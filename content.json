[{"title":"Hexo + GitHub Pages搭建静态博客","date":"2022-12-03T07:15:07.000Z","path":"2022/12/03/blog/hexo/","text":"使用hexo + BlueLake主题搭建简洁博客，专注写作。 1. 搭建Hexo环境 全局安装hexo工具：1npm install hexo-cli -g 创建hexo管理文件夹：1hexo init &lt;folder&gt; 初始化hexo文件夹：12cd &lt;folder&gt;npm install 此时文件目录如下：12345678.├── _config.yml├── package.json├── scaffolds├── source| ├──_drafts| └──_posts└── themes 本文中将&lt;foleder&gt;/_config.yml称为根配置文件。 写在&lt;foleder&gt;/_posts目录下的markdown文件会自动发布为博客文章。 2. 使用BlueLake主题 2.1 安装主题 进入&lt;foleder&gt;/themes目录，执行： 1git clone https://github.com/chaooo/hexo-theme-BlueLake.git themes/BlueLake 2.2 安装主题插件 进入&lt;foleder&gt;/themes目录，执行： 1234npm install hexo-renderer-jade --savenpm install hexo-renderer-stylus --savenpm install hexo-generator-feed --savenpm install hexo-generator-sitemap --save 2.3 启用主题 编辑&lt;foleder&gt;/_config.yml根配置文件，把主题设置修改为BlueLake： 1theme: BlueLake 2.4 配置主题 2.4.1 配置关于页面 在&lt;folder&gt;目录下执行： 1hexo new page &#x27;about&#x27; 此时会自动创建&lt;folder&gt;/about/index.md文件，该文件中不需要写任何东西。编辑 themes/BlueLake/_config.yml主题配置文件中的about项，修改为自己的信息。 2.4.2 更换markdown渲染器 在&lt;folder&gt;目录下执行，卸载原来的渲染器： 1npm uninstall hexo-renderer-marked --save 下载新渲染器 1npm i @upupming/hexo-renderer-markdown-it-plus --save hexo-renderer-marked渲染器提供了更为丰富的markdown语法支持，比如数学公式，如果不需要，也可以不用更换。 3. 安装插件 3.1 安装本地搜索插件 在&lt;folder&gt;目录下执行： 1npm install hexo-generator-json-content --save 编辑&lt;foleder&gt;/_config.yml根配置文件，添加配置： 123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 3.2 添加文章首页指定插件 在&lt;folder&gt;目录下执行： 12npm uninstall hexo-generator-index --savenpm install hexo-generator-index-pin-top --save 在需要置顶的文章的Front-matter中加上top: true。 123456title: Hexo + GitHub Pages搭建静态博客urlname: dh2ay6date: &#x27;2022-12-03 15:15:07 +0800&#x27;tags: [hexo]categories: [搭建博客]top: true 4. 使用Hexo写作 4.1 给文章添加标签 在markdown文章的Front-matter区，添加tags标签： 123tags:- PS3- Games 或： 1tags: [PS3, Games] 4.2 给文章添加分类 在markdown文章的Front-matter区，添加tags标签： 123categories: - Diary - Life 或： 1categories: [Diary, Life] 注意：Hexo 不支持指定多个同级分类，上述案例表示创建了Diary分类和Life分类，其中Life分类是Diary分类的子分类。 4.3 显示文章摘要 在markdown文章的Front-matter区后，markdown正文前，书写需要显示的摘要，写完后使用&lt;!-- more --&gt;分割正文。 参考文档： BlueLake主题安装 BlueLake主题配置 Heox官方文档 最全Hexo博客搭建+主题优化+插件配置+常用操作+错误分析 5. 部署GitHub Pages 5.1 安装 hexo-deployer-git 在&lt;folder&gt;目录下执行： 1npm install hexo-deployer-git --save 5.2 添加部署地址 编辑&lt;foleder&gt;/_config.yml根配置文件，填写github仓库地址： 12345deploy: type: git repo: github: https://github.com/sk370/sk370.github.io branch: master 5.3 推送网站到GitHub 在&lt;folder&gt;目录下执行： 1hexo d","tags":[{"name":"hexo","slug":"hexo","permalink":"https://sk370.github.io/tags/hexo/"}]},{"title":"性能监控","date":"2022-10-29T03:57:30.000Z","path":"2022/10/29/性能监控/性能监控/","text":"性能监控主要是对 jvm 的堆空间进行管理，涉及到对象创建与回收，所以需要使用 jvm 工具。 1. 工具简介 jvisualvm 是升级版的 jconsole，二者都是 JDK 自带的小工具，配置了 java 的环境变量后，可以通过命令行启动【我的怎么启动不了】， 可监控本地和远程应用。 远程应用需要配置。 2. jconsole 的使用 启动后选择要连接的进程（选中后双击）： 3. jvisualvm 3.1 基本使用 启动后选择要连接的进程 主要作用：监控内存泄露， 跟踪垃圾回收， 执行时内存、 cpu 分析， 线程分析… 线程分析的相关概念： 运行： 正在运行的 休眠： sleep 等待： wait 驻留： 线程池里面的空闲线程 监视： 阻塞的线程， 正在等待锁 3.2 安装插件 安装插件方便查看 gc 选择工具》插件》可用插件，找到 visualgc，然后安装 安装了插件后就可以查看了 3.3 安装插件 503 错误的解决 cmd 查看自己的 jdk 版本（查看小版本） 打开网址：https://visualvm.github.io/pluginscenters.html，找到对应的版本 点击下面的连接，进去后复制插件连接 在工具》插件》设置中粘贴上面复制的连接","tags":[{"name":"jconsole","slug":"jconsole","permalink":"https://sk370.github.io/tags/jconsole/"},{"name":"jvisualvm","slug":"jvisualvm","permalink":"https://sk370.github.io/tags/jvisualvm/"}]},{"title":"压力测试","date":"2022-10-29T03:56:03.000Z","path":"2022/10/29/压力测试/压力测试/","text":"压力测试考察当前软硬件环境下系统所能承受的最大负荷并帮助找出系统瓶颈所在。 压测都是为了系统在线上的处理能力和稳定性维持在一个标准范围内， 做到心中有数。 1. 相关概念 使用压力测试， 主要是希望找到用其他测试方法更难发现的错误。 有两种错误类型是:内存泄漏， 并发与同步。 有效的压力测试系统将应用以下这些关键条件:重复， 并发， 量级， 随机变化。 性能指标包括： 响应时间（Response Time: RT） 响应时间指用户从客户端发起一个请求开始， 到客户端接收到从服务器端返回的响应结束， 整个过程所耗费的时间。 HPS（Hits Per Second） ： 每秒点击次数， 单位是次/秒。 TPS（Transaction per Second）：系统每秒处理交易数， 单位是笔/秒。 QPS（Query per Second） ： 系统每秒处理查询次数， 单位是次/秒。 对于互联网业务中，如果某些业务有且仅有一个请求连接， 那么 TPS=QPS=HPS， 一般情况下用 TPS 来衡量整个业务流程， 用 QPS 来衡量接口查询次数， 用 HPS 来表示对服务器单击请求。 无论 TPS、 QPS、 HPS,此指标是衡量系统处理能力非常重要的指标，越大越好， 根据经验， 一般情况下： 金融行业： 1000TPS~50000TPS， 不包括互联网化的活动 保险行业： 100TPS~100000TPS， 不包括互联网化的活动 制造行业： 10TPS~5000TPS 互联网电子商务： 10000TPS~1000000TPS 互联网中型网站： 1000TPS~50000TPS 互联网小型网站： 500TPS~10000TPS 最大响应时间（Max Response Time） 指用户发出请求或者指令到系统做出反应（响应）的最大时间。 最少响应时间（Mininum ResponseTime） 指用户发出请求或者指令到系统做出反应（响应） 的最少时间。 90%响应时间（90% Response Time） 是指所有用户的响应时间进行排序， 第 90%的响应时间。 从外部看， 性能测试主要关注如下三个指标 吞吐量： 每秒钟系统能够处理的请求数、 任务数。 响应时间： 服务处理一个请求或一个任务的耗时。 错误率： 一批请求中结果出错的请求所占比例。 2. JMeter Apache JMeter 是一款开源免费的压测产品，最初被设计用于 Web 应用功能测试使用，如今 JMeter 被国内企业用于性能测试。对于 WEB 服务器（支持浏览器访问），不建议使用 Jmeter,因为 jmeter 的线程组都是线性执行的，与浏览器相差很大，测试结果不具有参考性。对于纯接口的部分场景（对接口调用顺序无严格要求）测试可以使用，但是要注意使用技巧，才能达到理想结果。【引用自别处，那么本项目还是否合适？】 2.1 基本使用 启动 JMeter 添加线程组（模拟同时请求同一接口的用户数）： 线程数： 虚拟用户数。 一个虚拟用户占用一个进程或线程。 设置多少虚拟用户数在这里也就是设置多少个线程数。 Ramp-Up Period(in seconds)准备时长： 设置的虚拟用户数需要多长时间全部启动。 如果线程数为 10， 准备时长为 2， 那么需要 2 秒钟启动 10 个线程， 也就是每秒钟启动 5 个线程。 循环次数： 每个线程发送请求的次数。 如果线程数为 10， 循环次数为 100， 那么每个线程发送 100 次请求。 总请求数为 10*100=1000 。 如果勾选了“永远”， 那么所有线程会一直发送请求， 一到手动停止运行脚本。 Delay Thread creation until needed： 直到需要时延迟线程的创建。 调度器： 设置线程组启动的开始时间和结束时间(配置调度器时， 需要勾选循环次数为永远) 持续时间（秒） ： 测试持续时间， 会覆盖结束时间 启动延迟（秒） ： 测试延迟启动时间， 会覆盖启动时间 启动时间： 测试启动时间， 启动延迟会覆盖它。 当启动时间已过， 手动只需测试时当前时间也会覆盖它。 结束时间： 测试结束时间， 持续时间会覆盖它。 添加 http 请求 添加监听器 查看结果树、汇总报告、聚合报告 启动测试 结果分析 汇总报告图 错误率：同开发确认， 确定是否允许错误的发生或者错误率允许在多大的范围内； 吞吐量：每秒请求的数大于并发数， 则可以慢慢的往上面增加； 若在压测的机器性能很好的情况下， 出现吞吐量小于并发数， 说明并发数不能再增加了， 可以慢慢的往下减， 找到最佳的并发数。 最大的 tps， 不断的增加并发数， 加到 tps 达到一定值开始出现下降， 那么那个值就是最大的 tps。 最大的并发数： 最大的并发数和最大的 tps 是不同的概率， 一般不断增加并发数， 达到一个值后， 服务器出现请求超时， 则可认为该值为最大的并发数。 压测过程出现性能瓶颈， 若压力机任务管理器查看到的 cpu、 网络和 cpu 都正常， 未达到 90%以上， 则可以说明服务器有问题， 压力机没有问题。 影响性能考虑点包括： 数据库、 应用程序、 中间件（tomact、 Nginx） 、 网络和操作系统等方面。 首先考虑自己的应用属于 CPU 密集型（大量计算）还是 IO 密集（大量读写） 2.2 JMeter Address Already in use 错误解决 windows 本身提供的端口访问机制的问题。 Windows 提供给 TCP/IP 链接的端口为 1024-5000， 并且要四分钟来循环回收他们。 就导致在短时间内跑大量的请求时将端口占满了。 cmd 中， 用 regedit 命令打开注册表 在HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters 下 修改端口数： 右击 parameters， 添加一个新的 DWORD， 名字为 MaxUserPort 然后双击 MaxUserPort， 输入数值数据为 65534， 基数选择十进制（如果是分布式运行的话， 控制机器和负载机器都需要这样操作） 修改端口回收时间： 右击 parameters， 添加一个新的 DWORD， 名字为 TCPTimedWaitDelay 然后双击 MaxUserPort， 输入数值数据为 30， 基数选择十进制（如果是分布式运行的话， 控制机器和负载机器都需要这样操作） 修改配置完毕之后记得重启机器才会生效 测试实验 nginx 单压测试 创建请求数： 指定请求地址 添加指标监控并进行运行： 虚拟机执行docker stats，可以查看 nginx 的内存占用、cpu 占用等 启动压力测试工具后可以看到 cpu 占用较高 手动停止测试工具，查看报告 网关单压测试 清空全部数据 然后修改网关地址： 使用 jvisual 查看运行： 手动停止压力测试工具，查看报告： 简单服务 请求不经过 nginx 和网关，服务也不进行数据库等其他操作，如： 12345678910 /** * 简单服务性能测试 * @return */@ResponseBody@GetMapping(&#123;&quot;/hello&quot;&#125;)public String hello()&#123; return &quot;hello&quot;;&#125; 简单服务+网关 上述简单服务经过网关处理，首先配置网关对请求的处理： 简单服务+网关+nginx 业务测试（单次数据库查询） 正常访问数据库请求，但不经过网关、nginx，直接访问微服务，不下载首页的其他数据（图片、css、js 等）。 此时 thymeleaf 缓存关闭的。 慢的原因为数据库访问和 thymeleaf 渲染。 业务测试（单次数据库查询、开 thymeleaf 缓存） 【懒得没测试】 ###业务测试（多次数据库查询） 慢的原因为数据库访问。 首页渲染 与 简单服务 的区别在在于，首页渲染需要下载静态资源。此时 thymeleaf 缓存关闭的。 首页渲染（开 thymeleaf 缓存） 【懒得没测试】 优化查询 通过调整日志级别、给查询字段添加索引进行测试。 【懒得没测试】 结论 中间件越多，性能损失越大，大多都损失在网络交互了。 业务优化思路： Db（MySQL 优化） 调整日志级别 给查询字段添加索引 提升模板的渲染速度（使用缓存） 静态资源分离 性能优化 动静分离 微服务，动静分离模型：由于访问 gulimall.com 会先访问到 nginx 上，再由 nginx 将请求发给网关，网关发给微服务。如果静态资源此时在 nginx 上，不经过网关、微服务则能提升一定的效率。 将 static 目录剪切到nginx/html/目录下 修改 index.html 页面静态资源的路径 修改 guilimall.conf 配置文件 优化复杂查询（三级分类查询） 获取三级分类的查询是个复杂查询，需要多次查询数据库，如果将三级分类提前查询好保存起来，然后对数据进行父子结构处理，而不进行数据库查询，将会大大提高吞吐量。 即将多次查询查询变为一次： 给查询字段添加索引 给查询的字段为 parent_cid 建立一个索引。 使用 redis 缓存 将数据修改不多，读取多的数据从数据库查到后，存放到redis缓存中，后续请求先从缓存中读取。","tags":[{"name":"JMeter","slug":"JMeter","permalink":"https://sk370.github.io/tags/JMeter/"}]},{"title":"Spring Cloud Gateway","date":"2022-10-29T03:48:28.000Z","path":"2022/10/29/springcloud/SpringCloudGateway/","text":"API 网关是一个搭建在客户端和微服务之间的服务，我们可以在 API 网关中处理一些非业务功能的逻辑，例如权限验证、监控、缓存、请求路由等。 1. 网关介绍 网关作为流量的入口，常用功能包括路由转发、权限校验、限流控制等。 spring cloud gateway 是 spring cloud 的第二代网关，取代了 zuul。 2.1.3.RELEASE 文档 网关的相关概念： route 路由：由 id、目标 uri、断言集合和过滤器组成。如果聚合断言的判定结果为真，就匹配到该路由。 predicate 断言：java8 的断言函数，输入类型是 spring 框架的serverwebexchange，允许开发人员匹配来自 http 请求的任何内容。 filter 过滤器：可以在返回请求之前或之后修改请求和响应的内容。 工作流程： 客户端向Spring Cloud Gateway发出请求。如果Gateway Handler Mapping确定请求与路由匹配，则将其发送到Gateway Web Handler。此handler通过特定于该请求的过滤器链处理请求。图中 filters 被虚线划分的原因是 filters 可以在发送代理请求之前或之后执行逻辑。先执行所有pre filter逻辑，然后进行请求代理。在请求代理执行完后，执行post filter逻辑。 2. 使用 gateway 创建 gateway 模块 修改依赖： 修改 gulimall-gateway 的 spring boot、spirng cloud 版本与项目一致 给 guilimall-gateway 添加 gulimall-common 依赖，使得网关也能够注册到注册中心 配置网关： 打开 com.atguigu.gateway.GulimallgatewayApplication 类的服务注册发现：@EnableDiscoveryClient 在 gulimall-gateway 的 resources 下的 application.properties 配置文件中，指定注册中心地址和该服务名称。【由于后面两部操作的作用是代替注册中心的配置文件，所以这一步也可以不要】 讲道理，创建 application.yml 文件，按照 6.2 的方式写应该也可以。 在 nacos 中创建 gulimall-getwa 的名称空间 在 gateway 的命名空间下，创建配置，指定名称，编辑注册中心application.yml文件的内容 上面的配置等价于： 1234spring.cloud.nacos.config.server-addr=127.0.0.1:8848spring.application.name=gulimall-gatewayserver.port=88 在 gulimall-gateway 的 resources 下创建 bootstrap.properties 配置文件，指定配置中心地址和该服务名称、以及命名空间。 由于当前在 gateway 的命名空间中只设置了一个配置，且为默认分组，所以设定配置的内容可以省略【测试不能省略！！！！】。 测试网关： 修改 com.atguigu.gateway.GulimallgatewayApplication 类的 spring boot 配置注解，排除与数据源有关的配置：@@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class&#125;) 因为 gulimall-geway 模块以来了 gulimall-common，而 gulimall-common 使用了 mysql，如果不配置数据源会报错，所以要排除。 启动 main 方法 3. 配置路由规则 本地文件写在reosurces/application.yml文件，也可写在配置中心中。这里采用写在本地的方式，测试访问 http://localhost:88?whw=baidu 跳转到www.baidu.com的网关设置；请求http://localhost:88?whw=qq跳转到www.qq.com。 12345678910111213spring: cloud: gateway: routes: - id: baidu_route uri: https://www.baidu.com predicates: - Query=whw,baidu - id: qq_route uri: https://www.qq.com predicates: - Query=whw,qq 上述案例中，uri 如果要映射至本地服务的路径，而且本地服务不止一个，所以就通过服务名进行映射，因此需要使用lb://的方式。 12345678910spring: cloud: gateway: routes: - id: product_route uri: lb://gulimall-product predicates: - Path=/api/product/** filters: - RewritePath=/api/(?&lt;segment&gt;.*),/$\\&#123;segment&#125; filter 表示路径重写，/api/product/表示前端的访问路径，gateway 接收到后，转换成/product/ 4. 配置跨域 跨域流程：根据文档，只有简单请求才会发生一次请求且不会发生跨域。否则会先发送预检（options）请求，用于询问能否跨域。 跨域的一般解决方式： 使用 nginx 代理服务器。 给 options 的相应添加响应头，使用@CrossOrigin注解，在方法、类前面可以进行响应头添加 使用过滤器简化给响应添加响应头，减少代码编写。【即全局配置】 创建配置类，将 CorsWebFilter 对象添加到 IOC 容器 1234567891011121314@Configurationpublic class GulimallCorsConfiguration &#123; @Bean public CorsWebFilter corsWebFilter()&#123; UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration corsConfiguration = new CorsConfiguration(); corsConfiguration.addAllowedHeader(&quot;*&quot;);//允许所有请求头 corsConfiguration.addAllowedMethod(&quot;*&quot;);//允许任意请求方法 corsConfiguration.addAllowedOrigin(&quot;*&quot;);//允许所有请求来源 corsConfiguration.setAllowCredentials(true);//允许携带cookie跨域 source.registerCorsConfiguration(&quot;/**&quot;, corsConfiguration);// /**表示所有路径都需要跨域 return new CorsWebFilter(source); &#125;&#125;","tags":[{"name":"Spring Cloud Gateway","slug":"Spring-Cloud-Gateway","permalink":"https://sk370.github.io/tags/Spring-Cloud-Gateway/"}]},{"title":"Spring Cloud Feign","date":"2022-10-29T03:44:24.000Z","path":"2022/10/29/springcloud/SpringCloudFeign/","text":"feign 是一个声明式的 http 客户端，目的是简化远程调用。feign 提供了 http 请求模板，通过编写简单的接口和注解，就可以定义好 http 请求的参数、格式、地址等信息。 1. 基本使用 feign 是一个声明式的 http 客户端，目的是简化远程调用。feign 提供了 http 请求模板，通过编写简单的接口和注解，就可以定义好 http 请求的参数、格式、地址等信息。 feign 整合了 ribbon（负载均衡）和 hystrix（服务熔断），可以不再显式地使用这两个组件。 使用步骤【以 gulimall-member 调用 gulimall-coupon 为例】： 引入依赖【创建项目时已引入】 在 gulimall-member 模块中，创建 com.atguigu.member.feign 包，创建接口 CouponFeignService.java，并使用@FeignClient(&quot;gulimall-coupon&quot;)指明类型及调用地址【gulimall-coupon】是注册中心 nacos 中的名称 在该接口中，写明要调用的 gulimall-coupon 中的方法 在 gulimall-member 的com.atguigu.gulimall.member.GulimallMemberApplication类使用@EnableFeignClients(basePackages=&quot;com.atguigu.gulimall.member.feign”)开启 feignclient 的调用功能，并指明 feign 客户端的全包名 在 gulimall-member 的com.atguigu.gulimall.member.MemberController类中编写测试方法 在 gulimall-coupon 的com.atguigu.gulimall.coupon.CouponController类中编写测试方法 重新启动 gulimall-member 和 gulimall-coupon，在浏览器地址栏访问 http://localhost:8000/member/member/coupons，检查是否有数据返回 请求及调用过程： 2. feign 调用流程 构造请求数据，将对象转换为 json RequestTemplate template = buildTemplateFromArgs.create(argv); 发送请求进行执行（执行成功解码响应数据） return executeAndDecode(template); 执行请求会有重试机制（有 5 次的机制，但默认不重试，直接抛出异常）","tags":[{"name":"Spring Cloud Feign","slug":"Spring-Cloud-Feign","permalink":"https://sk370.github.io/tags/Spring-Cloud-Feign/"}]},{"title":"Spring Cloud Alibaba","date":"2022-10-29T03:19:35.000Z","path":"2022/10/29/springcloudalibaba/SpringCloudAlibaba/","text":"Spring Cloud Alibaba 是阿里巴巴结合自身微服务实践，开源的微服务全家桶。 1. Spring Cloud Alibaba - Nacos 1.1 注册中心 一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。【注册中心及配置中心】 使用文档： 注册中心： 配置中心： 将 nacos 接入到 gulimall-common 中，作为注册中心： 下载 nacos 注册中心， 启动注册中心：解压文件，双击 bin 目录下 starup.cmd 在 5 个 gulimall 的微服务模块中，在 resources/application.yml 文件中填写 nacos 地址 127.0.0.1:8848，并指定名称 这里可以在 application.propeties 文件中写注册地址和名称吗？ 直觉觉得可以，propterties 和 yml 文件都是配置文件，只不过是文本格式不同 使用@EnableDiscoveryClient注解，分别在 5 个微服务的应用类前开启 5 个 gulimall 的注册功能，使得该服务可以注册到发现中心。 其他模块图略。 测试是否成功，启动com.atguigu.gulimall.coupon.GulimallCouponApplication，浏览器打开 127.0.0.1:8848/nacos，使用默认的登录名 nacos 和密码 nacos 登录。 同理注册其他 gulimall 的微服务模块 1.2 配置中心 1.2.1 使用及配置 在 gulimall-common 中，引入 nacos config starter 依赖 给需要使用配置中的微服务模块resources路径下创建bootstrap.properties【本例 gulimall-coupon】 12spring.application.name=gulimall-coupon #当前微服务/程序的名称spring.cloud.nacos.config.server-addr=127.0.0.1:8848 #配置中心的ip地址(同注册中心） 测试配置中心： 在需要测试的控制器类中编写方式，定义测试属性，并给该类添加@RefreshScope注解、属性添加@Value注解 在 nacos 配置中心给上面定义的测试属性进行初始化 nacos 配置中心相关概念 命令空间：默认为 public，默认新增的所有配置都在 public 空间，用于配置隔离，可以基于环境设置隔离，比如开发、测试、生产环境；也可以基于服务设置隔离， 创建命名空间后，可以在配置列表中进行使用，需要切换则需要在bootstrap.properties中设置spring.cloud.nacos.config.namespace=2c40b603-31cb-43e4-bab5-30a1b131af1c 配置集：所有配置的集合。 配置集 ID：Data Id，类似文件名 配置分组：默认所有的配置集都属于 DEFAULT_GROUP 切换分组：在bootstrap.properties文件中设置spring.cloud.nacos.config.group=DEFAULT_GROUP 使用 nacos 配置中心管理多个配置集，代替 resourcers/application.yml 文件，并在bootstrap.properties文件中进行设置，指定 nacos 配置中心要管理的配置文件。 按照上述配置后，就不需要 resourcers/application.yml 文件了 1.2.2 核心概念 命名空间：用于进行租户粒度的配置隔离。 不同的命名空间下， 可以存在相同的 Group 或 Data ID 的配置。 Namespace 的常用场景之一是不同环境的配置的区分隔离， 例如开发测试环境和生产环境的资源（如配置、 服务） 隔离等。 配置集：一组相关或者不相关的配置项的集合称为配置集。 在系统中， 一个配置文件通常就是一个配置集， 包含了系统各个方面的配置。 例如， 一个配置集可能包含了数据源、 线程池、 日志级别等配置项。 配置集 ID：Nacos 中的某个配置集的 ID。 配置集 ID 是组织划分配置的维度之一。 Data ID 通常用于组织划分系统的配置集。 一个系统或者应用可以包含多个配置集， 每个配置集都可以被一个有意义的名称标识。 Data ID 通常采用类 Java 包（ 如 com.taobao.tc.refund.log.level） 的命名规则保证全局唯一性。 此命名规则非强制。 配置分组：Nacos 中的一组配置集， 是组织配置的维度之一。 通过一个有意义的字符串（ 如 Buy 或 Trade ） 对配置集进行分组， 从而区分 Data ID 相同的配置集。 当您在 Nacos 上创建一个配置时， 如果未填写配置分组的名称， 则配置分组的名称默认采用 DEFAULT_GROUP 。 配置分组的常见场景： 不同的应用或组件使用了相同的配置类型， 如 database_url 配置和 MQ_topic 配置。 1.2.3 原理 自动注入：NacosConfigStarter 实现了 org.springframework.cloud.bootstrap.config.PropertySourceLocator 接口， 并将优先级设置成了最高。在 Spring Cloud 应用启动阶段， 会主动从 Nacos Server 端获取对应的数据， 并将获取到的数据转换成 PropertySource 且注入到 Environment 的 PropertySources 属性中， 所以使用@Value 注解也能直接获取 Nacos Server 端配置的内容。 动态刷新：Nacos Config Starter 默认为所有获取数据成功的 Nacos 的配置项添加了监听功能， 在监听到服务端配置发生变化时会实时触发 org.springframework.cloud.context.refresh.ContextRefresher 的 refresh 方法 。如果需要对 Bean 进行动态刷新， 请参照 Spring 和 Spring Cloud 规范。 推荐给类添加@RefreshScope 或 @ConfigurationProperties 注解。 1.2.4 加载多个配置文件 12345spring.cloud.nacos.config.server-addr=127.0.0.1:8848spring.cloud.nacos.config.namespace=31098de9-fa28-41c9-b0bd-c754ce319ed4spring.cloud.nacos.config.ext-config[0].data-id=gulimall-datasource.ymlspring.cloud.nacos.config.ext-config[0].refresh=falsespring.cloud.nacos.config.ext-config[0].group=dev 1.2.5 namespace 与 group 最佳实践 每个微服务创建自己的 namespace 进行隔离， group 来区分 dev， beta， prod 等环境。 01.阿里云OSS对象存储 普通上传方式 文件提交给微服务，微服务发送到阿里云 OSS 12345678910111213141516171819202122232425262728293031323334353637@RunWith(SpringRunner.class)@SpringBootTestpublic class GulimallThirdPartyApplicationTests &#123; @Resource OSSClient ossClient; @Test public void testUpload2()&#123; String bucketName = &quot;gulimall-brands-logo&quot;; String objectName = &quot;3-2.jpg&quot;; String filePath= &quot;C:\\\\Users\\\\iceri\\\\Documents\\\\3.jpg&quot;; try &#123; InputStream inputStream = null; try &#123; inputStream = new FileInputStream(filePath); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; ossClient.putObject(bucketName, objectName, inputStream); &#125; catch (OSSException oe) &#123; System.out.println(&quot;Caught an OSSException, which means your request made it to OSS, &quot; + &quot;but was rejected with an error response for some reason.&quot;); System.out.println(&quot;Error Message:&quot; + oe.getErrorMessage()); System.out.println(&quot;Error Code:&quot; + oe.getErrorCode()); System.out.println(&quot;Request ID:&quot; + oe.getRequestId()); System.out.println(&quot;Host ID:&quot; + oe.getHostId()); &#125; catch (ClientException ce) &#123; System.out.println(&quot;Caught an ClientException, which means the client encountered &quot; + &quot;a serious internal problem while trying to communicate with OSS, &quot; + &quot;such as not being able to access the network.&quot;); System.out.println(&quot;Error Message:&quot; + ce.getMessage()); &#125; finally &#123; if (ossClient != null) &#123; ossClient.shutdown(); &#125; &#125; &#125;&#125; 签名方式上传 浏览器找 gulimall-third-part 索要签名，浏览器拿到签名后，直接上传到 OSS 服务器 直接获取到的签名为 Map 格式，内部无 data 属性，前端在处理时存在不便。为了统一处理返回结果，设置返回对象 R，将 map 放到 R 里面。 1234567891011121314151617181920212223242526272829303132333435@RequestMapping(&quot;oss/policy&quot;)public R policy()&#123; // https://gulimall-brands-logo.oss-cn-chengdu.aliyuncs.com/3-1.jpg //上传后访问的路径 String host = &quot;https://&quot; + bucket + &quot;.&quot; + endpoint; // 设置上传到OSS文件的前缀，可置空此项。置空后，文件将上传至Bucket的根目录下。 String format = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;).format(new Date()); String dir = format + &quot;/&quot;; Map&lt;String, String&gt; respMap = null; try &#123; long expireTime = 30; long expireEndTime = System.currentTimeMillis() + expireTime * 1000; Date expiration = new Date(expireEndTime); PolicyConditions policyConds = new PolicyConditions(); policyConds.addConditionItem(PolicyConditions.COND_CONTENT_LENGTH_RANGE, 0, 1048576000); policyConds.addConditionItem(MatchMode.StartWith, PolicyConditions.COND_KEY, dir); String postPolicy = client.generatePostPolicy(expiration, policyConds); byte[] binaryData = postPolicy.getBytes(&quot;utf-8&quot;); String encodedPolicy = BinaryUtil.toBase64String(binaryData); String postSignature = client.calculatePostSignature(postPolicy); respMap = new LinkedHashMap&lt;String, String&gt;(); respMap.put(&quot;accessid&quot;, accessId); respMap.put(&quot;policy&quot;, encodedPolicy); respMap.put(&quot;signature&quot;, postSignature); respMap.put(&quot;dir&quot;, dir); respMap.put(&quot;host&quot;, host); respMap.put(&quot;expire&quot;, String.valueOf(expireEndTime / 1000)); // respMap.put(&quot;expire&quot;, formatISO8601Date(expiration)); &#125; catch (Exception e) &#123; // Assert.fail(e.getMessage()); System.out.println(e.getMessage()); &#125; return R.ok().put(&quot;data&quot;, respMap);&#125; 注意：这里要设置阿里云 OSS 允许跨域。 第3章 sms 4.1 阿里云短信工具类 将阿里云短信服务封装成工具类，会员登录时模块 12，工具模块是 05，而模块 12 依赖了模块 17，模块 17 依赖了模块 05，根据依赖的传递性，在模块 05 中添加阿里云短信服务 12345&lt;dependency&gt; &lt;groupId&gt;com.aliyun&lt;/groupId&gt; &lt;artifactId&gt;dysmsapi20170525&lt;/artifactId&gt; &lt;version&gt;2.0.18&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 /** * 阿里云短信服务 * @param accessKeyId 私有id * @param accessKeySecret 私有key * @param endpoint 短信服务域名 * @param signName 短信签名 * @param templateCode 短信模板 * @param phoneNum 电话号码 * @return 发送成功返回验证码，失败返回失败消息 */ public static ResultEntity&lt;String&gt; sendShortMessage(String accessKeyId, String accessKeySecret, String endpoint, String signName, String templateCode, String phoneNum) &#123; Config config = new Config(); config.setAccessKeyId(accessKeyId); config.setAccessKeySecret(accessKeySecret); config.endpoint = endpoint; Client client = null; try &#123; client = new Client(config); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; SendSmsRequest sendSmsRequest = new SendSmsRequest(); sendSmsRequest.setSignName(signName); sendSmsRequest.setTemplateCode(templateCode); sendSmsRequest.setPhoneNumbers(phoneNum); //生成验证码 String code = &quot;&quot;; int rand; for (int i = 0; i &lt; 4; i++) &#123; rand = (int) (Math.random() * 10); code += rand + &quot;&quot;; &#125; sendSmsRequest.setTemplateParam(&quot;&#123;\\&quot;code\\&quot;:\\&quot;&quot; + code + &quot;\\&quot;&#125;&quot;); RuntimeOptions runtimeOptions = new RuntimeOptions(); int statusCode; try &#123; SendSmsResponse sendSmsResponse = client.sendSmsWithOptions(sendSmsRequest, runtimeOptions);// System.out.println(sendSmsResponse.toString() + &quot;刚发送，看看sendSmsResponse是个啥++++++++++++++++++++++++++++++++++&quot;); statusCode = sendSmsResponse.statusCode; if (statusCode == 200) &#123; return ResultEntity.successWithData(code); &#125; return ResultEntity.failed(sendSmsResponse.toString()); &#125; catch (TeaException error) &#123; String errMessage = Common.assertAsString(error.message);// System.out.println(&quot;+++++++++++++++++++++TeaException&quot; + errMessage); return ResultEntity.failed(errMessage); &#125; catch (Exception e) &#123; TeaException error = new TeaException(e.getMessage(), e); String errMessage = Common.assertAsString(error.message);// System.out.println(&quot;+++++++++++++++++++++Exception&quot; + errMessage); return ResultEntity.failed(errMessage); &#125; &#125; 4.2 注册过程 基础功能（发送验证码）： 注册成功后的跳转，使用重定向，防止刷新页面重新提交注册请求。 防止点击 button 按钮表单刷新： button 按钮不设置 type 类型时，默认为 subbmit，默认事件会提交表单（即使表单不带 action 属性） 设置为 type=button 可以实现单纯的点击效果。 首次请求响应超时： 分布式环境中，使用了 Zuul、Feign 后，由于底层都通过 Ribbon 通过 service name 寻找 eureka，第一次请求需要建立缓存、连接等，操作较多，ribbon 的默认超时时间较短，容易报超时错误。 解决方式是在响应的微服务配置文件中配置（设置大一点的值）： 123ribbon: ReadTimeout: 1000 ConnectTimeout: 10000","tags":[{"name":"Spring Cloud Alibaba","slug":"Spring-Cloud-Alibaba","permalink":"https://sk370.github.io/tags/Spring-Cloud-Alibaba/"}]},{"title":"ElasticSearch","date":"2022-10-25T11:51:48.000Z","path":"2022/10/25/elasticsearch/ElasticSearch/","text":"ElasticSearch 是一款强大的全文搜索引擎，用于海量数据的搜索及分析。Elastic 的底层是开源库 Lucene，Elastic 是 Lucene 的封装， 提供了 REST API 的操作接口， 开箱即用。 在 vagrant 虚拟机中。 第 1 章 简介 1.1 什么是 ElasticSearch ElasticSearch 是一款强大的全文搜索引擎，用于海量数据的搜索及分析。 Elastic 的底层是开源库 Lucene，Elastic 是 Lucene 的封装， 提供了 REST API 的操作接口， 开箱即用。 1.2 基本概念 1.2.1 Index（索引） 做动词时， 相当于 MySQL 中的 insert。 做名词时， 相当于 MySQL 中的 Database。 1.2.2 Type（类型） 在 Index（索引） 中， 可以定义一个或多个类型。 类似于 MySQL 中的 Table； 每一种类型的数据放在一起。 ES7 及以上移除了 type 的概念。 这是因为 elasticsearch 是基于 Lucene 开发的搜索引擎，ES 中不同 type 下名称相同的 filed 最终在 Lucene 中的处理方式是一样的。 两个不同 type 下的两个 user_name， 在 ES 同一个索引下其实被认为是同一个 filed，所以必须在两个不同的 type 中定义相同的 filed 映射。 否则，不同 type 中的相同字段名称就会在处理中出现冲突的情况， 导致 Lucene 处理效率下降。 去掉 type 就是为了提高 ES 处理数据的效率。 Elasticsearch 7.x： URL 中的 type 参数为可选。 比如， 索引一个文档不再要求提供文档类型。 Elasticsearch 8.x： 不再支持 URL 中的 type 参数。 解决： 将索引从多类型迁移到单类型， 每种类型文档一个独立索引 将已存在的索引下的类型数据， 全部迁移到指定位置即可。 详见数据迁移 1.2.3 Document（文档） 保存在某个索引（Index） 下， 某种类型（Type） 的一个数据（Document） ， 文档是 JSON 格式的， Document 就像是 MySQL 中的某个 Table 里面的内容(一条记录)。 注意：ES6 移除了类型，即索引可以直接对应文档。 1.2.4 字段 ES 字段的类型主要有五大类： 1.2.5 映射 Mapping 是用来定义一个文档（ document），以及它所包含的属性（ field） 是如何存储和索引的【建立文档和属性的对应关系】。 比如， 使用 mapping 来定义： 哪些字符串属性应该被看做全文本属性（full text fields） 。 哪些属性包含数字， 日期或者地理位置。 文档中的所有属性是否都能被索引（_all 配置） 。 日期的格式。 自定义映射规则来执行动态添加属性 自动映射机制：ES 能够将存储的 json 数据类型进行自动猜测映射成对应的类型： 查看 mapping 信息：GET bank/_mapping 创建 mapping 信息（创建时）： 12345678910PUT /my-index&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;&#125; 创建 mapping 信息（添加新字段时）： 修改 mapping 信息：对于已经存在的映射字段，不能更新 mapping，必须创建新的索引进行数据迁移。 数据迁移： 先创建新的索引，并指定映射： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546PUT /newbank&#123; &quot;mappings&quot;: &#123; &quot;properties&quot; : &#123; &quot;account_number&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125;, &quot;address&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;age&quot; : &#123; &quot;type&quot; : &quot;integer&quot; &#125;, &quot;balance&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125;, &quot;city&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;email&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;employer&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;firstname&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;gender&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;lastname&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;state&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125; &#125; &#125;&#125; 迁移数据：【由于 bank 索引创建时使用了类型 account，所以在迁移的时候要指定该类型】POST /_reindex 12345678910POST /_reindex&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;bank&quot;, &quot;type&quot;: &quot;account&quot; &#125;, &quot;dest&quot;:&#123; &quot;index&quot;: &quot;newbank&quot; &#125;&#125; 1.2.6 倒排索引机制 比如要保存 5 条记录： 红海行动 探索红海行动 红海特别行动 红海记录篇 特工红海特别探索 ElasticSearch 存储是存储两张表：数据表和倒排索引表。 倒排索引表中存储数据时，先将要存储的记录按单词拆分，单词对应数据表中出现的位置。 如红海在 1、2、3、4、5 中均出现过，特别在 3、5 号出现过。 在检索时，先将检索的内容按单词拆分，如红海特工行动，拆分成红海、特工、行动，计算相关性得分，按照相关性得分的高低从高到低排序。 1.3 Docker 安装 ES 1.3.1 下载镜像文件 修改虚拟机内存为 1G（关机情况下进行）： docker pull elasticsearch:7.4.2 存储和检索数据 docker pull kibana:7.4.2 可视化检索数据 free-m：查看可用内存 1.3.2 创建实例 创建 ElasticSearch 实例 创建虚拟机的文件，用于后续挂载 docker 中的 ElasticSearch 文件 http.host: 0.0.0.0 表示所有用户都可以访问。 123mkdir -p /mydata/elasticsearch/configmkdir -p /mydata/elasticsearch/dataecho &quot;http.host: 0.0.0.0&quot; &gt;&gt; cat 挂载文件、配置 ElasticSearch 9200 是虚拟机端口，9300 是 docker 中的端口 特别注意：-e ES_JAVA_OPTS=&quot;-Xms64m -Xmx256m&quot; \\ 测试环境下， 设置 ES 的初始内存和最大内存， 否则导致过大启动不了 ES 1234567docker run --name elasticsearch -p 9200:9200 -p 9300:9300 \\-e &quot;discovery.type=single-node&quot; \\-e ES_JAVA_OPTS=&quot;-Xms64m -Xmx256m&quot; \\-v /mydata/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\-v /mydata/elasticsearch/data:/usr/share/elasticsearch/data \\-v /mydata/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\-d elasticsearch:7.4.2 启动不成功时查看日志：docker logs elasticsearch 提高目录权限 1chmod -R 777 /mydata/elasticsearch/ 保证权限 访问：http://192.168.56.10:9200/ psotman 测试 es： docker update 应用id --restart=always：设置开机自启 创建 Kibana 实例 12docker run --name kibana -e ELASTICSEARCH_HOSTS=http://192.168.56.10:9200 -p 5601:5601 \\-d kibana:7.4.2 docker update 应用id --restart=always：设置开机自启 访问：http://192.168.56.10:5601 1.3.3 修改 ES 实例 1.3.2 的 ES 实例中，分配了 128m 的最大内存，后续使用中存在不够用的情况。因此需要修改实例，过程为删掉旧实例，创建新实例。由于创建旧实例时使用了容器卷的挂载，相关的数据保存在 vagrant 的虚拟机中，所以数据不会丢失，新建的实例也会自动拥有这些数据。 停止 ES 实例 移除 ES 实例 查看旧 ES 实例的挂载数据： 创建新实例：设置最大内存 512m 1234567docker run --name elasticsearch -p 9200:9200 -p 9300:9300 \\-e &quot;discovery.type=single-node&quot; \\-e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot; \\-v /mydata/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\-v /mydata/elasticsearch/data:/usr/share/elasticsearch/data \\-v /mydata/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\-d elasticsearch:7.4.2 第 2 章 ES 的使用 2.1 基本检索 2.1.1 _cat 路径 GET /_cat/nodes： 查看所有节点 GET /_cat/health： 查看 es 健康状况 GET /_cat/master： 查看主节点 GET /_cat/indices： 查看所有索引，相当于 mysql 的 show databases; 2.1.2 索引一个文档（mysql 保存一条记录） 保存一个数据， 保存在哪个索引的哪个类型下， 指定用哪个唯一标识。【注意 ES6 已经移除了类型，ES7 会给予警告，ES8 将完全不支持类型】 PUT customer/external/1：在 customer 索引下的 external 类型下保存 id 为 1 的数据： 123&#123; &quot;name&quot;: &quot;John Doe&quot;&#125; PUT 和 POST 都可以： POST 新增。 如果不指定 id， 会自动生成 id。 指定 id 就会修改这个数据， 并新增版本号。 不指定 id，会生成唯一 id，多次发送均为 created，更新 id 指定 id，同 put。 PUT 可以新增可以修改。 必须指定 id； 由于 PUT 需要指定 id， 我们一般都用来做修改操作， 不指定 id 会报错。 同样的请求（请求地址一致）表示更新操作，会更新版本号，并显示为 update： 2.1.3 查询文档 将索引文档时的请求类型转为 GET 即为查询。如GET customer/external/1 12345678910111213&#123; &quot;_index&quot;: &quot;customer&quot;, //在哪个索引 &quot;_type&quot;: &quot;external&quot;, //在哪个类型 &quot;_id&quot;: &quot;1&quot;, //记录 id &quot;_version&quot;: 2, //版本号 &quot;_seq_no&quot;: 1, //并发控制字段， 每次更新就会+1， 用来做乐观锁 &quot;_primary_term&quot;: 1, //同上， 主分片重新分配， 如重启， 就会变化 &quot;found&quot;: true, &quot;_source&quot;: &#123; //真正的内容 &quot;name&quot;: &quot;John Doe&quot; &#125;&#125; 2.1.4 更新文档 除了上述保存的时候更新外，还可以使用 post 请求进行专门的更新： POST customer/external/1/_update 使用_update必须传递doc参数。 多次更新同一条数据会对比原数据，如果一致不会进行更新，即：result 变为 noop，_version 和_seq_no 不会变化。 12345&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;John Doew&quot; &#125;&#125; put 请求和 post 不带 update 的请求一样，总是进行更新，不会比较旧数据。 2.1.5 更新的并发控制 携带_seq_no和_primary_term字段，传递参数：?if_seq_no=0&amp;if_primary_term=1 上述为乐观锁的控制过程。post、put 请求均可，上述为使用了正确的锁，修改成功的情况。 2.1.6 删除文档&amp;索引 删除文档：DELETE customer/external/1 删除索引：DELETE customer 2.1.7 bulk 批量 API 语法格式：POST customer/external/_bulk 注意：数据格式不是 json 1234&#123; action: &#123; metadata &#125;&#125;&#123; request body &#125;&#123; action: &#123; metadata &#125;&#125;&#123; request body &#125; 简单案例： 1234&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;name&quot;: &quot;John Doe&quot; &#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125;&#123;&quot;name&quot;: &quot;Jane Doe&quot; &#125; POSTMAN 不能测试，需要使用 Kibana 每条数据都是独立执行的，上一条的失败不会影响下一条。 复杂案例： 1234567&#123; &quot;delete&quot;: &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot; &#125;&#125;&#123; &quot;create&quot;: &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot; &#125;&#125;&#123; &quot;title&quot;: &quot;My first blog post&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot; &#125;&#125;&#123; &quot;title&quot;: &quot;My second blog post&quot; &#125;&#123; &quot;update&quot;: &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;My updated blog post&quot;&#125; &#125; 导入测试数据：官方数据地址（百度即可找到）POST /bank/account/_bulk 在 postman 查询索引： 2.2 进阶检索（复杂检索） 官方文档： 2.2.1 ES 检索方式 ES 支持两种基本方式检索： 一个是通过使用 REST request URI 发送搜索参数（uri+检索参数） GET bank/_search?q=*&amp;sort=account_number:asc 另一个是通过使用 REST request body 来发送它们（uri+请求体） GET bank/_search 12345678910&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;account_number&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 响应结果解释： took - Elasticsearch 执行搜索的时间（ 毫秒） time_out - 告诉我们搜索是否超时 _shards - 告诉我们多少个分片被搜索了， 以及统计了成功/失败的搜索分片 hits.total - 搜索结果 hits.hits - 实际的搜索结果数组（ 默认为前 10 的文档） hits.hits.sort - 结果的排序 key（ 键）（没有则按 score 排序） score 和 max_score –相关性得分和最高得分（ 全文检索用） 一旦搜索的结果被返回， Elasticsearch 就完成了这次请求， 并且不会维护任何服务端的资源或者结果的 cursor（游标）。 2.2.2 Query DSL 上文的查询中，使用了 GET 请求+请求体的方式，而 HTTP 客户端工具（POSTMAN），get 请求不能携带请求体。 Elasticsearch 提供了一个可以执行查询的 Json 风格的 DSL（ domain-specific language 领域特定语言），称为 Query DSL。 2.2.2.1 语法格式 12345678910111213GET bank/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 0, &quot;size&quot;: 5, &quot;sort&quot;: [ &#123; &quot;account_number&quot;: &#123;&quot;order&quot;: &quot;desc&quot;&#125; &#125; ]&#125; query 定义一个查询 match_all 查询类型【代表查询所有的所有】 from+size 限定，完成分页功能 sort 排序，多字段排序，会在前序字段相等时后续字段内部排序，否则以前序为准 account_number：排序字段 order：排序规则 2.2.2.2 _source 返回部分字段 123456789GET bank/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 0, &quot;size&quot;: 5, &quot;_source&quot;: [&quot;age&quot;,&quot;balance&quot;]&#125; 2.2.2.3 match_all 匹配查询 查找当前索引所有数据，不能写匹配条件，写了会报错。 2.2.2.3 match 匹配查询 match 可以查询各种类型。 查询非字符串，表示精确匹配，查询结果必须与查询关键字完全一致 20 带不带引号都可以。 12345678GET bank/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;account_number&quot;: &quot;20&quot; &#125; &#125;&#125; 查询字符串表示全文索引，查询结果包含查询关键字即可 12345678GET bank/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;&#125; 查询多个字符串（ 分词+全文检索）表示包含任意一个检索词都算 最终查询出 address 中包含 mill 或者 road 或者 mill road 的所有记录， 并给出相关性得分。 12345678GET bank/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill road&quot; &#125; &#125;&#125; 2.2.2.4 match_phrase【 短语匹配】 将需要匹配的值当成一个整体单词（ 不分词） 进行检索 12345678GET bank/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;address&quot;: &quot;mill road&quot; &#125; &#125;&#125; 2.2.2.5 multi_match【 多字段匹配】 state 或者 address 包含 mill 123456789GET bank/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;mill&quot;, &quot;fields&quot;: [&quot;state&quot;,&quot;address&quot;] &#125; &#125;&#125; 2.2.2.6 bool【 复合查询】 复合语句可以合并 任何 其它查询语句， 包括复合语句， 使得复合语句之间可以互相嵌套， 可以表达非常复杂的逻辑。 must： 必须达到 must 列举的所有条件 1234567891011GET bank/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123; &quot;gender&quot;: &quot;M&quot;&#125;&#125; ] &#125; &#125;&#125; must_not 必须不是指定的情况： email 必须不包含 baluba.com 1234567891011121314GET bank/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;gender&quot;: &quot;M&quot; &#125; &#125; ], &quot;must_not&quot;: [ &#123; &quot;match&quot;: &#123; &quot;email&quot;: &quot;baluba.com&quot; &#125; &#125; ] &#125; &#125;&#125; should： 应该达到 should 列举的条件， 如果达到会增加相关文档的评分， 并不会改变查询的结果。 如果 query 中只有 should 且只有一种匹配规则， 那么 should 的条件就会被作为默认匹配条件而去改变查询结果。 1234567891011121314GET bank/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;gender&quot;: &quot;M&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;lane&quot; &#125; &#125; ] &#125; &#125;&#125; 也可以复合 filter 查询 2.2.2.7 filter【结果过滤】 与 must_not 相反，保留复合条件的结果。用于在不计算相关性得分时，自动检查场景并且优化查询。 123456789101112131415GET bank/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;balance&quot;: &#123; &quot;gte&quot;: 20000, &quot;lte&quot;: 30000 &#125; &#125; &#125; &#125; &#125;&#125; 2.2.2.8 term 和 terms【精确查询】 与 match 的区别在于 match 是模糊查询（全文检索），term 是精确查询。 term 和 terms 的区别在于，terms 可以匹配多个字段，满足其一。 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;age&quot;: &quot;28&quot; &#125; &#125;&#125; 如果 term 匹配 text 文本，则匹配不到，需要用 match match 配合属性.keyword可以精确匹配 text，如同 term 匹配非 text。与 match_phrase 配合属性的区别在于 match 配合属性.keyword必须完整匹配 text，而 match_phrase 配合属性只要包含即可。 2.2.2.9 aggregations【执行聚合】 聚合查询提供了从数据中分组和提取数据的能力，最简单的聚合方法大致等于 SQL GROUP BY 和 SQL 聚合函数。 语法结构：aggregations 和 aggs 等价。 12345678910&quot;aggregations&quot; : &#123; &quot;&lt;aggregation_name&gt;&quot; : &#123; &quot;&lt;aggregation_type&gt;&quot; : &#123; &lt;aggregation_body&gt; &#125; [,&quot;meta&quot; : &#123; [&lt;meta_data_body&gt;] &#125; ]? [,&quot;aggregations&quot; : &#123; [&lt;sub_aggregation&gt;]+ &#125; ]? &#125; [,&quot;&lt;aggregation_name_2&gt;&quot; : &#123; ... &#125; ]*&#125; 聚合的方式有超多种，这里简单举 3 个案例进行体验。 案例一：搜索 address 中包含 mill 的所有人的年龄分布以及平均年龄， 但不显示这些人的详情。 12345678910111213141516171819202122GET bank/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_state&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125;, &quot;avg_age&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;, &quot;size&quot;: 0&#125; size： 0 不显示搜索数据，size：10 取出前 10 条记录 案例二：按照年龄聚合， 并且请求这些年龄段的这些人的平均薪资 12345678910111213141516171819202122GET bank/account/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;aggs&quot;: &#123; &quot;age_avg&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;size&quot;: 1000 &#125;, &quot;aggs&quot;: &#123; &quot;banlances_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125; &#125;, &quot;size&quot;: 1000&#125; 案例三：查出所有年龄分布， 并且这些年龄段中 M（男） 的平均薪资和 F（女） 的平均薪资以及这个年龄段的总体平均薪资 1234567891011121314151617181920212223242526272829303132333435GET bank/account/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;aggs&quot;: &#123; &quot;age_agg&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;size&quot;: 100 &#125;, &quot;aggs&quot;: &#123; &quot;gender_agg&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;gender.keyword&quot;, &quot;size&quot;: 100 &#125;, &quot;aggs&quot;: &#123; &quot;balance_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125;, &quot;balance_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125; &#125;, &quot;size&quot;: 1000&#125; 注意 keyword 的使用。否则会报错。 2.4 分词 一个 tokenizer（ 分词器） 接收一个字符流， 将之分割为独立的 tokens（ 词元， 通常是独立的单词）， 然后输出 tokens 流。 例如， whitespace tokenizer 遇到空白字符时分割文本。 它会将文本 “Quick brown fox!” 分割为 [Quick, brown, fox!]。 该 tokenizer（分词器） 还负责记录各个 term（词条） 的顺序或 position 位置（用于 phrase 短语和 word proximity 词近邻查询） ， 以及 term（词条） 所代表的原始 word（单词） 的 start（起始） 和 end（结束） 的 character offsets（字符偏移量） （用于高亮显示搜索的内容） 。 Elasticsearch 提供了很多内置的分词器， 可以用来构建 custom analyzers（自定义分词器）。 2.4.1 ik 分词器 安装： 方式一：进入 ES 内部：docker exec -it 容器id /bin/bash，并进入/plugins文件中。执行安装命令：wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.4.2/elasticsearch-analysis-ik-7.4.2.zip 找到对应的 ES 版本，我的为 7.4.2 方式二：方式一存在的问题是 docker 中的 ES 太纯净，没有 wget 工具，安装会找不到。由于安装 ES 时文件路径与 Vagrant 路径进行了映射，所以可以在 Vagrant 中安装。 安装 wget 工具：yum install wget 安装分词器：wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.4.2/elasticsearch-analysis-ik-7.4.2.zip 方式三：方式二中，安装分词器会发现 github 不允许非浏览器的方式进行下载，会提示Unable to establish SSL connection。所以可以使用先下载好 ik 分词器的压缩包，使用 xshell 存放到/plugins目录中进行解压安装，如解压为 ik 文件夹： 提升文件夹权限： 重启 ES：docker restart elasticsearch 测试： 使用默认（不使用 ik 分词器）： 使用 ik 分词器一： 使用 ik 分词器二： 不同的分词器， 分词有明显的区别， 所以以后定义一个索引不能再使用默认的 mapping 了， 要手工建立 mapping, 因为要选择分词器。 2.4.2 自定义词库 目标：搭建一个 nginx 服务器，作为 es 的远程分词库。 在 nginx 服务器创建词库： 在 nginx 的 html 目录下创建一个新的目录，如 es： 在 es 目录中，创建一个 txt 文件，在里面输入分词，如：乔碧萝、殿下 连接 nginx 服务器的词库： 修改plugins/ik/config/IKAnalyzer.cfg.xml文件 重启 ES 实例 测试分词效果 设置自定义分词前： 设置自定义分词后： 第 3 章 Java 操作 ES 3.1 方式一：连接 9300 端口 9300 端口建立 TCP 长连接。 要求：spring-data-elasticsearch:transport-api.jar； 缺点：springboot 版本不同， transport-api.jar 不同， 不能适配 es 版本。7.x 已经不建议使用， 8 以后就要废弃。 3.2 方式二：连接 9200 端口 9200 建立 HTTP 短连接。 工具及特点： JestClient： 非官方， 更新慢 RestTemplate： 模拟发 HTTP 请求， ES 很多操作需要自己封装， 麻烦 HttpClient： 同上 Elasticsearch-Rest-Client： 官方 RestClient， 封装了 ES 操作， API 层次分明， 上手简单。 3.3 使用 Elasticsearch-Rest-Client 根据 3.2 的对比结论，最终选择 Elasticsearch-Rest-Client 作为连接工具。 注意：使用的是高阶工具 3.3.1 springboot 整合 ES 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.4.2&lt;/version&gt;&lt;/dependency&gt; 配置 ES 12345678@Configurationpublic class GulimallElasticSearchConfig &#123; @Bean RestHighLevelClient client() &#123; RestClientBuilder builder = RestClient.builder(new HttpHost(&quot;192.168.56.10&quot;, 9200,&quot;http&quot;)); return new RestHighLevelClient(builder); &#125;&#125; 使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Testpublic void testSearch()&#123;// 1. 创建检索请求SearchRequest searchRequest = new SearchRequest();searchRequest.indices(&quot;bank&quot;);//指定索引SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();searchRequest.source(searchSourceBuilder);//指定DSL器// 1.1 构造检索条件// searchSourceBuilder.query();// searchSourceBuilder.from();// searchSourceBuilder.size();// searchSourceBuilder.aggregation();searchSourceBuilder.query(QueryBuilders.matchQuery(&quot;address&quot;, &quot;mill&quot;));// 1.2 按照年龄的值分布进行聚合TermsAggregationBuilder ageAgg = AggregationBuilders.terms(&quot;ageAgg&quot;).field(&quot;age&quot;).size(10);searchSourceBuilder.aggregation(ageAgg);// 1.3 计算平均薪资TermsAggregationBuilder balanceAvg = AggregationBuilders.terms(&quot;balanceAvg&quot;).field(&quot;balance&quot;);searchSourceBuilder.aggregation(balanceAvg);System.out.println(&quot;检索条件&quot; + searchSourceBuilder.toString());searchRequest.source(searchSourceBuilder);// 2. 执行检索SearchResponse search = null;try &#123; search = restHighLevelClient.search(searchRequest, GulimallElasticSearchConfig.COMMON_OPTIONS);&#125; catch (IOException e) &#123; e.printStackTrace();&#125;// 3. 分析查询结果// System.out.println(&quot;检索结果&quot; + search.toString());//JSON.parseObject(search.toString(), Map.class);//使用Java的方式解析json// 3.1 使用es的api处理——获取命中记录集SearchHits hits = search.getHits();SearchHit[] hitsHits = hits.getHits();for (SearchHit hit : hitsHits)&#123; // hit.getIndex(); hit.getType();hit.getId(); String string = hit.getSourceAsString();//获取hit结果，转为json字符串 Account account = JSON.parseObject(string, Account.class);//利用fastjson工具转换为Account对象 System.out.println(&quot;account&quot; + account);&#125;// 3.2 获取本次检索到的聚合数据Aggregations aggregations = search.getAggregations();// for(Aggregation aggregation : aggregations.asList())&#123;// System.out.println(&quot;当前聚合&quot; + aggregation.getName());// &#125;Terms ageAggResult = aggregations.get(&quot;ageAgg&quot;);for(Terms.Bucket bucket : ageAggResult.getBuckets())&#123; String keyAsString = bucket.getKeyAsString(); System.out.println(&quot;年龄&quot; + keyAsString + &quot;====&gt;&quot; + bucket.getDocCount());&#125;Avg balanceAvgResult = aggregations.get(&quot;balanceAvg&quot;);System.out.println(&quot;平均薪资&quot; + balanceAvgResult.getValue());&#125; 附录：安装 Nginx 1. 获得 nginx 的配置文件 随便启动一个 nginx 实例， 只是为了复制出配置：docker run -p 80:80 --name nginx -d nginx:1.10 将容器内的配置文件拷贝到当前目录：docker container cp nginx:/etc/nginx . 终止原容器：docker stop nginx 执行命令删除原容器： docker rm $ContainerId 修改文件名称：mv nginx conf 新建 nginx 文件夹，把这个 conf 移动到/mydata/nginx 下 2. 创建新的 nginx 容器 12345docker run -p 80:80 --name nginx \\-v /mydata/nginx/html:/usr/share/nginx/html \\-v /mydata/nginx/logs:/var/log/nginx \\-v /mydata/nginx/conf:/etc/nginx \\-d nginx:1.10","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://sk370.github.io/tags/ElasticSearch/"}]},{"title":"JUC","date":"2022-10-03T01:23:34.000Z","path":"2022/10/03/juc/JUC/","text":"JUC 是 java.util.concurrent 包的简称，该包提供了一系列多线程开发的工具类，使用该包的工具类能够快速进行多线程开发。（Java 并发编程） 1. 多线程基础 1.1 什么是 JUC 在 Java 5.0 提供了 java.util.concurrent（简称 JUC） 包， 在此包中增加了在并发编程中很常用的实用工具类， 用于定义类似于线程的自定义子系统， 包括线程池、 异步 IO 和轻量级任务框架。提供可调的、 灵活的线程池。 还提供了设计用于多线程上下文中的 Collection 实现等。 使用并发编程的背景：2003 年后，CPU 的主频不再翻倍，硬件厂商采用了多核而不是更快的主频的发展方向。在不提高主频、而核心数在不断增加的背景下，要想让程序更快运行，就要用并行或者并发编程。 使用高并发的优势： 充分利用多核处理器。 使用高并发系统，提高程序性能。 提高程序吞吐量，满足异步+回调等场景的生产需求。 使用高并发的弊端及问题： 线程安全性问题，如 i++，集合类的不安全 线程锁问题，synchronized 重量级、死锁等问题，如何选用合适的锁，并适当的控制锁。 线程性能问题 1.2 进程和线程 进程（Process） 是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。 在当代面向线程设计的计算机结构中，进程是线程的容器。程序是指令、数据及其组织形式的描述，进程是程序的实体。进程是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。 线程（thread） 是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。 总结来说: 进程：指在系统中正在运行的一个应用程序；程序一旦运行就是进程。进程——资源分配的最小单位。 线程：系统分配处理器时间资源的基本单元，或者说进程之内独立执行的一个单元执行流。线程——程序执行（CPU 时序调度）的最小单位。 1.3 线程的状态 1.3.1 Thread.State Thread 类中的内部枚举类 State 定义了线程的 6 种状态： NEW：尚未启动的线程处于此状态。 RUNNABLE：在 Java 虚拟机中执行的线程处于此状态。 BLOCKED：被阻塞等待监视器锁定的线程处于此状态。 WAITING：无限期等待另一个线程执行特定操作的线程处于此状态。 TIMED_WAITING：正在等待另一个线程执行最多指定等待时间的操作的线程处于此状态。 TERMINATED：已退出的线程处于此状态。 线程在给定时间点只能处于一种状态。 这些状态是虚拟机状态，不反映任何操作系统线程状态。 1.3.2 wait 和 sleep sleep 是 Thread 的静态方法， wait 是 Object 的方法，任何对象实例都能调用。 sleep 不会释放锁，它也不需要占用锁(？)。wait 会释放锁，但调用它的前提是当前线程占有锁(即代码要在 synchronized 中)。 它们都可以被 interrupted 方法中断。 1.4 并行和并发 1.4.1 串行模式 串行表示所有任务都一一按先后顺序进行。串行意味着必须先装完一车柴才能运送这车柴，只有运送到了，才能卸下这车柴，并且只有完成了这整个三个步骤，才能进行下一个步骤。串行是一次只能取得一个任务，并执行这个任务。 1.4.2 并行模式 并行意味着可以同时取得多个任务，并同时去执行所取得的这些任务。并行模式相当于将长长的一条队列，划分成了多条短队列，所以并行缩短了任务队列的长度。并行的效率从代码层次上强依赖于多进程/多线程代码，从硬件角度上则依赖于多核 CPU。 1.4.3 并发 并发(concurrent)指的是多个程序可以同时运行的现象，更细化的是多进程可以同时运行或者多指令可以同时运行，描述的是多进程同时运行的现象。 但实际上，对于单核心 CPU 来说，同一时刻只能运行一个线程。所以，这里的&quot;同时运行&quot;表示的不是真的同一时刻有多个线程运行的现象，这是并行的概念，而是提供一种功能让用户看来多个程序同时运行起来了，但实际上这些程序中的进程不是一直霸占 CPU 的，而是执行一会停一会。 要解决大并发问题，通常是将大任务分解成多个小任务, 由于操作系统对进程的调度是随机的，所以切分成多个小任务后，可能会从任一小任务处执行。这可能会出现一些现象： 可能出现一个小任务执行了多次，还没开始下个任务的情况。这时一般会采用队列或类似的数据结构来存放各个小任务的成果。 可能出现还没准备好第一步就执行第二步的可能。这时，一般采用多路复用或异步的方式，比如只有准备好产生了事件通知才执行某个任务。 可以多进程/多线程的方式并行执行这些小任务。也可以单进程/单线程执行这些小任务，这时很可能要配合多路复用才能达到较高的效率。 并发： 同一时刻多个线程在访问同一个资源，多个线程对一个点。例子：春运抢票、电商秒杀… 并行： 多项工作一起执行，之后再汇总结果。 1.5 管程 管程(monitor—监视器—Java 中描述的锁)是保证了同一时刻只有一个进程在管程内活动，即管程内定义的操作在同一时刻只被一个进程调用(由编译器实现).但是这样并不能保证进程以设计的顺序执行。 JVM 中同步是基于进入和退出管程(monitor)对象实现的，每个对象都会有一个管程(monitor)对象，管程(monitor)会随着 java 对象一同创建和销毁。 来源于 JVM 中的定义：执行线程首先要持有管程对象，然后才能执行方法，当方法完成之后会释放管程，方法在执行时候会持有管程，其他线程无法再获取同一个管程。 1.6 用户线程和守护线程 用户线程：平时用到的普通线程，自定义线程。 守护线程：运行在后台，是一种特殊的线程，一种为其他线程服务的线程，比如垃圾回收。 当主线程结束后，用户线程还在运行，JVM 存活。 如果没有用户线程，都是守护线程，JVM 结束，自动退出。 Thread 类中的 final 方法 isDaemon()能够判断当前线程为用户线程（false）还是守护线程（true）。 setDaemon()能够设置当前线程为用户线程（false）还是守护线程（true）。 setDaemon()需要在 start()方法之前调用，否则不起作用。 1.7 start()方法源码 通过 Thread.java 的 start()方法可以看到，内部调用了 start0()方法，而 start0()是 Thread.java 中的 native 方法。start()源码 想要查看 native 方法，需要下载 openjdk 查看源码，下载地址：http://hg.openjdk.java.net/jdk8，结合本课程，需要下载 hotspot 源码和 jdk 源码。 jdk\\src\\sharr\\native\\java目录，里面有各种 xxx.c 文件，因为 JNI 机制（JVM 的本地方法接口调用的就是 native 方法），所以 Thread.java 文件中的 native 方法（jdk 中的文件与 Java 中的文件一一对应）能在 java\\lang\\Thread.c 文件中找到： Thread.c 中的 start0 对应着的 JVM_StartThread 在\\hotspot\\src\\share\\vm\\prims\\jvm.cpp文件中，jvm.cpp 中的 JVM_StartThread 调用了 Thread::start(native_thread)，该方法在\\hotspot\\src\\share\\vm\\prims\\thread.cpp中，thread.cpp 中的 Thread::start(native_thread)调用了 os::start_thread(thread)，这里的 os 代表操作系统。 总结： 123456789101112131415161718public synchronized void start() &#123; if (threadStatus != 0) throw new IllegalThreadStateException(); group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; &#125; &#125;&#125;private native void start0(); 2. Lock 接口 2.1 synchronized 关键字 同步锁：保证每个线程都能正常执行的标记锁。 每个 Java 对象都有且只有一个同步锁，在任何时刻，最多只允许一个线程拥有这把锁 synchronized 是 Java 中的关键字，是一种同步锁。它修饰的对象有以下几种： 修饰一个代码块，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象。 修饰一个方法，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象。 虽然可以使用 synchronized 来定义方法，但 synchronized 并不属于方法定义的一部分，因此，synchronized 关键字不能被继承。 如果在父类中的某个方法使用了 synchronized 关键字，而在子类中覆盖了这个方法，在子类中的这个方法默认情况下并不是同步的，而必须显式地在子类的这个方法中加上 synchronized 关键字才可以。 当然，还可以在子类方法中调用父类中相应的方法，这样虽然子类中的方法不是同步的，但子类调用了父类的同步方法，因此，子类的方法也就相当于同步了。 修饰一个静态的方法，其作用的范围是整个静态方法，作用的对象是这个类的所有对象。 修饰一个类，其作用的范围是 synchronized 后面括号括起来的部分，作用的对象是这个类的所有对象。 买票案例：假设 3 个售票员，每个都可以卖完所有票，所以要争夺资源（票数）。 123456789101112131415161718192021222324252627282930313233public class SaleTicket &#123; public static void main(String[] args) &#123; Ticket ticket = new Ticket(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 30; i++) &#123; ticket.sale(); &#125; &#125;, &quot;aa&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 30; i++) &#123; ticket.sale(); &#125; &#125;, &quot;bb&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 30; i++) &#123; ticket.sale(); &#125; &#125;, &quot;cc&quot;).start(); &#125;&#125;class Ticket&#123; private int number = 30;//30张票 /** * 售出 */ public synchronized void sale()&#123; if(number &gt; 0)&#123; System.out.println(&quot;当前票数：&quot; + this.number + &quot;，&quot; + Thread.currentThread().getName() + &quot;卖出了1张票，目前剩余&quot; + --number + &quot;张票&quot;); &#125; &#125;&#125; 这里没有使用 Ticket 继承 Thread 的方式，所以创建线程对象时也没有使用Tread ticket1 = new Ticket()的方式，这是因为继承具有局限性，一般不这么用，所以要去习惯采用函数式接口的匿名内部类的方式。 2.2 创建多线程 使用Thread(Runnable target, String name)及 Lambda 表达式创建多线程对象： 注意： 启动线程需要使用线程对象的名字，即 aa。 获取到的线程名字为Thread(Runnable target, String name)构造器种传入的字符串，即 bb。 创建了线程对象，调用 start()方法并不一定马上创建线程，因为 start()方法内调用了 navtie 修饰的 start0()方法，表示由操作系统（JVM）确定何时创建。 多线程编程步骤： 创建资源类，在资源类创建属性和操作方法。 创建多线程，调用资源类的操作方法。 2.3 Lock 概述 2.3.1 Lock 实现类 Lock 锁是 java.util.concurrent.locks 包下的 Lock 接口，它的实现类有 ReentrantLock，ReentrantReadWriteLock.ReadLock ， ReentrantReadWriteLock.WriteLock，提供了比使用同步方法和语句可以获得的更广泛的锁操作。它们允许更灵活的结构，可能具有非常不同的属性，并且可能支持多个关联的条件对象。 Lock 提供了比 synchronized 更多的功能。 ReentrantLock 是唯一实现了 Lock 接口的类 ，意思是“可重入锁”。 2.3.2 Lock 与 Synchronized 的区别 Lock 不是 Java 语言内置的，synchronized 是 Java 语言的关键字，因此是内置特性。Lock 是一个接口，通过实现类类可以实现同步访问。 synchronized 不需要用户去手动释放锁，当 synchronized 方法或者 synchronized 代码块执行完之后，系统会自动让线程释放对锁的占用；而 Lock 则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现象。 synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生。而 Lock 在发生异常时，如果没有主动通过 unLock()去释放锁，则很可能造成死锁现象，因此使用 Lock 时需要在 finally 块中释放锁。 Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，等待的线程会一直等待下去，不能够响应中断。通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。 总结：Lock 可以提高多个线程进行读操作的效率。 在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时 Lock 的性能要远远优于 synchronized。 使用 ReentrantLock 重写上述代码 123456789101112131415161718192021222324252627282930313233343536373839public class SaleTicket &#123; public static void main(String[] args) &#123; Ticket ticket = new Ticket(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 30; i++) &#123; ticket.sale(); &#125; &#125;, &quot;aa&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 30; i++) &#123; ticket.sale(); &#125; &#125;, &quot;bb&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 30; i++) &#123; ticket.sale(); &#125; &#125;, &quot;cc&quot;).start(); &#125;&#125;class Ticket&#123; private int number = 30;//30张票 private final ReentrantLock lock = new ReentrantLock();//创建可重入锁 /** * 售出 */ public void sale()&#123; lock.lock(); try &#123; if(number &gt; 0)&#123; System.out.println(&quot;当前票数：&quot; + this.number + &quot;，&quot; + Thread.currentThread().getName() + &quot;卖出了1张票，目前剩余&quot; + --number + &quot;张票&quot;); &#125; &#125; finally &#123; lock.unlock();//确保不论有无异常，均释放锁 &#125; &#125;&#125; 2.4 Lock 接口方法 12345678public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long time, TimeUnit unit) throws InterruptedException; void unlock(); Condition newCondition();&#125; 2.4.1 lock()、unlock() lock()方法是平常使用得最多的一个方法，就是用来获取锁。如果锁已被其他线程获取，则进行等待。 采用 Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁。因此一般来说，使用 Lock 必须在 try{}catch{}块中进行，并且将释放锁的操作放在 finally 块中进行，以保证锁一定被被释放，防止死锁的发生。通常使用 Lock 来进行同步的话，是以下面这种形式去使用的： 12345678Lock lock = ...;//获取锁lock.lock();try&#123; //处理任务&#125;catch(Exception ex)&#123;&#125;finally&#123; lock.unlock(); //释放锁&#125; 2.4.2 newCondition() 关键字 synchronized 与 wait()/notify()这两个方法一起使用可以实现等待/通知模式，Lock 锁的 newContition()方法返回 Condition 对象，Condition 类也可以实现等待/通知模式。 用 notify()通知时，JVM 会随机唤醒某个等待的线程， 使用 Condition 类可以进行选择性通知，Condition 比较常用的两个方法： await()会使当前线程等待,同时会释放锁,当其他线程调用 signal()时,线程会重新获得锁并继续执行。 signal()用于唤醒一个等待的线程。 注意：在调用 Condition 的 await()/signal()方法前，也需要线程持有相关的 Lock 锁，调用 await()后线程会释放这个锁，在 singal()调用后会从当前 Condition 对象的等待队列中，唤醒 一个线程，唤醒的线程尝试获得锁， 一旦获得锁成功就继续执行。 3. 线程间通信 3.1 概述 线程通信即线程按照既定的顺序执行。线程通信模型有共享内存和消息传递两种方式。 多线程通信编程步骤： 创建资源类，在资源类创建属性和操作方法。 在资源类操作方法中：（1）判断；（2）干活；（3）通知 创建多线程，调用资源类的操作方法。 3.2 synchronized 实现线程通信 使用 Object()对象的 wait()和 notify()方法实现线程的通信： 123456789101112131415161718192021222324252627282930313233343536373839404142public class ThreadCommunication &#123; public static void main(String[] args) &#123; Share share = new Share(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; share.incr(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;aa&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; share.decr(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;bb&quot;).start(); &#125;&#125;class Share&#123; private int number = 0; public synchronized void incr() throws InterruptedException &#123; if(this.number != 0)&#123; this.wait();//线程等待获取jvm资源 &#125; number++; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + number); this.notify();//通知其他线程 &#125; public synchronized void decr() throws InterruptedException &#123; if(this.number == 0)&#123; this.wait();//线程等待获取jvm资源 &#125; number--; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + number); this.notify();//通知其他线程 &#125;&#125; 存在问题：上述案例只有两个线程对 number 进行操作，当一个线程 wait()时，由于它只能被另外一个线程唤醒，两个线程的执行次数是一定的，而且也是交替执行，执行结果总是 0。 但是当有多个线程进行 number++、多个线程进行 number–操作时，由于 notify()唤醒的线程不确定，当它唤醒了一个已经在 wait()的线程时，会直接执行 if 语句后的代码，而不会再进行 if 判断，导致执行结果每次不一样，有可能发生阻塞，且不一定是 0。 线程可以在没有被通知，中断或超时的情况下唤醒 ，即所谓的虚假唤醒** **。上述例子使用 wile 条件判断即可解决运行结果不为 0 的问题。 3.3 Lock 的方法实现线程通信 使用 Lock 实现上述代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class ThreadCommunication &#123; public static void main(String[] args) &#123; Share share = new Share(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; share.incr(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;aa&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; share.decr(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;bb&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; share.incr(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;cc&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; share.decr(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;dd&quot;).start(); &#125;&#125;class Share&#123; private int number = 0; private final Lock lock = new ReentrantLock(); private final Condition condition = lock.newCondition(); public void incr() throws InterruptedException &#123; lock.lock(); try &#123; while (this.number != 0)&#123; condition.await();//线程等待获取jvm资源 &#125; number++; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + number); condition.signalAll();//通知其他线程 &#125; finally &#123; lock.unlock(); &#125; &#125; public void decr() throws InterruptedException &#123; lock.lock(); try &#123; while (this.number ==0 )&#123; condition.await();//线程等待获取jvm资源 &#125; number--; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + number); condition.signalAll();//通知其他线程 &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 注意：await()方法仍有虚假唤醒问题，所以要将 if 判断替换为 while() 3.4 线程间定制化通信 既实现线程的执行顺序控制，又实现每次线程的执行次序控制。 案例：A 线程打印 5 次 A，B 线程打印 10 次 B，C 线程打印 15 次 C，按照此顺序循环 10 轮。 思路：给每个线程定制一个标志位，执行次数到了修改标志位，通知下一个线程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class CustomizedCommunication &#123; public static void main(String[] args) &#123; ShareResource shareResource = new ShareResource(); new Thread(() -&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; shareResource.print5(i); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;AA&quot;).start(); new Thread(() -&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; shareResource.print10(i); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;BB&quot;).start(); new Thread(() -&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; shareResource.print15(i); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;CC&quot;).start(); &#125;&#125;class ShareResource &#123; private int flag = 1; private final Lock lock = new ReentrantLock(); private Condition c1 = lock.newCondition(); private Condition c2 = lock.newCondition(); private Condition c3 = lock.newCondition(); public void print5(int loop) throws InterruptedException &#123; lock.lock(); try &#123; while (flag != 1)&#123; c1.await(); &#125; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(Thread.currentThread().getName() + &quot;：：&quot; + i + &quot;轮数：&quot; + loop); &#125; flag = 2; c2.signalAll();//通知指定线程 &#125;finally &#123; lock.unlock(); &#125; &#125; public void print10(int loop) throws InterruptedException &#123; lock.lock(); try &#123; while (flag != 2)&#123; c2.await(); &#125; for (int i = 0; i &lt; 10; i++) &#123; System.out.println(Thread.currentThread().getName() + &quot;：：&quot; + i + &quot;轮数：&quot; + loop); &#125; flag = 3; c3.signalAll();//通知指定线程 &#125;finally &#123; lock.unlock(); &#125; &#125; public void print15(int loop) throws InterruptedException &#123; lock.lock(); try &#123; while (flag != 3)&#123; c3.await(); &#125; for (int i = 0; i &lt; 15; i++) &#123; System.out.println(Thread.currentThread().getName() + &quot;：：&quot; + i + &quot;轮数：&quot; + loop); &#125; flag = 1; c1.signalAll();//通知指定线程 &#125;finally &#123; lock.unlock(); &#125; &#125;&#125; 4. 集合的线程不安全 4.1 List 的线程不安全 4.1.1 ArryaList 线程不安全演示 并发修改异常：从集合中取数据的同时在存数据，出现 java.util.ConcurrentModificationException。原因是 ArrayList 的 add()方法没有 synchronized 修饰。 1234567891011public class ListDanger &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; list.add(UUID.randomUUID().toString().substring(0,8)); System.out.println(list); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 4.1.2 解决方案 使用 Vector 代替 ArrayList：Vector 的 add()方法使用了 synchronized 修饰。 使用 Collections 工具类的 synchronizedList()方法：List&lt;String&gt; list = Collections.synchronizedList(new ArrayList&lt;&gt;()); 使用 CopyOnWriteArrayList：CopyOnWriteArrayList 是 java.util.concurrent 包下的类，实际开发中使用这种方案。 原理：写时复制技术。每次写入数据时，先创建原对象的拷贝，在拷贝的对象中进行数据写入，写入结束再将拷贝对象和原对象进行合并，如果要读取则读取合并产生的新对象。这样做的优势是可以兼顾并发读操作与独立写操作。 源码： 使用场景：List 大小保持很小，只读操作远多于可变操作。 4.2 Set 的线程不安全 4.2.1 HashSet 的线程不安全演示 不安全原因：add()方法没有 synchronized 修饰。 1234567891011public class SetDanger &#123; public static void main(String[] args) &#123; Set&lt;String&gt; set = new HashSet&lt;&gt;();//线程不安全 for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; set.add(UUID.randomUUID().toString().substring(0,8)); System.out.println(set); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 4.2.2 解决方案 使用 CopyOnWriteArraySet：CopyOnWriteArraySet 是 java.util.concurrent 包下的类，实际开发中使用这种方案。 原理：写时复制技术。add()调用 addIfAbsent()方法： 4.3 Map 的线程不安全 4.3.1 HashMap 的线程不安全演示 HashSet 的底层就是 HashMap，put()方法没有 synchronized 修饰。 123456789101112public class MapDanger &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;();//线程不安全 for (int i = 0; i &lt; 10; i++) &#123; String key = String.valueOf(i); new Thread(() -&gt; &#123; map.put(key, UUID.randomUUID().toString().substring(0,8)); System.out.println(map); &#125;,key).start(); &#125; &#125;&#125; 4.3.2 解决方案 使用 ConcurrentHashMap：ConcurrentHashMap 是 java.util.concurrent 包下的类，实际开发中使用这种方案。 原理：put()方法调用了 putVal()方法，该方法中使用了 synchronized 修饰。 5. 多线程锁 5.1 synchronized 锁的 8 种情况 123456789101112class Phone &#123; public static synchronized void sendSMS() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(&quot;------sendSMS&quot;); &#125; public synchronized void sendEmail() throws Exception &#123; System.out.println(&quot;------sendEmail&quot;); &#125; public void getHello() &#123; System.out.println(&quot;------getHello&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930311 标准访问，先打印短信还是邮件------sendSMS------sendEmail2 停4秒在短信方法内，先打印短信还是邮件------sendSMS------sendEmail3 新增普通的hello方法，是先打短信还是hello------getHello------sendSMS4 现在有两部手机，先打印短信还是邮件------sendEmail------sendSMS5 两个静态同步方法，1部手机，先打印短信还是邮件------sendSMS------sendEmail6 两个静态同步方法，2部手机，先打印短信还是邮件------sendSMS------sendEmail7 1个静态同步方法,1个普通同步方法，1部手机，先打印短信还是邮件------sendEmail------sendSMS8 1个静态同步方法,1个普通同步方法，2部手机，先打印短信还是邮件------sendEmail------sendSMS main 线程中，多个分支线程的启动顺序不一定，为了”控制“执行顺序，可以在主线程代码中加入睡眠时间，让前面的线程先启动，后面的稍等待。 5.2 公平锁和非公平锁 公平锁：private final ReentrantLock lock = new ReentrantLock()构造器不传参或传入 true，此时各线程抢夺资源，相对公平，单效率较低。 非公平锁：private final ReentrantLock lock = new ReentrantLock()构造器传入 false，此时第一个抢到资源的线程优先执行，且可能执行完，导致其他线程饿死，相对不公，单效率高。 5.3 可重入锁（递归锁） synchronized 和 Lock 都是可重入锁。可重入锁指多层加锁的方法，只要获得了外层的锁，就可进入内层的方法（原理：实际上外层和内层都是同一把锁）。 由于 synchronized 加锁和释放锁自动完成，所以称为隐式可重入锁。 Lock 的加锁和释放锁需要手动操作，所以称为显式可重入锁。 5.4 死锁 死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。 产生原因： 系统资源不足。 进程运行推进顺序不合适。 资源分配不当 123456789101112131415161718192021222324252627282930313233public class DeadLock &#123; static Object a = new Object(); static Object b = new Object(); public static void main(String[] args) &#123; new Thread(()-&gt;&#123; synchronized (a)&#123; System.out.println(Thread.currentThread().getName() + &quot;持有锁a，试图获取锁b&quot;); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (b)&#123; System.out.println(Thread.currentThread().getName() + &quot;获取锁b&quot;); &#125; &#125; &#125;,&quot;A&quot;).start(); new Thread(()-&gt;&#123; synchronized (b)&#123; System.out.println(Thread.currentThread().getName() + &quot;持有锁b，试图获取锁a&quot;); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (a)&#123; System.out.println(Thread.currentThread().getName() + &quot;获取锁a&quot;); &#125; &#125; &#125;,&quot;B&quot;).start(); &#125;&#125; 上述案例中，需要加上 sleep()方法来让死锁更好的生效，不然执行太快，直接执行结束了，没有发生死锁。 验证是否是死锁： jsp 命令：该工具在 jdk/bin 目录下，要么将该目录添加到环境变量，以让在各地运行，要么进入该目录，打开命令行终端执行jps -l jstack 命令：同上，执行完jps -l命令后，执行如jstack 11056 5.5 乐观锁和悲观锁 悲观锁：不支持并发操作，效率低。每个线程都进行加锁、释放锁操作，一次只能执行一个线程，其他等待。 乐观锁：给原始数据添加版本号，每个线程操作原始数据就修改版本号，其他线程要操作原始数据时使用版本号进行比较，看是否发生变化。 5.6 读写锁 5.6.1 概念 特点：一个资源可以被多个读线程访问，或者被一个写线程访问，但是不能同时存在读写线程。即读写互斥，读读共享。 出现演变：多线程的场景下，① 如果没有锁，则各线程抢夺资源，线程运行较乱。② 使用了 synchronized 或 ReentrantLock 对资源加锁，由于是独占锁，每次只能有一个线程对资源进行读或者写操作，影响读读的效率。③ReentrantReadWriteLock 可以同时进行读读操作，但是读写互斥，也有可能影响一定的性能（可以对写锁进行降级，降级为读锁，参看 5.6.2）。 场景描述：共享资源有读和写的操作，且写操作没有读操作那么频繁。在没有写操作的时候，多个线程同时读一个资源没有任何问题，所以应该允许多个线程同时读取共享资源；但是如果一个线程想去写这些共享资源，就不应该允许其他线程对该资源进行读和写的操作了。 读锁：共享锁，可能发生死锁。 死锁情形描述：两个线程共同读取公共资源，此时持有读锁，而线程 1 要对内容进行修改，则要等到线程 2 读之后。线程 2 要修改时，相应地要等线程 1 读之后。 写锁：独占锁，可能发生死锁。 死锁情形描述：没明白。 线程进入读锁的前提条件： 没有其他线程的写锁 没有写请求, 或者有写请求，但调用线程和持有锁的线程是同一个(可重入锁)。 线程进入写锁的前提条件： 没有其他线程的读锁 没有其他线程的写锁 读写锁的三个重要的特性： 公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。 重进入：读锁和写锁都支持线程重进入。 锁降级：遵循获取写锁、获取读锁再释放写锁的次序，写锁能够降级成为读锁。 5.6.2 ReentrantReadWriteLock ReadWriteLock 是一个接口，内部只定义了两个方法：Lock readLock()和Lock writeLock()，一个用来获取读锁，一个用来获取写锁。也就是说将文件的读写操作分开，分成 2 个锁来分配给线程，从而使得多个线程可以同时进行读操作。 ReentrantReadWriteLock 实现了 ReadWriteLock 接口。 如果有一个线程已经占用了读锁，则此时其他线程如果要申请写锁，则申请写锁的线程会一直等待释放读锁。 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程会一直等待释放写锁。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class ReadWriteLockDemo &#123; public static void main(String[] args) &#123; MyCache myCache = new MyCache(); for (int i = 0; i &lt; 5; i++) &#123; final int num = i; new Thread(()-&gt;&#123; myCache.put(num+&quot;&quot;,num+&quot;&quot;); &#125;,String.valueOf(i)).start(); &#125; for (int i = 0; i &lt; 5; i++) &#123; final int num = i; new Thread(()-&gt;&#123; myCache.get(num+&quot;&quot;); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125;class MyCache&#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); public void put(String key, Object value)&#123; System.out.println(Thread.currentThread().getName() + &quot;正在进行写操作&quot; + key); try &#123; TimeUnit.MILLISECONDS.sleep(300);//模拟写入过程 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; map.put(key, value); System.out.println(Thread.currentThread().getName() + &quot;写完了&quot; + key); &#125; public Object get(String key)&#123; Object result = null; System.out.println(Thread.currentThread().getName() + &quot;正在进行读操作&quot; + key); try &#123; TimeUnit.MILLISECONDS.sleep(300);//模拟写入过程 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; result = map.get(key); System.out.println(Thread.currentThread().getName() + &quot;读完了&quot; + result); return result; &#125;&#125; 没有使用读写锁的情况：读写顺序不受控制，可能读在写之前，导致得到空值。 改造： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class ReadWriteLockDemo &#123; public static void main(String[] args) &#123; MyCache myCache = new MyCache(); for (int i = 0; i &lt; 5; i++) &#123; final int num = i; new Thread(()-&gt;&#123; myCache.put(num+&quot;&quot;,num+&quot;&quot;); &#125;,String.valueOf(i)).start(); &#125; for (int i = 0; i &lt; 5; i++) &#123; final int num = i; new Thread(()-&gt;&#123; myCache.get(num+&quot;&quot;); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125;class MyCache&#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); private ReadWriteLock rwLock = new ReentrantReadWriteLock(); public void put(String key, Object value)&#123; rwLock.writeLock().lock();//添加写锁 try &#123; System.out.println(Thread.currentThread().getName() + &quot;正在进行写操作&quot; + key); TimeUnit.MILLISECONDS.sleep(300);//模拟写入过程 map.put(key, value); System.out.println(Thread.currentThread().getName() + &quot;写完了&quot; + key); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; rwLock.writeLock().unlock();//释放写锁 &#125; &#125; public Object get(String key)&#123; rwLock.readLock().lock();//添加读锁 Object result = null; try &#123; System.out.println(Thread.currentThread().getName() + &quot;正在进行读操作&quot; + key); TimeUnit.MILLISECONDS.sleep(300);//模拟写入过程 result = map.get(key); System.out.println(Thread.currentThread().getName() + &quot;读完了&quot; + result); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; rwLock.readLock().lock();//释放锁 &#125; return result; &#125;&#125; 在线程持有读锁的情况下，该线程不能取得写锁(因为获取写锁的时候，如果发现当前的读锁被占用，就马上获取失败，不管读锁是不是被当前线程持有)。 在线程持有写锁的情况下，该线程可以继续获取读锁（获取读锁时如果发现写锁被占用，只有写锁没有被当前线程占用的情况才会获取失败）。 原因: 当线程获取读锁的时候，可能有其他线程同时也在持有读锁，因此不能把获取读锁的线程“升级” 为写锁；而对于获得写锁的线程，它一定独占了读写锁，因此可以继续让它获取读锁，当它同时获取了写锁和读锁后，还可以先释放写锁继续持有读锁，这样一个写锁就“降级” 为了读锁。 volatile 关键字在一个多线程应用中，出于计算性能的考虑，每个线程默认是从主内存将该变量拷贝到线程所在 CPU 的缓存中，然后进行读写操作的。现在电脑基本都是多核 CPU，不同的线程可能运行的不同的核上，而每个核都会有自己的缓存空间。 这里存在一个问题，JVM 既不会保证什么时候把 CPU 缓存里的数据写到主内存，也不会保证什么时候从主内存读数据到 CPU 缓存。也就是说，不同 CPU 上的线程，对同一个变量可能读取到的值是不一致的，这也就是通常说的：线程间的不可见问题。 volatile 关键字解决了线程间不可见性，通过 volatile 修饰的变量，都会变得线程间可见。即被 volatile 关键字修饰的变量会直接存储到主内存中。 5.6.2 写锁降级 实现过程：获取写锁、获取读锁、释放写锁、释放读锁，写锁能够降级成为读锁。但是读锁不能升级为写锁。 降级演示： 1234567891011121314public class WriteLockDown &#123; public static void main(String[] args) &#123; ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock();//可重入读写锁 ReentrantReadWriteLock.ReadLock readLock = rwLock.readLock();//读锁 ReentrantReadWriteLock.WriteLock writeLock = rwLock.writeLock();//写锁 writeLock.lock();//获取写锁 System.out.println(1); readLock.lock();//获取读锁 System.out.println(2); writeLock.unlock();//释放写锁 readLock.unlock();//释放读锁 &#125;&#125; 上述案例中，释放写锁之前，进行了读操作，可以看到可以正常输出 2，证明写锁没有影响到读锁（降级为了读锁，读读共享），没有出现读写互斥的情况，提升了效率和性能。 读锁升级写锁演示; 1234567891011121314public class WriteLockDown &#123; public static void main(String[] args) &#123; ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock();//可重入读写锁 ReentrantReadWriteLock.ReadLock readLock = rwLock.readLock();//读锁 ReentrantReadWriteLock.WriteLock writeLock = rwLock.writeLock();//写锁 readLock.lock();//获取读锁 System.out.println(2); writeLock.lock();//获取写锁 System.out.println(1); writeLock.unlock();//释放写锁 readLock.unlock();//释放读锁 &#125;&#125; 可以看到先加了读锁之后，程序不能继续进行，由于读写互斥，而读锁不能升级为写锁。 6. Callable 接口 6.1 Callable 和 Runalbe 接口比较 Runnable 缺少的一项功能是，当线程终止时（即 run()完成时），无法使线程返回结果。为了支持此功能，Java 5 中提供了 Callable 接口。 总结来说，二者有以下三点不同： Callable 的执行方法有返回值，Runnable 没有 Callable 的执行方法可以抛出异常，Runnable 不可以 Callable 的执行方法叫 call()，Runnable 的执行方法叫 run() 12345678910class MyThread1 implements Runnable&#123; @Override public void run() &#123; &#125;&#125;class MyThread2 implements Callable&#123; @Override public Object call() throws Exception &#123; return null; &#125;&#125; 6.2 使用 Callable 创建线程 6.2.1 创建方式 要想使用 new Thread(Runnable, String)的方式创建 Callable 的线程，则需要找到 Callable 和 Runable 产生交集的部分。 FutureTask 是 Runnable 接口的实现类，而其构造方法 FutureTask(Callable callable)可以使用 Callable，此时相当于创建了 Runable 接口的实现类对象，这样就可以使用 new Thread(runnable, string)了。 123456789public class CompareInterface &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(()-&gt;&#123; return 200; &#125;); new Thread(futureTask, &quot;lucy&quot;).start(); System.out.println(futureTask.get()); &#125;&#125; 6.2.2 FurtureTask 原理 在主线程中需要执行比较耗时的操作时，但又不想阻塞主线程时，可以把这些作业交给 Future 对象在后台完成。 当主线程将来需要时，就可以通过 Future 对象获得后台作业的计算结果或者执行状态。 一般 FutureTask 多用于耗时的计算，主线程可以在完成自己的任务后，再去获取结果。 仅在计算完成时才能检索结果；如果计算尚未完成，则阻塞 get 方法一旦计算完成，就不能再重新开始或取消计算。 get 方法而获取结果只有在计算完成时获取，否则会一直阻塞直到任务转入完成状态，然后会返回结果或者抛出异常。 get 只计算一次，因此 get 方法放到最后。 6.2.3 FutureTask 优缺点 优点：使用 Future 配合线程池创建多线程异步任务，能够提升程序运行效率。 1234567891011121314151617181920212223public static void main(String[] args) throws ExecutionException, InterruptedException&#123; //3个任务，目前开启多个异步任务线程来处理，请问耗时多少？ ExecutorService threadPool = Executors.newFixedThreadPool(3); long startTime = System.currentTimeMillis(); FutureTask&lt;String&gt; futureTask1 = new FutureTask&lt;String&gt;(() -&gt; &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;task1 over&quot;; &#125;); threadPool.submit(futureTask1); FutureTask&lt;String&gt; futureTask2 = new FutureTask&lt;String&gt;(() -&gt; &#123; try &#123; TimeUnit.MILLISECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;task2 over&quot;; &#125;); threadPool.submit(futureTask2); System.out.println(futureTask1.get()); System.out.println(futureTask2.get()); try &#123; TimeUnit.MILLISECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; long endTime = System.currentTimeMillis(); System.out.println(&quot;----costTime: &quot;+(endTime - startTime) +&quot; 毫秒&quot;); System.out.println(Thread.currentThread().getName()+&quot;\\t -----end&quot;); threadPool.shutdown();&#125; 缺点：Future 对于结果获取不是很友好，只能通过使用阻塞或轮询地方式得到任务地结果。 get()方法会发生阻塞。由于 get()方法只能等线程运行完才能获得结果，当线程运行时间较长 get()一直得不到结果，会导致后面的程序造成等待。 所以一般将 get()放在程序的后面。 避免程序阻塞可使用get(long timeout, TimeUnit unit)方法，让在等待的时间内得不到结果时抛出异常，然后捕获异常再做其他处理。 isDone()轮询消耗 CPU 资源：使用 isDone()方法可以判断任务是否完成，完成了再调用 get()方法，但由于不停的进行 isDone()判断，不停地进入 while 循环，导致 CPU 一直要执行此代码，消耗资源。 6.3 使用 FutureTask 使 Runnable 有返回值 7. JUC 同步器 7.1 CountDownLatch（计数器） CountDownLatch 类可以设置一个计数器，然后通过 countDown 方法来进行减 1 的操作，使用 await 方法等待计数器不大于 0，然后继续执行 await 方法之后的语句。 CountDownLatch 主要有两个方法，当一个或多个线程调用 await 方法时，这些线程会阻塞。 其它线程调用 countDown 方法会将计数器减 1(调用 countDown 方法的线程不会阻塞) 当计数器的值变为 0 时，因 await 方法阻塞的线程会被唤醒，继续执行 12345678910public class CountDownLatchDemo &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 6; i++) &#123; new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;号同学离开了教师&quot;); &#125;,String.valueOf(i)).start(); &#125; System.out.println(&quot;都走光了，锁门！&quot;); &#125;&#125; 上述代码的执行异常： 使用 CountDownLatch 进行改造： 1234567891011121314151617public class CountDownLatchDemo &#123; public static void main(String[] args) &#123; CountDownLatch countDownLatch = new CountDownLatch(6);//创建对象并设置初始值 for (int i = 0; i &lt; 6; i++) &#123; new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;号同学离开了教师&quot;); countDownLatch.countDown();//每次让计数器减一 &#125;,String.valueOf(i)).start(); &#125; try &#123; countDownLatch.await();//等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;都走光了，锁门！&quot;); &#125;&#125; 7.2 CyclicBarrier（循环栅栏） CyclicBarrier 英文是循环阻塞的意思，在使用中 CyclicBarrier 的构造方法第一个参数是目标障碍数，每次执行 CyclicBarrier 一次障碍数会加一，如果达到了目标障碍数，才会执行 cyclicBarrier.await()之后的语句。可以将 CyclicBarrier 理解为加 1 操作。 123456789101112131415161718public class CyclicBarrierDemo &#123; private static final int NUMBER = 7; public static void main(String[] args) &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(NUMBER, ()-&gt;&#123; System.out.println(&quot;集齐七颗许愿&quot;); &#125;); for (int i = 0; i &lt; 8; i++) &#123; new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;星龙珠收集到了&quot;); try &#123; cyclicBarrier.await();//等待 &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 上述案例中，当 i 最大数为 7 时，正好输出 7 次，程序正常结束。小于 7，输出不够，无法许愿（没有输出），程序没有正常停止。大于 7，输出已经多了，可以许愿，但是程序也没正常停止。 7.3 Semaphore（信号灯） Semaphore 的构造方法中传入的第一个参数是最大信号量（可以看成最大线程池），每个信号量初始化为一个最多只能分发一个许可证。使用 acquire 方法获得许可证（消耗完阻塞，看作调用一次许可证-1），release 方法释放许可（看作调用一次许可证+1）。 1234567891011121314151617181920public class SemaphoreDemo &#123; public static void main(String[] args) &#123; Semaphore semaphore = new Semaphore(3); for (int i = 0; i &lt; 6; i++) &#123; new Thread(()-&gt;&#123; try &#123; semaphore.acquire();//抢占 System.out.println(Thread.currentThread().getName() + &quot;抢到了车位&quot;); TimeUnit.SECONDS.sleep(new Random().nextInt(5));//设置停车时间，模拟线程占用 System.out.println(Thread.currentThread().getName() + &quot;离开了车位&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; semaphore.release();//释放车位 &#125; &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 8. 阻塞队列（Blocking Queue） 8.1 概述 阻塞队列，顾名思义，首先它是一个队列，通过一个共享的队列，可以使得数据由队列的一端输入，从另外一端输出。 当队列是空的，从队列中获取元素的操作将会被阻塞。 当队列是满的，从队列中添加元素的操作将会被阻塞。 试图从空的队列中获取元素的线程将会被阻塞，直到其他线程往空的队列插入新的元素。 试图向已满的队列中添加新元素的线程将会被阻塞，直到其他线程从队列中移除一个或多个元素或者完全清空，使队列变得空闲起来并后续新增。 分类： 先进先出（FIFO）：先插入的队列的元素也最先出队列，类似于排队的功能。从某种程度上来说这种队列也体现了一种公平性。 后进先出（LIFO）：后插入队列的元素最先出队列，这种队列优先处理最近发生的事件(栈) 。 在多线程领域：所谓阻塞，在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤起。 8.2 Blocking Queue BlockingQueue 是 Concurrent 包中的接口，用于解决多线程高效安全“传输” 数据的问题。 生产者、消费者模型中： 当队列中没有数据的情况下，消费者端的所有线程都会被自动阻塞（挂起），直到有数据放入队列。 当队列中填满数据的情况下，生产者端的所有线程都会被自动阻塞（挂起），直到队列中有空的位置，线程被自动唤醒。 实现类： 核心方法： 8.3 常见 Blocking Queue 8.3.1 ArrayBlockingQueue 由数组结构组成的有界阻塞队列 基于数组的阻塞队列实现，在 ArrayBlockingQueue 内部，维护了一个定长数组，以便缓存队列中的数据对象，这是一个常用的阻塞队列，除了一个定长数组外，ArrayBlockingQueue 内部还保存着两个整形变量，分别标识着队列的头部和尾部在数组中的位置。 ArrayBlockingQueue 在生产者放入数据和消费者获取数据，都是共用同一个锁对象，由此也意味着两者无法真正并行运行，这点尤其不同于 LinkedBlockingQueue；按照实现原理来分析， ArrayBlockingQueue 完全可以采用分离锁，从而实现生产者和消费者操作的完全并行运行。 Doug Lea 之所以没这样去做，也许是因为 ArrayBlockingQueue 的数据写入和获取操作已经足够轻巧，以至于引入独立的锁机制，除了给代码带来额外的复杂性外，其在性能上完全占不到任何便宜。 ArrayBlockingQueue 和 LinkedBlockingQueue 间还有一个明显的不同之处在于，前者在插入或删除元素时不会产生或销毁任何额外的对象实例，而后者则会生成一个额外的 Node 对象。这在长时间内需要高效并发地处理大批量数据的系统中，其对于 GC 的影响还是存在一定的区别。而在创建 ArrayBlockingQueue 时，我们还可以控制对象的内部锁是否采用公平锁，默认采用非公平锁。 8.3.2 LinkedBlockingQueue 由链表结构组成的有界（但大小默认值为 Integer.MAX_VALUE）阻塞队列 基于链表的阻塞队列，同 ArrayListBlockingQueue 类似，其内部也维持着一个数据缓冲队列（该队列由一个链表构成），当生产者往队列中放入一个数据时，队列会从生产者手中获取数据，并缓存在队列内部，而生产者立即返回；只有当队列缓冲区达到最大值缓存容量时（LinkedBlockingQueue 可以通过构造函数指定该值），才会阻塞生产者队列，直到消费者从队列中消费掉一份数据，生产者线程会被唤醒，反之对于消费者这端的处理也基于同样的原理。 而 LinkedBlockingQueue 之所以能够高效的处理并发数据，还因为其对于生产者端和消费者端分别采用了独立的锁来控制数据同步，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。 8.3.3 DelayQueue 使用优先级队列实现的延迟无界阻塞队列。 DelayQueue 中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。 DelayQueue 是一个没有大小限制的队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。 8.3.4 PriorityBlockingQueue 支持优先级排序的无界阻塞队列。 基于优先级的阻塞队列（优先级的判断通过构造函数传入的 Compator 对象来决定），但需要注意的是 PriorityBlockingQueue 并不会阻塞数据生产者，而只会在没有可消费的数据时，阻塞数据的消费者。 因此使用的时候要特别注意， 生产者生产数据的速度绝对不能快于消费者消费数据的速度，否则时间一长，会最终耗尽所有的可用堆内存空间。 在实现 PriorityBlockingQueue 时，内部控制线程同步的锁采用的是公平锁。 8.3.5 SynchronousQueue 不存储元素的阻塞队列，也即单个元素的队列。 一种无缓冲的等待队列，类似于无中介的直接交易，有点像原始社会中的生产者和消费者，生产者拿着产品去集市销售给产品的最终消费者，而消费者必须亲自去集市找到所要商品的直接生产者，如果一方没有找到合适的目标，那么对不起，大家都在集市等待。 相对于有缓冲的 BlockingQueue 来说，少了一个中间经销商的环节（缓冲区），如果有经销商，生产者直接把产品批发给经销商，而无需在意经销商最终会将这些产品卖给那些消费者，由于经销商可以库存一部分商品，因此相对于直接交易模式，总体来说采用中间经销商的模式会吞吐量高一些（可以批量买卖）；但另一方面，又因为经销商的引入，使得产品从生产者到消费者中间增加了额外的交易环节，单个产品的及时响应性能可能会降低。 声明一个 SynchronousQueue 有两种不同的方式，公平模式和非公平模式： 公平模式：SynchronousQueue 会采用公平锁，并配合一个 FIFO 队列来阻塞 多余的生产者和消费者，从而体系整体的公平策略。 非公平模式（SynchronousQueue 默认）：SynchronousQueue 采用非公平锁，同时配合一个 LIFO 队列来管理多余的生产者和消费者，而后一种模式，如果生产者和消费者的处理速度有差距，则很容易出现饥渴的情况，即可能有某些生产者或者是消费者的数据永远都得不到处理。 8.3.6 LinkedTransferQueue 由链表组成的无界阻塞队列。 LinkedTransferQueue 是一个由链表结构组成的无界阻塞 TransferQueue 队列。相对于其他阻塞队列，LinkedTransferQueue 多了 tryTransfer 和 transfer 方法。 LinkedTransferQueue 采用一种预占模式。意思就是消费者线程取元素时，如果队列不为空，则直接取走数据，若队列为空，那就生成一个节点（节点元素为 null）入队，然后消费者线程被等待在这个节点上，后面生产者线程入队时发现有一个元素为 null 的节点，生产者线程就不入队了，直接就将元素填充到 该节点，并唤醒该节点等待的线程，被唤醒的消费者线程取走元素，从调用的方法返回。 8.3.7 LinkedBlockingDeque 由链表组成的双向阻塞队列 LinkedBlockingDeque 是一个由链表结构组成的双向阻塞队列， 即可以从队列的两端插入和移除元素。 对于一些指定的操作，在插入或者获取队列元素时如果队列状态不允许该操作可能会阻塞住该线程直到队列状态变更为允许操作，这里的阻塞一般有两种情况： 插入元素时: 如果当前队列已满将会进入阻塞状态，一直等到队列有空的位置时再将该元素插入，该操作可以通过设置超时参数，超时后返回 false 表示操作失败，也可以不设置超时参数一直阻塞，中断后抛出 InterruptedException 异常。 读取元素时: 如果当前队列为空会阻塞住直到队列不为空然后返回元素，同样可以通过设置超时参数 。 9. 线程池 9.1 概述 线程池（thread pool）：一种线程使用模式。线程过多会带来调度开销，进而影响缓存局部性和整体性能。而线程池维护着多个线程，等待着监督管理者分配可并发执行的任务。这避免了在处理短时间任务时创建与销毁线程的代价。线程池不仅能够保证内核的充分利用，还能防止过分调度。 优势：线程池做的工作只要是控制运行的线程数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量，超出数量的线程排队等候，等其他线程执行完毕，再从队列中取出任务来执行。 它的主要特点为： 降低资源消耗: 通过重复利用已创建的线程降低线程创建和销毁造成的销耗。 提高响应速度: 当任务到达时，任务可以不需要等待线程创建就能立即执行。 提高线程的可管理性: 线程是稀缺资源，如果无限制的创建，不仅会销耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控，并提高系统稳定性。 Java 中的线程池是通过 Executor 框架实现的，该框架中用到了 Executor， Executors（工具类），ExecutorService，ThreadPoolExecutor 这几个类 。 常用方法来自于 ExecutorService 接口： 9.2 创建方式 9.2.1 Executors.newCachedThreadPool 作用：创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。（一池可扩容线程） 特点： core 线程数为 0 线程池中数量没有固定，可达到最大值（Interger. MAX_VALUE） 线程池中的线程可进行缓存重复利用和回收（回收默认时间为 1 分钟） 当线程池中，没有可用线程，会重新创建一个线程。 场景: 适用于创建一个可无限扩大的线程池，服务器负载压力较轻，执行时间较短，任务多的场景。 9.2.2 Executors.newFixedThreadPool 作用：创建一个可重用固定线程数的线程池，以共享的无界队列方式来运行这些线程。在任意点，在大多数线程会处于处理任务的活动状态。如果在所有线程处于活动状态时提交附加任务，则在有可用线程之前，附加任务将在队列中等待。如果在关闭前的执行期间由于失败而导致任何线程终止，那么一个新线程将代替它执行后续的任务（如果需要）。在某个线程被显式地关闭之前，池中的线程将一直存在。 （一池 N 线程） 特征： 线程池中的线程处于一定的量，可以很好的控制线程的并发量 线程可以重复被使用，在显示关闭之前，都将一直存在 超出一定量的线程被提交时候需在队列中等待 场景: 适用于可以预测线程数量的业务中，或者服务器负载较重，对线程数有严格限制的场景。 9.2.3 Executors.newSingleThreadExecutor 作用：创建一个使用单个 worker 线程的 Executor，以无界队列方式来运行该线程。（注意，如果因为在关闭前的执行期间出现失败而终止了此单个线程，那么如果需要，一个新线程将代替它执行后续的任务）。可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。与其他等效的 newFixedThreadPool 不同，可保证无需重新配置此方法所返回的执行程序即可使用其他的线程。（一池一线程） 特征：线程池中最多执行 1 个线程，之后提交的线程活动将会排在队列中以此执行。 场景: 适用于需要保证顺序执行各个任务，并且在任意时间点，不会同时有多个线程的场景。 9.2.4 Executors.newScheduleThreadPool 作用: 线程池支持定时以及周期性执行任务，创建一个 corePoolSize 为传入参数，最大线程数为整形的最大数的线程池。（定时线程） 特征： 线程池中具有指定数量的线程，即便是空线程也将保留。 可定时或者延迟执行线程活动。 场景: 适用于需要多个后台线程执行周期任务的场景。 9.2.5 Executors.newWorkStealingPool jdk1.8 提供的线程池，底层使用的是 ForkJoinPool 实现，创建一个拥有多个任务队列的线程池，可以减少连接数，创建当前可用 cpu 核数的线程来并行执行任务。（根据 CPU 核数创建线程） 场景: 适用于大耗时，可并行执行的场景。 9.3 底层原理 线程池底层是创建了 ThreadPoolExecutor 对象： 参数说明： corePoolSize：常驻线程数量，创建好以后就准备就绪（调用 start()）。核心线程数一直存在，除非设置 allowCoreThreadTimeOut。 maximumPoolSize：能容纳的最大线程数。控制资源。 keepAliveTime：空闲线程存活时间。释放的是（maximumPoolSize - corePoolSize）的线程。 unit：存活的时间单位。 workQueue：存放提交但未执行任务的队列。用来存储等待执行的任务， 如果当前对线程的需求超过了 corePoolSize 大小， 就会放在这里等待空闲线程执行。 threadFactory：创建线程的工厂类。 handler：等待队列满后的拒绝策略 。 123456789101112131415161718192021222324* @param corePoolSize the number of threads to keep in the pool, even* if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set池中一直保持的线程的数量， 即使线程空闲。 除非设置了 allowCoreThreadTimeOut* @param maximumPoolSize the maximum number of threads to allow in the* pool池中允许的最大的线程数* @param keepAliveTime when the number of threads is greater than* the core, this is the maximum time that excess idle threads* will wait for new tasks before terminating.当线程数大于核心线程数的时候， 线程在最大多长时间没有接到新任务就会终止释放，最终线程池维持在 corePoolSize 大小* @param unit the time unit for the &#123;@code keepAliveTime&#125; argument时间单位* @param workQueue the queue to use for holding tasks before they are* executed. This queue will hold only the &#123;@code Runnable&#125;* tasks submitted by the &#123;@code execute&#125; method.阻塞队列， 用来存储等待执行的任务， 如果当前对线程的需求超过了 corePoolSize大小， 就会放在这里等待空闲线程执行。* @param threadFactory the factory to use when the executor* creates a new thread创建线程的工厂， 比如指定线程名等* @param handler the handler to use when execution is blocked* because the thread bounds and queue capacities are reached拒绝策略， 如果线程满了， 线程池就会使用拒绝策略。 在创建了线程池后，线程池中的线程数为零。 当调用 execute()方法添加一个请求任务时，线程池会做出如下判断： 如果正在运行的线程数量小于 corePoolSize，那么马上创建 corePoolSize 数量的线程运行这个任务。 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列。 如果这个时候队列满了且正在运行的线程数量还小于 maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务。队列中的继续等待。 如果队列满了且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会启动饱和拒绝策略来执行。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做超过一定的时间（keepAliveTime）时，线程会判断： 如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。 所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。 9.3.1 面试题 问：一个线程池 core 7； max 20 ， queue： 50， 100 并发进来怎么分配的？ 答：先有 7 个能直接得到执行， 接下来 50 个进入队列排队，再多开 13 个继续执行。 现在 70 个被安排上了。 剩下 30 个默认拒绝策略。 9.4 拒绝策略 AbortPolicy：丢弃任务，并抛出拒绝执行 RejectedExecutionException 异常信息。线程池默认的拒绝策略。必须处理好抛出的异常，否则会打断当前的执行流程，影响后续的任务执行。 CallerRunsPolicy：当触发拒绝策略，只要线程池没有关闭的话，则使用调用线程直接运行任务。一般并发比较小，性能要求不高，不允许失败。但是，由于调用者自己运行任务，如果任务提交速度过快，可能导致程序阻塞，性能效率上必然的损失较大。 DiscardOldestPolicy: 当触发拒绝策略，只要线程池没有关闭的话，丢弃阻塞队列 workQueue 中最老的一个任务，并将新任务加入。 DiscardPolicy：直接丢弃，其他啥都没有。 9.5 自定义线程池 项目中创建多线程时，使用常见的三种线程池（单一、可变、定长）都有一定问题，原因是 FixedThreadPool 和 SingleThreadExecutor 底层都是用 LinkedBlockingQueue 实现的，这个队列最大长度为 Integer.MAX_VALUE，容易导致 OOM。 1234567891011public class MyThreadPool &#123; public static void main(String[] args) &#123; ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor( 2, 5, 2L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); &#125;&#125; 10. 分支/合并（Fork/Join）框架 10.1 概述 10.1.1 介绍 Fork/Join 可以将一个大的任务拆分成多个子任务进行并行处理，最后将子任务结果合并成最后的计算结果，并进行输出。 Fork/Join 框架要完成两件事情： 任务分割：首先 Fork/Join 框架需要把大的任务分割成足够小的子任务，如果子任务比较大的话还要对子任务进行继续分割。 执行任务并合并结果：分割的子任务分别放到双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都放在另外一个队列里，启动一个线程从队列里取数据，然后合并这些数据。 10.1.2 创建 在 Java 的 Fork/Join 框架中，使用两个类完成上述操作： ForkJoinTask：创建一个 Fork/Join 任务。该类提供了在任务中执行 fork 和 join 的机制。通常情况下不需要直接继承 ForkJoinTask 类，只需要继承它的子类： RecursiveAction：用于没有返回结果的任务。 RecursiveTask：用于有返回结果的任务。继承后可以实现递归(自己调自己)调用的任务。 ForkJoinPool：ForkJoinTask 需要通过 ForkJoinPool 来执行。 10.1.3 Fork/Join 框架的实现原理： ForkJoinPool 由 ForkJoinTask 数组和 ForkJoinWorkerThread 数组组成，ForkJoinTask 数组负责将存放以及将程序提交给 ForkJoinPool，而 ForkJoinWorkerThread 负责执行这些任务。 10.1.4 异常 ForkJoinTask 在执行的时候可能会抛出异常，但是没办法在主线程里直接捕获异常，所以 ForkJoinTask 提供了 isCompletedAbnormally()方法来检查任务是否已经抛出异常或已经被取消了，并且可以通过 ForkJoinTask 的 getException()方法获取异常。 getException 方法返回 Throwable 对象，如果任务被取消了则返回 CancellationException。如果任务没有完成或者没有抛出异常则返回 null。 10.2 fork()方法 一般使用 ForkJoinPool 和 RecursiveTask 对象的 fork()实现拆分过程。 实现原理：当调用 ForkJoinTask 的 fork 方法时，程序会把任务放在 ForkJoinWorkerThread 的 workQueue 中，异步地执行这个任务，然后立即返回结果。 fork()源码： 12345678public final ForkJoinTask&lt;V&gt; fork() &#123; Thread t; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ((ForkJoinWorkerThread)t).workQueue.push(this); else ForkJoinPool.common.externalPush(this); return this;&#125; push()方法把当前任务存放在 ForkJoinTask 数组队列里。然后再调用 ForkJoinPool 的 signalWork()方法唤醒或创建一个工作线程来执行任务。push()源码如下： 123456789101112131415final void push(ForkJoinTask&lt;?&gt; task) &#123; ForkJoinTask&lt;?&gt;[] a; ForkJoinPool p; int b = base, s = top, n; if ((a = array) != null) &#123; // ignore if queue removed int m = a.length - 1; // fenced write for task visibility U.putOrderedObject(a, ((m &amp; s) &lt;&lt; ASHIFT) + ABASE, task); U.putOrderedInt(this, QTOP, s + 1); if ((n = s - b) &lt;= 1) &#123; if ((p = pool) != null) p.signalWork(p.workQueues, this); &#125; else if (n &gt;= m) growArray(); &#125;&#125; 10.3 join()方法 join()方法的主要作用是阻塞当前线程并等待获取结果。 源码： 123456public final V join() &#123; int s; if ((s = doJoin() &amp; DONE_MASK) != NORMAL) reportException(s); return getRawResult();&#125; join()方法首先调用 doJoin()方法，通过 doJoin()方法得到当前任务的状态来判断返回什么结果： 已完成（NORMAL）：直接返回任务结果, 被取消（CANCELLED）：抛出 CancellationException。 信号（SIGNAL）： 出现异常（EXCEPTIONAL） ：直接抛出对应的异常。 123456789private int doJoin() &#123; int s; Thread t; ForkJoinWorkerThread wt; ForkJoinPool.WorkQueue w; return (s = status) &lt; 0 ? s : ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ? (w = (wt = (ForkJoinWorkerThread)t).workQueue). tryUnpush(this) &amp;&amp; (s = doExec()) &lt; 0 ? s : wt.pool.awaitJoin(w, this, 0L) : externalAwaitDone();&#125; 10.4 案例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 计算 1+2+3.........+1000,==每 100 个数切分一个子任务 * 二分法拆分 */public class ForkJoinDemo&#123; public static void main(String[] args) &#123; MyTask myTask = new MyTask(0, 1000); ForkJoinPool forkJoinPool = new ForkJoinPool(); ForkJoinTask&lt;Integer&gt; forkJoinTask = forkJoinPool.submit(myTask); try &#123; Integer res = forkJoinTask.get(); System.out.println(res); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125;finally &#123; forkJoinPool.shutdown(); &#125; &#125;&#125;class MyTask extends RecursiveTask&lt;Integer&gt; &#123; private final static Integer VALUE = 10;//拆分差值不能超过10 private int begin; private int end; private int result; public MyTask(int begin, int end) &#123; this.begin = begin; this.end = end; &#125; @Override protected Integer compute() &#123; if(end - begin &lt;= VALUE)&#123; for (int i = begin; i &lt;= end; i++) &#123; result += i; &#125; &#125;else &#123; int middle = (begin + end)/2; MyTask taskExample1 = new MyTask(begin, middle);//拆分左边 MyTask taskExample2 = new MyTask(middle + 1, end);//拆分右边 taskExample1.fork(); taskExample2.fork(); result = taskExample1.join() + taskExample2.join();//合并结果 &#125; return result; &#125;&#125; 11. CompletableFuture 11.1 再谈 FutureTask Futrue 接口定义了操作异步任务的一些方法，如获取异步任务的执行结果，取消任务、判断任务是否被取消、判断任务是否执行完毕等。 FutureTask 在 Java 里面通常用来表示一个异步任务的引用，比如将任务提交到线程池里面，就会得到一个 Futrue。FutureTask 对象的 isDone 方法判断任务是否处理结束，get 方法可以一直阻塞直到任务结束然后获取结果。但整体来说这种方式，还是同步的，因为需要客户端不断阻塞等待或者不断轮询才能知道任务是否完成。（详见FutureTask 的优缺点） Future 的主要缺点如下： 不支持手动完成： 我提交了一个任务，但是执行太慢了，我通过其他路径已经获取到了任务结果，现在没法把这个任务结果通知到正在执行的线程，所以必须主动取消或者一直等待它执行完成。 不支持进一步的非阻塞调用： 通过 Future 的 get 方法会一直阻塞到任务完成，但是想在获取任务之后执行额外的任务，因为 Future 不支持回调函数，所以无法实现这个功能。 不支持链式调用： 对于 Future 的执行结果，我们想继续传到下一个 Future 处理使用，从而形成一个链式的 pipline 调用，这在 Future 中是没法实现的。 不支持多个 Future 合并： 比如我们有 10 个 Future 并行执行，我们想在所有的 Future 运行完毕之后，执行某些函数，是没法通过 Future 实现的。 不支持异常处理： Future 的 API 没有任何的异常处理的 api，所以在异步运行时，如果出了问题 是不好定位的。 11.2 CompletableFuture 概述 由于 FutureTask 阻塞的方式与异步编程的设计理念违背，轮询又会消耗资源，所以 jdk8 设计了 CompletableFuture，提供了一种类似观察者模式的机制，可以让任务再执行完成后通知监听的一方。 CompletableFuture 实现了 Future、 CompletionStage 接口。实现了 Future 接口就可以兼容现在有线程池框架，而 CompletionStage 接口才是异步编程的接口抽象，里面定义多种异步方法，通过这两者集合，从而打造出了强大的 CompletableFuture 类。 CompletionStage 代表异步计算过程中的某一个阶段，一个阶段完成后就可能触发另外一个阶段。 CompletableFuture 的优点： 异步任务结束后，会自动回调某个对象的方法。 主线程设置好回调后，不再关心异步任务的执行，异步任务之间可以顺序执行。 异步任务出错时，会自动回调某个对象的方法。 11.3 创建异步对象 CompletableFuture 提供了四个静态方法来创建一个异步操作。 11.3.1 没有返回值的异步任务 runAsync(Runnable runnable)：默认使用 ForkJoinPool.commonPool()创建的线程池 123456789101112131415161718192021/** * 没有返回值的异步任务 */public class Test02 &#123; public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); //运行一个没有返回值的异步任务 CompletableFuture&lt;Void&gt; future = CompletableFuture.runAsync(() -&gt; &#123; try &#123; System.out.println(&quot;子线程启动干活&quot;); Thread.sleep(5000); System.out.println(&quot;子线程完成&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); //主线程阻塞 future.get(); System.out.println(&quot;主线程结束&quot;); &#125;&#125; runAsync(Runnable runnable, Executor executor)：指定线程池 11.3.2 有返回值的异步任务 supplyAsync(Supplier&lt;U&gt; supplier)：默认使用 ForkJoinPool.commonPool()创建的线程池 123456789101112131415161718192021/** * 有返回值的异步任务 */public class Test03 &#123; public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); //运行一个有返回值的异步任务 CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; try &#123; System.out.println(&quot;子线程开始任务&quot;); Thread.sleep(5000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return &quot;子线程完成了!&quot;; &#125;); //主线程阻塞 String s = future.get(); System.out.println(&quot;主线程结束, 子线程的结果为:&quot; + s); &#125;&#125; supplyAsync(Supplier&lt;U&gt; supplier, Executor executor)：【开发使用这个】 11.3.3 异步任务对象常用方法 11.4 链式调用方法 Lambda 表达式参数编程经验： 11.4.1 whenComplete()和 whenCompleteAsync() whenComplete 和 whenCompleteAsync 表示异步对象计算完成时的回调，二者的区别是： whenComplete： 执行当前任务的线程执行继续执行 whenComplete 的任务。 whenCompleteAsync： 把 whenCompleteAsync 这个任务继续提交给线程池来进行执行。 方法不以 Async 结尾， 意味着 Action 使用相同的线程执行， 而 Async 可能会使用其他线程执行（如果是使用相同的线程池， 也可能会被同一个线程选中执行） 特点：正常完成返回结果，发生异常只能感知。 whenComplete(v,e)：结果是 v，异常是 e，发生异常时不能提供返回结果，只能感知有无异常。 123456789101112131415161718192021public class Test11 &#123; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; try &#123; System.out.println(&quot;子线程开始任务&quot;); Thread.sleep(5000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return &quot;子线程完成了!&quot;; &#125;).whenComplete((v, e) -&gt; &#123; if(e==null)&#123; System.out.println(&quot;子线程执行完没有发生异常。&quot;); &#125; &#125;); //主线程阻塞 String s = future.get(); System.out.println(&quot;主线程结束, 子线程的结果为:&quot; + s); &#125;&#125; 11.4.2 异常处理 11.4.2.1 exceptionally 异常处理 出现异常时触发。有返回结果。 特点：正常完成返回结果，发生异常返回指定结果（有异常才进入 exceptionally()方法）。 12345678910111213141516171819/** * 异常处理 */public class Test06 &#123; private static Integer num = 10; public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; int i= 1/0; System.out.println(&quot;加 10 任务开始&quot;); num += 10; return num; &#125;).exceptionally(ex -&gt; &#123; System.out.println(ex.getMessage()); return -1; &#125;); System.out.println(future.get()); &#125;&#125; 11.4.2.2 handle 异常处理 handle 类似于 thenAccept/thenRun 方法，是最后一步的处理调用，但是同时可以处理异常。 特点：正常完成返回结果，发生异常返回指定结果（有无异常均进入 handle()方法）。 1234567891011121314151617181920212223/** * 异常处理2 */public class Test07 &#123; private static Integer num = 10; public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;加 10 任务开始&quot;); num += 10; return num; &#125;).handle((i,ex) -&gt;&#123; System.out.println(&quot;进入 handle 方法&quot;); if(ex != null)&#123; System.out.println(&quot;发生了异常,内容为:&quot; + ex.getMessage()); return -1; &#125;else&#123; System.out.println(&quot;正常完成,内容为: &quot; + i); return i; &#125;&#125;); System.out.println(future.get()); &#125;&#125; 11.4.3 线程串行化 都要前置任务成功完成。 11.4.3.1 线程依赖——thenApply() 当一个线程依赖另一个线程时， 获取上一个任务返回的结果， 并返回当前任务的返回值。 特点：接受前一个任务的返回结果，同时自身处理结果还要返回。 12345678910111213141516171819202122/** * 依赖线程串行化：先对一个数加 10,然后取平方 */public class Test04 &#123; private static Integer num = 10; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; try &#123; System.out.println(&quot;加 10 任务开始&quot;); num += 10; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return num; &#125;).thenApply(integer -&gt; &#123; return num * num; &#125;); Integer integer = future.get(); System.out.println(&quot;主线程结束, 子线程的结果为:&quot; + integer); &#125;&#125; 11.4.3.2 消费处理结果——thenAccept thenAccept 消费处理结果, 接收任务的处理结果，并消费处理，无返回结果。 特点：接受前一个任务的返回结果，但自身无返回结果。 12345678910111213141516171819202122232425/** * 消费处理结果 */public class Test05 &#123; private static Integer num = 10; public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); CompletableFuture.supplyAsync(() -&gt; &#123; try &#123; System.out.println(&quot;加 10 任务开始&quot;); num += 10; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return num; &#125;).thenApply(integer -&gt; &#123; return num * num; &#125;).thenAccept(new Consumer&lt;Integer&gt;() &#123; @Override public void accept(Integer integer) &#123; System.out.println(&quot;子线程全部处理完成,最后调用了 accept,结果为:&quot; + integer); &#125; &#125;); &#125;&#125; 11.4.3.3 thenRun 只要上面的任务执行完成， 就开始执行 thenRun——继续执行一个新线程，新线程与前一个线程共用同一个线程。thenRunAsync 表示新开的线程异步执行。 特点：不接受前一个任务的返回结果。 多个 thenRun 链式调用和多个 thenRunAsync 链式调用的异同： 相同： 没有传入自定义线程池，默认都是 ForkJoinPool 不同： 第一个任务传入了自定义线程池，则 thenRun 的后续任务都使用该线程池 第一个任务传入了自定义线程池，则 thenRunAsync 只有第一个任务使用指定线程池，后续任务都是 ForkJoinPool 特殊： 如果分支线程执行太快，根据系统优化切换原则，后续线程有可能直接使用 main 线程处理。 11.4.4 结果合并 11.4.5.1 两任务组合——两个任务都要完成 两个任务必须都完成， 触发该任务。 thenCombine： 组合两个 future， 获取两个 future 的返回结果， 并返回当前任务的返回值。 thenAcceptBoth： 组合两个 future， 获取两个 future 任务的返回结果， 然后处理任务， 没有返回值。 runAfterBoth： 组合两个 future， 不需要获取 future 的结果， 只需两个 future 处理完任务后，处理该任务，无返回值。 thenCompose：组合两个 future， 获取两个 future（二者的返回结果有依赖关系） 的返回结果， 并返回当前任务的返回值。 thenCompose：合并两个有依赖关系的 CompletableFuture 的执行结果。 1234567891011121314151617181920212223/** * 结果合并1 */public class Test08 &#123; private static Integer num = 10; public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); //第一步加 10 CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;加 10 任务开始&quot;); num += 10; return num; &#125;); //合并 CompletableFuture&lt;Integer&gt; future1 = future.thenCompose(i -&gt; //再来一个 CompletableFuture CompletableFuture.supplyAsync(() -&gt; &#123; return i + 1; &#125;)); System.out.println(future.get()); System.out.println(future1.get()); &#125;&#125; thenCombine：合并两个没有依赖关系的 CompletableFutures 任务 1234567891011121314151617181920212223242526272829/** * 结果合并2 */public class Test09 &#123; private static Integer num = 10; public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;加 10 任务开始&quot;); num += 10; return num; &#125;); CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; &#123;System.out.println(&quot;乘以 10 任务开始&quot;); num = num * 10; return num; &#125;); //合并两个结果 CompletableFuture&lt;Object&gt; future = job1.thenCombine(job2, new BiFunction&lt;Integer, Integer, List&lt;Integer&gt;&gt;() &#123; @Override public List&lt;Integer&gt; apply(Integer a, Integer b) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(a); list.add(b); return list; &#125; &#125;); System.out.println(&quot;合并结果为:&quot; + future.get()); &#125;&#125; 11.4.5.2 两任务组合 - 一个完成 当两个任务中， 任意一个 future 任务完成的时候， 执行任务。 applyToEither： 两个任务有一个执行完成， 获取它的返回值， 处理任务并有新的返回值。 acceptEither： 两个任务有一个执行完成， 获取它的返回值， 处理任务， 没有新的返回值。 runAfterEither： 两个任务有一个执行完成， 不需要获取 future 的结果， 处理任务， 也没有返回值。 applyToEither：选择并获取执行速度快的线程 12345678910111213141516public class CompletableFutureFastDemo&#123; public static void main(String[] args)&#123; CompletableFuture&lt;String&gt; playA = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;A come in&quot;); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;playA&quot;; &#125;); CompletableFuture&lt;String&gt; playB = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;B come in&quot;); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;playB&quot;; &#125;); CompletableFuture&lt;String&gt; result = playA.applyToEither(playB, f -&gt; f + &quot; is winer&quot;); System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;-----: &quot;+result.join()); &#125;&#125; 11.4.5.3 多任务组合 allOf： 一系列独立的 future 任务，等其所有的任务执行完后做一些事情 12345678910111213141516171819202122232425262728293031323334353637/** * 合并多个结果1 */public class Test10_1 &#123; private static Integer num = 10; public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); List&lt;CompletableFuture&gt; list = new ArrayList&lt;&gt;(); CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;加 10 任务开始&quot;); num += 10; return num; &#125;); list.add(job1); CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;乘以 10 任务开始&quot;);num = num * 10; return num; &#125;); list.add(job2); CompletableFuture&lt;Integer&gt; job3 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;减以 10 任务开始&quot;); num = num * 10; return num; &#125;); list.add(job3); CompletableFuture&lt;Integer&gt; job4 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;除以 10 任务开始&quot;); num = num * 10; return num; &#125;); list.add(job4); //多任务合并 List&lt;Integer&gt; collect = list.stream().map(CompletableFuture&lt;Integer&gt;::join).collect(Collectors.toList()); System.out.println(collect); &#125;&#125; anyOf：只要在多个 future 里面有一个返回，整个任务就可以结束，而不需要等到每一个 future 结束 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 多结果合并2 */public class Test10_2 &#123; private static Integer num = 10; public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt;[] futures = new CompletableFuture[4]; CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; &#123; try&#123; Thread.sleep(5000); System.out.println(&quot;加 10 任务开始&quot;);num += 10; return num; &#125;catch (Exception e)&#123; return 0; &#125; &#125;); futures[0] = job1; CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; &#123; try&#123; Thread.sleep(2000); System.out.println(&quot;乘以 10 任务开始&quot;); num = num * 10; return num; &#125;catch (Exception e)&#123; return 1; &#125; &#125;); futures[1] = job2; CompletableFuture&lt;Integer&gt; job3 = CompletableFuture.supplyAsync(() -&gt; &#123; try&#123; Thread.sleep(3000); System.out.println(&quot;减以 10 任务开始&quot;); num = num * 10; return num; &#125;catch (Exception e)&#123; return 2; &#125; &#125;); futures[2] = job3; CompletableFuture&lt;Integer&gt; job4 = CompletableFuture.supplyAsync(() -&gt; &#123; try&#123; Thread.sleep(4000); System.out.println(&quot;除以 10 任务开始&quot;);num = num * 10; return num; &#125;catch (Exception e)&#123; return 3; &#125; &#125;); futures[3] = job4; CompletableFuture&lt;Object&gt; future = CompletableFuture.anyOf(futures); System.out.println(future.get()); &#125;&#125; 11.4.5 手动终止线程 1234567891011121314151617181920212223/** * 主线程里面创建一个CompletableFuture，然后主线程调用get方法阻塞，最后在一个子线程中使其终止 */public class Test01 &#123; public static void main(String[] args) throws Exception&#123; CompletableFuture&lt;String&gt; future = new CompletableFuture&lt;&gt;(); new Thread(() -&gt; &#123; try&#123; System.out.println(Thread.currentThread().getName() + &quot;子线程开始干活&quot;); //子线程睡 5 秒 Thread.sleep(5000); //在子线程中完成主线程 future.complete(&quot;success&quot;); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;, &quot;A&quot;).start(); //主线程调用get方法阻塞 System.out.println(&quot;主线程调用 get 方法获取结果为: &quot; + future.get()); System.out.println(&quot;主线程完成,阻塞结束!!!!!!&quot;); &#125;&#125; 一般不建议通过 new 的方式创建 CompletableFuture 对象。 11.5 电商比价案例 11.5.1 需求描述 需求说明： 同一款产品，同时搜索出同款产品在各大电商平台的售价。 同一款产品，同时搜索出本产品在同一电商平台下不同卖家的售价。 输出返回： 返回同款产品在不同地方（平台或卖家）的价格清单列表——List 《mysql》 in jd price is 88.5 《mysql》 in dangdang price is 86.11 《mysql》 in taobao price is 90.43 技术要求： 函数式编程 链式编程 Strream 流式计算 解决方案： 按部就班：一个一个查 万箭齐发：同时查多个，进行结果合并 11.5.2 按部就班 123456789101112131415161718192021222324252627282930313233343536373839public class CompletableFutureMallDemo &#123; static List&lt;NetMall&gt; list = Arrays.asList( new NetMall(&quot;jd&quot;), new NetMall(&quot;dangdang&quot;), new NetMall(&quot;taobao&quot;), new NetMall(&quot;pdd&quot;), new NetMall(&quot;tmall&quot;) ); public static List&lt;String&gt; getPrice(List&lt;NetMall&gt; list,String productName) &#123; return list .stream() .map(netMall -&gt; String.format(productName + &quot; in %s price is %.2f&quot;, netMall.getNetMallName(), netMall.calcPrice(productName))) .collect(Collectors.toList()); &#125; public static void main(String[] args) &#123; long startTime = System.currentTimeMillis(); List&lt;String&gt; list1 = getPrice(list, &quot;mysql&quot;); for (String element : list1) &#123; System.out.println(element); &#125; long endTime = System.currentTimeMillis(); System.out.println(&quot;----costTime: &quot;+(endTime - startTime) +&quot; 毫秒&quot;); &#125;&#125;class NetMall&#123; @Getter private String netMallName; public NetMall(String netMallName) &#123; this.netMallName = netMallName; &#125; public double calcPrice(String productName) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;//模拟查询耗时，1s return ThreadLocalRandom.current().nextDouble() * 2 + productName.charAt(0);//模拟书的价格 &#125;&#125; 11.5.3 万箭齐发 123456789101112131415161718192021222324252627282930313233343536373839404142public class CompletableFutureMallDemo &#123; static List&lt;NetMall&gt; list = Arrays.asList( new NetMall(&quot;jd&quot;), new NetMall(&quot;dangdang&quot;), new NetMall(&quot;taobao&quot;), new NetMall(&quot;pdd&quot;), new NetMall(&quot;tmall&quot;) );//List&lt;NetMall&gt; -----&gt;List&lt;CompletableFuture&lt;String&gt;&gt;------&gt; List&lt;String&gt;public static List&lt;String&gt; getPriceByCompletableFuture(List&lt;NetMall&gt; list,String productName) &#123; return list.stream().map(netMall -&gt; CompletableFuture.supplyAsync(() -&gt; String.format(productName + &quot; in %s price is %.2f&quot;, netMall.getNetMallName(), netMall.calcPrice(productName)))) .collect(Collectors.toList()) .stream() .map(s -&gt; s.join()) .collect(Collectors.toList()); &#125; public static void main(String[] args) &#123; long startTime2 = System.currentTimeMillis(); List&lt;String&gt; list2 = getPriceByCompletableFuture(list, &quot;mysql&quot;); for (String element : list2) &#123; System.out.println(element); &#125; long endTime2 = System.currentTimeMillis(); System.out.println(&quot;----costTime: &quot;+(endTime2 - startTime2) +&quot; 毫秒&quot;); &#125;&#125;class NetMall&#123; @Getter private String netMallName; public NetMall(String netMallName) &#123; this.netMallName = netMallName; &#125; public double calcPrice(String productName) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;//模拟查询耗时，1s return ThreadLocalRandom.current().nextDouble() * 2 + productName.charAt(0);//模拟书的价格 &#125;&#125; 方法解析：","tags":[{"name":"Java","slug":"Java","permalink":"https://sk370.github.io/tags/Java/"},{"name":"JUC","slug":"JUC","permalink":"https://sk370.github.io/tags/JUC/"}]},{"title":"EhCache","date":"2022-10-02T03:39:52.000Z","path":"2022/10/02/ehcache/EhCache/","text":"EhCache 是一种广泛使用的开源 Java 分布式缓存。主要面向通用缓存,Java EE 和轻量级容器。可以和大部分 Java 项目无缝整合，例如： Hibernate 中的缓存就是基于 EhCache 实现的。 1 介绍 ehcache 可以保存登录信息到缓存里，防止高并发情况下数据库的访问压力，主要解决本地缓存的问题，分布式缓存共享不便。 EhCache 是一种广泛使用的开源 Java 分布式缓存。主要面向通用缓存,Java EE 和轻量级容器。可以和大部分 Java 项目无缝整合，例如： Hibernate 中的缓存就是基于 EhCache 实现的。 EhCache 支持内存和磁盘存储，默认存储在内存中，如内存不够时把缓存数据同步到磁盘中。 EhCache 支持基于 Filter 的 Cache 实现，也支持 Gzip 压缩算法。 优点：EhCache 直接在 JVM 虚拟机中缓存，速度快，效率高。 缺点：缓存共享麻烦，集群分布式应用使用不方便。 2 基本使用 2.1 引入依赖 12345678&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache&lt;/artifactId&gt; &lt;version&gt;2.6.11&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.2 配置 EhCache 在 resouces 目录下创建 ehcache.xml 文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ehcache&gt; &lt;!--磁盘的缓存位置--&gt; &lt;diskStore path=&quot;java.io.tmpdir/ehcache&quot;/&gt; &lt;!--默认缓存--&gt; &lt;defaultCache maxEntriesLocalHeap=&quot;10000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;120&quot; timeToLiveSeconds=&quot;120&quot; maxEntriesLocalDisk=&quot;10000000&quot; diskExpiryThreadIntervalSeconds=&quot;120&quot; memoryStoreEvictionPolicy=&quot;LRU&quot;&gt; &lt;persistence strategy=&quot;localTempSwap&quot;/&gt; &lt;/defaultCache&gt; &lt;!--helloworld缓存--&gt; &lt;cache name=&quot;HelloWorldCache&quot; maxElementsInMemory=&quot;1000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;5&quot; timeToLiveSeconds=&quot;5&quot; overflowToDisk=&quot;false&quot; memoryStoreEvictionPolicy=&quot;LRU&quot;/&gt; &lt;!-- defaultCache：默认缓存策略，当ehcache找不到定义的缓存时，则使用这个缓存策略。只能定义一个。 --&gt; &lt;!-- name:缓存名称。 maxElementsInMemory:缓存最大数目 maxElementsOnDisk：硬盘最大缓存个数。 eternal:对象是否永久有效，一但设置了，timeout将不起作用。 overflowToDisk:是否保存到磁盘，当系统宕机时 timeToIdleSeconds:设置对象在失效前的允许闲置时间（单位：秒）。仅当eternal=false对象不是永久有效时使用，可选属性，默认值是0，也就是可闲置时间无穷大。 timeToLiveSeconds:设置对象在失效前允许存活时间（单位：秒）。最大时间介于创建时间和失效时间之间。仅当eternal=false对象不是永久有效时使用，默认是0.，也就是对象存活时间无穷大。 diskPersistent：是否缓存虚拟机重启期数据 Whether the disk store persists between restarts of the Virtual Machine. The default value is false. diskSpoolBufferSizeMB：这个参数设置DiskStore（磁盘缓存）的缓存区大小。默认是30MB。每个Cache都应该有自己的一个缓冲区。 diskExpiryThreadIntervalSeconds：磁盘失效线程运行时间间隔，默认是120秒。 memoryStoreEvictionPolicy：当达到maxElementsInMemory限制时，Ehcache将会根据指定的策略去清理内存。默认策略是LRU（最近最少使用）。你可以设置为FIFO（先进先出）或是LFU（较少使用）。 clearOnFlush：内存数量最大时是否清除。 memoryStoreEvictionPolicy:可选策略有：LRU（最近最少使用，默认策略）、FIFO（先进先出）、LFU（最少访问次数）。 FIFO，first in first out，这个是大家最熟的，先进先出。 LFU， Less Frequently Used，就是上面例子中使用的策略，直白一点就是讲一直以来最少被使用的。如上面所讲，缓存的元素有一个hit属性，hit值最小的将会被清出缓存。 LRU，Least Recently Used，最近最少使用的，缓存的元素有一个时间戳，当缓存容量满了，而又需要腾出地方来缓存新的元素的时候，那么现有缓存元素中时间戳离当前时间最远的元素将被清出缓存。 --&gt;&lt;/ehcache&gt; 2.3 测试基本使用 1234567891011121314151617public class TestEH &#123; public static void main(String[] args) &#123; //获取编译目录下的资源的流对象 InputStream input = TestEH.class.getClassLoader().getResourceAsStream(&quot;ehcache.xml&quot;); //获取 EhCache 的缓存管理对象 CacheManager cacheManager = new CacheManager(input); //获取缓存对象 Cache cache = cacheManager.getCache(&quot;HelloWorldCache&quot;); //创建缓存数据 Element element = new Element(&quot;name&quot;,&quot;zhang3&quot;); //存入缓存 cache.put(element); //从缓存中取出 Element element1 = cache.get(&quot;name&quot;); System.out.println(element1.getObjectValue()); &#125;&#125; 3 整合 Shiro 3.1 引入依赖 1234567891011&lt;!--Shiro整合EhCache--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-ehcache&lt;/artifactId&gt; &lt;version&gt;1.4.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt;&lt;/dependency&gt; 3.2 配置 EhCache 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ehcache name=&quot;ehcache&quot; updateCheck=&quot;false&quot;&gt; &lt;!--磁盘的缓存位置--&gt; &lt;diskStore path=&quot;java.io.tmpdir&quot;/&gt; &lt;!--默认缓存--&gt; &lt;defaultCache maxEntriesLocalHeap=&quot;1000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;3600&quot; timeToLiveSeconds=&quot;3600&quot; overflowToDisk=&quot;false&quot;&gt; &lt;/defaultCache&gt; &lt;!--登录认证信息缓存：缓存用户角色权限--&gt; &lt;cache name=&quot;loginRolePsCache&quot; maxEntriesLocalHeap=&quot;2000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;600&quot; timeToLiveSeconds=&quot;0&quot; overflowToDisk=&quot;false&quot; statistics=&quot;true&quot;/&gt;&lt;/ehcache&gt; 3.3 将要保存的信息添加到缓存管理器 参照01.Shiro第 4 节的内容，创建 springboot 和 shiro 的整合环境，在此基础上，对01.Shiro的内容进行扩展。在配置类中添加代码，获取缓存管理器： 1234567891011121314151617181920212223242526272829303132333435363738@Configurationpublic class ShiroConfig &#123; //配置 SecurityManager @Bean public DefaultWebSecurityManager defaultWebSecurityManager() &#123; //1 创建 defaultWebSecurityManager 对象 DefaultWebSecurityManager defaultWebSecurityManager = new DefaultWebSecurityManager(); //2 创建加密对象，并设置相关属性 HashedCredentialsMatcher matcher = new HashedCredentialsMatcher(); //2.1 采用 md5 加密 matcher.setHashAlgorithmName(&quot;md5&quot;); //2.2 迭代加密次数 matcher.setHashIterations(3); //3 将加密对象存储到 myRealm 中 myRealm.setCredentialsMatcher(matcher); //4 将 myRealm 存入 defaultWebSecurityManager 对象 defaultWebSecurityManager.setRealm(myRealm); //4.1 设置 rememberMe defaultWebSecurityManager.setRememberMeManager(rememberMeManager()); // 4.6设置缓存管理器 defaultWebSecurityManager.setCacheManager(getEhCacheManager()); //5 返回 return defaultWebSecurityManager; &#125; //缓存管理器 public EhCacheManager getEhCacheManager()&#123; EhCacheManager ehCacheManager = new EhCacheManager(); InputStream is =null; try &#123; is = ResourceUtils.getInputStreamForPath(&quot;classpath:ehcache-shiro.xml&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; CacheManager cacheManager = new CacheManager(is); ehCacheManager.setCacheManager(cacheManager); return ehCacheManager; &#125;&#125; 此时再进行登录测试，第一次登录会查询数据库获取角色、权限信息，后续登出，再登录时，可以正常进行页面访问，但没有再进行数据库查询，说明使用了缓存。","tags":[{"name":"EhCache","slug":"EhCache","permalink":"https://sk370.github.io/tags/EhCache/"}]},{"title":"Shiro","date":"2022-10-01T03:20:36.000Z","path":"2022/10/01/shiro/Shiro/","text":"ApacheShiro 是一个功能强大且易于使用的 Java 安全(权限)框架。Shiro 可以完成：认证、授权、加密、会话管理、与 Web 集成、缓存等。 1 Shrio 概述 1.1 是什么 ApacheShiro 是一个功能强大且易于使用的 Java 安全(权限)框架。 Shiro 可以完成：认证、授权、加密、会话管理、与 Web 集成、缓存等。 1.2 特点 易于使用：使用 Shiro 构建系统安全框架非常简单。就算第一次接触也可以快速掌握。 全面：Shiro 包含系统安全框架需要的功能，满足安全需求的“一站式服务”。 灵活：Shiro 可以在任何应用程序环境中工作。虽然它可以在 Web、EJB、IoC 环境中工作，但不需要依赖它们。Shiro 也没有强制要求任何规范，甚至没有很多依赖项。 强力支持 Web：Shiro 具有出色的 Web 应用程序支持，可以基于应用程序 URL 和 Web 协议（例如 REST）创建灵活的安全策略，同时还提供一组 JSP 库来控制页面输出。 兼容性强：Shiro 的设计模式使其易于与其他框架和应用程序集成。Shiro 与 Spring、Grails、Wicket、Tapestry、Mule、ApacheCamel、Vaadin 等框架无缝集成。 1.3 对比 SrpingSecurity SpringSecurity 基于 Spring 开发，项目若使用 Spring 作为基础，配合 Spring Security 做权限更加方便，而 Shiro 需要和 Spring 进行整合开发。 SpringSecurity 功能比 Shiro 更加丰富些，例如安全维护方面。 SpringSecurity 社区资源相对比 Shiro 更加丰富。 Shiro 的配置和使用比较简单，SpringSecurity 上手复杂些。 Shiro 依赖性低，不需要任何框架和容器，可以独立运行.SpringSecurity 依赖 Spring 容器。 Shiro 不仅仅可以使用在 web 中，它可以工作在任何应用环境中。在集群会话时 Shiro 最重要的一个好处或许就是它的会话是独立于容器的。 1.4 基本功能 Authentication：身份认证/登录，验证用户是不是拥有相应的身份。 Authorization：授权，即权限验证，验证某个已认证的用户是否拥有某个权限；即判断用户是否能进行什么操作，如：验证某个用户是否拥有某个角色。或者细粒度的验证某个用户某个资源是否具有某个权限。 SessionManager：会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；会话可以是普通 JavaSE 环境，也可以是 Web 环境的。 Cryptography：加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储。 WebSupport：Web 支持，可以非常容易的集成到 Web 环境。 Caching：缓存，比如用户登录后，其用户信息、拥有的角色/权限不必每次去查，这样可以提高效率。 Concurrency：Shiro 支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去。 Testing：提供测试支持。 RunAs：允许一个用户假装为另一个用户（如果他们允许）的身份进行访问。 RememberMe：记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了。 1.5 原理 1.5.1 外部视角 即从应用程序角度观察 Shiro 如何完成工作。 Subject：应用代码直接交互的对象是 Subject，也就是说 Shiro 的对外 API 核心 就是 Subject。Subject 代表了当前“用户”，这个用户不一定是一个具体的人，与当前应用交互的任何东西都是 Subject，如网络爬虫，机器人等；与 Subject 的所有交互都会委托给 SecurityManager；Subject 其实是一个门面，SecurityManager 才是实际的执行者。 SecurityManager：安全管理器；即所有与安全有关的操作都会与 SecurityManager 交互；且其管理着所有 Subject；可以看出它 Shiro 的核心，它负责与 Shiro 的其他组件进行交互，它相当于 SpringMVC 中 DispatcherServlet 的角色。 Realm：Shiro 从 Realm 获取安全数据（如用户、角色、权限），就是说 SecurityManager 要验证用户身份，那么它需要从 Realm 获取相应的用户进行比较以确定用户身份是否合法；也需要从 Realm 得到用户相应的角色/权限进行验证用户是否能进行操作；可以把 Realm 看成 DataSource。 1.5.2 内部视角 即从 Shiro 架构角度观察 Shiro 如何完成工作。 Subject：主体，可以看到主体可以是任何可以与应用交互的“用户”。 SecurityManager：相当于 SpringMVC 中的 DispatcherServlet 或者 Struts2 中的 FilterDispatcher；是 Shiro 的心脏；所有具体的交互都通过 SecurityManager 进行控制；它管理着所有 Subject、且负责进行认证和授权、及会话、缓存的管理。 Authenticator：认证器，负责主体认证的，这是一个扩展点，如果用户觉得 Shiro 默认的不好，可以自定义实现；其需要认证策略（AuthenticationStrategy），即什么情况下算用户认证通过了。 Authorizer：授权器，或者访问控制器，用来决定主体是否有权限进行相应的操作；即控制着用户能访问应用中的哪些功能。 Realm：可以有 1 个或多个 Realm，可以认为是安全实体数据源，即用于获取安全实体的；可以是 JDBC 实现，也可以是 LDAP 实现，或者内存实现等等；由用户提供；注意：Shiro 不知道你的用户/权限存储在哪及以何种格式存储；所以我们一般在应用中都需要实现自己的 Realm； SessionManager：如果写过 Servlet 就应该知道 Session 的概念，Session 需要有人去管理它的生命周期，这个组件就是 SessionManager；而 Shiro 并不仅仅可以用在 Web 环境，也可以用在如普通的 JavaSE 环境、EJB 等环境；所以 Shiro 就抽象了一个自己的 Session 来管理主体与应用之间交互的数据；这样的话，比如在 Web 环境用，刚开始是一台 Web 服务器；接着又上了台 EJB 服务器；这时想把两台服务器的会话数据放到一个地方，这个时候就可以实现自己的分布式会话（如把数据放到 Memcached 服务器）。 SessionDAO：DAO 大家都用过，数据访问对象，用于会话的 CRUD，比如想把 Session 保存到数据库，那么可以实现自己的 SessionDAO，通过如 JDBC 写到数据库；比如想把 Session 放到 Memcached 中，可以实现自己的 MemcachedSessionDAO；另外 SessionDAO 中可以使用 Cache 进行缓存，以提高性能。 CacheManager：缓存控制器，来管理如用户、角色、权限等的缓存的；因为这些数据基本上很少去改变，放到缓存中后可以提高访问的性能。 Cryptography：密码模块，Shiro 提供了一些常见的加密组件用于如密码加密/解密的。 2 权限概念 2.1 登录认证 2.1.1 概念 身份验证：一般需要提供如身份 ID 等一些标识信息来表明登录者的身份，如提供 email，用户名/密码来证明。 在 shiro 中，用户需要提供 principals（身份）和 credentials（证明）给 shiro，从 而应用能验证用户身份： principals：身份，即主体的标识属性，可以是任何属性，如用户名、邮箱等，唯一即可。一个主体可以有多个 principals，但只有一个 Primaryprincipals，一般是用户名/邮箱/手机号。 credentials：证明/凭证，即只有主体知道的安全值，如密码/数字证书等。 最常见的 principals 和 credentials 组合就是用户名/密码。 2.1.2 基本流程 执行登录：Subject 对象调用 login(token)方法。login(token)方法中，调用了this.securityManager.login(this,token)。 这里的this.securityManager是程序员通过SecurityUtils.setSecurityManager(securityManager)创建的。 123Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro.ini&quot;);SecurityManager securityManager = factory.getInstance();SecurityUtils.setSecurityManager(securityManager); SecurityManager 负责真正的身份验证逻辑；它会委托给 Authenticator 进行身份验证； Authenticator 才是真正的身份验证者，ShiroAPI 中核心的身份认证入口点，此处可以自定义插入自己的实现； Authenticator 可能（多个 realm 时）会委托给相应的 AuthenticationStrategy 进行多 Realm 身份验证，默认 ModularRealmAuthenticator 会调用 AuthenticationStrategy 进行多 Realm 身份验证； Authenticator 会把相应的 token 传入 Realm，从 Realm 获取身份验证信息，如果没有返回/抛出异常表示身份验证成功了。此处可以配置多个 Realm，将按照相应的顺序及策略进行访问。 底层：SecurityManager 要验证用户身份，那么它需要从 Realm 获取相应的用户进行比较以确定用户身份是否合法；也需要从 Realm 得到用户相应的角色/权限进行验证用户是否能进行操作；可以把 Realm 看成 DataSource，即安全数据源。 2.1.3 多个 Realm 认证 当应用程序配置多个 Realm 时，例如：用户名密码校验、手机号验证码校验等等。 Shiro 的 ModularRealmAuthenticator 会使用内部的 AuthenticationStrategy 组件判断认证是成功还是失败。 AuthenticationStrategy 是一个无状态的组件，它在身份验证尝试中被询问 4 次（这 4 次交互所需的任何必要的状态将被作为方法参数）： （1） 在所有 Realm 被调用之前 （2） 在调用 Realm 的 getAuthenticationInfo 方法之前 （3） 在调用 Realm 的 getAuthenticationInfo 方法之后 （4） 在所有 Realm 被调用之后 认证策略的另外一项工作就是聚合所有 Realm 的结果信息封装至一个 AuthenticationInfo 实例中，并将此信息返回，以此作为 Subject 的身份信息。 Shiro 中定义了 3 种认证策略的实现： AuthenticationStrategy class 描述 AtLeastOneSuccessfulStrategy 只要有一个（或更多）的 Realm 验证成功，那么认证将视为成功 FirstSuccessfulStrategy 第一个 Realm 验证成功，整体认证将视为成功，且后续 Realm 将被忽略 AllSuccessfulStrategy 所有 Realm 成功，认证才视为成功 ModularRealmAuthenticator 内置的认证策略默认实现是 AtLeastOneSuccessfulStrategy 方式。可以通过配置修改策略 。 2.2 角色、授权 2.2.1 概念 授权，也叫访问控制，即在应用中控制谁访问哪些资源（如访问页面/编辑数据/页面操作等）。在授权中需了解的几个关键对象：主体（Subject）、资源（Resource）、权限（Permission）、角色（Role）。 主体(Subject)：访问应用的用户，在 Shiro 中使用 Subject 代表该用户。用户只有授权后才允许访问相应的资源。 资源(Resource)：在应用中用户可以访问的 URL，比如访问 JSP 页面、查看/编辑某些数据、访问某个业务方法、打印文本等等都是资源。用户只要授权后才能访问。 权限(Permission)：安全策略中的原子授权单位，通过权限我们可以表示在应用中用户有没有操作某个资源的权力。即权限表示在应用中用户能不能访问某个资源，如：访问用户列表页面查看/新增/修改/删除用户数据（即很多时候都是 CRUD 式权限控制）等。权限代表了用户有没有操作某个资源的权利，即反映在某个资源上的操作允不允许。 Shiro 支持粗粒度权限（如用户模块的所有权限）和细粒度权限（操作某个用户的权限，即实例级别的）。 角色(Role)：权限的集合，一般情况下会赋予用户角色而不是权限，即这样用户可以拥有一组权限，赋予权限时比较方便。典型的如：项目经理、技术总监、CTO、开发工程师等都是角色，不同的角色拥有一组不同的权限。 2.2.2 授权方式 Shiro 支持三种方式的授权： 编程式：通过写 if/else 授权代码块完成： 123456Subject subject = SecurityUtils.getSubject();if(subject.hasRole(“admin”)) &#123; //有权限&#125; else &#123; //无权限&#125; 注解式：通过在执行的 Java 方法上放置相应的注解完成： 1234@RequiresRoles(&quot;admin&quot;)public void hello() &#123; //有权限&#125; 没有权限将抛出相应的异常； JSP/GSP 标签：在 JSP/GSP 页面通过相应的标签完成： 123&lt;shiro:hasRole name=&quot;admin&quot;&gt;&lt;!— 有权限 —&gt;&lt;/shiro:hasRole&gt; 2.2.3 授权流程 首先调用 Subject.isPermitted*/hasRole*接口，其会委托给 SecurityManager，而 SecurityManager 接着会委托给 Authorizer。 Authorizer 是真正的授权者，如果我们调用如 isPermitted(“user:view”)，其首先会通过 PermissionResolver 把字符串转换成相应的 Permission 实例。 在进行授权之前，其会调用相应的 Realm 获取 Subject 相应的角色/权限用于匹配传入的角色/权限。 Authorizer 会判断 Realm 的角色/权限是否和传入的匹配，如果有多个 Realm，会委托给 ModularRealmAuthorizer 进行循环判断，如果匹配如 isPermitted*/hasRole*会返回 true，否则返回 false 表示授权失败。 3 基本使用（不结合其他环境） 3.1 搭建 Maven 环境 引入 shiro 和日志包 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 3.2 测试登录 3.2.1 添加登录认证信息 既可以使用数据库的用户信息，也可以使用 INI 文件进行配置。本例使用 INI 文件。 在 resources 目录下创建shiro.ini文件（名称任意），写入用户信息： 123[users]zhangsan=z3lisi=l4 3.2.2 测试登录 调用subject.login(token)方法。 1234567891011121314151617181920212223242526272829303132333435package top.sk370.shiro;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.*;import org.apache.shiro.config.IniSecurityManagerFactory;import org.apache.shiro.mgt.SecurityManager;import org.apache.shiro.subject.Subject;import org.apache.shiro.util.Factory;public class App &#123; public static void main(String[] args) &#123; // 1 初始化获取 SecurityManager Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro.ini&quot;); SecurityManager securityManager = factory.getInstance(); SecurityUtils.setSecurityManager(securityManager); //2 获取 Subject 对象 Subject subject = SecurityUtils.getSubject(); //3 创建 token 对象， web 应用用户名密码从页面传递 AuthenticationToken token = new UsernamePasswordToken(&quot;zhangsan&quot;, &quot;z3&quot;); //4 完成登录 try &#123; subject.login(token); System.out.println(&quot;登录成功&quot;); &#125; catch (UnknownAccountException e) &#123; e.printStackTrace(); System.out.println(&quot;用户不存在&quot;); &#125; catch (IncorrectCredentialsException e) &#123; e.printStackTrace(); System.out.println(&quot;密码错误&quot;); &#125; catch (AuthenticationException ae) &#123; //unexpected condition? error? &#125; &#125;&#125; 3.3 测试角色 3.3.1 添加角色信息 在 shiro.ini 文件中添加角色配置： 123[users]zhangsan=z3,role1,role2lisi=l4 3.3.2 测试角色 调用subject.hasRole(role)方法。 登录成功后确认是否具有相关角色： 3.4 测试权限 3.4.1 添加角色的权限的对应关系 12345[users]zhangsan=z3,role1,role2lisi=l4[roles]role1=user:insert,user:select 3.4.2 测试角色的权限 调用subject.isPermitted(auth)方法。 也可以用 checkPermission 方法，但没有返回值，没权限抛 AuthenticationException。 3.5 测试加密 Shiro 内嵌很多常用的加密算法，比如 MD5 加密、盐值加密、父类加密： MD5 加密：new Md5Hash(str) 盐值加密：new Md5Hash(str, &quot;salt&quot;) 多次盐值加密：new Md5Hash(str, &quot;salt&quot;, 3) 父类加密：Md5Hash 继承了 SimpleHash 类，使用父类可以指定加密方式，如 MD5 加密——new SimpleHash(&quot;MD5&quot;,password,&quot;salt&quot;,3) 1234567891011121314151617public static void main(String[] args) &#123; //密码明文 String password = &quot;z3&quot;; //使用md5加密 Md5Hash md5Hash = new Md5Hash(password); System.out.println(&quot;md5加密 = &quot; + md5Hash.toHex()); //带盐的md5加密，盐就是在密码明文后拼接新字符串，然后再进行加密 Md5Hash md5Hash2 = new Md5Hash(password,&quot;salt&quot;); System.out.println(&quot;带盐的md5加密 = &quot; + md5Hash2.toHex()); //为了保证安全，避免被破解还可以多次迭代加密，保证数据安全 Md5Hash md5Hash3 = new Md5Hash(password,&quot;salt&quot;,3); System.out.println(&quot;md5带盐的3次加密 = &quot; + md5Hash3.toHex()); //使用父类进行加密 SimpleHash simpleHash = new SimpleHash( &quot;MD5&quot;,password,&quot;salt&quot;,3); System.out.println(&quot;父类带盐的3次加密 = &quot; + simpleHash.toHex());&#125; 3.6 自定义带加密的登录认证 shiro 默认的登录不带加密，想要使用自定义加密登录认证，需要创建自定义 Realm 类，继承 org.apache.shiro.realm.AuthenticatingRealm 或 AuthorizingRealm 类，实现doGetAuthenticationInfo()方法： org.apache.shiro.realm.AuthenticatingRealm 和 org.apache.shiro.realm.AuthorizingRealm 的区别是： AuthenticatingRealm：至有认证 AuthenticationInfo 这一个 doGetAuthenticationInfo()方法。 AuthorizingRealm：有认证 AuthenticationInfo 和授权 AuthenticationInfo 两个 doGetAuthenticationInfo()方法。 编写自定义 Realm 类： 1234567891011121314151617181920212223242526public class MyRealm extends AuthenticatingRealm &#123; //自定义登录认证方法，shiro的login方法底层会调用该类的认证方法进行认证 //需要配置自定义的realm生效，在ini文件中配置，在Springboot中配置 //该方法只是获取进行对比的信息，认证逻辑还是按照shiro底层认证逻辑完成 protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException &#123; //1获取前台的身份信息 String principal = authenticationToken.getPrincipal().toString(); //2获取前台的凭证信息 String password = new String((char[])authenticationToken.getCredentials()); System.out.println(&quot;认证用户信息：&quot;+principal+&quot;---&quot;+password); //3获取数据库中存储的用户信息——模拟 if(principal.equals(&quot;zhangsan&quot;))&#123; //3.1数据库中存储的加盐3次迭代的密码，这里没有读取数据库，也没读取shiro.ini文件，直接写出来了 String pwdInfo = &quot;7174f64b13022acd3c56e2781e098a5f&quot;; //4创建封装校验逻辑对象，封装数据返回 AuthenticationInfo info = new SimpleAuthenticationInfo( authenticationToken.getPrincipal(), pwdInfo, ByteSource.Util.bytes(&quot;salt&quot;), authenticationToken.getPrincipal().toString() ); return info; &#125; return null; &#125;&#125; shiro.ini 文件中配置，使自定义 Realm 类生效 123456789101112131415[main]md5CredentialsMatcher=org.apache.shiro.authc.credential.Md5CredentialsMatcher #加密方式md5CredentialsMatcher.hashIterations=3 #盐值加密次数myrealm=top.sk370.shiro.MyRealmmyrealm.credentialsMatcher=$md5CredentialsMatchersecurityManager.realms=$myrealm[users]zhangsan=7174f64b13022acd3c56e2781e098a5f,role1,role2 # 密码视为数据库中已加密的的密码lisi=l4[roles]role1=user:insert,user:select 测试登录：原先的登录代码不变 1234567891011121314151617181920212223242526272829303132333435package top.sk370.shiro;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.*;import org.apache.shiro.config.IniSecurityManagerFactory;import org.apache.shiro.mgt.SecurityManager;import org.apache.shiro.subject.Subject;import org.apache.shiro.util.Factory;public class App &#123; public static void main(String[] args) &#123; // 1 初始化获取 SecurityManager Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro.ini&quot;); SecurityManager securityManager = factory.getInstance(); SecurityUtils.setSecurityManager(securityManager); //2 获取 Subject 对象 Subject subject = SecurityUtils.getSubject(); //3 创建 token 对象， web 应用用户名密码从页面传递 AuthenticationToken token = new UsernamePasswordToken(&quot;zhangsan&quot;, &quot;z3&quot;); //4 完成登录 try &#123; subject.login(token); System.out.println(&quot;登录成功&quot;); &#125; catch (UnknownAccountException e) &#123; e.printStackTrace(); System.out.println(&quot;用户不存在&quot;); &#125; catch (IncorrectCredentialsException e) &#123; e.printStackTrace(); System.out.println(&quot;密码错误&quot;); &#125; catch (AuthenticationException ae) &#123; //unexpected condition? error? &#125; &#125;&#125; 4 整合 SpringBoot 搭建目标：实现访问数据库登录的过程。 4.1 搭建环境 引入依赖： 123456789101112131415161718192021222324252627282930313233&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring-boot-web-starter&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--mybatis-plus--&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.46&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--配置Thymeleaf与Shrio的整合依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.theborakompanioni&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-extras-shiro&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置 mysql 等 12345678910111213141516mybatis-plus: configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl mapper-locations: classpath:mapper/*.xmlspring: datasource: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:13306/shirodb?characterEncoding=utf-8&amp;useSSL=false username: root password: dimitre123 jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8shiro: loginUrl: /myController/login # 登录页面的路径 编写后台 4.2 测试登录 4.2.1 创建自定义 Realm 自定义 realm 实现与数据源的链接，代替 shiro.ini 文件，完成认证过程。 1234567891011121314151617181920212223242526272829303132333435363738394041@Componentpublic class MyRealm extends AuthorizingRealm &#123; @Autowired private UserService userService; /** * 自定义登录认证 * * @param token * @return * @throws AuthenticationException */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; //1 获取用户身份信息 String name = token.getPrincipal().toString(); //2 调用业务层获取用户信息（数据库中） User user = userService.getUserInfoByName(name); //3 判断并将数据完成封装 if (user != null) &#123; AuthenticationInfo info = new SimpleAuthenticationInfo( token.getPrincipal(), user.getPwd(), ByteSource.Util.bytes(&quot;salt&quot;), token.getPrincipal().toString() ); return info; &#125; return null; &#125; /** * 自定义授权 * @param principals * @return */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; return null; &#125;&#125; 这里的 token 对象是 handler 中封装的：AuthenticationToken token = new UsernamePasswordToken(name, pwd); token.getPrincipal()：获取用户名对象 token.getPrincipal().toString()：将用户名转为字符串 4.2.2 创建自定义配置类 自定义配置类完成 DefaultWebSecurityManager 和 DefaultShiroFilterChainDefinition 组件的创建： DefaultWebSecurityManager：用于认证操作，对获取到的前端用户信息进行处置， DefaultShiroFilterChainDefinition：用于授权操作（过滤器链接，对符合规则的请求进行放行） 1234567891011121314151617181920212223242526272829303132333435@Configurationpublic class ShiroConfig &#123; @Autowired private MyRealm myRealm; //配置 SecurityManager @Bean public DefaultWebSecurityManager defaultWebSecurityManager() &#123; //1 创建 defaultWebSecurityManager 对象 DefaultWebSecurityManager defaultWebSecurityManager = new DefaultWebSecurityManager(); //2 创建加密对象，并设置相关属性 HashedCredentialsMatcher matcher = new HashedCredentialsMatcher(); //2.1 采用 md5 加密 matcher.setHashAlgorithmName(&quot;md5&quot;); //2.2 迭代加密次数 matcher.setHashIterations(3); //3 将加密对象存储到 myRealm 中 myRealm.setCredentialsMatcher(matcher); //4 将 myRealm 存入 defaultWebSecurityManager 对象 defaultWebSecurityManager.setRealm(myRealm); //5 返回 return defaultWebSecurityManager; &#125; //配置 Shiro 内置过滤器拦截范围 @Bean public DefaultShiroFilterChainDefinition shiroFilterChainDefinition() &#123; DefaultShiroFilterChainDefinition definition = new DefaultShiroFilterChainDefinition(); //设置不认证可以访问的资源 definition.addPathDefinition(&quot;/myController/userLogin&quot;, &quot;anon&quot;); definition.addPathDefinition(&quot;/login&quot;, &quot;anon&quot;); //设置需要进行登录认证的拦截范围 definition.addPathDefinition(&quot;/**&quot;, &quot;authc&quot;); return definition; &#125;&#125; 4.2.3 编写 handler 方法 1234567891011121314151617181920212223242526272829303132@Controller@RequestMapping(&quot;myController&quot;)public class MyController &#123; /** * 访问登录页面 */ @GetMapping(&quot;login&quot;) public String login()&#123; return &quot;login&quot;; &#125; /** * 执行登录 */ @GetMapping(&quot;userLogin&quot;) public String userLogin(String name, String pwd, HttpSession session) &#123; //1 获取 Subject 对象 Subject subject = SecurityUtils.getSubject(); //2 封装请求数据到 token 对象中 AuthenticationToken token = new UsernamePasswordToken(name, pwd); //3 调用 login 方法进行登录认证 try &#123; subject.login(token); session.setAttribute(&quot;user&quot;, token.getPrincipal().toString()); return &quot;main&quot;;//跳转页面 &#125; catch (AuthenticationException e) &#123; e.printStackTrace(); System.out.println(&quot;登录失败&quot;); return &quot;登录失败&quot;; &#125; &#125;&#125; 此时就可以正常进行登陆了。 4.3 测试退出 退出功能只需要在配置类的 DefaultShiroFilterChainDefinition 组件中，添加退出的路径即可： 123456789101112131415161718@Configurationpublic class ShiroConfig &#123;…… //配置 Shiro 内置过滤器拦截范围 @Bean public DefaultShiroFilterChainDefinition shiroFilterChainDefinition() &#123; DefaultShiroFilterChainDefinition definition = new DefaultShiroFilterChainDefinition(); //设置不认证可以访问的资源 definition.addPathDefinition(&quot;/myController/userLogin&quot;, &quot;anon&quot;); definition.addPathDefinition(&quot;/login&quot;, &quot;anon&quot;); //设置登出过滤器，设置了登出过滤器，就不用设置handler方法了 definition.addPathDefinition(&quot;/logout&quot;,&quot;logout&quot;); //设置需要进行登录认证的拦截范围 definition.addPathDefinition(&quot;/**&quot;, &quot;authc&quot;); return definition; &#125;&#125; 注意：设置了登出过滤器，就不用设置 handler 方法了 4.4 多个 Realm 在配置类中修改： 疑问：这里怎么实现 MyRealm 的 md5 加密功能？ 4.5 Remberme 4.5.1 基本流程 RememberMe 功能，比如访问一些网站时，关闭了浏览器，下次再打开时还是能记住你是谁， 无需再登录即可访问。 在登录页面选中 RememberMe 然后登录成功；如果是浏览器登录，一般会把 RememberMe 的 Cookie 写到客户端并保存下来。 但是，如果访问电商平台时，如果要查看我的订单或进行支付时，此时还是需要再进行身份认证的，以确保当前用户还是你。 4.5.2 修改配置类 12345678910111213141516171819202122232425262728293031323334353637383940414243@Configurationpublic class ShiroConfig &#123; @Autowired private MyRealm myRealm; //配置 SecurityManager @Bean public DefaultWebSecurityManager defaultWebSecurityManager() &#123; …… //4 将 myRealm 存入 defaultWebSecurityManager 对象 defaultWebSecurityManager.setRealm(myRealm); //4.5 设置 rememberMe defaultWebSecurityManager.setRememberMeManager(rememberMeManager()); //5 返回 return defaultWebSecurityManager; &#125; //cookie 属性设置——记住我功能 public SimpleCookie rememberMeCookie() &#123; SimpleCookie cookie = new SimpleCookie(&quot;rememberMe&quot;); //设置跨域 //cookie.setDomain(domain); cookie.setPath(&quot;/&quot;); cookie.setHttpOnly(true); cookie.setMaxAge(30 * 24 * 60 * 60); return cookie; &#125; //创建 Shiro 的 cookie 管理对象 public CookieRememberMeManager rememberMeManager() &#123; CookieRememberMeManager cookieRememberMeManager = new CookieRememberMeManager(); cookieRememberMeManager.setCookie(rememberMeCookie()); cookieRememberMeManager.setCipherKey(&quot;1234567890987654&quot;.getBytes()); return cookieRememberMeManager; &#125; //配置 Shiro 内置过滤器拦截范围 @Bean public DefaultShiroFilterChainDefinition shiroFilterChainDefinition() &#123; …… //rememberMe的用户无需登录 definition.addPathDefinition(&quot;/**&quot;,&quot;user&quot;); return definition; &#125;&#125; 4.5.3 修改 handler 方法 接收前台是否传过来 remberme 的勾选： 4.5.4 登录表单增加记住我 4.6 角色、权限 4.6.1 角色、权限访问过程 角色认证即执行权限判断的过程，共有两种方式： （1） 在前端页面中通过shiro:****属性判断。 （2） 在接口服务中通过注解@Requires****进行判断。 给接口服务方法添加注解可以加在控制器方法上，也可以加在 service 业务方法上，一般加在控制器方法上。常用注解如下： @RequiresAuthentication： 验证用户是否登录，等同于方法 subject.isAuthenticated()。 @RequiresUser： 验证用户是否被记忆：等同于方法 subject.isAuthenticated()+subject.isRemembered()。 @RequiresGuest： 验证是否是一个游客请求：等同于方法 subject.getPrincipal()，注意返回结果为 null 代表为游客请求。 @RequiresRoles： 验证 subject 是否有相应角色，等同于方法 subject.hasRole()，没有则抛出 AuthorizationException 异常。可以编写异常类返回指定内容。 @RequiresPermissions： 验证 subject 是否有相应权限，等同于方法 subject.isPermitted(auth)。没有则抛出 AuthorizationException 异常。可以编写异常类返回指定内容。 例如： @RequiresPermissions (“file:read”,”wite:aFile.txt”)，subject 必须同时含有 file:read 和 wite:aFile.txt 权限才能访问方法。 4.6.2 编写角色、权限查询方法，完成数据库角色查询 数据库表及关联关系如下： 登录时访问数据库的方法使用了 Mybatis-plus 的方法，但进行角色、权限检查时没有现成的方法，所以需要自己编写。 mapper 方法 123456789101112131415161718192021222324252627@Repositorypublic interface UserMapper extends BaseMapper&lt;User&gt; &#123; /** * 自定义mapper方法：角色检查 * @param principal * @return */ @Select(&quot;SELECT NAME FROM role WHERE id IN (SELECT rid FROM role_user WHERE uid=(SELECT id FROM USER WHERE NAME=#&#123;principal&#125;))&quot;) List&lt;String&gt; getUserRoleInfoMapper(@Param(&quot;principal&quot;)String principal ); /** * 自定义mapper方法：权限检查 * @param roles * @return */ @Select(&#123; &quot;&lt;script&gt;&quot;, &quot;select info FROM permissions WHERE id IN &quot;, &quot;(SELECT pid FROM role_ps WHERE rid IN (&quot;, &quot;SELECT id FROM role WHERE NAME IN &quot;, &quot;&lt;foreach collection=&#x27;roles&#x27; item=&#x27;name&#x27; open=&#x27;(&#x27; separator=&#x27;,&#x27; close=&#x27;)&#x27;&gt;&quot;, &quot;#&#123;name&#125;&quot;, &quot;&lt;/foreach&gt;&quot;, &quot;))&quot;, &quot;&lt;/script&gt;&quot; &#125;) List&lt;String&gt; getUserPermissionInfoMapper(@Param(&quot;roles&quot;)List&lt;String&gt; roles);&#125; service 及 serviceimpl 方法： 1234public interface UserService &#123; List&lt;String&gt; getUserRoleInfo(@Param(&quot;principal&quot;)String principal ); List&lt;String&gt; getUserPermissionInfo(List&lt;String&gt; roles);&#125; 123456789101112@Servicepublic class UserServiceImpl implements UserService &#123; @Override public List&lt;String&gt; getUserRoleInfo(String principal) &#123; return userMapper.getUserRoleInfoMapper(principal); &#125; @Override public List&lt;String&gt; getUserPermissionInfo(List&lt;String&gt; roles) &#123; return userMapper.getUserPermissionInfoMapper(roles); &#125;&#125; 4.6.3 角色、权限认证过程 自定义 MyRealm 类的 doGetAuthenticationInfo 方法执行登录认证过程，而 doGetAuthorizationInfo 方法进行角色、权限检验过程。 角色认证：创建 SimpleAuthorizationInfo 对象，调用 addRole()方法、或 addRoles()方法，再将该对象返回： 权限认证：创建 SimpleAuthorizationInfo 对象，调用 addStringPermission()方法、或 addStringPermissions()方法，再将该对象返回： 12345678910111213141516171819@Overrideprotected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; // 1. 获取当前用户身份信息 String principal = principals.getPrimaryPrincipal().toString(); // 2.1 调用接口方法获取用户的角色信息 List&lt;String&gt; roles = userService.getUserRoleInfo(principal); System.out.println(&quot;当前用户角色信息： &quot;+roles); // 2.2 调用接口方法获取用户角色的权限信息 List&lt;String&gt; permissions = userService.getUserPermissionInfo(roles); System.out.println(&quot;当前用户权限信息： &quot;+permissions); // 3. 创建对象，存储当前登录的用户的权限和角色 SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); // 4.1 存储角色 info.addRoles(roles); // 4.2 存储权限信息 info.addStringPermissions(permissions); // 5.返回 return info;&#125; 4.6.4 测试角色及授权 handler 方法添加具有角色、权限才能访问的路径（控制器方法）： 123456789101112131415161718192021222324252627@Controller@RequestMapping(&quot;myController&quot;)public class MyController &#123; /** * 访问该页面，测试有无相应角色 * @return */ @RequiresRoles(&quot;admin&quot;) @GetMapping(&quot;userLoginRoles&quot;) @ResponseBody//由于引入了thymelaf，这里不写ResponseBody则相当于返回页面，加了相当于在页面返回内容 public String userLoginRoles() &#123; System.out.println(&quot;登录认证验证角色&quot;); return &quot;验证角色成功&quot;; &#125; /** * 访问该页面，测试有无相应权限 * @return */ @RequiresPermissions(&quot;user:delete&quot;) @GetMapping(&quot;userPermissions&quot;) @ResponseBody public String userLoginPermissions() &#123; System.out.println(&quot;登录认证验证权限&quot;); return &quot;验证权限成功&quot;; &#125;&#125; 在登录成功后的 main.html 页面编写链接，进行 handler 路径（方法）的访问： 123456&lt;a shiro:hasRole=&quot;admin&quot; href=&quot;/myController/userLoginRoles&quot; &gt;测试授权-角色验证&lt;/a&gt;&lt;a shiro:hasPermission=&quot;user:delete&quot; href=&quot;/myController/userPermissions&quot; &gt;测试授权-权限验证&lt;/a&gt; 4.7 异常处理 @RequiresRoles 验证失败会抛出 AuthorizationException 异常。@RequiresPermissions 验证失败会抛出 AuthorizationException 异常。 相应的，可以编写异常处理类进行对应的异常处理： 12345678910111213@ControllerAdvicepublic class PermissionsException &#123; @ResponseBody @ExceptionHandler(UnauthenticatedException.class) public String unauthenticatedException(Exception e)&#123; return &quot;未验证&quot;; &#125; @ResponseBody @ExceptionHandler(AuthorizationException.class) public String authorizationException(Exception e)&#123; return &quot;权限认证失败&quot;; &#125;&#125; 4.8 前端的授权验证 4.8.1 添加 thymeleaf 与 shiro 的整合依赖 4.6.1 节中描述了还可以使用前端标签的方式进行验证，这里利用了 thymeleaf 标签的功能（jsp 同理，但是要有相应的依赖支持）。 123456&lt;!--配置 Thymeleaf 与 Shrio 的整合依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.theborakompanioni&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-extras-shiro&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 4.8.2 配置类注册 ShiroDialect 组件 用于解析 thymeleaf 中的&lt;shiro:&gt;标签相关属性： 12345678@Configurationpublic class ShiroConfig &#123; @Bean public ShiroDialect shiroDialect()&#123; return new ShiroDialect(); &#125;&#125; 4.8.3 常用标签 guest 标签：&lt;shiro:guest&gt;&lt;/shiro:guest&gt; 用户没有身份验证时显示相应信息，即游客访问信息。 user 标签：&lt;shiro:user&gt;&lt;/shiro:user&gt; 用户已经身份验证/记住我登录后显示相应的信息。 authenticated 标签：&lt;shiro:authenticated&gt;&lt;/shiro:authenticated&gt; 用户已经身份验证通过，即 Subject.login 登录成功，不是记住我登录的。 notAuthenticated 标签：&lt;shiro:notAuthenticated&gt;&lt;/shiro:notAuthenticated&gt; 用户已经身份验证通过，即没有调用 Subject.login 进行登录，包括记住我自动登录的，也属于未进行身份验证。 principal 标签：&lt;shiro: principal/&gt;&lt;shiro:principal property=&quot;username&quot;/&gt; 相当于((User)Subject.getPrincipals()).getUsername()，判断是否是某个用户。 lacksPermission 标签：&lt;shiro:lacksPermission name=&quot;org:create&quot;&gt;&lt;/shiro:lacksPermission&gt; 如果当前 Subject 没有权限将显示标签内的内容。 hasRole 标签：&lt;shiro:hasRole name=&quot;admin&quot;&gt;&lt;/shiro:hasRole&gt; 如果当前 Subject 有角色将显示标签内的内容。 hasAnyRoles 标签：&lt;shiro:hasAnyRoles name=&quot;admin,user&quot;&gt;&lt;/shiro:hasAnyRoles&gt; 如果当前 Subject 有任意一个角色（或的关系）将显示标签内的内容。 lacksRole 标签：&lt;shiro:lacksRole name=&quot;abc&quot;&gt;&lt;/shiro:lacksRole&gt; 如果当前 Subject 没有角色将显示 body 体内容。 hasPermission 标签：&lt;shiro:hasPermission name=&quot;user:create&quot;&gt;&lt;/shiro:hasPermission&gt; 如果当前 Subject 有权限将显示标签内的内容 5. SessionManager 5.1 介绍 会话管理器，负责创建和管理用户的会话（Session）生命周期，它能够在任何环境中 在本地管理用户会话，即使没有 Web/Servlet/EJB 容器，也一样可以保存会话。 默认情况下，Shiro 会检测当前环境中现有的会话机制（比如 Servlet 容器）进行适配，如果没有（比如独立应用程序或者非 Web 环境），它将会使用内置的企业会话管理器来提供相应的会话管理服务，其中还涉及一个名为 SessionDAO 的对象。 SessionDAO 负责 Session 的持久化操作（CRUD），允许 Session 数据写入到后端持久化数据库。 5.2 会话管理实现 SessionManager 由 SecurityManager 管理。 Shiro 提供了三种实现： DefaultSessionManager：用于 JavaSE 环境 ServletContainerSessionManager：用于 web 环境，直接使用 Servlet 容器的会话 DefaultWebSessionManager：用于 web 环境，自己维护会话（不使用 Servlet 容器的会话管理） 5.3 获取 session 实现： Session session = SecurityUtils.getSubject().getSession() session.setAttribute(&quot;key&quot;,&quot;value&quot;) 说明： Controller 中的 request，在 shiro 过滤器中的 doFilerInternal 方法，被包装成 ShiroHttpServletRequest。 SecurityManager 和 SessionManager 会话管理器决定 session 来源于 ServletRequest，还是由 Shiro 管理的会话。 无论是通过 request.getSession 或 subject.getSession 获取到 session，操作 session，两者都是等价的。","tags":[{"name":"Shiro","slug":"Shiro","permalink":"https://sk370.github.io/tags/Shiro/"}]},{"title":"Spring Session","date":"2022-09-09T08:58:05.000Z","path":"2022/09/09/springsession/SpringSession/","text":"Http协议是无状态的，这样对于服务端来说，没有办法区分是新的访客还是旧的访客。但是，有些业务场景，需要追踪用户多个请求，此时就需要Session。Spring Session实现了无侵入式操作session的方式。 1. 登录状态保持及检查 http 协议时无状态的，无类型的，所有的信息都会被解析为字符串，也不会记录两次请求之间的联系。 1.1 会话控制机制 Cookie 的工作机制： 服务器端返回 Cookie 信息给浏览器 Java 代码： response.addCookie(cookie 对象); HTTP 响应消息头： Set-Cookie: Cookie 的名字=Cookie 的值 浏览器接收到服务器端返回的 Cookie， 以后的每一次请求都会把 Cookie 带上 HTTP 请求消息头： Cookie： Cookie 的名字=Cookie 的值 Session 的工作机制 获取 Session 对象： request.getSession() 检查当前请求是否携带了 JSESSIONID 这个 Cookie 带了： 根据这个 JSESSIONID 在服务器端查找对应的 Session 对象 能找到： 就把找到的 Session 对象返回 没找到： 新建 Session 对象返回， 同时返回 JSESSIONID 的 Cookie 没带： 新建 Session 对象返回， 同时返回 JSESSIONID 的 Cookie 1.2 session 共享 在分布式和集群环境下， 每个具体模块运行在单独的 Tomcat 上， 而 Session 是被不同 Tomcat 所“区隔” 的， 所以不能互通， 会导致程序运行时， 用户会话数据发生错误。 有的服务器上有， 有的服务器上没有。 1.2.1 Session 同步 问题 1： 造成 Session 在各个服务器上“同量” 保存。 TomcatA 保存了 1G 的 Session 数据， TomcatB 也需要保存 1G 的 Session 数据。 数据量太大的会导 致 Tomcat 性能下降。 问题 2： 数据同步对性能有一定影响。 1.2.2 将 Session 数据存储在 Cookie 中 做法： 所有会话数据在浏览器端使用 Cookie 保存， 服务器端不存储任何会话数据。 好处： 服务器端大大减轻了数据存储的压力。 不会有 Session 不一致问题 缺点：Cookie 能够存储的数据非常有限。 一般是 4KB。 不能存储丰富的数据。 Cookie 数据在浏览器端存储， 很大程度上不受服务器端控制， 如果浏览器端清理 Cookie， 相关数据会丢失。 1.2.3 反向代理 hash 一致性 问题 1： 具体一个浏览器， 专门访问某一个具体服务器， 如果服务器宕机，会丢失数据。 存在单点故障风险。 问题 2： 仅仅适用于集群范围内， 超出集群范围， 负载均衡服务器无效。 1.2.4 后端统一存储 Session 数据 后端存储 Session 数据时， 一般需要使用 Redis 这样的内存数据库， 而一般不采用 MySQL 这样的关系型数据库。 原因如下： Session 数据存取比较频繁。 内存访问速度快。 Session 有过期时间， Redis 这样的内存数据库能够比较方便实现过期释放。 优点 访问速度比较快。 虽然需要经过网络访问， 但是现在硬件条件已经能够达到网络访问比硬盘访问还要快。 硬盘访问速度： 200M/s 网络访问速度： 1G/s Redis 可以配置主从复制集群， 不担心单点故障。 1.3 SpringSession 使用 表面上调用的原生的 getAttribute，实际上已经是被 springsession 包装了的。 SpringSession 允许使用各种库作为 session 的存储位置，redis 只是其中一种。 需要在 springboot 环境中使用，暂未研究 springmvc。 12345678910&lt;!-- 引入springboot&amp;redis整合场景 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 引入springboot&amp;springsession整合场景 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 原理：SpringSession 从底层全方位“接管”了 Tomcat 对 Session 的管理。 SessionRepositoryFilter 利用 Filter 原 理，在每次请求到达目标方法之前，将原生HttpServletRequest/HttpServletResponse对象包装为SessionRepositoryRequest/ResponseWrapper。 包装 request 对象时要做到： 包装后和包装前类型兼容。 所谓类型兼容：“包装得到的对象 instanceof 包装前类型” 返回 true。 只有做到了类型的兼容， 后面使用包装过的对象才能够保持使用方法不变。 包装过的对象类型兼容、 使用方法不变， 才能实现“偷梁换柱”。 但是如果直接实现 HttpServletRequest 接口， 我们又不知道如何实现各个抽象方法。 这个问题可以借助原始被包装的对象来解决：包装对象并不是凭空创建一个相同类型的对象， 而是借助原生对象的主体功能， 修改有特殊需要的功能。 HttpSessionStrategy 封装 Session 的存取策略：cookie 还是 http headers 等方式。 SessionRepository 指定存取/删除/过期 session 操作的 repository RedisOperationsSessionRepository 使用 Redis 将 session 保存维护起来","tags":[{"name":"Spring Session","slug":"Spring-Session","permalink":"https://sk370.github.io/tags/Spring-Session/"}]},{"title":"Thymeleaf","date":"2022-09-06T07:54:59.000Z","path":"2022/09/06/thymeleaf/Thymeleaf/","text":"Thymeleaf是一种用于Web和独立环境的现代服务器端的Java模板引擎，主要目标是将优雅的自然模板带到开发工作流程中，并将HTML在浏览器中正确显示，并且可以作为静态原型，让开发团队能更容易地协作。 第 1 章 简介 封捷：第八章 Thymeleaf | 代码重工 雷丰阳：05、Web 开发 第 2 章 基本语法 html 页面 th:href 标签，以/开头不影响请求处理 2.1 基本语法 2.1.1 表达式 表达式名字 语法 用途 变量取值 ${…} 获取请求域、session 域、对象等值 选择变量 *{…} 获取上下文对象值 消息 #{…} 获取国际化等值 链接 @{…} 生成链接 片段表达式 ~{…} jsp:include 作用，引入公共页面片段 第 3 章 使用 3.1 springmvc 环境使用 使用步骤 添加 thymeleaf 的 jar 包 新建一个 Servlet 类ViewBaseServlet.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class ViewBaseServlet extends HttpServlet &#123; private TemplateEngine templateEngine; @Override public void init() throws ServletException &#123; // 1.获取ServletContext对象 ServletContext servletContext = this.getServletContext(); // 2.创建Thymeleaf解析器对象 ServletContextTemplateResolver templateResolver = new ServletContextTemplateResolver(servletContext); // 3.给解析器对象设置参数 // ①HTML是默认模式，明确设置是为了代码更容易理解 templateResolver.setTemplateMode(TemplateMode.HTML); // ②设置前缀 String viewPrefix = servletContext.getInitParameter(&quot;view-prefix&quot;); templateResolver.setPrefix(viewPrefix); // ③设置后缀 String viewSuffix = servletContext.getInitParameter(&quot;view-suffix&quot;); templateResolver.setSuffix(viewSuffix); // ④设置缓存过期时间（毫秒） templateResolver.setCacheTTLMs(60000L); // ⑤设置是否缓存 templateResolver.setCacheable(true); // ⑥设置服务器端编码方式 templateResolver.setCharacterEncoding(&quot;utf-8&quot;); // 4.创建模板引擎对象 templateEngine = new TemplateEngine(); // 5.给模板引擎对象设置模板解析器 templateEngine.setTemplateResolver(templateResolver); &#125; protected void processTemplate(String templateName, HttpServletRequest req, HttpServletResponse resp) throws IOException &#123; // 1.设置响应体内容类型和字符集 resp.setContentType(&quot;text/html;charset=UTF-8&quot;); // 2.创建WebContext对象 WebContext webContext = new WebContext(req, resp, getServletContext()); // 3.处理模板数据 templateEngine.process(templateName, webContext, resp.getWriter()); &#125;&#125; 在 web.xml 文件中添加配置【init 方法中会使用到】 业务 Servlet 继承 ViewBaseServlet 调用父类（ViewBaseServlet）的processTemplate方法 html 页面使用 thymeleaf 的标签 修改 html 文件的表头：&lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; &lt;th:if&gt; &lt;th:unless&gt; &lt;th:each&gt; &lt;th:text&gt; 如果要利用 Java8 中，反射获取方法真实的形参名称的话，需要开启上述设置。 3.2 springboot 整合 引入 Starter 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 2、自动配置好了 thymeleaf 12345@Configuration(proxyBeanMethods = false)@EnableConfigurationProperties(ThymeleafProperties.class)@ConditionalOnClass(&#123; TemplateMode.class, SpringTemplateEngine.class &#125;)@AutoConfigureAfter(&#123; WebMvcAutoConfiguration.class, WebFluxAutoConfiguration.class &#125;)public class ThymeleafAutoConfiguration &#123; &#125; 自动配好的策略 1、所有 thymeleaf 的配置值都在 ThymeleafProperties 2、配置好了 **SpringTemplateEngine ** **3、配好了 ThymeleafViewResolver ** 4、我们只需要直接开发页面 1234567891011/** * 访问该页面，测试有无相应角色 * @return */@RequiresRoles(&quot;admin&quot;)@GetMapping(&quot;userLoginRoles&quot;)@ResponseBody//由于引入了thymelaf，这里不写ResponseBody则相当于返回页面，加了相当于在页面返回内容public String userLoginRoles() &#123; System.out.println(&quot;登录认证验证角色&quot;); return &quot;验证角色成功&quot;;&#125; 第 4 章 使用案例 4.1 遍历元素 1234567&lt;ul&gt; &lt;li th:each=&quot;category : $&#123;categories&#125;&quot;&gt; &lt;a href=&quot;#&quot; class=&quot;header_main_left_a&quot; th:attr=&quot;ctg-data=$&#123;category.catId&#125;&quot; &gt;&lt;b th:text=&quot;$&#123;category.name&#125;&quot;&gt;家用电器&lt;/b&gt;&lt;/a &gt; &lt;/li&gt;&lt;/ul&gt; th:attr=&quot;ctg-data=$&#123;category.catId&#125;&quot;给自定义属性赋值。","tags":[{"name":"Thymeleaf","slug":"Thymeleaf","permalink":"https://sk370.github.io/tags/Thymeleaf/"}]},{"title":"分布式理论","date":"2022-09-05T12:43:17.000Z","path":"2022/09/05/技术理论/分布式理论/","text":"分布式理论是应用部署的一种演进，是由传统的单一架构向微服务划分的技术实践理论指导。分布式理论下，主要考虑一致性（C:Consistency)，可用性（A: Availability）和分区容错性（P：Partition tolerance）这三个基本需求。 1. 单一架构 1.0 概念 一个 war 包 → 运行在一个 Tomcat 上 1.1 演变 1.1.1 水平拆分 把原来的一个工程拆分成多个模块分别进行开发，一定程度上提高了模块化程度。最后部署时还是一个 war 包。 1.1.2 垂直拆分 按照业务功能把项目拆分成多个模块工程，但是由于彼此之间不存在模块之间的调用，所以还不能算作一个真正的分布式架构。相当于把一个项目拆分成多个小项目分别开发。 1.2 互联网时代的挑战 1.2.1 高可扩 项目设计架构的时候要考虑到功能的持续更新。 1.2.2 高性能 提高响应速度，系统处理一个请求的时间尽可能短，减少用户等待的时间，优化用户体验。 1.2.3 高并发 用户访问量非常大时，会增加系统的并发压力。 2. 分布式架构 2.1 概念 一个项目拆分成多个模块工程，每个模块工程都是一个 war 包，运行在各自的 Tomcat 上，模块之间可以通过网络互相调用。 2.2 方法的远程调用 2.2.1 本地调用 在同一个项目内部，不经过网络直接调用方法。不管是我们自己声明的方法还是第三方 jar 包中的方法都算本地调用。 2.2.2 远程调用 本质是获取远程方法的执行结果。利用网络请求与响应。 2.2.3 意义 对内：能够实现分布式架构 对外：能够调用第三方接口 2.3 思想 2.3.1 SOA ServiceOrientedArchitecture 面向服务的架构 在整个系统中，把相同的功能抽取出来作为一个服务，供系统中的其他模块调用，提高代码的复用性。 有了服务，就有了提供服务和使用服务的相对方。 2.3.2 微服务 微服务是 SOA 的进一步发展。微服务强调的特征：独立、可部署。 2.4 技术演进 2.4.1 WebService 解决应用程序之间的跨平台访问问题。 基于 SOAP/WSDL 协议， 让应用程序之间可以进行远程通信。 2.4.2 Dubbo+Zookeeper Dubbo： 基于 RPC 的远程过程调用框架。 Zookeeper： 基于树形目录结构、 异步通知机制的注册中心。 2.4.3 spring boot+spring cloud SpringBoot： 开发具体微服务， 使用“场景启动器（starter） ” 快速整合第三 方中间件 SpringCloud： 提供的微服务架构整体管理的一站式解决方案 Eureka： 注册中心 Ribbon： 客户端负载均衡 Feign： 远程接口的声明式调用 Hystrix： 服务的熔断、 降级、 监控 Zuul： 网关 2.5 相关概念 2.5.1 接口 分布式环境下的服务模块。 2.5.2 远程接口的声明式调用 远程：调用方法会发送请求。 接口：上述功能模块。 声明式调用：只需要声明调用模块的对象。 2.5.3 注册中心 远程接口的声明式调用之所以能够实现， 就是因为 Dubbo 或 Feign 这样的框架把服务的具体信息存入了注册中心， 对程序员在上层进行的具体代码编写全部屏蔽细节。 2.6 分布式架构的优缺点 2.6.1 优点 模块化程度更高， 有利于分工 有利于提升项目性能 整体提升：整个项目中每个模块都可以独占一台服务器， 整个项目分配到的服务器资源更多。 局部提升：由于对项目进行了拆分， 所以可以有针对性的对项目中局部的模块进行专门的优化。 纵向： 给当前模块所在的服务器增加硬件资源 横向： 给当前模块配置集群 2.6.2 缺点 结构复杂 调用关系复杂 部署复杂 数据不一致问题 Session 不一致问题 分布式事务问题 2.7 分布式和集群 2.7.1 相同点 都需要使用多台服务器 2.7.2 不同点 分布式： 每台服务器上运行的模块不同——异构 集群： 每台服务器上运行模块相同——同构","tags":[{"name":"分布式理论","slug":"分布式理论","permalink":"https://sk370.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/"}]},{"title":"Spring Security","date":"2022-09-03T16:43:34.000Z","path":"2022/09/04/springsecurity/SpringSecurity/","text":"在web应用开发中，安全无疑是十分重要的，选择Spring Security来保护web应用是一个非常好的选择。Spring Security 是spring项目之中的一个安全模块，可以非常方便与spring项目无缝集成。 1. 权限管理 1.1 从 springsecurity 认识权限管理 1.1.1 权限管理的作用 主要用于权限认证。 即：尚筹网前部分做的登录验证、需要登录状态访问的拦截、密码加密（使用盐值）。 图中虚线框内即为 springsecurity 的作用。用户登录系统时我们协助 SpringSecurity 把用户对应的角色、 权限组装好， 同时把各个资源所要求的权限信息设定好， 剩下的“ 登录验证”、 “ 权限验证” 等等工作都交给 SpringSecurity。 1.2 权限管理中的相关概念 主体：principal 使用系统的用户或设备或从其他系统远程登录的用户等等。 简单说就是谁使用系统谁就是主体。 认证：authentication 权限管理系统确认一个主体的身份， 允许主体进入系统。 简单说就是“主体” 证明自己是谁。 笼统的认为就是以前所做的登录操作。 授权：authorization 将操作系统的“权力”“授予”“主体”， 这样主体就具备了操作系统中特定功能的能力。 授权就是给用户分配权限。 1.3 作用流程 1.2 权限管理的主流框架 1.2.1 springsecurity Spring 技术栈的组成部分。 和 Spring 无缝整合。 全面的权限控制。 专门为 Web 开发而设计。 旧版本不能脱离 Web 环境使用。 新版本对整个框架进行了分层抽取， 分成了核心模块和 Web 模块。 单独 引入核心模块就可以脱离 Web 环境。 重量级 1.2.2 shiro Apache 旗下的轻量级权限控制框架。 轻量级。 Shiro 主张的理念是把复杂的事情变简单。 针对对性能有更高要求的 互联网应用有更好表现。 通用性。 好处： 不局限于 Web 环境， 可以脱离 Web 环境使用。 缺陷： 在 Web 环境下一些特定的需求需要手动编写代码定制。 2. 基本使用 2.1 渐进式增强 2.1.1 实验一：新建 springmvc 项目 创建工程 springmvc 的 web 工程 123456789101112131415161718192021&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.20.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 引入Servlet容器中相关依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- JSP页面使用的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.1.3-b06&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; web.xml 中配置 DispatcherServlet 123456789101112131415&lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- Map all requests to the DispatcherServlet for handling --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 加载 springmvc 配置文件，创建 DispatcherServlet springmvc.xml 文件中配置 springsceurity 的扫描包、开启注解驱动、开启 servlet 默认前端控制器 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;iceriver.springsecurity&quot;/&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/views/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;mvc:annotation-driven/&gt; &lt;mvc:default-servlet-handler/&gt;&lt;/beans&gt; 此时还未引入 springsecurity，创建了一个解析/WEB-INF/views/*.jsp文件的普通项目 2.1.2 实验二：引入 springsecurity 添加依赖： 123456789101112131415161718&lt;!-- SpringSecurity 对 Web 应用进行权限管理 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-web&lt;/artifactId&gt; &lt;version&gt;4.2.10.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- SpringSecurity 配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-config&lt;/artifactId&gt; &lt;version&gt;4.2.10.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- SpringSecurity 标签库 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-taglibs&lt;/artifactId&gt; &lt;version&gt;4.2.10.RELEASE&lt;/version&gt;&lt;/dependency&gt; 没法看 web.xml 文件中配置过滤器 12345678&lt;filter&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; SpringSecurity 使用的是过滤器 Filter 而不是拦截器 Interceptor，意味着SpringSecurity能够管理的不仅仅是 SpringMVC 中的 handler 请求，还包含 Web 应用中所有请求。 比如：项目中的静态资源也会被拦截， 从而进行权限控制。 特别注意：springSecurityFilterChain 标 签 中 必 须 是 springSecurityFilterChain。 因为 springSecurityFilterChain 在 IOC 容器中对应真正执行权限控制的二十几个 Filter， 只有叫这个名字才能够加载到这些 Filter。 创建配置类，使用@EnableWebSecurity 注解开启 springsecurity 安全控制功能 1234567891011121314151617package iceriver.springsecurity.config;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;/** * @author zhuyuqi * @version v0.0.1 * @className WebSecurityConfig * @description https://developer.aliyun.com/profile/sagwrxp2ua66w * @date 2022/09/04 10:03 */@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123;&#125; 此时所有请求都被拦截，要求登录才能访问，登录失败有错误提示 静态资源也被拦截，要求登录 2.1.3 实验三：放行首页和静态资源 在 WebSecurityConfig 中重写父类的 configure(HttpSecurity security)方法。 12345678910111213@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity security) throws Exception &#123; //super.configure(security); 注释掉将取消父类方法中的默认规则 security.authorizeRequests() // 对请求进行授权 .antMatchers(&quot;/index.jsp&quot;) // 针对/index.jsp路径进行授权 .permitAll() // 可以无条件访问 .antMatchers(&quot;/layui/**&quot;) // 针对/layui目录下所有资源进行授权 .permitAll() // 可以无条件访问 &#125;&#125; 效果：访问/layui 路径下的 index.jsp 会放行，访问/layui 路径下的其他资源会报 403 错误 2.1.4 实验四：未认证请求跳转到登录页 目标：解决访问/layui 路径下的其他资源报 403 错误，实现跳转到登录页。 继续重写父类的 configure(HttpSecurity security)方法。 123456789101112131415161718192021222324@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity security) throws Exception &#123; //super.configure(security); 注释掉将取消父类方法中的默认规则 security.authorizeRequests() // 对请求进行授权 .antMatchers(&quot;/index.jsp&quot;) // 针对/index.jsp路径进行授权 .permitAll() // 可以无条件访问 .antMatchers(&quot;/layui/**&quot;) // 针对/layui目录下所有资源进行授权 .permitAll() // 可以无条件访问 .and() .authorizeRequests() // 对请求进行授权 .anyRequest() // 任意请求 .authenticated() // 需要登录以后才可以访问 .and() .formLogin() // 使用表单形式登录 // 指定登录页的同时会影响到：“提交登录表单的地址”、“退出登录地址”、“登录失败地址“ .loginPage(&quot;/index.jsp&quot;) // 指定登录页面（如果没有指定会访问SpringSecurity自带的登录页） // loginProcessingUrl()方法指定了登录地址，就会覆盖loginPage()方法中设置的默认值/index.jsp POST .loginProcessingUrl(&quot;/do/login.html&quot;); // 指定form表单提交的地址 &#125;&#125; loginPage()方法的说明 1：没有指定我们自己的地址时，拦截了会跳转到 springsecurity 的登录页 loginPage()方法的说明 2： index.jsp GET 请求——去登录页面 index.jsp POST 请求——提交登录表单 index.jsp?error GET 请求——登录失败 index.jsp?logout GET 请求退出登录 2.1.5 实验五：实现登录 登录策略： 设置登录页 index.jsp 的 form 表单地址、inputname 属性 123456&lt;p&gt;$&#123;SPRING_SECURITY_LAST_EXCEPTION.message&#125;&lt;/p&gt;&lt;form action=&quot;$&#123;pageContext.request.contextPath &#125;/do/login.html&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;$&#123;_csrf.parameterName&#125;&quot; value=&quot;$&#123;_csrf.token&#125;&quot; /&gt; &lt;input name=&quot;loginAcct&quot; /&gt; &lt;input name=&quot;userPswd&quot; /&gt;&lt;/form&gt; form 表单中第一个 input 隐藏域设置的是跨域，未写该句提交表单不论账号密码正不正确，都会报 403 错误。（如果 spring security 设置了 crsf().disable()就不需要这句了） SpringSecurity 默认账号的请求参数名： username SpringSecurity 默认密码的请求参数名： password 要么修改页面上的表单项的 name 属性值， 要么修改配置。 修改配置可以调用 usernameParameter()和 passwordParameter()方法。 这里采用了在配置类中修改。 1234567891011121314public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity security) throws Exception &#123; //super.configure(security); 注释掉将取消父类方法中的默认规则 security.authorizeRequests() // 对请求进行授权 …… .rocessingUrl()方法指定了登录地址，就会覆盖loginPage()方法中设置的默认值/index.jsp .loginProcessingUrl(&quot;/do/login.html&quot;) // 指定form表单提交的地址 .usernameParameter(&quot;loginAcct&quot;) // 定制登录账号的请求参数名 .passwordParameter(&quot;userPswd&quot;) // 定制登录密码的请求参数名 .defaultSuccessUrl(&quot;/main.jsp&quot;); //设置登录成功后默认前往的 URL 地址 &#125;&#125; 这里 defaultSuccessUrl 地址写”main.html“是为了将请求发给 controlle 处理 重写另外一个父类的方法 configure(AuthenticationManagerBuilder builder)， 来设置登录系统的账号密码 （内存中校验） 12345678910 @Overrideprotected void configure(AuthenticationManagerBuilder builder) throws Exception &#123; //super.configure(auth); 一定要禁用默认规则 builder.inMemoryAuthentication() .withUser(&quot;tom&quot;).password(&quot;123123&quot;) //设置账号密码 .roles(&quot;ADMIN&quot;) //设置角色 .and() .withUser(&quot;jerry&quot;).password(&quot;456456&quot;)//设置另一个账号密码 .authorities(&quot;SAVE&quot;,&quot;EDIT&quot;); //设置权限&#125; Cross-site request forgery 跨站请求伪造：发送登录请求时要携带_csrf 值 举例：webA 为银行，webB 为钓鱼网站，UserC 在未登出 WebA 的情况下访问了 webB，导致 webB 伪装成 UserC 给 webA 发了个转账请求，webA 通过 cookie 验证了 UserC 的身份，执行了转账请求。 如果此时加入了_csrf 值，即 token 隐藏令牌，WebA 会比对该值，一致才执行操作。 问题：上面博客中防范措施一 token 隐藏令牌也是保存在 cookie 里的，那肯定能通过验证啊。 而且如果使用防范措施二：请求头中哪个是？ 2.1.6 实验六：开启注销功能（禁用 csrf） 实现方式：调用 HttpSecurity 对象的一系列方法设置注销功能。 logout()方法： 开启注销功能 logoutUrl()方法： 自定义注销功能的 URL 地址 logoutSuccessUrl()方法： 退出成功后前往的 URL 地址 addLogoutHandler()方法： 添加退出处理器 logoutSuccessHandler()方法： 退出成功处理器 禁用 CSRF 功能 12345678910111213141516public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity security) throws Exception &#123; //super.configure(security); 注释掉将取消父类方法中的默认规则 security.authorizeRequests() // 对请求进行授权 …… .defaultSuccessUrl(&quot;/main.jsp&quot;); //设置登录成功后默认前往的 URL 地址 .and() .csrf() .disable()//禁用csrf功能 .logout()//开启退出功能 .logoutUrl(&quot;/do/logout.html&quot;)//退出请求地址 .logoutSuccessUrl(&quot;/index.jsp&quot;);//退出后展示地址 &#125;&#125; 修改退出前端页面 123&lt;a id=&quot;logoutAnchor&quot; href=&quot;$&#123;pageContext.request.contextPath &#125;/do/logout.html&quot; &gt;退出&lt;/a&gt; 由于禁用了 crsf，此时登录页的 csrf 隐藏域会失效。不会报 403 错误 2.1.7 实验七：开启注销功能（开启 csrf） 如果 CSRF 功能没有禁用， 那么要求页面上的请求必须是 POST 方式。 将退出的 a 标签换为 form 表单 1234567891011121314151617&lt;form id=&quot;logoutForm&quot; action=&quot;$&#123;pageContext.request.contextPath &#125;/my/logout&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;$&#123;_csrf.parameterName&#125;&quot; value=&quot;$&#123;_csrf.token&#125;&quot; /&gt;&lt;/form&gt;&lt;a id=&quot;logoutAnchor&quot; href=&quot;&quot;&gt;退出&lt;/a&gt;&lt;script type=&quot;text/javascript&quot;&gt; window.onload = function () &#123; var anchor = document.getElementById(&quot;logoutAnchor&quot;); anchor.onclick = function () &#123; document.getElementById(&quot;logoutForm&quot;).submit(); return false; &#125;; &#125;;&lt;/script&gt; 修改 configure(HttpSecurity security)方法 12345678910111213141516public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity security) throws Exception &#123; //super.configure(security); 注释掉将取消父类方法中的默认规则 security.authorizeRequests() // 对请求进行授权 …… .defaultSuccessUrl(&quot;/main.jsp&quot;); //设置登录成功后默认前往的 URL 地址 .and() //.csrf() //.disable()//禁用csrf功能 .logout()//开启退出功能 .logoutUrl(&quot;/do/logout.html&quot;)//退出请求地址 .logoutSuccessUrl(&quot;/index.jsp&quot;);//退出后展示地址 &#125;&#125; 2.1.8 实验八：基于角色或权限进行访问控制 通过 HttpSecurity 对象设置资源的角色要求 ，只要角色或权限有一个满足即可。再通过 AuthenticationManagerBuilder 对象设置用户登录时具备的角色 configure(HttpSecurity security)中规划权限，控制要访问的资源 123456789101112131415161718public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity security) throws Exception &#123; //super.configure(security); 注释掉将取消父类方法中的默认规则 security.authorizeRequests() // 对请求进行授权 .antMatchers(&quot;/index.jsp&quot;) // 针对/index.jsp路径进行授权 .permitAll() // 可以无条件访问 .antMatchers(&quot;/layui/**&quot;) // 针对/layui目录下所有资源进行授权 .permitAll() // 可以无条件访问 .antMatchers(&quot;/level1/**&quot;) // 针对/level1/**路径设置访问要求 .hasRole(&quot;学徒&quot;) // 要求用户具备“学徒”角色才可以访问 .antMatchers(&quot;/level2/**&quot;) // 针对/level2/**路径设置访问要求 .hasAuthority(&quot;内门弟子&quot;) // 要求用户具备“内门弟子”权限才可以访问 …… .logoutSuccessUrl(&quot;/index.jsp&quot;);//退出后展示地址 &#125;&#125; 注意设置权限的顺序，上面的要相对小。 /level1/**等路径在 handler 中做了请求映射 在 configure(AuthenticationManagerBuilder builder)中指定登录账号的角色/权限 1234567891011@Overrideprotected void configure(AuthenticationManagerBuilder builder) throws Exception &#123; //super.configure(auth); 一定要禁用默认规则 builder.inMemoryAuthentication() .withUser(&quot;tom&quot;).password(&quot;123123&quot;) //设置账号密码 .roles(&quot;ADMIN&quot;,&quot;学徒&quot;) // 指定当前用户的角色 .and() .withUser(&quot;jerry&quot;) // 指定账号 .password(&quot;123123&quot;) // 指定密码 .authorities(&quot;UPDATE&quot;,&quot;内门弟子&quot;); // 指定当前用户的权限&#125; 如果被拒绝，则报 403 错误 注意3 步：调用 hasRole()方法时，SpringSecurity 会在角色字符串前面加“ROLE_” 前缀 ，使用数据库时要注意。这也是 hasRole()方法和 hasAuthority()方法的区别 相应的，在给角色分配权限的时候 roles()会给角色加上&quot;ROLE*&quot;，然后再调用 authorities()方法。 总结：SpringSecurity 会在底层用“ ROLE*” 区分角色和权限。 角色信息会被附加 “ROLE*” 前缀。 hasRole()会添加&quot;ROLE*&quot;前缀，分配权限是应该使用 roles()方法保持对应。或者使用 hasAuthority()和 authorities()方法对应。 2.1.9 实验九：403-error 目前访问被拒绝时会跳转到 403 页面。 为了优化体验，在 configure(HttpSecurity security)方法中定义 403 后的跳转页面 方式一使用 handler 和 security 分类的方式 方式二利用了 AccessDeniedHandler 接口的匿名实现，冲洗了 handle 方法 1234567891011121314151617181920@Override protected void configure(HttpSecurity security) throws Exception &#123; //super.configure(security); 注释掉将取消父类方法中的默认规则 security.authorizeRequests() // 对请求进行授权 …… .logoutSuccessUrl(&quot;/index.jsp&quot;)//退出后展示地址 . and() .exceptionHandling() // 指定异常处理器 // .accessDeniedPage(&quot;/to/no/auth/page.html&quot;) // 方式一：访问被拒绝时前往的页面 //方式二： .accessDeniedHandler(new AccessDeniedHandler() &#123; @Override public void handle(HttpServletRequest request, HttpServletResponse response, AccessDeniedException accessDeniedException) throws IOException, ServletException &#123; request.setAttribute(&quot;message&quot;, &quot;抱歉！您无法访问这个资源！☆☆☆&quot;); request.getRequestDispatcher(&quot;/WEB-INF/views/no_auth.jsp&quot;).forward(request, response); &#125; &#125;); &#125; 2.1.10 实验十：记住我（内存版） 功能解释：第一次登陆后关闭浏览器，再次打开不需要登录。 在 configure(HttpSecurity security)方法中调用 rememberMe()方法 ，开启功能。 1234567891011121314151617@Override protected void configure(HttpSecurity security) throws Exception &#123; //super.configure(security); 注释掉将取消父类方法中的默认规则 security.authorizeRequests() // 对请求进行授权 …… .accessDeniedHandler(new AccessDeniedHandler() &#123; @Override public void handle(HttpServletRequest request, HttpServletResponse response, AccessDeniedException accessDeniedException) throws IOException, ServletException &#123; request.setAttribute(&quot;message&quot;, &quot;抱歉！您无法访问这个资源！☆☆☆&quot;); request.getRequestDispatcher(&quot;/WEB-INF/views/no_auth.jsp&quot;).forward(request, response); &#125; &#125;) .and() .rememberMe();//记录等级状态 &#125; 登录表单设置”记住我“checkbox，name 属性为&quot;rember-me&quot; 1&lt;input type=&quot;checkbox&quot; name=&quot;remember-me&quot; lay-skin=&quot;primary&quot; title=&quot;记住我&quot; /&gt; 原理：将账户信息保存到 cookie 中 个性化配置 1234.rememberMe() .rememberMeParameter(&quot;remember-me-new&quot;) .rememberMeCookieName(&quot;remember-me-cookie&quot;) .tokenValiditySeconds(2 * 24 * 60 * 60); okenValiditySeconds 用于设置 token 的有效期，即多长时间内可以免除重复登录，单位是秒。不修改配置情况下默认是 2 周。 通过 rememberMeParameter 设置 from 表单“自动登录”勾选框的参数名称。如果这里改了，from 表单中 checkbox 的 name 属性要对应的更改。如果不设置默认是 remember-me。 rememberMeCookieName 设置了保存在浏览器端的 cookie 的名称，如果不设置默认也是 remember-me。 2.1.11 实验十一：记住我（数据库版） 上述记住我的方式将登录信息保存在内存中，虽然关闭浏览器不失效，但服务区重启后，本地保存的 cookie 与服务区的验证不一致，所以也无法通过，所以需要在数据库中保存登录状态更稳定。 添加数据库依赖 123456789101112131415161718&lt;!-- https://mvnrepository.com/artifact/com.alibaba/druid --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql 驱动 --&gt;&lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;4.3.20.RELEASE&lt;/version&gt;&lt;/dependency&gt; springmvc 配置文件中添加数据源 1234567891011&lt;!-- 配置数据源 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;dimitre123&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:13306/security?useSSL=false&quot;/&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;&lt;/bean&gt;&lt;!-- jdbcTemplate--&gt;&lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt; 这里还在数据库创建了 security 的数据库。 在配置类中注入数据源 在 configure(HttpSecurity security)方法中调用 tokenRepository()方法 ，开启令牌仓库功能 12345678910111213141516171819public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired private DataSource dataSource; @Override protected void configure(HttpSecurity security) throws Exception &#123; JdbcTokenRepositoryImpl tokenRepository = new JdbcTokenRepositoryImpl(); tokenRepository.setDataSource(dataSource); //super.configure(security); 注释掉将取消父类方法中的默认规则 security.authorizeRequests() // 对请求进行授权 …… .and() .rememberMe()//记录等级状态 .tokenRepository(tokenRepository);//开启令牌仓库功能 &#125; ……&#125; 按照 JdbcTokenRepositoryImpl 中的代码创建数据库表： 123456CREATE TABLE persistent_logins ( username VARCHAR (64) NOT NULL, series VARCHAR (64) PRIMARY KEY, token VARCHAR (64) NOT NULL, last_used TIMESTAMP NOT NULL); 这里虽然 JdbcTokenRepositoryImpl 类中的 initDao()调用了创表语句，但是 initDao()不是自动执行，需要外部调用以下。 但是又因为 initDao()方法设置成了 protected 修饰，导致只能是同包或子类调用。 而 WebSecurityConfig 已经继承了 WebSecurityConfigurerAdapter，且让他继承 JdbcTokenRepositoryImpl 不合语义。 要么就按照上述的方式手动创建。 要么就修改源码，手动调用：创建一个一模一样的包，把 JdbcTokenRepositoryImpl 原封不动拷贝过来，WebSecurityConfig 中注入了 tokenRepository 后，执行 tokenRepository.initDao() 此时登录后关闭浏览器、重启服务器，只要 cookie 还在，就能自动登录，而退出登录，也会删除本地 cookie，删除数据库表。 2.1.12 实验十二：数据库数据登录 前面的功能中，登录账号和密码写死在了方法中 springsecurity中实现数据库登录使用的是configure(AuthenticationManagerBuilder builder)方法中的builder.jdbcAuthentication().usersByUsernameQuery(&quot;tom&quot;);，在usersByUsernameQuery(&quot;tom&quot;)等方法中最终调用 JdbcDaoImpl 类的方法查询数据库。 而 SpringSecurity 的默认实现已经将 SQL 语句硬编码在了JdbcDaoImpl 类中。 此时要利用 springsecurity 的功能，只有三个选择; 按照 JdbcDaoImpl 类中 SQL 语句设计表结构。（与项目的适配性不高） 修改 JdbcDaoImpl 类的源码。（使用了 mybatis 框架时，数据库操作通过 mybaits 执行，不经过 springsecurity） 不使用 jdbcAuthentication()。 创建数据库： 自定义数据库查询方法：创建自定义类，实现 userDetailsService 接口。 12345678910111213141516171819@Componentpublic class MyUserDetailsService implements UserDetailsService &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; //1.使用 SQL 语句根据用户名查询用户对象 String sql = &quot;SELECT id,loginAcct,userPswd,username,email,cretaTime FROM t_admin WHERE loginAcct = ?&quot;; List&lt;Admin&gt; list = jdbcTemplate.query(sql, new BeanPropertyRowMapper&lt;&gt;(Admin.class)); Admin admin = list.get(0); String userPswd = admin.getUserPswd(); //2.设置权限信息 List&lt;GrantedAuthority&gt; authorities = new ArrayList&lt;&gt;(); authorities.add(new SimpleGrantedAuthority(&quot;ROLE_ADMIN&quot;)); authorities.add(new SimpleGrantedAuthority(&quot;UPDATE&quot;)); //3.把admin对象和authorities对象封装到UserDetails中 return new User(username, userPswd, authorities); &#125;&#125; 这里的 Admin 是 t_admin 的实体 User 是org.springframework.security.core.userdetails.User; 修正 configure(AuthenticationManagerBuilder builder) 1234567891011121314151617181920212223@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired private DataSource dataSource; @Autowired private MyUserDetailsService myUserDetailsService; @Override protected void configure(HttpSecurity security) throws Exception &#123; …… .usernameParameter(&quot;loginAcct&quot;) // 定制登录账号的请求参数名 .passwordParameter(&quot;userPswd&quot;) // 定制登录密码的请求参数名 .defaultSuccessUrl(&quot;/main.html&quot;) //设置登录成功后默认前往的 URL 地址 …… &#125; @Override protected void configure(AuthenticationManagerBuilder builder) throws Exception &#123; builder .userDetailsService(myUserDetailsService); &#125;&#125; 此时可以登陆了，但是我的没访问权限，老师的不知道怎么样。 但是按照做下来来说，应该有权限的。 configure(HttpSecurity security)方法中的 loginAcct 和 MyUserDetailsService 类中的没有关系，MyUserDetailsService 只是用到了 configure(HttpSecurity security)方法中获取到的参数值，没有用参数名。 2.1.13 实验十三：密码加密（md5） 自定义类实现 org.springframework.security.crypto.password.PasswordEncoder 接口。 重写抽象方法： encode()方法对明文进行加密。 matches()方法对明文加密后和密文进行比较 12345678910111213141516171819202122232425262728293031323334353637383940@Componentpublic class MyPasswordEncoder implements PasswordEncoder &#123; @Override public String encode(CharSequence rawPassword) &#123; return privateEncode(rawPassword); &#125; @Override public boolean matches(CharSequence rawPassword, String encodedPassword) &#123; String formPassword = privateEncode(rawPassword); String databasePassword = encodedPassword; return Objects.equals(formPassword, databasePassword); &#125; /** * 加密方法 * @param rawPassword * @return */ private String privateEncode(CharSequence rawPassword)&#123; //使用Javasecurity提供的md5加密工具类 if (rawPassword == null || rawPassword.length() == 0) &#123; throw new RuntimeException(&quot;传入字符不能为空&quot;); &#125; String algorithm = &quot;md5&quot;; try &#123; MessageDigest messageDigest = MessageDigest.getInstance(algorithm);// Java提供的md5加密类 byte[] bytes = ((String)rawPassword).getBytes();//将rawPassword转换为字符串后转换为字节数组 byte[] digest = messageDigest.digest(bytes);// 执行加密 int signum = 1; BigInteger bigInteger = new BigInteger(signum, digest);// 创建一个正大整数，signum为-1表示负数，为0表示0 int radix = 16; String encoded = bigInteger.toString(radix).toUpperCase();// 将正大整数转换为16进制字符串 return encoded; &#125; catch (NoSuchAlgorithmException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 在 WebSecurityConfig 中进行使用 12345678910111213141516171819@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; …… @Autowired private MyPasswordEncoder myPasswordEncoder; @Override protected void configure(HttpSecurity security) throws Exception &#123; …… &#125; @Override protected void configure(AuthenticationManagerBuilder builder) throws Exception &#123; builder .userDetailsService(myUserDetailsService)//权限校验 .passwordEncoder(myPasswordEncoder);//登录密码加密与数据库比较 &#125;&#125; 2.1.14 实验十四：密码加密（盐值） md5 加密的问题：md5 使用 hash 值加密，一旦输入确定，输出就确定了。假如目前有个明文的密码库，当把它全部转成 md5 加密的字符串后，采用撞库的办法有可能猜出来。 以上问题可以采用循环进行 md5 加密、使用其他进制转换等方法避免。 但盐值加密是个更好的办法。 盐值加密介绍： springseurity 提供了PasswordEncoder 接口的实现类BCryptPasswordEncoder，其内部使用了盐值加密的相关算法。 123456789101112131415161718192021222324252627282930@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired private DataSource dataSource; @Autowired private MyUserDetailsService myUserDetailsService;// @Autowired// private MyPasswordEncoder myPasswordEncoder; @Bean BCryptPasswordEncoder getBCryptPasswordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; @Autowired private BCryptPasswordEncoder myPasswordEncoder; @Override protected void configure(HttpSecurity security) throws Exception &#123; …… &#125; @Override protected void configure(AuthenticationManagerBuilder builder) throws Exception &#123; …… builder .userDetailsService(myUserDetailsService)//权限校验 .passwordEncoder(myPasswordEncoder);//登录密码加密与数据库比较 &#125;&#125; 2.2 工作原理 在初始化时或第一次请求时准备好过滤器链。具体任务由具体过滤器来完成。 org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter org.springframework.security.web.context.SecurityContextPersistenceFilter org.springframework.security.web.header.HeaderWriterFilter org.springframework.security.web.csrf.CsrfFilter org.springframework.security.web.authentication.logout.LogoutFilter org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter org.springframework.security.web.authentication.ui.DefaultLoginPageGeneratingFilter org.springframework.security.web.authentication.www.BasicAuthenticationFilter org.springframework.security.web.savedrequest.RequestCacheAwareFilter org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter org.springframework.security.web.authentication.AnonymousAuthenticationFilter org.springframework.security.web.session.SessionManagementFilter org.springframework.security.web.access.ExceptionTranslationFilter org.springframework.security.web.access.intercept.FilterSecurityInterceptor","tags":[{"name":"Spring Security","slug":"Spring-Security","permalink":"https://sk370.github.io/tags/Spring-Security/"}]},{"title":"RabbitMQ","date":"2022-08-28T09:35:50.000Z","path":"2022/08/28/rabbitmq/RabbitMQ/","text":"RabbitMQ 是流行的消息队列服务软件，是开源的 AMQP（高级消息队列协议）实现。支持多种客户端，如：Java、Python、C、PHP、Ruby、JavaScript 等，用于在分布式系统中存储转发消息，可以实现异步处理、流量削峰、系统解耦，在易用性、扩展性、高可用等方面表现优异。 第 1 章消息队列介绍 1.1MQ 相关概念 1.1.1 含义 MQ(messagequeue)，从字面意思上看，本质是个队列，FIFO 先入先出，只不过队列中存放的内容是 message 而已，还是一种跨进程的通信机制，用于上下游传递消息。在互联网架构中，MQ 是一种非常常见的上下游“逻辑解耦+物理解耦”的消息通信服务。使用了 MQ 之后，消息发送上游只需要依赖 MQ，不用依赖其他服务。 1.1.2 意义 流量消峰：将短时、大量请求使用消息队列做缓冲，排队执行，减少服务器压力。 应用解耦：故障服务出现故障后，操作缓存在消息队列里，等到服务恢复后再执行，减少了不同服务间的强依赖关系，提高了系统的可用性。 异步处理：服务间调用过程需要很长时间完成时，使用消息总线去传送执行结果信息，不需要服务去等待服务。 传统方式 1：A 过一段时间去调用 B 的查询 api 查询。 传统方式 2：A 提供一个 callbackapi，B 执行完之后调用 api 通知 A 服务。 1.1.3 分类 ActiveMQ： 优点：单机吞吐量万级，时效性 ms 级，可用性高，基于主从架构实现高可用性，消息可靠性较低的概率丢失数据 缺点:官方社区现在对 ActiveMQ5.x 维护越来越少，高吞吐量场景较少使用。 Kafka： 大数据的杀手锏。吞吐量百万级，分布式、不会丢失数据。消费者采用 Pull 方式获取消息,消息有序,通过控制能够保证所有消息被消费且仅被消费一次;有优秀的第三方 KafkaWeb 管理界面 Kafka-Manager；在日志领域比较成熟，被多家公司和多个开源项目使用；功能支持：功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用。 缺点：Kafka 单机超过 64 个队列/分区，Load 会发生明显的飙高现象，队列越多，load 越高，发送消息响应时间变长，使用短轮询方式，实时性取决于轮询间隔时间，消费失败不支持重试；支持消息顺序，但是一台代理宕机后，就会产生消息乱序，社区更新较慢； RocketMQ：RocketMQ 出自阿里巴巴的开源产品，用 Java 语言实现，在设计时参考了 Kafka，并做出了自己的一些改进。被阿里巴巴广泛应用在订单，交易，充值，流计算，消息推送，日志流式处理，binglog 分发等场景。 优点:单机吞吐量十万级,可用性非常高，分布式架构,消息可以做到 0 丢失,MQ 功能较为完善，还是分布式的，扩展性好,支持 10 亿级别的消息堆积，不会因为堆积导致性能下降,源码是 java 我们可以自己阅读源码，定制自己公司的 MQ。 缺点：支持的客户端语言不多，目前是 java 及 c++，其中 c++不成熟；社区活跃度一般,没有在 MQ 核心中去实现 JMS 等接口,有些系统要迁移需要修改大量代码 RabbitMQ：2007 年发布，是一个在 AMQP(高级消息队列协议)基础上完成的，可复用的企业消息系统，是当前最主流的消息中间件之一。 优点:由于 erlang 语言的高并发特性，性能较好；吞吐量到万级，MQ 功能比较完备,健壮、稳定、易用、跨平台、支持多种语言如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP 等，支持 AJAX 文档齐全；开源提供的管理界面非常棒，用起来很好用,社区活跃度高；更新频率相当高。 缺点：商业版需要收费,学习成本较高 1.1.4 消息队列的选择 kafka：大型公司、大数据公司、日志采集需求的公司使用。 RocketMQ：金融领域。 RabbitMQ：中小型公司的优选。 1.2 RabbitMQ 1.2.1 RabbitMQ 的概念 RabbitMQ 是一个消息中间件：它接受并转发消息。你可以把它当做一个快递站点，当你要发送一个包裹时，你把你的包裹放到快递站，快递员最终会把你的快递送到收件人那里，按照这种逻辑 RabbitMQ 是一个快递站，一个快递员帮你传递快件。 RabbitMQ 与快递站的主要区别在于，它不处理快件而是接收，存储和转发消息数据。 1.2.2 RabbitMQ 的四大核心概念 生产者：产生数据发送消息的程序是生产者 交换机：交换机是 RabbitMQ 非常重要的一个部件，一方面它接收来自生产者的消息，另一方面它将消息推送到队列中。 交换机必须确切知道如何处理它接收到的消息，是将这些消息推送到特定队列还是推送到多个队列，亦或者是把消息丢弃，这个得有交换机类型决定 队列：队列是 RabbitMQ 内部使用的一种数据结构， 尽管消息流经 RabbitMQ 和应用程序，但它们只能存储在队列中。队列仅受主机的内存和磁盘限制的约束，本质上是一个大的消息缓冲区。 许多生产者可以将消息发送到一个队列，许多消费者可以尝试从一个队列接收数据。 消费者：消费与接收具有相似的含义。消费者大多时候是一个等待接收消息的程序。 请注意生产者，消费者和消息中间件很多时候并不在同一机器上。同一个应用程序既可以是生产者又是可以是消费者。 1.2.3 RabbitMQ 模式 简单模式： 工作模式： 发布订阅模式： 路由模式： 主题模式： RPC 模式： 发布确认模式： 1.2.4 工作原理 Producer：生产者 Broker：接收和分发消息的应用， RabbitMQ Server 就是 Message Broker Exchange： 交换机。message 到达 broker 的第一站，根据分发规则，匹配查询表中的 routing key，分发消息到 queue 中去。常用的类型有： direct (point-to-point), topic (publish-subscribe) and fanout(multicast) Queue： 消息最终被送到这里等待 consumer 取走 Connection： publisher／ consumer 和 broker 之间的 TCP 连接 Channel：信道。如果每一次访问 RabbitMQ 都建立一个 Connection，在消息量大的时候建立 TCPConnection 的开销将是巨大的，效率也较低。 Channel 是在 connection 内部建立的逻辑连接，如果应用程序支持多线程，通常每个 thread 创建单独的 channel 进行通讯， AMQP method 包含了 channel id 帮助客户端和 message broker 识别 channel，所以 channel 之间是完全隔离的。 Channel 作为轻量级的 Connection 极大减少了操作系统建立 TCP connection 的开销 Consumer：消费者 图中没有画的还有 Virtual host，Broker 包含多个 Virtual host， Virtual host 包含多个 Exchange 和 Channel。 1.3 rabbitmq 安装（3.8.8 版本） 1.3.1 安装过程 RabbitMQ 的使用需要 erlang 的运行环境，相关步骤如下： 根据本人习惯，在/usr/local 下新建/erlang 和/rabbitmq 目录 分别将 erlang-21.3-1.el7.x86_64.rpm 和 rabbitmq-server-3.8.8-1.el7.noarch.rpm 防止对应目录下 安装 Erlang 语言环境：rpm -ivh erlang-21.3-1.el7.x86_64.rpm emmmm，erlang 应该是分散安装了…… 安装 socat 依赖：yum install socat -y 安装 rabbitmq：rpm -ivh rabbitmq-server-3.8.8-1.el7.noarch.rpm 设置开机启动：chkconfig rabbitmq-server on 启动服务：/sbin/service rabbitmq-server start或者systemctl start rabbitmq-server 重启服务：systemctl restart rabbitmq-server 查看运行状态：/sbin/service rabbitmq-server status或者systemctl status rabbitmq-server 停止服务：/sbin/service rabbitmq-server stop或者systemctl stop rabbitmq-server 开启 web 管理插件：rabbitmq-plugins enable rabbitmq_management 访问虚拟机 ip+端口：[http://192.168.150.100:15672/](http://192.168.150.100:15672/) 默认账号 guest、密码 guest，此时还不能登录，需要开启权限，但一般设置新账户。 创建账户：rabbitmqctl add_user admin 123 设置 admin 添加 administrator 角色：rabbitmqctl set_user_tags admin administrator 此时就已经可以登录了 设置用户权限：rabbitmqctl set_permissions -p &quot;/&quot; admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 查看用户和角色：rabbitmqctl list_users 关闭应用：rabbitmqctl stop_app 清除应用：rabbitmqctl reset 重启应用：rabbitmqctl start_app 1.3.2 rabbitmqctl 命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152[root@localhost rabbitmq-server-3.8.8]# rabbitmqctl -helpUsagerabbitmqctl [--node &lt;node&gt;] [--timeout &lt;timeout&gt;] [--longnames] [--quiet] &lt;command&gt; [&lt;command options&gt;]Available commands:Help: autocomplete Provides command name autocomplete variants help Displays usage information for a command version Displays CLI tools versionNodes: await_startup Waits for the RabbitMQ application to start on the target node reset Instructs a RabbitMQ node to leave the cluster and return to its virgin state rotate_logs Instructs the RabbitMQ node to perform internal log rotation shutdown Stops RabbitMQ and its runtime (Erlang VM). Monitors progress for local nodes. Does not require a PID file path. start_app Starts the RabbitMQ application but leaves the runtime (Erlang VM) running stop Stops RabbitMQ and its runtime (Erlang VM). Requires a local node pid file path to monitor progress. stop_app Stops the RabbitMQ application, leaving the runtime (Erlang VM) running wait Waits for RabbitMQ node startup by monitoring a local PID file. See also &#x27;rabbitmqctl await_online_nodes&#x27;Cluster: await_online_nodes Waits for &lt;count&gt; nodes to join the cluster change_cluster_node_type Changes the type of the cluster node cluster_status Displays all the nodes in the cluster grouped by node type, together with the currently running nodes force_boot Forces node to start even if it cannot contact or rejoin any of its previously known peers force_reset Forcefully returns a RabbitMQ node to its virgin state forget_cluster_node Removes a node from the cluster join_cluster Instructs the node to become a member of the cluster that the specified node is in rename_cluster_node Renames cluster nodes in the local database update_cluster_nodes Instructs a cluster member node to sync the list of known cluster members from &lt;seed_node&gt;Replication: cancel_sync_queue Instructs a synchronising mirrored queue to stop synchronising itself sync_queue Instructs a mirrored queue with unsynchronised mirrors (follower replicas) to synchronise themUsers: add_user Creates a new user in the internal database authenticate_user Attempts to authenticate a user. Exits with a non-zero code if authentication fails. change_password Changes the user password clear_password Clears (resets) password and disables password login for a user delete_user Removes a user from the internal database. Has no effect on users provided by external backends such as LDAP list_users List user names and tags set_user_tags Sets user tagsAccess Control: clear_permissions Revokes user permissions for a vhost clear_topic_permissions Clears user topic permissions for a vhost or exchange list_permissions Lists user permissions in a virtual host list_topic_permissions Lists topic permissions in a virtual host list_user_permissions Lists permissions of a user across all virtual hosts list_user_topic_permissions Lists user topic permissions list_vhosts Lists virtual hosts set_permissions Sets user permissions for a vhost set_topic_permissions Sets user topic permissions for an exchangeMonitoring, observability and health checks: list_bindings Lists all bindings on a vhost list_channels Lists all channels in the node list_ciphers Lists cipher suites supported by encoding commands list_connections Lists AMQP 0.9.1 connections for the node list_consumers Lists all consumers for a vhost list_exchanges Lists exchanges list_hashes Lists hash functions supported by encoding commands list_queues Lists queues and their properties list_unresponsive_queues Tests queues to respond within timeout. Lists those which did not respond ping Checks that the node OS process is up, registered with EPMD and CLI tools can authenticate with it report Generate a server status report containing a concatenation of all server status information for support purposes schema_info Lists schema database tables and their properties status Displays status of a nodeParameters: clear_global_parameter Clears a global runtime parameter clear_parameter Clears a runtime parameter. list_global_parameters Lists global runtime parameters list_parameters Lists runtime parameters for a virtual host set_global_parameter Sets a runtime parameter. set_parameter Sets a runtime parameter.Policies: clear_operator_policy Clears an operator policy clear_policy Clears (removes) a policy list_operator_policies Lists operator policy overrides for a virtual host list_policies Lists all policies in a virtual host set_operator_policy Sets an operator policy that overrides a subset of arguments in user policies set_policy Sets or updates a policyVirtual hosts: add_vhost Creates a virtual host clear_vhost_limits Clears virtual host limits delete_vhost Deletes a virtual host list_vhost_limits Displays configured virtual host limits restart_vhost Restarts a failed vhost data stores and queues set_vhost_limits Sets virtual host limits trace_off trace_onConfiguration and Environment: decode Decrypts an encrypted configuration value encode Encrypts a sensitive configuration value environment Displays the name and value of each variable in the application environment for each running application set_cluster_name Sets the cluster name set_disk_free_limit Sets the disk_free_limit setting set_log_level Sets log level in the running node set_vm_memory_high_watermark Sets the vm_memory_high_watermark settingDefinitions: export_definitions Exports definitions in JSON or compressed Erlang Term Format. import_definitions Imports definitions in JSON or compressed Erlang Term Format.Feature flags: enable_feature_flag Enables a feature flag on target node list_feature_flags Lists feature flagsOperations: close_all_connections Instructs the broker to close all connections for the specified vhost or entire RabbitMQ node close_connection Instructs the broker to close the connection associated with the Erlang process id eval Evaluates a snippet of Erlang code on the target node eval_file Evaluates a file that contains a snippet of Erlang code on the target node exec Evaluates a snippet of Elixir code on the CLI node force_gc Makes all Erlang processes on the target node perform/schedule a full sweep garbage collection resume_listeners Resumes client connection listeners making them accept client connections again suspend_listeners Suspends client connection listeners so that no new client connections are acceptedQueues: delete_queue Deletes a queue purge_queue Purges a queue (removes all messages in it)Deprecated: hipe_compile DEPRECATED. This command is a no-op. HiPE is no longer supported by modern Erlang versions node_health_check DEPRECATED. Performs intrusive, opinionated health checks on a fully booted node. See https://www.rabbitmq.com/monitoring.html#health-checks insteadUse &#x27;rabbitmqctl help &lt;command&gt;&#x27; to learn more about a specific command 1.3.3 rabbitmq-plugins 命令 123456789101112131415161718192021222324252627[root@localhost rabbitmq-server-3.8.8]# rabbitmq-pluginsUsagerabbitmq-plugins [--node &lt;node&gt;] [--timeout &lt;timeout&gt;] [--longnames] [--quiet] &lt;command&gt; [&lt;command options&gt;]Available commands:Help: autocomplete Provides command name autocomplete variants help Displays usage information for a command version Displays CLI tools versionMonitoring, observability and health checks: directories Displays plugin directory and enabled plugin file paths is_enabled Health check that exits with a non-zero code if provided plugins are not enabled on target nodePlugin Management: disable Disables one or more plugins enable Enables one or more plugins list Lists plugins and their state set Enables one or more plugins, disables the restUse &#x27;rabbitmq-plugins help &lt;command&gt;&#x27; to learn more about a specific command 1.3.4 web 管理界面 1.4 使用 1.4.1 maven 整合 123456789101112131415161718192021222324252627&lt;!--指定 jdk 编译版本--&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;8&lt;/source&gt; &lt;target&gt;8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;&lt;dependencies&gt; &lt;!--rabbitmq 依赖客户端--&gt; &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;5.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--操作文件流的一个依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.4.2 整合 springboot 依赖： 123456789101112&lt;dependencies&gt; &lt;!--RabbitMQ 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.amqp&lt;/groupId&gt; &lt;artifactId&gt;spring-rabbit-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件： 1234spring.rabbitmq.host=192.168.150.100spring.rabbitmq.port=5672spring.rabbitmq.username=adminspring.rabbitmq.password=123 使用@EnableRabbit 注解开启功能 队列声明在配置类中，提供者写在 controller 中，消费者写在监听器中。 第 2 章 6 种模式及消息策略 2.1 模式 1：简单模式 创建 maven 工程。 引入依赖： 创建生产者： 1234567891011121314151617181920212223242526272829303132333435363738public class Producer &#123; private final static String QUEUE_NAME = &quot;hello&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory factory = new ConnectionFactory();//创建一个连接工厂 factory.setHost(&quot;192.168.150.100&quot;);//连接rabbitmq主机// factory.setPort(5672);//设置端口号，默认5672可以则可以不写这句// factory.setVirtualHost(&quot;/&quot;);//设置连接哪个虚拟主机，默认虚拟主机/时可以不写这句 factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;123&quot;); Connection connection = factory.newConnection();//获取连接对象 Channel channel = connection.createChannel();//获取连接中通道 /** * 生成一个队列，参数解释如下: * 1.队列名称，不存在时则自动创建 * 2.队列里面的消息是否持久化 默认消息存储在内存中【true 持久化队列】 * 3.exclusive 是否独占队列（只允许当前队列使用）【true 独占队列】 * 4.是否自动删除 最后一个消费者断开连接以后 该队列是否自动删除 【true 自动删除】 * 5.其他参数 */ channel.queueDeclare(QUEUE_NAME, false, false, false, null); String message = &quot;hello world&quot;; /** * 发送一个消息，参数解释如下: * 1.发送到那个交换机 * 2.路由的 key 是哪个 * 3.其他的参数信息 * 4.发送消息的消息体 */ channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes());//简单模式发送 System.out.println(&quot;消息发送完毕&quot;); channel.close(); connection.close(); &#125;&#125; 执行 main 方法， 查看 web 管理插件，可以看到生成了一个 hello 队列，且准备就绪，等待被消费。 创建消费者： 1234567891011121314151617181920212223242526272829303132333435public class Consumer &#123; private final static String QUEUE_NAME = &quot;hello&quot;; public static void main(String[] args) throws Exception &#123; ConnectionFactory factory = new ConnectionFactory();//创建一个连接工厂 factory.setHost(&quot;192.168.150.100&quot;);//连接rabbitmq主机// factory.setPort(5672);//设置端口号，默认5672可以则可以不写这句// factory.setVirtualHost(&quot;/&quot;);//设置连接哪个虚拟主机，默认虚拟主机/时可以不写这句 factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;123&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); System.out.println(&quot;等待接收消息....&quot;); //推送的消息如何进行消费的接口回调 DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; String message = new String(delivery.getBody()); System.out.println(message); &#125;; //取消消费的一个回调接口 如在消费的时候队列被删除掉了 CancelCallback cancelCallback = (consumerTag) -&gt; &#123; System.out.println(&quot;消息消费被中断&quot;); &#125;; /** * 消费者消费消息 * 1.消费哪个队列 * 2.消费成功之后是否要自动应答 true 代表自动应答 【false 手动应答】 * 3.消费者成功消费的回调 * 4.消费者取消消费的回调 */ channel.basicConsume(QUEUE_NAME, true, deliverCallback, cancelCallback); channel.close(); connection.close(); &#125;&#125; 执行 main 方法，此时 hello 队列中的消息会被消费掉 2.2 模式 2：工作模式 2.2.1 工作模式代码 工作队列(又称任务队列)的主要思想是队列中产生大量待处理的消息，消费（处理）能力不足，导致不能即使得到响应。工作模式提供了多个消费线程，共同处理生产者生产的消息。 但需要注意的是，一个消息只能被处理一次，不能被多次处理，不同消费者处理消息的机制是轮循方式。 优化简单模式连接工厂代码，创建工具类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class RabbitMQUtils &#123; private static Connection connection = null; private static Channel channel = null; /** * 得到一个通道 * @return * @throws Exception */ public static Channel getChannel() &#123; try &#123; ConnectionFactory factory = new ConnectionFactory();//创建一个连接工厂 factory.setHost(&quot;192.168.150.100&quot;);//连接rabbitmq主机// factory.setPort(5672);//设置端口号，默认5672可以则可以不写这句// factory.setVirtualHost(&quot;/&quot;);//设置连接哪个虚拟主机，默认虚拟主机/时可以不写这句 factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;123&quot;); connection = factory.newConnection();//获取连接对象 channel = connection.createChannel();//获取连接中通道 &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (TimeoutException e) &#123; e.printStackTrace(); &#125; return channel; &#125; public static void close()&#123; if (channel != null)&#123; try &#123; channel.close(); &#125; catch (IOException | TimeoutException e) &#123; e.printStackTrace(); &#125; &#125; if (connection != null)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建生产者，使用输入实现源源不断地消息，生产者生成队列（queueDeclare）、发送消息（basicPublish）的方法与简单模式没有区别。 1234567891011121314151617181920212223242526272829303132333435363738public class Producer &#123; private final static String QUEUE_NAME = &quot;worker&quot;; public static void main(String[] args) throws Exception &#123; Channel channel = RabbitMQUtils.getChannel(); /** * 生成一个队列，参数解释如下: * 1.队列名称 * 2.队列里面的消息是否持久化 默认消息存储在内存中 * 3.该队列是否只供一个消费者进行消费 是否进行共享 【true 可以多个消费者消费】 * 4.是否自动删除 最后一个消费者端开连接以后 该队列是否自动删除 【true 自动删除】 * 5.其他参数 */ channel.queueDeclare(QUEUE_NAME, false, false, false, null); Scanner scanner = new Scanner(System.in); boolean flag = true; while (flag)&#123; System.out.println(&quot;输入要发送的消息内容，按0退出&quot;); String str = scanner.nextLine(); if(&quot;0&quot;.equals(str))&#123; break; &#125;else&#123; /** * 发送一个消息，参数解释如下: * 1.发送到那个交换机 * 2.路由的 key 是哪个 * 3.其他的参数信息 * 4.发送消息的消息体 */ channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, str.getBytes());//简单模式发送 System.out.println(&quot;本次消息发送完毕&quot;); &#125; &#125; System.out.println(&quot;输入结束！&quot;); RabbitMQUtils.close(); &#125;&#125; 使用多线程创建 3 个消费者，消费者消费消息（basicConsume）的方法与简单模式没有区别 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Consumer extends Thread &#123; @Override public void run() &#123; try &#123; Channel channel = RabbitMQUtils.getChannel(); String name = Thread.currentThread().getName(); System.out.println( name + &quot;等待接收消息....&quot;); //推送的消息如何进行消费的接口回调 DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; String message = new String(delivery.getBody()); System.out.println(name + &quot;消费了一条消息，内容是&quot; + message); &#125;; //取消消费的一个回调接口 如在消费的时候队列被删除掉了 CancelCallback cancelCallback = (consumerTag) -&gt; &#123; System.out.println(&quot;消息消费被中断&quot;); &#125;; /** * 消费者消费消息 * 1.消费哪个队列 * 2.消费成功之后是否要自动应答 true 代表自动应答 【false 手动应答】 * 3.消费者成功消费的回调 * 4.消费者取消消费的回调 */ channel.basicConsume(QUEUE_NAME, true, deliverCallback, cancelCallback); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private final static String QUEUE_NAME = &quot;worker&quot;; public static void main(String[] args) throws Exception &#123; Consumer thread1 = new Consumer(); Consumer thread2 = new Consumer(); Consumer thread3 = new Consumer(); thread1.setName(&quot;消费者1&quot;); thread2.setName(&quot;消费者2&quot;); thread3.setName(&quot;消费者3&quot;); thread1.start(); thread2.start(); thread3.start(); &#125;&#125; 执行测试：注意，需要先启动生产者创建消息队列，否则消费者会报错。 2.3 模式 3：发布订阅模式 2.3.1 交换机及交换机与队列绑定 简单模式和工作模式中，生产者直接将消息发送给队列，消息被不同的消费者只能处理一次。 实际生产中，一个消息可能需要多个消费者处理，为了满足上述要求，rabbitmq 引入了 Exchanges（交换机）。 RabbitMQ 消息传递模型的核心思想是: 生产者生产的消息从不会直接发送到队列。实际上，通常生产者甚至都不知道这些消息传递传递到了哪些队列中。相反， 生产者只能将消息发送到交换机(exchange)，交换机工作的内容非常简单， 一方面它接收来自生产者的消息，另一方面将它们推入队列。 交换机必须确切知道如何处理收到的消息。 是应该把这些消息放到特定队列还是说把他们到许多队列中还是说应该丢弃它们。 这就的由交换机的类型来决定。 Exchanges 类型： 直接(direct) 该方法的第一个参数即为交换机的名称，空字符串表示默认或无名称交换机 主题(topic) 标题(headers) 扇出(fanout) ——即发布订阅 临时队列： 临时需要一个空队列，当断开与消费者的连接时，队列自动删除。 创建方式：String queueName = channel.queueDeclare().getQueue(); 绑定（bindings）： binding 其实是 exchange 和 queue 之间的桥梁，它告诉我们 exchange 和那个队列进行了绑定关系。比如说下面这张图告诉我们的就是 X 与 Q1 和 Q2 进行了绑定 。 绑定时使用唯一的 Routing key，从而实现队列与交换机的唯一绑定。当多个队列持有这个 Routing key 时，表示多个队列获得了消息. 2.4.2 Fanout 介绍 Fanout（扇出） 这种类型非常简单，它将接收到的所有消息广播到它知道的所有队列中。 系统中默认有 exchange 类型： 2.4.3 示例 创建一个生产者，发送消息后，由两个消费者接收，一个消费者在控制台输出，一个进行本地保存。 实现思路：创建一个扇出交换机，绑定两个队列（两个队列使用相同的 Routing key），利用扇出的特性——广播数据即可实现。 1234567891011121314产者发送消息：channel.basicPublish(EXCHANGE_NAME,&quot;&quot;,null,str.getBytes());消费者获取指定的交换机：channel.exchangeDeclare(EXCHANGE_NAME,&quot;fanout&quot;);生成临时队列：Stringqueue=channel.queueDeclare().getQueue();临时队列绑定交换机channel.queueBind(queue,EXCHANGE_NAME,&quot;&quot;);消费消息：channel.basicConsume(queue,true,deliverCallback,consumerTag-&gt;&#123;&#125;); 2.4 模式 4：路由模式 2.4.1 介绍 Direct exchange ——直接交换机，与 fanout 的区别是 Routing key 不同。 2.4.2 多重绑定 Routing key 相同情况时的直接交换机。与扇出交换机相同。 2.4.3 示例 实现上述模式： 12345678910111213141516171819202122生产者发送消息：channel.basicPublish(EXCHANGE_NAME,&quot;info&quot;,null,str.getBytes());channel.basicPublish(EXCHANGE_NAME,&quot;error&quot;,null,str.getBytes());消费者获取指定的交换机：channel.exchangeDeclare(EXCHANGE_NAME,&quot;direct&quot;);生成队列：channel.queueDeclare(&quot;console&quot;,false,false,false,null);channel.queueDeclare(&quot;console&quot;,false,false,false,null);channel.queueDeclare(&quot;disk&quot;,false,false,false,null);队列绑定交换机channel.queueBind(&quot;console&quot;,EXCHANGE_NAME,&quot;info&quot;);channel.queueBind(&quot;console&quot;,EXCHANGE_NAME,&quot;warning&quot;);channel.queueBind(&quot;disk&quot;,EXCHANGE_NAME,&quot;error&quot;);消费消息：channel.basicConsume(&quot;console&quot;,true,deliverCallback,consumerTag-&gt;&#123;&#125;);channel.basicConsume(&quot;console&quot;,true,deliverCallback,consumerTag-&gt;&#123;&#125;);channel.basicConsume(&quot;console&quot;,true,deliverCallback,consumerTag-&gt;&#123;&#125;); 上述交换机章 ，消费者消费消息只与 Routing key 相关。 2.5 模式 5：话题模式 2.5.1 介绍 fanout 只能广播、direct 只能单线发送（路由唯一），如果想要一个消费者能够接收多个路由的消息，则无法实现。 topic 交换机的消息的 routing_key 是一个长度不超过 255 字节的单词列表，单词以点号分隔开。如 “stock.usd.nyse”, “nyse.vmw”,“quick.orange.rabbit”。其中： *(星号)可以代替一个单词 #(井号)可以替代零个或多个单词 当一个队列绑定键是#,那么这个队列将接收所有数据，就有点像 fanout 如果队列绑定键当中没有#和*出现，那么该队列绑定类型就是 direct 了 2.5.2 示例 123456789101112131415161718192021222324252627生产者发送消息：for(Map.Entry&lt;String,String&gt;bindingKeyEntry:bindingKeyMap.entrySet())&#123; StringbindingKey=bindingKeyEntry.getKey(); Stringmessage=bindingKeyEntry.getValue(); channel.basicPublish(EXCHANGE_NAME,bindingKey,null, message.getBytes(&quot;UTF-8&quot;)); System.out.println(&quot;生产者发出消息&quot;+message);&#125;消费者获取指定的交换机：channel.exchangeDeclare(EXCHANGE_NAME,&quot;topic&quot;);生成队列：channel.queueDeclare(&quot;Q2&quot;,false,false,false,null);channel.queueDeclare(&quot;Q1&quot;,false,false,false,null);channel.queueDeclare(&quot;Q2&quot;,false,false,false,null);队列绑定交换机channel.queueBind(&quot;Q2&quot;,EXCHANGE_NAME,&quot;*.*.rabbit&quot;);channel.queueBind(&quot;Q1&quot;,EXCHANGE_NAME,&quot;*.ogange.*&quot;);channel.queueBind(&quot;Q2&quot;,EXCHANGE_NAME,&quot;lazy.#&quot;);消费消息：channel.basicConsume(&quot;Q2&quot;,true,deliverCallback,consumerTag-&gt;&#123;&#125;);channel.basicConsume(&quot;Q1&quot;,true,deliverCallback,consumerTag-&gt;&#123;&#125;);channel.basicConsume(&quot;Q2&quot;,true,deliverCallback,consumerTag-&gt;&#123;&#125;); 2.7 模式 7：发布确认策略 2.7.1 原理 针对工作模式消息持久化仍然存在丢失的风险，rabbitmq 通过引入发布确认模式。 生产者将信道设置成 confirm 模式（channel.confirmSelect()），该信道上面发布的消息都将会被指派一个唯一的 ID(从 1 开始)。 消息被投递到所有匹配的队列之后， broker 就会发送一个确认 ack 给生产者(包含消息的唯一 ID)，这就使得生产者知道消息已经正确到达目的队列了。 如果消息和队列是可持久化的，那么确认消息会在将消息写入磁盘之后发出。 broker 回传给生产者的确认消息中 delivery-tag 域包含了确认消息的序列号。 broker 也可以设置 basic.ack 的 multiple 域，表示到这个序列号之前的所有消息都已经得到了处理。 理解：发布确认针对的是 producer 和 broker 之间的关系，而不是 producer 和 consumer 的关系。 2.7.2 单个发布确认策略——同步方式 发布一个消息之后只有它被确认发布，后续的消息才能继续发布,waitForConfirmsOrDie(long)这个方法只有在消息被确认的时候才返回，如果在指定时间范围内这个消息没有被确认那么它将抛出异常。 相较于其他策略，单个发布确认策略发布速度较慢，因为没有确认发布的消息就会阻塞，这种方式最多提供每秒不超过数百条的发布消息的吞吐量。 2.7.3 批量发布确认策略——同步方式 单个发布确认存在消息发布速度较慢的问题。当消息发送固定条数后进行确认，确认成功后发送下一批次的消息，能够稍微提高点发送速率。 但是还是没法解决发生故障时是哪条消息的问题。 总结：同步的方式总是存在阻塞的问题。 2.7.4 异步确认发布策略 解决上述两策略信息丢失无法定位问题。 该策略利用消费者的 ackCallback 和 nackCallback 的回调函数，分别确认发送成功的消息和失败的消息。 2.7.5 处理未确认消息 最好的解决的解决方案就是把未确认的消息放到一个基于内存的能被发布线程访问的队列，比如说用 ConcurrentLinkedQueue 这个队列在 confirm callbacks 与发布线程之间进行消息的传递。 ConcurrentSkipListMap 是一个线程安全、有序、支持高并发的哈希表。 2.7.6 发布策略的选择 单独发布消息：同步等待确认， 简单，但吞吐量非常有限。 批量发布消息：批量同步等待确认， 简单，合理的吞吐量， 一旦出现问题但很难推断出是哪条消息出现了问题。 异步处理：最佳性能和资源使用，在出现错误的情况下可以很好地控制，但是实现起来稍微难些 。 2.8 可靠投递——消费端 2.8.1 概念 概念：为了保证消息在发送过程中不丢失，rabbitmq 引入消息应答机制，消息应答就是:消费者在接收到消息并且处理该消息之后，告诉 rabbitmq 它已经处理了， rabbitmq 可以把该消息删除了。 分类： 自动应答：消息发送后立即被认为传送成功。对消费者的处理效率和连接或 channel 的稳定性要求较高，仅适用于系统环境非常好的情况下。 手动应答： Channel.basicAck(用于肯定确认) Channel.basicNack(用于否定确认) Channel.basicReject(用于否定确认) ——比 Channel.basicNack 少一个 multiple 参数。 12345678910111213141516171819202122232425262728293031public void basicAck(long deliveryTag, boolean multiple) throws IOException &#123; long realTag = deliveryTag - this.activeDeliveryTagOffset; if (multiple &amp;&amp; deliveryTag == 0L) &#123; realTag = 0L; &#125; else if (realTag &lt;= 0L) &#123; return; &#125; this.transmit(new Ack(realTag, multiple)); this.metricsCollector.basicAck(this, deliveryTag, multiple);&#125;public void basicNack(long deliveryTag, boolean multiple, boolean requeue) throws IOException &#123; long realTag = deliveryTag - this.activeDeliveryTagOffset; if (multiple &amp;&amp; deliveryTag == 0L) &#123; realTag = 0L; &#125; else if (realTag &lt;= 0L) &#123; return; &#125; this.transmit(new Nack(realTag, multiple, requeue)); this.metricsCollector.basicNack(this, deliveryTag);&#125;public void basicReject(long deliveryTag, boolean requeue) throws IOException &#123; long realTag = deliveryTag - this.activeDeliveryTagOffset; if (realTag &gt; 0L) &#123; this.transmit(new Reject(realTag, requeue)); this.metricsCollector.basicReject(this, deliveryTag); &#125;&#125; 手动应答的优势：批量应答并且减少网络拥堵。 multiple 参数： true：代表批量应答 channel 上所有未应答的消息。 false：代表只应答 channel 上队列的第一个消息。 消息重新入队列 消费者由于某些原因失去连接(其通道已关闭，连接已关闭或 TCP 连接丢失)， 导致消息未发送 ACK 确认， RabbitMQ 将了解到消息未完全处理，并将对其重新排队。如果此时其他消费者可以处理，它将很快将其重新分发给另一个消费者。这样，即使某个消费者偶尔死亡，也可以确保不会丢失任何消息。 ACK 确认：TCP 协议中的消息确认 2.8.2 手动应答实现——消费者 basicConsume 方法 要实现手动应答及消息重新入队列，则需要将 basicConsume 方法的第二个参数传入 false，同时第三个参数 deliverCallback 的回调中，将未被消费的消息放回队列。 代码实现：生产者在 worker 基础上不变，消费者 1 设置睡眠时间，当它处理消息时停止程序，再次重启消费者程序，可以看到消息立即被其他消费者获取。 2.9 可靠投递——发送端 发送端可靠投递主要靠消费者发送成功时，Broker 的 confirmCallback 回调和队列的 returnCallback，具体使用见第 4 章。 2.10 消息持久化 2.10.1 队列持久化——生产者 queueDeclare 方法 解决 rabbitmq 服务停止，队列丢失的问题。 创建队列时，将生产者的 queueDeclare 方法第二个参数传入 true。 注意：要持久化的队列要在创建时设定好，不能再后期修改。 2.10.2 消息持久化——生产者 basicPublish 方法 解决 rabbitmq 服务停止，消息丢失的问题。 创建消息时，将生产者的发送消息方法（basicPublish）第 3 个参数设置为 MessageProperties.PERSISTENT_TEXT_PLAIN。 这种方式并不能 完全保证消息不丢失，该方式的可能再存盘的 io 过程中遇到异常情况，因而持久性保证并不强。 2.11 其他消息处理机制 2.11.1 不公平分发 在消费者执行 baseConsume 方法前，调用channe.basicQos(1)。channe.basicQos(0)表示轮询分发，即默认值。 不公平分发情况下，如果该消费者当前消息没处理完，则不会接收新的消息，消息会被派送给其他消费者。 2.11.2 预取值 当 channe.basicQos(）传入&gt;=2 的数时，表示获取队列中的 2 条信息，而不管该消费者的执行速率。 第 3 章 死信队列和延迟队列 3.1 死信队列 3.1.1 死信介绍 死信，就是无法被消费的消息。 一般来说， producer 将消息投递到 broker 或者直接到 queue 里了， consumer 从 queue 取出消息进行消费，但某些时候由于特定的原因导致 queue 中的某些消息无法被消费，这样的消息如果没有后续的处理，就变成了死信。 有死信自然就有了死信队列。 来源： 消息 TTL 过期 队列达到最大长度(队列满了，无法再添加数据到 mq 中) 消息被拒绝(basic.reject 或 basic.nack)并且 requeue=false 应用： 下单未成功支付订单自动失效。 3.1.2 死信代码架构——过期时间 C1 代码： 生产者正常生产即可，C2 消费者正常消费即可。 死信队列代码的要点是图中红框部分。 3.1.3 死信代码架构——队列最大长度 3.1.4 死信代码架构——消息被拒绝 3.2 延迟队列 3.2.1 介绍 本质上是死信队列的一种。延时队列就是用来存放需要在指定时间被处理的元素的队列。 使用场景： 订单在十分钟之内未支付则自动取消 新创建的店铺，如果在十天内都没有上传过商品，则自动发送消息提醒。 用户注册成功后，如果三天内没有登陆则进行短信提醒。 用户发起退款，如果三天内没有得到处理则通知相关运营人员。 预定会议后，需要在预定的时间点前十分钟通知各个与会人员参加会议 3.2.2 TTL TTL 是 RabbitMQ 中一个消息或者队列的属性，表明一条消息或者该队列中的所有消息的最大存活时间，单位是毫秒。 如果一条消息设置了 TTL 属性或者进入了设置 TTL 属性的队列，那么这条消息如果在 TTL 设置的时间内没有被消费，则会成为&quot;死信&quot;。 如果同时配置了队列的 TTL 和消息的 TTL，那么较小的那个值将会被使用。 队列 TTL 和消息 TTL 的区别： 如果设置了队列的 TTL 属性，那么一旦消息过期，就会被队列丢弃(如果配置了死信队列被丢到死信队列中)。 如果只设置了消息的过期时间，消息即使过期，也不一定会被马上丢弃，因为消息是否过期是在即将投递到消费者之前判定的，如果当前队列有严重的消息积压情况，则已过期的消息也许还能存活较长时间。 如果不设置 TTL，表示消息永远不会过期。 如果将 TTL 设置为 0，则表示除非此时可以直接投递该消息到消费者，否则该消息将会被丢弃。 3.2.3 延迟信息和延迟队列示例 创建交换机、队列、并进行绑定 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091@Configurationpublic class TTLQueueConfig &#123; public static final String X_EXCHANGE = &quot;X&quot;; public static final String Y_DEAD_LETTER_EXCHANGE = &quot;Y&quot;; public static final String QUEUEA = &quot;QA&quot;; public static final String QUEUEB = &quot;QB&quot;; public static final String DEAD_LETTER_QUEUE = &quot;QD&quot;;//死信队列 /** * 声明普通交换机 * @return */ @Bean(&quot;xExchange&quot;)//注入时可以使用别名 public DirectExchange xExchange()&#123; return new DirectExchange(X_EXCHANGE); &#125; /** * 声明死信交换机 * @return */ @Bean(&quot;yExchange&quot;)//注入时可以使用别名 public DirectExchange yExchange()&#123; return new DirectExchange(Y_DEAD_LETTER_EXCHANGE); &#125; /** * 声明普通队列 * @return */ @Bean(&quot;queueA&quot;)//注入时可以使用别名 public Queue queueA()&#123; Map&lt;String, Object&gt; arguments = new HashMap&lt;&gt;(); arguments.put(&quot;x-dead-letter-exchange&quot;, Y_DEAD_LETTER_EXCHANGE);//设置死信交换机 arguments.put(&quot;x-dead-letter-routing-key&quot;, &quot;YD&quot;);//设置死信ROUTITNGKEY arguments.put(&quot;x-message-ttl&quot;, 10000);//设置消息过期时间 return QueueBuilder.durable(QUEUEA).withArguments(arguments).build(); &#125; /** * 声明普通队列 * @return */ @Bean(&quot;queueB&quot;)//注入时可以使用别名 public Queue queueB()&#123; Map&lt;String, Object&gt; arguments = new HashMap&lt;&gt;(); arguments.put(&quot;x-dead-letter-exchange&quot;, Y_DEAD_LETTER_EXCHANGE);//设置死信交换机 arguments.put(&quot;x-dead-letter-routing-key&quot;, &quot;YD&quot;);//设置死信ROUTITNGKEY arguments.put(&quot;x-message-ttl&quot;, 40000);//设置消息过期时间 return QueueBuilder.durable(QUEUEB).withArguments(arguments).build(); &#125; /** * 声明死信队列 * @return */ @Bean(&quot;queueD&quot;)//注入时可以使用别名 public Queue queueD()&#123; return QueueBuilder.durable(DEAD_LETTER_QUEUE).build(); &#125; /** * 队列A绑定普通交换机 * @param queueA * @param xExchange * @return */ @Bean public Binding queueABindingX(@Qualifier(&quot;queueA&quot;) Queue queueA, @Qualifier(&quot;xExchange&quot;) DirectExchange xExchange)&#123; return BindingBuilder.bind(queueA).to(xExchange).with(&quot;XA&quot;);//with()参数XA为routingkey &#125; /** * 队列B绑定普通交换机 * @param queueB * @param xExchange * @return */ @Bean public Binding queueABindingB(@Qualifier(&quot;queueB&quot;) Queue queueB, @Qualifier(&quot;xExchange&quot;) DirectExchange xExchange)&#123; return BindingBuilder.bind(queueB).to(xExchange).with(&quot;XB&quot;);//with()参数XB为routingkey &#125; /** * 队列D绑定死信交换机 * @param queueD * @param yExchange * @return */ @Bean public Binding queueABindingD(@Qualifier(&quot;queueD&quot;) Queue queueD, @Qualifier(&quot;yExchange&quot;) DirectExchange yExchange)&#123; return BindingBuilder.bind(queueD).to(yExchange).with(&quot;YD&quot;);//with()参数XD为routingkey &#125;&#125; 生产者 1234567891011121314@RestController@RequestMapping(&quot;ttl&quot;)@Slf4jpublic class SendMsgController &#123; @Autowired private RabbitTemplate rabbitTemplate; @GetMapping(&quot;sendMsg/&#123;message&#125;&quot;) public void sendMsg(@PathVariable String message)&#123; log.info(&quot;当前时间为&#123;&#125;，发送一条信息给两个队列&#123;&#125;&quot;, new Date().toString(), message); rabbitTemplate.convertAndSend(&quot;X&quot;, &quot;XA&quot;,&quot;消息来自ttl为10s的队列&quot; + message); rabbitTemplate.convertAndSend(&quot;X&quot;, &quot;XB&quot;,&quot;消息来自ttl为40s的队列&quot; + message); &#125;&#125; 消费者 123456789@Slf4j@Componentpublic class DeadLetterQueueConsumer &#123; @RabbitListener(queues = &quot;QD&quot;)//接收消息，队列QD public void receiveD(Message message, Channel channel)&#123; String msg = new String(message.getBody()); log.info(&quot;当前时间为&#123;&#125;，收到的死信队列消息&#123;&#125;&quot;, new Date().toString(), msg); &#125;&#125; 测试：浏览器请求：http://localhost:8080/ttl/sendMsg/嘻嘻嘻 存在问题：代码灵活性不高，队列没有扩展性，新增延时时就需要重新创建队列。 优化：不设置过期时间的队列 QC，让生产者设置消息过期时间。 存在问题：虽然实现了在发送消息时设置过期时间，但同时发两条消息时，如果第一个消息未发送，则会阻塞后续消息的发送（队列特征），导致延迟时间不起作用。 3.2.4 Rabbitmq 插件实现延迟队列（新增延迟交换机） 插件用于优化上述第 7 点问题。 安装插件：找到/usr/lib/rabbitmq/lib/rabbitmq_server-3.8.8/plugins目录 将插件包rabbitmq_delayed_message_exchange-3.8.0.ez拷贝至 plugins 目录中： 使用：rabbitmq-plugins enable rabbitmq_delayed_message_exchange安装插件 重启 rabbitmq：执行rabbitmqctl start_app或者systemctl restart rabbitmq-server重启服务。 登录 web 管理控制台，在 exchanges 界面新增 exchange，检查是否出现x-delayed-message选项： 插件作用分析：默认情况下，延时发生在队列中，安装插件后，演示发生在交换机。 3.2.5 延迟交换机示例 消费者、生产者没有变化，创建交换机发生变化： 3.3 延迟队列总结 延时队列在需要延时处理的场景下非常有用，使用 RabbitMQ 来实现延时队列可以很好的利用 RabbitMQ 的特性，如：消息可靠发送、消息可靠投递、死信队列来保障消息至少被消费一次以及未被正确处理的消息不会被丢弃。另外，通过 RabbitMQ 集群的特性，可以很好的解决单点故障问题，不会因为单个节点挂掉导致延时队列不可用或者消息丢失。 当然，延时队列还有很多其它选择，比如利用 Java 的 DelayQueue，利用 Redis 的 zset，利用 Quartz 或者利用 kafka 的时间轮，这些方式各有特点,看需要适用的场景 第 4 章 发布确认高级 4.1 介绍 4.1.1 问题来源 在生产环境中由于一些不明原因，导致 rabbitmq 重启，在 RabbitMQ 重启期间生产者消息投递失败，导致消息丢失，需要手动处理和恢复。 4.1.2 解决思路 如果 rabbitmq 宕机，发送者根本不知道消息发到了那里，为了解决 rabbitmq 宕机，思路一：引入缓存，当交换机不可用时，由缓存接收数据。思路二：为了让发送者知道交换机不可用，则需要交换机给发送者一个确认收到消息的回调，为了保证消息不丢失，再将消息返回给生产者，生产者再进一步处置。 4.2 代码实现 4.2.1 配置文件开启发布确认功能 spring.rabbitmq.publisher-confirm-type=correlated NONE：禁用发布确认模式，是默认值 CORRELATED：发布消息成功到交换器后会触发回调方法 SIMPLE：（相当于单个确认，参见 2.3） 其一效果和 CORRELATED 值一样会触发回调方法， 其二在发布消息成功后使用 rabbitTemplate 调用 waitForConfirms 或 waitForConfirmsOrDie 方法，等待 broker 节点返回发送结果，根据返回结果来判定下一步的逻辑。 注意：waitForConfirmsOrDie 方法如果返回 false 则会关闭 channel，则接下来无法发送消息到 broker 4.2.2 增加交换机回调接口 该回调接口可以单独写，也可以让生产者实现，这里采用了单独写的方式。 消费者和提供者代码不变（同 2.3），测试结果如下： 测试交换机不可用（如改名字）、routingkey 不可用，会看到相应的提示。 此时已经实现了交换机给发送者返回是否收到的消息，但队列还没收到消息的问题还没解决，首先肯定的是队列收不到消息，则需要对收不到的消息进行回退，或者加入死信队列。 4.2.3 回退消息 在仅开启了生产者确认机制的情况下，交换机接收到消息后，会直接给消息生产者发送确认消息。 但由于该消息不可路由，该消息会被直接丢弃，此时生产者是不知道消息被丢弃这个消息。 RabbitTemplate 中，mandatory 参数可以在当消息传递过程中不可达目的地时将消息返回给生产者。 核心配置文件中增加spring.rabbitmq.publisher-returns=true，开启回退功能。 在 callback 类中新增实现接口RabbitTemplate.ReturnCallback，重写returnedMessage()，并将当前类注入 RabbitTemplate。 setMandatory 设置为 false 的结果。 4.3 备份交换机 4.3.1 介绍 mandatory 参数和回退消息，使 rabbitmq 获得了对无法投递消息的感知能力，有机会在生产者的消息无法被投递时发现并处理。 但此时，只是知道了这些信息，还不知道如何处理。仅打印日志来处理这些无法路由的消息是很不优雅的做法。 前文中为队列设置死信交换机来存储那些处理失败的消息，但是这些不可路由消息根本没有机会进入到队列，因此无法使用死信队列来保存消息。 在 RabbitMQ 中，有一种备份交换机的机制存在，可以很好的应对这个问题。当为某一个交换机声明一个对应的备份交换机时，就是为它创建一个备胎，当交换机接收到一条不可路由消息时，将会把这条消息转发到备份交换机中，由备份交换机来进行转发和处理。 通常备份交换机的类型为 Fanout ，这样就能把所有消息都投递到与其绑定的队列中，然后我们在备份交换机下绑定一个队列，这样所有那些原交换机无法被路由的消息，就会都进入这个队列了。 同时，还可以建立一个报警队列，用独立的消费者来进行监测和报警。 4.3.2 代码实现 配置类——声明交换机，备份交换机声明同普通声明，确认交换机需要构造。 报警消费者与其他消费者无异。 注意：mandatory 参数与备份交换机同时使用时，备份交换机优先级高 。 第 5 章 rabbitmq 扩展知识 5.1 幂等性 5.1.1 使用场景 幂等性即消息重复消费。 消费者在消费 MQ 中的消息时， MQ 已把消息发送给消费者，消费者在给 MQ 返回 ack 时网络中断，故 MQ 未收到确认信息，该条消息会重新发给其他的消费者，或者在网络重连后再次发送给该消费者，但实际上该消费者已成功消费了该条消息，造成消费者消费了重复的消息。 5.1.2 解决方案 思路：MQ 消费者的幂等性的解决一般使用全局 ID 或者写个唯一标识比如时间戳 或者 UUID 或者订单消费者消费 MQ 中的消息也可利用 MQ 的该 id 来判断，或者可按自己的规则生成一个全局唯一 id，每次消费消息时用该 id 先判断该消息是否已消费过。 操作方法： 唯一 ID+指纹码机制 ： 用户规则+时间戳+任意唯一信息码。利用查询语句进行判断这个 id 是否存在数据库中。优势是实现简单；劣势就是在高并发时，如果是单个数据库就会有写入性能瓶颈，当然也可以采用分库分表提升性能，但也不是最推荐的方式。 Redis 原子性：利用 redis 执行 setnx 命令，天然具有幂等性。从而实现不重复消费。 5.2 优先级队列 5.2.1 使用场景 订单催付的场景。 曾经后端系统是使用 redis 来存放的定时轮询，redis 只能用 List 做一个简单的消息队列，并不能实现一个优先级的场景。 RabbitMQ 可以设定消息的优先级，否则就是默认优先级。 5.2.2 设置方式 web 控制台设置方式： 代码实现方式： 1234567//队列设置优先级Map&lt;String, Object&gt; params = new HashMap();params.put(&quot;x-max-priority&quot;, 10);channel.queueDeclare(&quot;hello&quot;, true, false, false, params);// 消息设置优先级AMQP.BasicProperties properties = new AMQP.BasicProperties().builder().priority(5).build();channel.basicPublish(&quot;&quot;, QUEUE_NAME, properties, message.getBytes()); 注意：要让队列实现优先级需要做的事情有如下事情:队列需要设置为优先级队列，消息需要设置消息的优先级，消费者需要等待消息已经发送到队列中才去消费因为，这样才有机会对消息进行排序。 5.3 惰性队列 5.3.1 使用场景 惰性队列：消息保存在磁盘中，防止队列中消息积压占满内存。 默认情况下，当生产者将消息发送到 RabbitMQ 的时候，队列中的消息会尽可能的存储在内存之中，这样可以更加快速的将消息发送给消费者。 即使是持久化的消息，在被写入磁盘的同时也会在内存中驻留一份备份。当 RabbitMQ 需要释放内存的时候，会将内存中的消息换页至磁盘中，这个操作会耗费较长的时间，也会阻塞队列的操作，进而无法接收新的消息。 5.3.2 模式 队列具备两种模式： default 和 lazy。默认的为 default 模式。 lazy 模式即为惰性队列的模式，可以通过调用 channel.queueDeclare 方法的时候在参数中设置，也可以通过 Policy 的方式设置，如果一个队列同时使用这两种方式设置的话，那么 Policy 的方式具备更高的优先级。 rabbitmqctl set_policy Lazy &quot;^myqueue$&quot; '&#123;&quot;queue-mode&quot;:&quot;lazy&quot;&#125;' --apply-to queues 如果要通过声明的方式改变已有队列的模式的话，那么只能先删除队列，然后再重新声明一个新的。 在队列声明的时候可以通过“x-queue-mode”参数来设置队列的模式，取值为“default”和“lazy”。 123Map&lt;String, Object&gt; args = new HashMap&lt;String, Object&gt;();args.put(&quot;x-queue-mode&quot;, &quot;lazy&quot;);channel.queueDeclare(&quot;myqueue&quot;, false, false, false, args); 第 6 章 RabbitMQ 集群 6.1 搭建集群 6.1.1 搭建集群要求 集群必须使用相同的 cookie。 6.1.2 搭建步骤 搭建 node2 备用 node1，node3 备用 node2 的集群。 复制 node1 机器的 cookie 给 node2 和 node3。在 node1 上执行远程操作命令： scp /var/lib/rabbitmq/.erlang.cookie root@node2:/var/lib/rabbitmq/.erlang.cookie scp /var/lib/rabbitmq/.erlang.cookie root@node3:/var/lib/rabbitmq/.erlang.cookie 启动 RabbitMQ 服务,顺带启动 Erlang 虚拟机和 RbbitMQ 应用服务(在三台节点上分别执行以下命令)：rabbitmq-server -detached 在 node2 执行（将 node2 添加给 node1）： rabbitmqctl stop_app：rabbitmqctl stop 会将 Erlang 虚拟机关闭， rabbitmqctl stop_app 只关闭 RabbitMQ 服务 rabbitmqctl reset rabbitmqctl join_cluster rabbit@node1 rabbitmqctl start_app(只启动应用服务) 在 node3 执行（将 node3 添加给 node2）： rabbitmqctl stop_app：rabbitmqctl stop 会将 Erlang 虚拟机关闭， rabbitmqctl stop_app只关闭 RabbitMQ 服务 rabbitmqctl reset rabbitmqctl join_cluster rabbit@node1 rabbitmqctl start_app(只启动应用服务) 查看集群状态（在任意节点执行）：rabbitmqctl cluster_status 需要重新设置用户（任选一台） 创建账号rabbitmqctl add_user admin 123 设置用户角色rabbitmqctl set_user_tags admin administrator 设置用户权限rabbitmqctl set_permissions -p &quot;/&quot; admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 登录 web 控制台查看： 解除集群节点(演示解除 node2，在 node1 机器执行) rabbitmqctl stop_app rabbitmqctl reset rabbitmqctl start_app rabbitmqctl cluster_status rabbitmqctl forget_cluster_node rabbit@node2 6.2 镜像队列 6.2.1 介绍 上述过程搭建了一个集群，但此时还没有生效，不能共享队列。 如果 RabbitMQ 集群中只有一个 Broker 节点，那么该节点的失效将导致整体服务的临时性不可用，并且也可能会导致消息的丢失。 可以将所有消息都设置为持久化，并且对应队列的 durable 属性也设置为 true，但是这样仍然无法避免由于缓存导致的问题：因为消息在发送之后和被写入磁盘井执行刷盘动作之间存在一个短暂却会产生问题的时间窗。 通过 publisherconfirm 机制能够确保客户端知道哪些消息己经存入磁盘，尽管如此， 一般不希望遇到因单点故障导致的服务不可用。 引入镜像队列(Mirror Queue)的机制，可以将队列镜像到集群中的其他 Broker 节点之上，如果集群中的一个节点失效了，队列能自动地切换到镜像中的另一个节点上以保证服务的可用性。 6.2.2 搭建过程 任意登录一台 node 的 web 管理控制台，在 admin 界面，选择右侧 policy： 此时，如果 node1 停机，node2 会成为镜像队列。 6.3 Haproxy+Keepalive 实现高可用负载均衡 6.3.1 介绍 前面已经实现了镜像队列，保证收到的消息不丢失。 但目前还是固定连接到 node1，如果 node1 下线，则存在收不到消息的情况。 6.3.2 Haproxy 实现负载均衡 HAProxy 提供高可用性、负载均衡及基于 TCPHTTP 应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案，包括 Twitter,Reddit,StackOverflow,GitHub 在内的多家知名互联网公司在使用。 HAProxy 实现了一种事件驱动、单一进程模型，此模型支持非常大的井发连接数。 注意：负载均衡的代理服务器还有 nginx,lvs。之间的区别: http://www.ha97.com/5646.html 以下演示将本地主机作为负载均衡服务器。 在所有节点上下载 haproxy：yum -y install haproxy 修改所有节点的 ip 为本地主机 ip：vim /etc/haproxy/haproxy.cfg 分别启动 haproxy： haproxy -f /etc/haproxy/haproxy.cfg ps -ef | grep haproxy 访问：http://localhost:8888/stats 查看 6.3.3 Keepalived 实现双机(主备)热备 上述过程使用本地主机模拟了 haproxy 代理服务器，如果该服务器下线，仍然不能保证服务与集群的有效连接。 下面没懂，就不写了。 网上找了一张架构图，大概意思应该是 master 和 backup 两台主机均安装 keeplived 和 haproxy。但是请求应该发给谁呢？图中的 192.168.1.4 是请求来源呢，还是接收请求的服务器？请求接到后怎么转给 master 或 backup 呢？ 6.4 Federation Exchange（联合交换机） 异地 broker，数据不一致。数据在 A 地，请求发到了 B 地。在搭建了集群的情况下，能够实现数据从 A 调到 B 地，但远距离传输存在延迟，影响性能。 Federation Exchange 主要作用是从 A 地共享数据给 B 地，但不能从 B 地到 A 地。（不能双向共享有毛用啊！） 须安装 Federation 插件： 在每台机器上开启 federation 相关插件： rabbitmq-plugins enable rabbitmq_federation rabbitmq-plugins enable rabbitmq_federation_management 先运行 consumer 在 node2 创建 fed_exchange。 downstream(node2)配置 upstream(node1) amqp://admin:123@node1 admin：账号 123：密码 node1：上游主机名 添加 policy： 成功状态： 6.5 Federation Queue（联合队列） 联邦队列可以在多个 Broker 节点(或者集群)之间为单个队列提供均衡负载的功能。一个联邦队列可以连接一个或者多个上游队列(upstream queue)，并从这些上游队列中获取消息以满足本地消费者消费消息的需求。 图示表示的是节点 2 联邦到节点 1，数据使用节点 1 流向节点 2 的。 编写代码 添加 stream（同联邦交换机） 添加 policy 6.6 Shovel 与 Federation 具备的数据转发功能类似， Shovel 够可靠、持续地从一个 Broker 中的队列(作为源端，即 source)拉取数据并转发至另一个 Broker 中的交换器(作为目的端，即 destination)。 作为源端的队列和作为目的端的交换器可以同时位于同一个 Broker，也可以位于不同的 Broker 上。 开启插件(需要的机器都开启) rabbitmq-plugins enable rabbitmq_shovel rabbitmq-plugins enable rabbitmq_shovel_management 添加 shovel 源和目的地 6.6 Federation/Shovel 与集群的对比 Federation/Shovel cluster exchange 是逻辑分离的,可能有不同拥有者 单个逻辑 exchange 不限制 rabbitmq 和 erlang 版本 rabbitmq 和 erlang 版本要保持一致 exchange 可以通过不可靠(公网)网络连接,直接使用 amqp 连接,但是需要设置用户权限. 可靠(内网)网络,通信依赖 Erlang interode,共享 erlang cookie 拓扑结构可以是单项或双向 节点两两互联 cap 理论中的 ap cap 理论中的 cp exchange 可以有单独的信息,有些消息是本地的 每个节点的消息都是相同的 客户端只能看到连接的服务器队列 客户端可以看到集群内所有队列 shovel 跟 federation 使用类似,但使用的是 erlang client,更底层,更多配置,也更灵活. 默认使用动态配置。 shovel 特性: 松耦合 不限制 vhost,user 甚至 erlang/rabbitmq 的版本 网络友好 不限制网络范围,遵循 amqp 协议即可 高自由度 每个节点可以随意组合其他节点","tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://sk370.github.io/tags/RabbitMQ/"}]},{"title":"Kubernetes","date":"2022-08-26T09:54:29.000Z","path":"2022/08/26/kubernetes/kubernetes/","text":"Kubernetes，简称K8s，是用8代替名字中间的8个字符“ubernete”而成的缩写。是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制。 1. 介绍 1.1 出现原因 传统部署：应用部署在一台主机上，一个应用的问题会扩散至整个主机上时，会导致整个主机崩溃，导致整个应用系统不可用。 虚拟化部署：应用部署在不同的虚拟机上，减小了应用问题影响的范围，但虚拟机太笨重，占用了过多的主机资源。 容器化部署：既达到了虚拟化部署的优点，同时由于容器占用资源小的优点，解决了大多数问题。但是业务庞大时，容器会部署过多，管理会存在很大的难度。所以急需一个对大量容器管理的系统——容器编排系统。 1.2 Kubernetes 特性 服务发现和负载均衡 Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。 存储编排 Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。 自动部署和回滚 你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。 自动完成装箱计算 Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。 自我修复 Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 密钥与配置管理 Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。 1.3 架构 工作方式：Kubernetes Cluster = N * Master Node + N * Worker * Node（N 主节点+N 工作节点， N&gt;=1） Kubernetes Cluister：k8s 集群——集团公司 Controll plane：Control Plane Components，控制平面组件——集团公司总部 c-m：控制器管理器——决策领导 etcd：键值数据库——公司资料库 api：kube-apiserver，控制面的前端——给领导干活儿的秘书 sched：kube-scheduler，监视、调度节点——调度 c-c-m：cloud-controller-manaager，云控制器管理器——商务部门 Node：节点——分公司 kubelet：——分公司总经理 kube-proxy：——门卫大爷，他们之间保持通信 2. 安装 docker 2.1 开 3 个云服务器 选择任意云服务，购买 2 核 4g 的服务区 3 台（因为集群最少 3 台）。本例选择腾讯云，因为原先有账号。 腾讯云特点： 网络测试： 本地主机 ping3 台远程主机的公网 ip 和私网 ip 公网 ip： 私网 ip： 远程主机互相 ping 公网 ip 和私网 ip 2.2 本地连接到 3 台服务区并安装 docker 准备工作——实现一个窗口输入，另外两个也完成输入 配置 yum 源 1234sudo yum install -y yum-utilssudo yum-config-manager \\--add-repo \\http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装 docker 1234sudo yum install -y docker-ce docker-ce-cli containerd.io#以下是在安装k8s的时候使用（本测试指定版本）yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7 containerd.io-1.4.6 启动（设置开机启动并让本次启动立即生效） 1systemctl enable docker --now 配置阿里云加速 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123; &quot;registry-mirrors&quot;: [&quot;https://bnzui6g7.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 3. kubeadm 创建集群 3.1 集群架构 master、node 节点均需要： kubelet、kubeadm、kubectl master 节点需要： kube-apiserver、kube-proxy、kube-controller-manager、kube-scheduler、coredns、etcd、pause。 node 节点需要： kube-proxy 3.2 准备工作 Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令，但官方要求： 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存) 2 CPU 核或更多 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以) 设置防火墙放行规则（虚拟机互相 ping 公网 ip 或私网 ip） 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见这里了解更多详细信息。 设置不同 hostname：hostnamectl set-hostname xxx 分别命名为 k8s-master、k8s-node1、k8s-node2 开启机器上的某些端口。请参见这里 了解更多详细信息。 内网互信 禁用交换分区。为了保证 kubelet 正常工作，你 必须 禁用交换分区。 永久关闭 禁用 SELinux 允许 iptables 检查桥接流量（将 ipv6 的流量转到 ipv4，便于统计） 12345678910111213141516171819202122#各个机器设置自己的域名hostnamectl set-hostname xxxx# 将 SELinux 设置为 permissive 模式（相当于将其禁用）sudo setenforce 0sudo sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=permissive/&#x27; /etc/selinux/config#关闭swapswapoff -ased -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab#允许 iptables 检查桥接流量cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.confbr_netfilterEOFcat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsudo sysctl --system 3.3 安装 kubelet、kubeadm、kubectl 官方教程： kubelet 是在每个 Node 节点上运行的主要 “节点代理”。 kubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具 kubectl 是使用 Kubernetes API 与 Kubernetes 集群的控制面进行通信的命令行工具。 12345678910111213141516171819# 配置k8s下载地址cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgexclude=kubelet kubeadm kubectlEOF# 安装kubelet、kubeadm、kubectlsudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes=kubernetes# 设置开机启动并立即重启sudo systemctl enable --now kubelet# 查看kubelet服务状态systemctl status kubelet 安装后效果：kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。 3.4 下载节点镜像文件 一个 k8s-master 节点需要 kube-apiserver、kube-proxy、kube-controller-manager、kube-scheduler、coredns、etcd、pause。 k8s-node 节点需要 kube-proxy。而 kubelet 是运行在虚拟机上的，其他部分是运行在容器内的，所以为了下载顺利，下面的流程先建立了一个下载上述镜像文件的可执行脚本./images.sh，并赋予了可执行权限进行下载。 12345678910111213141516171819202122# 创建脚本sudo tee ./images.sh &lt;&lt;-&#x27;EOF&#x27;#!/bin/bashimages=(kube-apiserver:v1.20.9kube-proxy:v1.20.9kube-controller-manager:v1.20.9kube-scheduler:v1.20.9coredns:1.7.0etcd:3.4.13-0pause:3.2)for imageName in $&#123;images[@]&#125; ; dodocker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_$imageNamedoneEOF# master节点赋予权限并执行chmod +x ./images.sh &amp;&amp; ./images.sh# node节点执行docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_kube-proxy 注意：这里docker-pull使用了 lfy 的阿里云镜像仓库。类似下面这种。 这里建议使用 lfy 的仓库，因为镜像的名字是他设计的，别人没有。 3.5 初始化主节点 12345678910111213#所有机器添加master域名映射，声明谁是master节点：私有ip要修改为自己的echo &quot;172.31.0.12 cluster-endpoint&quot; &gt;&gt; /etc/hosts#主节点初始化kubeadm init \\--apiserver-advertise-address=172.31.0.12 \\--control-plane-endpoint=cluster-endpoint \\--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\--kubernetes-version v1.20.9 \\--service-cidr=10.96.0.0/16 \\--pod-network-cidr=192.168.0.0/16#所有网络范围不重叠——指上述ip（下面两个ip保持默认，其中192.168.0.0/16是为了适配master节点安装calico） 设定完 master 节点后，可以ping cluster-endpoint测试是否接通： 主节点安装完成后： 123456789101112131415161718192021222324252627Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of control-plane nodes by copying certificate authoritiesand service account keys on each node and then running the following as root: kubeadm join cluster-endpoint:6443 --token y53lrq.sca3goa3xm11y94q \\ --discovery-token-ca-cert-hash sha256:985e0f6815ee7b4f9b70c0d7cfc899ffaa7b4577c817ce6acbf925059d535030 \\ --control-planeThen you can join any number of worker nodes by running the following on each as root:kubeadm join cluster-endpoint:6443 --token y53lrq.sca3goa3xm11y94q \\ --discovery-token-ca-cert-hash sha256:985e0f6815ee7b4f9b70c0d7cfc899ffaa7b4577c817ce6acbf925059d535030 根据提示执行如下命令（步骤一）： 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 安装网络组件，这里选择calico 官网（步骤二）： 下载 calico 配置文件：curl [https://docs.projectcalico.org/manifests/calico.yaml](https://docs.projectcalico.org/manifests/calico.yaml) -O curl https://docs.projectcalico.org/v3.20/manifests/calico.yaml -O 安装 calico：kubectl apply -f calico.yaml 小总结： 运行中的应用在 docker 里面叫容器，在 k8s 里面叫 Pod kubetcl 常用命令： 查看节点状态（只能在主节点运行）：kubectl get nodes 根据配置文件，给集群创建资源：kubectl apply -f xxxx.yaml 查看集群部署了哪些应用（pod）：kubectl get pods -A 3.5 node 节点加入 master 根据 master 节点安装完的提示： 在 node1 和 node2 中执行（该命令有 24 小时有效期，如果过期了可以在 master 节点执行kubeadm token create --print-join-command重新获取： 12kubeadm join cluster-endpoint:6443 --token y53lrq.sca3goa3xm11y94q \\ --discovery-token-ca-cert-hash sha256:985e0f6815ee7b4f9b70c0d7cfc899ffaa7b4577c817ce6acbf925059d535030 遇到报错： node1、node2 执行：sysctl -w net.ipv4.ip_forward=1 网上解释：Linux 系统默认是禁止数据包转发的。所谓转发即当主机拥有多于一块的网卡时，其中一块收到数据包，根据数据包的目的 ip 地址将包发往本机另一网卡，该网卡根据路由表继续发送数据包。 在主节点查看节点状态``： 3.6 集群的自我修复 机器宕机后，重启集群可恢复。可以看到 AGE 延续的之前，STATUS 在逐渐恢复到 Running。 3.7 k8s 可视化界面 dashboard 是 k8s 官方提供的 web 可视化界面： 安装命令（在 master 节点执行）： 下载配置文件并安装：kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 设置访问端口： 暴露端口：kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard type: ClusterIP 改为 type: NodePort 查看暴露的端口：kubectl get svc -A |grep kubernetes-dashboard 在云服务器安全组中找到找到端口，在安全组放行上面的 32428 端口 现在即可使用 k8s 集群的任意一台公网 ip+端口号即可访问 为啥我只有子节点的 ip 能访问？ 创建访问者账号： vi dash-user.yaml 12345678910111213141516171819#创建访问账号，准备一个yaml文件； vi dash-user.yamlapiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard 执行kubectl apply -f dash-user.yaml 获取 token 令牌： 12#获取访问令牌kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=&quot;&#123;.secrets[0].name&#125;&quot;) -o go-template=&quot;&#123;&#123;.data.token | base64decode&#125;&#125;&quot; 1eyJhbGciOiJSUzI1NiIsImtpZCI6IjR5RUhNVExPeXJORmkyaHFWWDNTMFBoZi1zbmVuRm0tY2ZMc1M3S1dPUDAifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXE3Z25iIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhYTMxYmNkNi03NmM1LTRjNDUtOTM2Mi0xNzEwNjdlM2FjZGMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.pXvfXxXF_PsjeWLhCJWim2Amqus8zi1LwEWMiUNz4ECp7nTKVJ-bV8vZXvHjcPA3m5k3WKpvbW-5rVnGTjBTik_9-eCkY2QFrQlJHo3PEP2ggpbp-IdCd7fW8uBmM0tNeb0nhI4QIqwTJOaZ3xC9aB6w4gl06227VT-5Eq7b3L18NyudIe_wE7wq4oYjJxMwfC52UpXMdeUw5WVyDPK1CvB1l7jydZV8jpCXprcK-aSC_yyyjbDDPZp31olnSIfcIVmUsgKz0Y2HtNNaMBjeenKya9lgUdLijYnq7cvUDznY-D7oftGqApdZcpN4OzbfBvIel-iRJKKdvbcTQT4jxA[r 将生成的令牌代码输入 dashboard： 我在 master 节点生成的 token，这个 dashboard 是 node1 节点，不知道是不是这个原因，反正登录不了。 4. k8s 资源理解 4.1 创建资源方式 yaml 文件 kubectl apply -f xxxx.yaml 命令行 4.2 namespace 名称空间：用来隔离资源，不隔离网络。 命令行操作： kubectl get ns或kubectl get namespace获取当前所有的名称空间 kubectl get pods获取 default 名称空间下的 pods kubectl get pods -A获取所有名称空间下的 pods kubectl create ns xxx创建名称空间 kubectl delete ns xxx删除名称空间（会将该名称空间下的资源一同删除） 配置文件操作： 1234apiVersion: v1kind: Namespacemetadata: name: xxxx kubectl delete -f xxxx.yaml：删除 xxxx.yaml 文件创建的名称空间及资源。 4.3 Pod 运行中的一组容器， Pod 是 kubernetes 中应用的最小单位。 pod 是对 docker 中的多个 container 的封装吗？一个 docker 里有多个 pod 吗？ 是的。是的。【2022.08.27】 命令行操作： kubectl run mynginx --image=nginx：从镜像文件创建并运行 nginx，并命名为 mynginx。不指定名称空间是，默认为 default kubectl describe mynginx：查看 mynginx 这个 pod 的详细运行信息 kubectl delete pod Pod名字：删除 pod kubectl get pod -owide：查看 k8s 给每个 pod 分配的 ip，主要是-owide 参数 kubectl get pod -w：持续监控并在控制台打印变化过程 curl 192.168.169.136集群中的每个 pod，k8s 都会给它分配一个 ip，使用 Pod 的 ip+pod 的端口，就能访问到里面运行容器 kubectl exec -it mynginx -- /bin/bash 配置文件操作： 12345678910111213apiVersion: v1kind: Podmetadata: labels: run: myapp name: myapp# namespace: defaultspec: containers: - image: nginx name: nginx - image: tomcat:8.5.68 # 镜像的名字 name: tomcat # pod容器的名字 kubectl delete -f xxxx.yaml：删除 xxxx.yaml 文件创建的 pod 及资源。 pod 的访问具有以下特点： 集群中的任意一个机器以及任意的应用都能通过 Pod 分配的 ip 来访问这个 Pod 同一个 pod 容器中的不同应用，可以通过本地连接访问，他们共享本 pod 的内存，存储空间。 同一个 pod 内的不同应用，不能安装两个同端口的应用，会报错。 4.4 Deployment 表示一次部署，产生一个或多个 pod。具有自愈、故障转移、扩缩容等多种优势。 kubectl create deployment mytomcat --image=tomcat:8.5.68：创建应用 与kubectl run mynginx --image=nginx区别在于，部署的应用崩溃、被删除，会自动创建新的应用——即自愈能力。 kubectl delete deploy pod名：删除 deployment 部署的应用 kubectl get deploy或 kubectl get deployment：查看 deployment 部署的应用 kubectl create deployment my-dep --image=nginx --replicas=3：多副本部署，3 表示 3 个 pod 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: labels: app: my-dep name: my-depspec: replicas: 3 selector: matchLabels: app: my-dep template: metadata: labels: app: my-dep spec: containers: - image: nginx name: nginx 扩缩容： kubectl scale --replicas=5 deployment/my-dep：部署 5 个名为 my-dp 的 pod（如果原先有 3 个，那就总数变为 5） 当 5 大于既有 pod 数量，称为扩容 当 5 小于既有 pod 数量，称为缩容 kubectl edit deployment my-dep：扩缩容的另一种操作 自愈：比如应用在 docker 中意外停止，k8s 会自动重启——自愈。但是如果机器停机，k8s 在监控周期内（阈值 5 分钟），k8s 会在其他机器上创建一个新的 pod。 此时停机的机器重启会怎么样？ 滚动更新（不停机维护、部署新版本后，旧版本暂时不下线，等到新版本生效了正常运行了再去更新） kubectl set image deployment/my-dep nginx=nginx:1.16.1 --record nginx=nginx:1.16.1是更新的新版本 --record记录本次更新 版本回退： kubectl rollout history deployment/my-dep：查看历史记录 kubectl rollout history deployment/my-dep --revision=2：查看第 2 次的历史记录 kubectl rollout undo deployment/my-dep：回滚(回到上次) kubectl rollout undo deployment/my-dep --to-revision=2： 4.5 工作负载 除了Deployment，k8s 还有 StatefulSet 、DaemonSet 、Job 等 类型资源。我们都称为 工作负载。 有状态应用使用 StatefulSet 部署，无状态应用使用 Deployment 部署。 5. service pod 的服务发现与负载均衡。相当于 pod 的网关入口，让原本需要通过 pod 的 IP 访问的资源，通过 service 的 ip 可以得到。 5.1 Service kubectl expose deployment my-dep --port=8000 --target-port=80：将 my-dep 部署的应用暴露端口，即访问 serv 的 ip+8000，相当于访问 my-dep 部署的应用的 ip+80（上图） 等价于kubectl expose deploy my-dep --port=8000 --target-port=80 --type=ClusterIP 访问返回结果的的负载均衡方式是随机的 IP 的方式可以在集群内（3 个虚拟机上）、pod 容器内访问。【不可以在其他机器上访问】 service 的域名会自动生成：规则为my-dep.default.svc（服务名.所在名称空间.svc） 域名的方式只能在 pod 容器内（进入某一个确定的程序内，如 tomcat）访问，但不能在集群内（3 个虚拟机上）访问，更不能在其他机器上访问 deployment 部署的 pod 的有固定的标签，service 通过这个标签可以发现应用，可以通过kubectl get pod --show -labels查看 12345678910111213apiVersion: v1kind: Servicemetadata: labels: app: my-dep name: my-depspec: selector: app: my-dep ports: - port: 8000 protocol: TCP targetPort: 80 kubectl get service：获取服务的信息，可以查看 my-dep 的 ip service 部署的 ip 容器下线后不会再访问到，但是重新上线（本 pod 扩缩容）就又可以访问到 5.2 service-nodeport kubectl expose deployment my-dep --port=8000 --target-port=80 --type=NodePort：通过任意节点的 ip+外部端口（如 30948）就可以访问到集群内的所有应用。 比较type=NodePort和type=ClusterIP type=ClusterIP暴露的端口只能在集群内访问 type=NodePort暴露的端口能在任意外部访问 1234567891011121314apiVersion: v1kind: Servicemetadata: labels: app: my-dep name: my-depspec: ports: - port: 8000 protocol: TCP targetPort: 80 selector: app: my-dep type: NodePort NodePort 范围在 30000-32767 之间 6. Ingress 6.1 介绍 service 的统一网关入口。service 相当于 pod 的统一网关入口，让原本需要通过 service 的 IP 访问的资源，通过 ingress（集群？）的 ip 可以得到。 ingress 底层是 nginx 做的，k8s 默认不带，需要手动安装。 6.2 安装 执行步骤 1234567891011wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.47.0/deploy/static/provider/baremetal/deploy.yaml#修改镜像vi deploy.yaml#将image的值改为如下值：registry.cn-hangzhou.aliyuncs.com/lfy_k8s_ingress-nginx-controller:v0.46.0# 检查安装的结果kubectl get pod,svc -n ingress-nginx# 最后别忘记在服务器上把svc暴露的端口要放行 本例中没有执行wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.47.0/deploy/static/provider/baremetal/deploy.yaml，直接创建的 yaml 文件，因为怕下载不下来耽搁时间。 yaml 文件内容如下（已修改 image 镜像地址为 lfy 的） 不用点开 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652apiVersion: v1kind: Namespacemetadata: name: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx---# Source: ingress-nginx/templates/controller-serviceaccount.yamlapiVersion: v1kind: ServiceAccountmetadata: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginxautomountServiceAccountToken: true---# Source: ingress-nginx/templates/controller-configmap.yamlapiVersion: v1kind: ConfigMapmetadata: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginxdata:---# Source: ingress-nginx/templates/clusterrole.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm name: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io # k8s 1.14+ resources: - ingresses verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - extensions - networking.k8s.io # k8s 1.14+ resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io # k8s 1.14+ resources: - ingressclasses verbs: - get - list - watch---# Source: ingress-nginx/templates/clusterrolebinding.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm name: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginxsubjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx---# Source: ingress-nginx/templates/controller-role.yamlapiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - namespaces verbs: - get - apiGroups: - &quot;&quot; resources: - configmaps - pods - secrets - endpoints verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io # k8s 1.14+ resources: - ingresses verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io # k8s 1.14+ resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io # k8s 1.14+ resources: - ingressclasses verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - configmaps resourceNames: - ingress-controller-leader-nginx verbs: - get - update - apiGroups: - &quot;&quot; resources: - configmaps verbs: - create - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch---# Source: ingress-nginx/templates/controller-rolebinding.yamlapiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginxsubjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx---# Source: ingress-nginx/templates/controller-service-webhook.yamlapiVersion: v1kind: Servicemetadata: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller-admission namespace: ingress-nginxspec: type: ClusterIP ports: - name: https-webhook port: 443 targetPort: webhook selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller---# Source: ingress-nginx/templates/controller-service.yamlapiVersion: v1kind: Servicemetadata: annotations: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginxspec: type: NodePort ports: - name: http port: 80 protocol: TCP targetPort: http - name: https port: 443 protocol: TCP targetPort: https selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller---# Source: ingress-nginx/templates/controller-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginxspec: selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller revisionHistoryLimit: 10 minReadySeconds: 0 template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller spec: dnsPolicy: ClusterFirst containers: - name: controller image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_ingress-nginx-controller:v0.46.0 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE runAsUser: 101 allowPrivilegeEscalation: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 ports: - name: http containerPort: 80 protocol: TCP - name: https containerPort: 443 protocol: TCP - name: webhook containerPort: 8443 protocol: TCP volumeMounts: - name: webhook-cert mountPath: /usr/local/certificates/ readOnly: true resources: requests: cpu: 100m memory: 90Mi nodeSelector: kubernetes.io/os: linux serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission---# Source: ingress-nginx/templates/admission-webhooks/validating-webhook.yaml# before changing this value, check the required kubernetes version# https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#prerequisitesapiVersion: admissionregistration.k8s.io/v1kind: ValidatingWebhookConfigurationmetadata: labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook name: ingress-nginx-admissionwebhooks: - name: validate.nginx.ingress.kubernetes.io matchPolicy: Equivalent rules: - apiGroups: - networking.k8s.io apiVersions: - v1beta1 operations: - CREATE - UPDATE resources: - ingresses failurePolicy: Fail sideEffects: None admissionReviewVersions: - v1 - v1beta1 clientConfig: service: namespace: ingress-nginx name: ingress-nginx-controller-admission path: /networking/v1beta1/ingresses---# Source: ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yamlapiVersion: v1kind: ServiceAccountmetadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginx---# Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhookrules: - apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations verbs: - get - update---# Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrolebinding.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhookroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx-admissionsubjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx---# Source: ingress-nginx/templates/admission-webhooks/job-patch/role.yamlapiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - secrets verbs: - get - create---# Source: ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yamlapiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx-admissionsubjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx---# Source: ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yamlapiVersion: batch/v1kind: Jobmetadata: name: ingress-nginx-admission-create annotations: helm.sh/hook: pre-install,pre-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginxspec: template: metadata: name: ingress-nginx-admission-create labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: create image: docker.io/jettech/kube-webhook-certgen:v1.5.1 imagePullPolicy: IfNotPresent args: - create - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc - --namespace=$(POD_NAMESPACE) - --secret-name=ingress-nginx-admission env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission securityContext: runAsNonRoot: true runAsUser: 2000---# Source: ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yamlapiVersion: batch/v1kind: Jobmetadata: name: ingress-nginx-admission-patch annotations: helm.sh/hook: post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginxspec: template: metadata: name: ingress-nginx-admission-patch labels: helm.sh/chart: ingress-nginx-3.33.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.47.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: patch image: docker.io/jettech/kube-webhook-certgen:v1.5.1 imagePullPolicy: IfNotPresent args: - patch - --webhook-name=ingress-nginx-admission - --namespace=$(POD_NAMESPACE) - --patch-mutating=false - --secret-name=ingress-nginx-admission - --patch-failure-policy=Fail env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission securityContext: runAsNonRoot: true runAsUser: 2000 ingress 安装完后，产生了两个 service，type 类型一个是 cluster IP（集群内访问），一个是 nodeport（其他机器访问呢）。通过集群的 ip+暴露的端口，能够实现外部请求的统一处理。 http：虚拟机 ip+31405—&gt;请求发给 http：clusterip+80 https：虚拟机 ip+32401—&gt;请求发给 https：clusterip+443 6.3 使用 示例： 部署 pod 和 service： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667apiVersion: apps/v1kind: Deploymentmetadata: name: hello-serverspec: replicas: 2 selector: matchLabels: app: hello-server template: metadata: labels: app: hello-server spec: containers: - name: hello-server image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_hello-server ports: - containerPort: 9000---apiVersion: apps/v1kind: Deploymentmetadata: labels: app: nginx-demo name: nginx-demospec: replicas: 2 selector: matchLabels: app: nginx-demo template: metadata: labels: app: nginx-demo spec: containers: - image: nginx name: nginx---apiVersion: v1kind: Servicemetadata: labels: app: nginx-demo name: nginx-demospec: selector: app: nginx-demo ports: - port: 8000 protocol: TCP targetPort: 80---apiVersion: v1kind: Servicemetadata: labels: app: hello-server name: hello-serverspec: selector: app: hello-server ports: - port: 8000 protocol: TCP targetPort: 9000 结构 pod 结构图示： service 结构图示： 可以从上面的 yaml 文件看到，service 的 type 是默认的是，所以外部不能通过集群的 ip 访问到。 需求： hello.atguigu.com:31405 把请求转给 hello-server 进行处理 demo.atguigu.com:31405 把请求转给 nginx-demo 进行处理 配置 ingress 规则： 123456789101112131415161718192021222324252627apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: ingress-host-barspec: ingressClassName: nginx rules: - host: &quot;hello.atguigu.com&quot; http: paths: - pathType: Prefix path: &quot;/&quot; backend: service: name: hello-server port: number: 8000 - host: &quot;demo.atguigu.com&quot; http: paths: - pathType: Prefix path: &quot;/nginx&quot; # 把请求会转给下面的服务，service的地址也要带/nginx，否则就是404 backend: service: name: nginx-demo port: number: 8000 6.4 路径重写 开启路径重写功能： 1234567891011121314151617181920212223242526272829apiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 name: ingress-host-barspec: ingressClassName: nginx rules: - host: &quot;hello.atguigu.com&quot; http: paths: - pathType: Prefix path: &quot;/&quot; backend: service: name: hello-server port: number: 8000 - host: &quot;demo.atguigu.com&quot; http: paths: - pathType: Prefix path: &quot;/nginx(/|$)(.*)&quot; # 把请求会转给下面的服务，下面的服务一定要能处理这个路径，不能处理就是404 backend: service: name: nginx-demo port: number: 8000 此时 ingress 层的/nginx 请求转发给 service 层时，会去掉 nginx 6.5 流量限制 12345678910111213141516171819apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: ingress-limit-rate annotations: nginx.ingress.kubernetes.io/limit-rps: &quot;1&quot;spec: ingressClassName: nginx rules: - host: &quot;haha.atguigu.com&quot; http: paths: - pathType: Exact path: &quot;/&quot; backend: service: name: nginx-demo port: number: 8000 7. k8s 网络模型 8. 存储抽象 8.1 介绍 由于 k8s 的自愈功能，如果某 pod 数据存储在一台机器上，自愈后 pod 启动在其他机器上，会导致原先的数据丢失。 k8s 为了解决这一问题，提出了存储层的概念，它将所有机器存储的内容统一放到一起，并在所有机器间进行共享，实现了数据的稳定。 k8s 实现存储层的技术有 Glusterfs、NFS、CephFS，本学习中，使用了 NFS。 8.2 NFS 的使用 所有节点安装 nft：yum install -y nfs-utils 主节点暴露目录，并给各个从节点同步数据： 12345678#nfs主节点echo &quot;/nfs/data/ *(insecure,rw,sync,no_root_squash)&quot; &gt; /etc/exportsmkdir -p /nfs/datasystemctl enable rpcbind --now #systemctl enable nfs-server --now # 设置开机启动#配置生效exportfs -r 从节点同步主节点的数据 1234567showmount -e 172.31.0.4 # 在从节点看主节点有哪些目录可以挂载mkdir -p /nfs/data #执行以下命令挂载 创建本地的 /nfs/datamount -t nfs 172.31.0.4:/nfs/data /nfs/data # 执行数据同步# 写入一个测试文件echo &quot;hello nfs server&quot; &gt; /nfs/data/test.txt 8.3 设置 pod 的数据挂载 123456789101112131415161718192021222324252627apiVersion: apps/v1kind: Deploymentmetadata: labels: app: nginx-pv-demo name: nginx-pv-demospec: replicas: 2 selector: matchLabels: app: nginx-pv-demo template: metadata: labels: app: nginx-pv-demo spec: containers: - image: nginx name: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html volumes: - name: html nfs: server: 172.31.0.4 path: /nfs/data/nginx-pv 将 nginx 的/usr/share/nginx/html 挂载到了 nfs 存储层的/nfs/data/nginx-pv 8.4 PV&amp;PVC 挂载 上述存储挂载存在两个问题，一是 pod 删除后，所挂载的数据不会在数据层删除，二是挂载的数据没有进行限制，如果还有其他挂载，存在数据不受控的情况。 PV：持久卷（Persistent Volume），将应用需要持久化的数据保存到指定位置。 PVC：持久卷申明（Persistent Volume Claim），申明需要使用的持久卷规格。 思想：先创建 pv 池（各种存储目录），当 pod 需要存储空间时，提交 pvc（申请书，写明需要多大空间），然后 k8s 从 pv 池分配该大小的空间。当 pod 不使用空间了，k8s 对不适用的数据进行回收。 8.5 示例 创建 pv 池：主节点执行： 首先创建 pv 池指向的目录（静态供应方式，也有动态供应方式，动态供应不需要提前创建好） mkdir -p /nfs/data/01 mkdir -p /nfs/data/02 mkdir -p /nfs/data/03 创建 pv，交给 k8s 管理 1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: v1kind: PersistentVolumemetadata: name: pv01-10mspec: capacity: storage: 10M accessModes: - ReadWriteMany storageClassName: nfs nfs: path: /nfs/data/01 server: 172.31.0.4---apiVersion: v1kind: PersistentVolumemetadata: name: pv02-1gispec: capacity: storage: 1Gi accessModes: - ReadWriteMany storageClassName: nfs nfs: path: /nfs/data/02 server: 172.31.0.4---apiVersion: v1kind: PersistentVolumemetadata: name: pv03-3gispec: capacity: storage: 3Gi accessModes: - ReadWriteMany storageClassName: nfs nfs: path: /nfs/data/03 server: 172.31.0.4 创建 pvc 使用空间——没有指明谁使用，只是告诉 pv 使用了（以需要 200m 的空间演示） 1234567891011kind: PersistentVolumeClaimapiVersion: v1metadata: name: nginx-pvcspec: accessModes: - ReadWriteMany resources: requests: storage: 200Mi storageClassName: nfs 创建 pod 绑定 pvc 1234567891011121314151617181920212223242526apiVersion: apps/v1kind: Deploymentmetadata: labels: app: nginx-deploy-pvc name: nginx-deploy-pvcspec: replicas: 2 selector: matchLabels: app: nginx-deploy-pvc template: metadata: labels: app: nginx-deploy-pvc spec: containers: - image: nginx name: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html volumes: - name: html persistentVolumeClaim: claimName: nginx-pvc 9. ConfigMap 抽取配置文件，同时可以自动更新。以 redis 示例 创建配置集：kubectl create cm redis-conf --from-file=redis.conf 前面的 redis-conf 是 k8s 识别的，后面的是本地新建的 redis.conf，注意，这是路径写法。 创建前需要在本地先创建出 redis.conf，创建完就可以删了 生成的 redis.conf 部分内容如下： 12345678apiVersion: v1data: #data是所有真正的数据 redis.conf: # key：默认是文件名 appendonly yes # value：配置文件的内容kind: ConfigMapmetadata: name: redis-conf namespace: default 创建 pod 时使用配置集： 123456789101112131415161718192021222324252627apiVersion: v1kind: Podmetadata: name: redisspec: containers: - name: redis image: redis command: - redis-server - &quot;/redis-master/redis.conf&quot; #指的是redis容器内部的位置 ports: - containerPort: 6379 volumeMounts: - mountPath: /data name: data - mountPath: /redis-master name: config volumes: - name: data emptyDir: &#123;&#125; - name: config configMap: name: redis-conf items: - key: redis.conf path: redis.conf 10. secret Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像 中来说更加安全和灵活。 1234567891011kubectl create secret docker-registry leifengyang-docker \\--docker-username=leifengyang \\--docker-password=Lfy123456 \\--docker-email=534096094@qq.com##命令格式kubectl create secret docker-registry regcred \\ --docker-server=&lt;你的镜像仓库服务器&gt; \\ --docker-username=&lt;你的用户名&gt; \\ --docker-password=&lt;你的密码&gt; \\ --docker-email=&lt;你的邮箱地址&gt;","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://sk370.github.io/tags/Kubernetes/"}]},{"title":"zookeeper","date":"2022-08-26T02:54:16.000Z","path":"2022/08/26/zookeeper/zookeeper/","text":"Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目。 在这个虚拟机中 第 1 章 zookeeper 介绍 1.1 介绍 1.1.1 概述 Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目。 1.1.2 工作机制 Zookeeper 从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架， 它负责存储和管理大家都关心的数据， 然后接受观察者的注册， 一旦这些数据的状态发生变化， Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些观察者做出相应的反应。 Zookeeper=文件系统+通知机制 1.1.3 特点 Zookeeper：一个领导者（ Leader） ， 多个跟随者（ Follower） 组成的集群。 集群中只要有半数以上节点存活， Zookeeper 集群就能正常服务。 所以 Zookeeper 适合安装奇数台服务器。 全局数据一致：每个 Server 保存一份相同的数据副本， Client 无论连接到哪个 Server， 数据都是一致的。 更新请求顺序执行， 来自同一个 Client 的更新请求按其发送顺序依次执行。 数据更新原子性， 一次数据更新要么成功， 要么失败。 实时性， 在一定时间范围内， Client 能读到最新数据。 1.1.4 数据结构 ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个节点称做一个 ZNode。每一个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过其路径唯一标识。 1.1.4 应用场景 统一命名服务： 在分布式环境下，经常需要对应用/服务进行统一命名，便于识别。例如：IP 不容易记住，而域名容易记住。 统一配置管理： 分布式环境下，配置文件同步非常常见。一般要求一个集群中，所有节点的配置信息是一致的，比如 Kafka 集群。对配置文件修改后， 希望能够快速同步到各个节点上。 配置管理可交由 ZooKeeper 实现。 可将配置信息写入 ZooKeeper 上的一个 Znode。 各个客户端服务器监听这个 Znode。一旦 Znode 中的数据被修改， ZooKeeper 将通知各个客户端服务器。 统一集群管理： 分布式环境中， 实时掌握每个节点的状态是必要的。可根据节点实时状态做出一些调整。 ZooKeeper 可以实现实时监控节点状态变化，可将节点信息写入 ZooKeeper 上的一个 ZNode。 监听这个 ZNode 可获取它的实时状态变化。 服务器动态上下线：客户端能实时洞察到服务器上下线的变化。 软负载均衡：在 Zookeeper 中记录每台服务器的访问数， 让访问数最少的服务器去处理最新的客户端请求 1.2 安装 1.2.1 本地模式安装 安装 jdk：【参见 linxuLinux】 在/usr/local/目录下创建 zookeeper 解压安装包至指定路径：tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /usr/local/zookeeper/ 进入/usr/local/zookeeper/apache-zookeeper-3.5.7-bin/conf目录，拷贝一份 zoo_sample.cfg，命名为 zoo.cfg 在/root 目录下创建 root/data/zkData 文件夹： 修改 zoo.cfg 文件，修改为：dataDir=/root/data/zkData 注意：zookeeper3.5.7 及之后的版本会占用 8080 端口，需要按照如下修改： 进入/usr/local/zookeper/apache-zookeeper-3.5.7-bin/bin 目录，启动 zookeeper：./zkServer.sh start bin 目录下查看启动状态：./zkServer.sh status 进入/usr/local/zookeper/bin 目录，启动 zookeeper 客户端：./zkCli.sh 退出命令：quit","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://sk370.github.io/tags/zookeeper/"}]},{"title":"Nginx","date":"2022-08-25T02:48:14.000Z","path":"2022/08/25/nginx/Nginx/","text":"Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务。因其稳定性、丰富的功能集、简单的配置文件和低系统资源的消耗而闻名。在BSD-like 协议下发行。其特点是占有内存少，并发能力强。 在这个虚拟机中 0. 准备 0.1 安装 Centos 迷你版 极简安装：一路默认。 0.2 网络配置 centos7 中，查看本机 ip 指令变成了ip addr 修改网卡配置，设置为开机即启动网卡：vi /etc/sysconfig/network-scripts/ifcfg-ens33 重启网路服务：systemctl restart network 0.3 配置静态 ip 修改网卡配置：vi /etc/sysconfig/network-scripts/ifcfg-ens33 重启网路服务：systemctl restart network 0.4 解决设置静态 IP 后无法联网问题 无法联网表现： 0.4.1 排查网关设置是否正确 可以看到网关地址没有设置正确。按照 0.3 修改即可。 0.5 安装 Tomcat 服务器 安装 tomcat 服务器，作为应用服务器。 复制文件： 解压缩：tar -zxvf apache-tomcat-8.5.82.tar.gz 将解压目录移动至/usr/local/tomcat 文件夹下：mv apache-tomcat-8.5.82 /usr/local/tomcat mv apache-tomcat-8.5.82 /usr/local/tomcat与mv apache-tomcat-8.5.82/ /usr/local/tomcat的区别 mv apache-tomcat-8.5.82 /usr/local/tomcat会把 apache-tomcat-8.5.82 里面的内容移到 tomcat 文件夹下，apache-tomcat-8.5.82 文件夹删除。 mv apache-tomcat-8.5.82/ /usr/local/tomcat会把 apache-tomcat-8.5.82 整个文件夹放在 tomcat 文件夹下 进入 tomcat/bin 目录，执行./startup.sh 访问测试： 注：这里已经提前关闭里 linux 的防火墙。同时也设置了阿里云域名解析。 1. Nginx 介绍 1.1 版本介绍 Nginx 开源版：http://nginx.org/ 无其他额外功能 Nginx plus 商业版：https://www.nginx.com 什么都有，但收费 openresty：http://openresty.org/cn/ 以 lua 脚本对 nginx 进行功能扩展，免费。 Tengine：http://tengine.taobao.org/ 淘宝开源的以 C 语言对 nginx 进行扩展的项目，集成度非常高。 1.2 Nginx 开源版安装 复制文件： 解压缩：tar -zxvf nginx-1.21.6.tar.gz 进入解压目录：cd nginx-1.21.6 执行安装文件：./configure 安装到指定目录：./configure --prefix=/usr/local/nginx 这里安装到了**/usr/local/nginx**，相当于 c 盘 programfile 解决安装时报的 error： 安装 gcc（c 语言编译器）：yum install -y gcc 安装 perl 库：yum install -y pcre pcre-devel 安装 zlib 库：yum install -y zlib zlib-devel 安装成功提示： 进行编译：make 进行安装：make install 检查是否安装成功： 关闭防火墙：systemctl stop firewalld 本地主机浏览器查看：访问地址是设定的虚拟机的 ip，nginx 端口默认为 80，可以省略。 Nginx 命令： ./nginx 启动 ./nginx -s stop 快速停止 ./nginx -s quit 优雅关闭，在退出前完成已经接受的连接请求 ./nginx -s reload 重新加载配置，而不用重启 nginx 服务器。机制是 nginx 杀死原线程，开启一个新线程。 给 nginx 设置脚本，快速启动 nignx，而不用进入安装目录去执行文件： 创建服务脚本：vi /usr/lib/systemd/system/nginx.service 写入文件： 123456789101112131415[Unit]Description=nginx - web serverAfter=network.target remote-fs.target nss-lookup.target[Service]Type=forkingPIDFile=/usr/local/nginx/logs/nginx.pidExecStartPre=/usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.confExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s stopExecQuit=/usr/local/nginx/sbin/nginx -s quitPrivateTmp=true[Install]WantedBy=multi-user.target 重启系统加载服务：systemctl daemon-reload 停止旧的 nginx 进程。 使用脚本启动新 nginx：systemctl start nginx.service 查看 nginx 状态：systemctl status nginx或者查看进程 利用脚本设置 nginx 开机启动：systemctl enable nginx.service 1.3 防火墙补充知识 关闭防火墙 systemctl stop firewalld.service 禁止防火墙开机启动 systemctl disable firewalld.service 放行端口 firewall-cmd --zone=public --add-port=80/tcp --permanent 重启防火墙 firewall-cmd --reload 查看防火墙状态 systemctl status firewalld 重载防火墙规则 firewall-cmd --reload 查看已配置防火墙规则 firewall-cmd --list-all 限定访问端口与访问 ip firewall-cmd --permanent --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;192.168.44.101&quot; port protocol=&quot;tcp&quot; port=&quot;8080&quot; accept 移除规则 firewall-cmd --permanent --remove-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;192.168.44.101&quot; port port=&quot;8080&quot; protocol=&quot;tcp&quot; accept 1.4 文件结构 默认只有红框的几个文件，带 temp 的文件是运行过程的缓存文件。 1.5 基本运行原理 nginx 启动后会启动 master 主进程和 work 子进程，主进程负责协调子进程，主进程主要进行配置文件校验，子进程负责处理请求。 1.6 nginx 配置文件 1.6.1 最小配置文件 1234567891011121314151617181920212223242526272829worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name localhost; location / &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; &#125; 1.6.2 配置解析 worker_processes 1; 默认为 1，表示开启一个业务进程 worker_connections 1024; 单个业务进程可接受连接数 include mime.types; 引入 http mime 类型。是一个文件，记录了常见的文件类型。 default_type application/octet-stream; 如果 mime 类型没匹配上，默认使用二进制流的方式传输。 sendfile on; 使用 linux 的 sendfile(socket, file, len) 高效网络传输，也就是数据 0 拷贝。 keepalive_timeout 65; server：虚拟主机配置，一个 server 就是一个虚拟主机。 listen：端口号 server_name：主机名 location：匹配路径 root：文件目录 index：默认页文件名称 error_page：报错编码对应页面 2. Nginx 实战 2.0 实战基础 2.0.1 虚拟主机与域名解析 域名、dns、ip 地址的关系： 浏览器、Nginx 与 http 协议的关系： 虚拟主机原理：原本一台服务器只能对应一个站点，通过虚拟主机技术可以虚拟化成多个站点同时对外提供服务 使用 server 配置，与虚拟机文件目录建立对应关系 2.0.2 server_name server_name 多域名配置：使用空格将多个域名分开 域名匹配根据从上到下的顺序，只有一个入口。 如果域名全部都没精确匹配，且域名解析设置了通配符匹配，则按照统配符的匹配规则进行。 server_name 也可以设置通配符匹配，匹配规则同阿里云解析。 支持正则匹配 2.0.3 域名解析技术架构 本机 hosts 文件：本机设置了 127.0.0.1 与www.123.com的域名映射，所以访问www.123.com也能访问到。 阿里云 dns 服务：通过阿里云平台设置 dns 与 ip 的映射，可以测试到可以正常访问。 多用户二级域名 短网址 httpdns 2.1 反向代理 2.1.1 正向代理 正向代理 是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。 2.1.2 反向代理 反向代理（Reverse Proxy）实际运行方式是指以代理服务器来接受 internet 上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给 internet 上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 2.1.3 反向代理配置 在虚拟机中开启了 nginx，在虚拟机中直接访问 localhost、127.0.0.1、192.168.150.110 三个地址均能访问到页面，其中 localhost、127.0.0.1 为虚拟机本机访问，用不到代理。代理服务器就是应用服务器的作用。 实现效果：本地或者网络的方式访问 nginx 服务器的地址，可以访问到其他服务器的内容，而请求地址没有发生变化。 如： 可以设计如下反向代理场景： 反向代理服务器 1：请求 192.168.150.110:80，返回http://www.baidu.com的页面 反向代理服务器 2：请求 192.168.150.110:81，返回http://www.sk370.top:8080 注意：1.由于 192.168.150.110 与www.sk370.top是域名解析，由阿里云服务器完成，不涉及代理服务器，二者本质是来自一个地方的请求。（www.sk370.top的请求会被阿里云服务器解析成ip地址去请求） 2.同时，在本地主机不能使用 localhost:80 或 81 和 127.0.0.1:80 或 81 去访问呢，这两个访问只能在虚拟机中进行。 3.http://www.sk370.top也可以写成http://127.0.0.1或者http😕/localhost:8080或者htttp://192.168.150.100 12345678910111213141516171819202122232425262728worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name localhost; location / &#123; proxy_pass http://www.baidu.com; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 总结：反向代理的关键配置语句为 proxy_pass，地址必须写全协议，注意末尾分号。 2.1.4 多 webapp（服务）配置 1234567891011121314151617181920212223242526272829303132333435http &#123; #此处省略一些基本配置 upstream product_server&#123; server www.helloworld.com:8081; &#125; upstream admin_server&#123; server www.helloworld.com:8082; &#125; upstream finance_server&#123; server www.helloworld.com:8083; &#125; server &#123; #此处省略一些基本配置 #默认指向product的server location / &#123; proxy_pass http://product_server; &#125; location /product/&#123; proxy_pass http://product_server; &#125; location /admin/ &#123; proxy_pass http://admin_server; &#125; location /finance/ &#123; proxy_pass http://finance_server; &#125; &#125;&#125; 2.1.5 https 的反向代理 HTTPS 的固定端口号是 443，不同于 HTTP 的 80 端口 SSL 标准需要引入安全证书，所以在 nginx.conf 中你需要指定证书和它对应的 key 12345678910111213141516171819202122232425#HTTP服务器server &#123; #监听443端口。443为知名端口号，主要用于HTTPS协议 listen 443 ssl; #定义使用www.xx.com访问 server_name www.helloworld.com; #ssl证书文件位置(常见证书文件格式为：crt/pem) ssl_certificate cert.pem; #ssl证书key位置 ssl_certificate_key cert.key; #ssl配置参数（选择性配置） ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; #数字签名，此处使用MD5 ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; root /root; index index.html index.htm; &#125;&#125; 2.2 负载均衡 2.2.1 介绍 2.2.2 简版负载均衡 将服务器 100 作为负载均衡器 2.2.3 配置权重 2.2.4 取消代理作用 2.2.5 backup 作用 2.2.6 其他策略 轮询存在问题：无法保持会话。（解决会话：使用基于客户端的无状态 token 会话保持机制，每次请求进行发送，服务器接收到后进行解密） ip_hash 策略：根据客户端的 ip 转发同一台服务器。移动端 ip 根据基站位置确定，ip 根据位置变化而变化，也无法保持会话。 12345678910upstream bck_testing_01 &#123; ip_hash; # with default weight for all (weight=1) server 192.168.250.220:8080 server 192.168.250.221:8080 server 192.168.250.222:8080&#125; least_conn：将请求转发到当前连接数最少的用户。一般而言，连接少可能是因为权重低（设置低权重是因为配置低），这是与当初设计违背的。同时不支持动态上下线，比如新增了一台服务区，肯定当前连接数为 0，需要其他服务器将连接数转到新上线的服务器，过程耗时。下面是加权最小连接的配置，去掉权重就是普通最小连接的配置。 1234567upstream bck_testing_01 &#123; least_conn; server 192.168.250.220:8080 weight=3 server 192.168.250.221:8080 # default weight=1 server 192.168.250.222:8080 # default weight=1&#125; url_hash：根据用于访问访问的 url 转发请求（定向流量转发），也不能维持会话。适用于访问固定资源不在同一服务器。 fair：根据后端的服务器响应时间转发请求。响应快的服务器接收的请求多，存在流量倾斜风险，且需要第三方插件实现。官方不推荐。 2.3 动静分离 2.3.1 动静分离配置 概念：将静态资源放在 nginx 服务器或其他服务器。 123456789101112131415location / &#123; proxy_pass http://127.0.0.1:8080;&#125;location /css &#123; root /usr/local/nginx/static; index index.html index.htm;&#125;location /images &#123; root /usr/local/nginx/static; index index.html index.htm;&#125;location /js &#123; root /usr/local/nginx/static; index index.html index.htm;&#125; 2.3.2 正则匹配优化 location 数目 location 前缀解释： /通用匹配，所有请求都会匹配到。 =精确匹配，不是以指定模式开头 ~正则匹配，区分大小写 ~*正则匹配，不区分大小写。 ^~非正则匹配，匹配以指定模式开头的 location 匹配顺序： 多个正则 location 直接按书写顺序匹配，成功后就不会继续往后面匹配 普通（非正则）location 会一直往下，直到找到匹配度最高的（最大前缀匹配） 当普通 location 与正则 location 同时存在，如果正则匹配成功,则不会再执行普通匹配 所有类型 location 存在时，“=”匹配 &gt; “^~”匹配 &gt; 正则匹配 &gt; 普通（最大前缀匹配） 2.3.3 alias 与 root 1234location ~*/(css|img|js) &#123; root /usr/local/nginx/static; index index.html index.htm;&#125; 1234location /css &#123; alias /usr/local/nginx/static/css; index index.html index.htm;&#125; root 用来设置根目录，而 alias 在接受请求的时候在路径上不会加上 location。 alias 指定的目录是准确的，即 location 匹配访问的 path 目录下的文件直接是在 alias 目录下查找的； root 指定的目录是 location 匹配访问的 path 目录的上一级目录,这个 path 目录一定要是真实存在 root 指定目录下的； 使用 alias 标签的目录块中不能使用 rewrite 的 break（具体原因不明）；另外，alias 指定的目录后面必须要加上&quot;/&quot;符号！！ alias 虚拟目录配置中，location 匹配的 path 目录如果后面不带&quot;/“，那么访问的 url 地址中这个 path 目录后面加不加”/“不影响访问，访问时它会自动加上”/“； 但是如果 location 匹配的 path 目录后面加上”/“，那么访问的 url 地址中这个 path 目录必须要加上”/“，访问时它不会自动加上”/“。如果不加上”/&quot;，访问就会失败！ root 目录配置中，location 匹配的 path 目录后面带不带&quot;/&quot;，都不会影响访问。 2.4 UrlRewrite 使用意义： 配置短地址 隐藏真实请求地址 设置防火墙隔离 2.4.1 rewrite 语法格式及参数语法 语法：rewrite &lt;regex&gt; &lt;replacement&gt; [flag] flag 标记说明： last #本条规则匹配完成后，继续向下匹配新的 location URI 规则 break #本条规则匹配完成即终止，不再匹配后面的任何规则 redirect #返回 302 临时重定向，浏览器地址会显示跳转后的 URL 地址【用于给爬虫展示】 permanent #返回 301 永久重定向，浏览器地址栏会显示跳转后的 URL 地址 【用于给爬虫展示】 2.4.2 示例 1234location / &#123; rewrite ^/([0-9]+).html$ /index.jsp?pageNum=$1 break; proxy_pass http://127.0.0.1:8080;&#125; 效果：访问http://192.168.150.100:8080/index.jsp?pageNum=1浏览器地址栏显式的是http://192.168.150.100:8080/1.html 2.5 解决跨域 解决跨域问题一般有两种思路： CORS：在后端服务器设置 HTTP 响应头，把你需要允许访问的域名加入 Access-Control-Allow-Origin 中。 jsonp：把后端根据请求，构造 json 数据，并返回，前端用 jsonp 跨域。 举例：www.helloworld.com 网站是由一个前端 app ，一个后端 app 组成的。前端端口号为 9000， 后端端口号为 8080。 前端和后端如果使用 http 进行交互时，请求会被拒绝，因为存在跨域问题。nginx 解决跨域如下： 首先，新建 enable-cors.conf 文件，设置 cors ： 1234567891011121314151617181920212223242526# allow origin listset $ACAO &#x27;*&#x27;;# set single originif ($http_origin ~* (www.helloworld.com)$) &#123; set $ACAO $http_origin;&#125;if ($cors = &quot;trueget&quot;) &#123; add_header &#x27;Access-Control-Allow-Origin&#x27; &quot;$http_origin&quot;; add_header &#x27;Access-Control-Allow-Credentials&#x27; &#x27;true&#x27;; add_header &#x27;Access-Control-Allow-Methods&#x27; &#x27;GET, POST, OPTIONS&#x27;; add_header &#x27;Access-Control-Allow-Headers&#x27; &#x27;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&#x27;;&#125;if ($request_method = &#x27;OPTIONS&#x27;) &#123; set $cors &quot;$&#123;cors&#125;options&quot;;&#125;if ($request_method = &#x27;GET&#x27;) &#123; set $cors &quot;$&#123;cors&#125;get&quot;;&#125;if ($request_method = &#x27;POST&#x27;) &#123; set $cors &quot;$&#123;cors&#125;post&quot;;&#125; 在 nginx.conf 中 include enable-cors.conf 来引入跨域配置： 12345678910111213141516171819202122232425262728# ----------------------------------------------------# 此文件为项目 nginx 配置片段# 可以直接在 nginx config 中 include（推荐）# 或者 copy 到现有 nginx 中，自行配置# www.helloworld.com 域名需配合 dns hosts 进行配置# 其中，api 开启了 cors，需配合本目录下另一份配置文件# ----------------------------------------------------upstream front_server&#123; server www.helloworld.com:9000;&#125;upstream api_server&#123; server www.helloworld.com:8080;&#125;server &#123; listen 80; server_name www.helloworld.com; location ~ ^/api/ &#123; include enable-cors.conf; proxy_pass http://api_server; rewrite &quot;^/api/(.*)$&quot; /$1 break; &#125; location ~ ^/ &#123; proxy_pass http://front_server; &#125;&#125; 2.6 网关 前面给 nginx 服务器配置了反向代理、负载均衡、UrlWrite、跨域等功能，已经不能叫一个普通的服务器了，此时叫做网关服务器。 网关服务器配置需要利用反向代理、防火墙实现。实现效果：外部请求只能发送到网关服务器，tomcat 只接受网关服务器的请求。 指定请求地址与请求端口，让 tomcat 只接收网关的请求：firewall-cmd --permanent --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;192.168.44.101&quot; port protocol=&quot;tcp&quot; port=&quot;8080&quot; accept&quot; 网关配置： 123456789upstream httpds &#123; server 192.168.44.102 weight=8 down; server 192.168.44.103:8080 weight=2; server 192.168.44.104:8080 weight=1 backup;&#125;location / &#123; rewrite ^/([0-9]+).html$ /index.jsp?pageNum=$1 redirect; proxy_pass http://httpds ;&#125;","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://sk370.github.io/tags/Nginx/"}]},{"title":"Docker","date":"2022-08-20T07:15:07.000Z","path":"2022/08/20/docker/docker/","text":"Docker 是基于 Go 语言实现的云开源项目。主要目标是“Build，Ship and Run Any App,Anywhere”，也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的 APP（可以是一个 WEB 应用或数据库应用等等）及其运行环境能够做到“一次镜像，处处运行”。 前提知识（最好情况），不了解可学基础篇：redis+mysql+tomcat+linux+centos 虚拟机位置 第 1 章 Docker 简介 1.1 Doceker 是什么 1.1.1 Docker 出现背景 假定您在开发一个尚硅谷的谷粒商城，您使用的是一台笔记本电脑而且您的开发环境具有特定的配置。其他开发人员身处的环境配置也各有不同。您正在开发的应用依赖于您当前的配置且还要依赖于某些配置文件。此外，您的企业还拥有标准化的测试和生产环境，且具有自身的配置和一系列支持文件。您希望尽可能多在本地模拟这些环境而不产生重新创建服务器环境的开销。请问？ 您要如何确保应用能够在这些环境中运行和通过质量检测？并且在部署过程中不出现令人头疼的版本、配置问题，也无需重新编写代码和进行故障修复？ 答案就是使用容器。Docker 之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案-----系统平滑移植，容器虚拟化技术。 环境配置相当麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。开发人员利用 Docker 可以消除协作编码时“在我的机器上可正常工作”的问题。 之前在服务器配置一个应用的运行环境，要安装各种软件，就拿尚硅谷电商项目的环境来说，Java/RabbitMQ/MySQL/JDBC 驱动包等。安装和配置这些东西有多麻烦就不说了，它还不能跨平台。假如我们是在 Windows 上安装的这些环境，到了 Linux 又得重新装。况且就算不跨操作系统，换另一台同样操作系统的服务器，要移植应用也是非常麻烦的。 传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等(java 为例)。而为了让这些程序可以顺利执行，开发团队也得准备完整的部署文件，让维运团队得以部署应用程式，开发需要清楚的告诉运维部署团队，用的全部配置文件+所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。Docker 的出现使得 Docker 得以打破过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运作。 1.1.2 Docker 理念 Docker 是基于 Go 语言实现的云开源项目。 Docker 的主要目标是“Build，Ship and Run Any App,Anywhere”，也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的 APP（可以是一个 WEB 应用或数据库应用等等）及其运行环境能够做到“一次镜像，处处运行”。 Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用打成镜像，通过镜像成为运行在 Docker 容器上面的实例，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作。 一句话描述：解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术。 1.1.3 虚拟机与容器的对比 虚拟机：virtual machine是带环境安装的一种解决方案。 它可以在一种操作系统里面运行另一种操作系统，比如在Windows10系统里面运行Linux系统CentOS7。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序、操作系统和硬件三者之间的逻辑不变。 主要缺点是：资源占用多、冗余步骤多、启动慢 容器：容器是Linux的另一种虚拟化技术，Linux容器(Linux Containers，缩写为 LXC)。 Linux容器是与系统其他部分隔离开的一系列进程，从另一个镜像运行，并由该镜像提供支持进程所需的全部文件。容器提供的镜像包含了应用的所有依赖项，因而在从开发到测试再到生产的整个过程中，它都具有可移植性和一致性。 Linux容器不是模拟一个完整的操作系统而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。 容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。 二者对比（为什么docker快）： 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程； 容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。 1.2 Docker 的作用 职业发展：coder→programmer→software engineer→DevOps engineer 更快速的应用交付和部署：Docker 化之后只需要交付少量容器镜像文件，在正式生产环境加载镜像并运行即可，应用安装配置在镜像里已经内置好，大大节省部署配置和测试验证时间。 更便捷的升级和扩缩容：当现有的容器不足以支撑业务处理时，可通过镜像运行新的容器进行快速扩容，使应用系统的扩容从原先的天级变成分钟级甚至秒级。 更简单的系统运维：应用容器化运行后，生产环境运行的应用可与开发、测试环境的应用高度一致。当出现程序异常时，也可以通过测试环境的相同容器进行快速定位和修复。 更高效的计算资源利用：Docker 是内核级虚拟化，其不像传统的虚拟化技术一样需要额外的 Hypervisor 支持，所以在一台物理机上可以运行很多个容器实例，可大大提升物理服务器的 CPU 和内存的利用率。 1.3 Docker 安装 Docker 官网：https://www.docker.com/ Docker 仓库：https://hub.docker.com/ 1.3.1 安装要求 也可以在 Windows 直接安装，参见： 目前，CentOS 仅发行版本中的内核支持 Docker。Docker 运行要求系统为 64 位、Linux 系统内核版本为 3.8 以上。可以使用以下命令查看自己版本： 1.3.2 Docker 的三大组成 仓库(repository)：存放镜像文件的地方。 容器(container)：使用镜像创建的运行实例，可以看作简易的 Linux 系统 镜像(image)：容器的模板，一个镜像可以创建多个模板。 1.3.3 Docker 平台架构 Docker是一个 C/S 模式的架构，后端是一个松耦合架构，众多模块各司其职。 1.3.4 安装过程（基于 CentOS） 确认gcc安装情况：gcc --version. yum -y install gcc yum -y install gcc-c++ 设置docker存储库： 安装yun-utils：yum install -y yum-utils 设置docker镜像仓库： 官方：yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 阿里云仓库：yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 【可选】更新 yum 索引（为了以后下载东西方便、快）：yum makecache fast 安装 docker 引擎：yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin 启动 docker：systemctl start docker 检查是否安装成功： docker version docker run hello-world 卸载 docker： systemctl stop docker yum remove docker-ce docker-ce-cli containerd.io rm -rf /var/lib/docker rm -rf /var/lib/containerd 阿里云镜像加速（提升获取 Docker 官方镜像的速度）： 默认情况无 daemon.json 文件 直接执行： 执行结果： 加载配置文件：systemctl daemon-reload 重启 docker：systemctl restart docker 重启完成可以执行docker run hello-world看是否成功 第 2 章 Docker 操作 2.1 Docker 帮助指令 2.1.1 帮助启动类命令 2.1.2 镜像命令 docker images：列出本地主机上的镜像 -a：列出本地所有的镜像（含历史映像层） -q：只显示镜像 ID。 docker search镜像名： --limit: 只列出 N 个镜像，默认 25 个。docker search --limit 5 redis docker pull 镜像名：下载镜像 docker pull 镜像名字[:TAG]：tag 表示版本 docker system df：查看镜像/容器/数据卷所占的空间 docker rmi 镜像名/ID：删除指定镜像 docker rmi -f 镜像ID：强制删除 docker rmi -f 镜像名1:TAG 镜像名2:TAG：删除多个 docker rmi -f $(docker images -qa)：删除全部镜像 【什么是虚悬镜像？】 仓库名、标签都是的镜像，俗称虚悬镜像 dangling image 使用 Dockerfile 创建：12from ubuntuCMD echo &#x27;action is success&#x27; 查看虚悬镜像：docker image ls -f dangling=true 删除虚悬镜像：虚悬镜像没有价值，可以删除docker image prune 2.1.3 容器命令 docker run [OPTIONS] IMAGE [COMMAND] [ARG...]：新建+启动容器 –name=“容器新名字”：为容器指定一个名称，等号可以用空格代替； -d: 后台运行容器并返回容器 ID，也即启动守护式容器(后台运行)； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用；也即启动交互式容器(前台有伪终端，等待交互)； docker run -it centos /bin/bash ：使用镜像 centos:latest 以交互模式启动一个容器,在容器内执行/bin/bash 命令。 -P: 随机端口映射，大写 P -p: 指定端口映射，小写 p docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名：挂载容器卷 docker create -it --name xxx 容器名/id bash：创建容器，不启动。 docker inspect 容器ID：查看容器数据卷在宿主机中的挂载（数据卷映射） docker ps [OPTIONS]：列出当前所有正在运行的容器 -a：列出当前所有正在运行的容器+历史上运行过的 -l：显示最近创建的容器。 -n x：显示最近 x 个创建的容器。 -q：静默模式，只显示容器编号。 exit：退出容器，并停止运行容器 ctrl+p+q：退出容器、容器不停止运行 docker start 容器名/id：启动指定容器 docker restart 容器id：重启指定容器 docker stop 容器名/id：停止容器 docker kill 容器名/id：强制停止容器 docker rm 容器名/id：删除已停止容器 docker rm -f 容器名/id：强制删除 docker rm -f $(docker ps -a -q)：删除多个 docker ps -a -q | xargs docker rm：删除多个 docker logs 容器ID：查看容器日志 docker top 容器ID：查看容器内运行的进程 docker inspect 容器ID：查看容器内部细节 进入正在运行的容器，并以命令行交互 L: docker exec -it 容器ID /bin/bash 新建一个进程连接到原先的容器，exit 退出时不会停止原先容器的运行 docker attach 容器ID 连接到原先运行的容器进程，exit 退出时也把容器进程停掉。 docker cp 容器ID:容器内路径 目的主机路径：将容器内文件拷贝到主机 docker export 容器ID &gt; 文件名.tar：导出容器的内容作为一个 tar 归档文件 cat 文件名.tar | docker import - 镜像用户/镜像名:镜像版本号：将 tar 包中的内容导入到一个新的文件系统，并导入为镜像。 docker save：docker save -o my_ubuntu_v3.tar runoob/ubuntu:v3，将镜像保存成 tar 文件归档 -o指定目录 docker load：导入 docker save 导出的镜像 docker commit：提交容器副本使之成为一个新的镜像 docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器ID 要创建的目标镜像名:[标签名] 2.2 Docker 镜像组成 2.2.1 UnionFS（联合文件系统） Union 文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。 Union 文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 2.2.2 Docker 镜像结构 最底层是引导文件系统 bootfs。这一层与我们典型的 Linux/Unix 系统是一样的，包含 boot 加载器和内核。当 boot 加载完成之后整个内核就都在内存中了，此时内存的使用权已由 bootfs 转交给内核，此时系统也会卸载 bootfs。 rootfs (root file system) ，在 bootfs 之上。包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。rootfs 就是各种不同的操作系统发行版，比如 Ubuntu，Centos 等等。 对于一个精简的 OS，rootfs 可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用 Host 的 kernel，自己只需要提供 rootfs 就行了。 对于不同的 linux 发行版, bootfs 基本是一致的, rootfs 会有差别, 不同的发行版可以公用 bootfs。 当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 所有对容器的改动 - 无论添加、删除、还是修改文件都只会发生在容器层中。只有容器层是可写的，容器层下面的所有镜像层都是只读的。 新镜像是从 base 镜像一层一层叠加生成的。每安装一个软件，就在现有镜像的基础上增加一层 2.2.3 镜像分层好处 镜像分层最大的一个好处就是共享资源，方便复制迁移，就是为了复用。 比如说有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像；同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。 2.3 本地镜像发布到阿里云 镜像发布分为发布到官方仓库、远程个人（组织）公开仓库、本地个人（组织）私有仓库。这里指发布到远程个人（组织）公开仓库、本地个人（组织）私有仓库。 2.3.1 发布到阿里云公共仓库 使用 docker commit 命令生成一个镜像 在阿里云中开启开启镜像仓库。 登录阿里云 Docker Registry：docker login --username=14124*****@qq.com registry.cn-hangzhou.aliyuncs.com 推送到公共仓库： docker tag [ImageId] registry.cn-angzhou.aliyuncs.com/iceriver/cnetos:[镜像版本号] registry.cn-angzhou.aliyuncs.com是我阿里云镜像的地址 iceriver是命名空间 centos是上传的镜像名称 docker push registry.cn-hangzhou.aliyuncs.com/iceriver/cnetos:[镜像版本号] 从公共仓库拉取：docker pull registry.cn-hangzhou.aliyuncs.com/iceriver/cnetos:[镜像版本号] 2.3.2 发布到私有库 Docker Registry 是 docker 官方提供的工具，可以用于构建私有镜像仓库。 下载镜像 Docker Registry：docker pull registry 运行私有库 Registry，相当于本地有个私有 Docker hub docker run -d -p 5000:5000 -v /opt/data/registry:/var/lib/registry --privileged=true registry 默认情况，容器的仓库被创建在容器的/var/lib/registry 目录下，建议自行用容器卷映射，方便于宿主机联调 –restart=always此模式容器会跟 docker daemon 会随着 docker 服务的重启而自动恢复 -v /opt/data/registry:/var/lib/registry把本地磁盘挂载到容器磁盘/var/lib/registry（默认情况下仓库存放镜像于容器内的/var/lib/registry 目录下） –name myregistry 定义容器名 -p 5000:5000端口映射，本地端口 5000 映射到容器端口 5000 –-privileged=true：配置了-v /opt/data/registry:/var/lib/registry ，如果没有关闭安全模块 selinux，容器将没有权限访问本地目录，设置此参数可以给容器加特权。如果没有关闭 selinux 以及没有加上此参数，上传传镜像时可能会报权限错误(OSError: [Errno 13] Permission denied: ‘/var/lib/registry/repositories/library’)或者（Received unexpected HTTP status: 500 Internal Server Error）错误 查看私有仓库有什么容器： curl -XGET http://192.168.111.162:5000/v2/_catalog ip 为本地虚拟机 ip 将新镜像 Tag 修改符合私服规范：docker tag 镜像:Tag Host:Port/Repository:Tag Host 是本地虚拟机 ip 例子： docker tag zzyyubuntu:1.2 192.168.111.162:5000/zzyyubuntu:1.2 修改/etc/docker/daemon.json 文件，添加本地 ip，取消 http 推送限制。 ip 是本地虚拟机 ip docker 默认不允许 http 方式推送镜像，通过配置选项来取消这个限制。====&gt; 修改完后如果不生效，建议重启 docker push 到私服库：docker push 192.168.111.162:5000/zzyyubuntu:1.2 pull 到本地运行：docker pull 192.168.111.162:5000/zzyyubuntu:1.2 2.4 Docker 容器数据卷 2.4.1 数据卷介绍 容器数据卷：将 docker 容器内的数据保存在宿主机的磁盘中。方便数据管理、避免数据丢失。 数据卷可在容器之间共享或重用数据 卷中的更改可以直接实时生效 数据卷中的更改不会包含在镜像的更新中 数据卷的生命周期一直持续到没有容器使用它为止 Docker 挂载主机目录访问如果出现 cannot open directory .: Permission denied 解决办法：在挂载目录后多加一个–privileged=true 参数即可 如果是 CentOS7 安全模块会比之前系统版本加强，不安全的会先禁止，所以目录挂载的情况被默认为不安全的行为，在 SELinux 里面挂载目录被禁止掉了，如果要开启，一般使用–privileged=true 命令，扩大容器的权限解决挂载目录没有权限的问题，也即使用该参数，container 内的 root 拥有真正的 root 权限，否则，container 内的 root 只是外部的一个普通用户权限。 2.4.2 宿主机与容器映射容器卷及读写规则 docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名 默认规则：宿主机与容器读写权限互通 docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:rw 镜像名 限制容器写： docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 2.4.2 数据卷的继承和共享 指容器 2 继承容器 1 的卷规则： 容器 1 完成和宿主机的映射：docker run -it --privileged=true -v /mydocker/u:/tmp --name u1 ubuntu 完成容器 2 继承容器 1 的卷规则：docker run -it --privileged=true --volumes-from 父类 --name u2 ubuntu 容器 1 失去挂载不会影响容器 2 的数据卷 容器 1 重新挂载会自动建立容器 2 的继承 第 3 章 安装实战（单体应用） 3.1 总体步骤 搜索镜像 拉取镜像 查看镜像 启动镜像 停止容器 移除容器 3.2 安装 tomcat 3.2.1 操作过程 搜索镜像：docker search tomcat 拉取镜像：docker pull tomcat 查看镜像：docker images tomcat 启动镜像：docker run -d -p 8080:8080 --name mytomcat tomcat 主机端口:docker 容器端口——访问宿主机的端口，会自动映射到容器端口 停止容器：docker stop tomcat 移除容器：docker rm tomcat 3.2.2 解决启动 404 问题 由于新版 tomcat 对存放资源的文件发生了变化，不再直接放在 webapps 目录下，而是在 webapps.dist 目录下，需要将 webapps.dist 改名为 webapps。 可以通过以下步骤解决： 连接 tomcat：docker exec -it 98610673d3b0 /bin/bash 查看当前文件夹：ls -l 可以进入 webapps 和 webapps.dist 目录下看下： 删除 webapps：rm -r webapps 将 webapps.dist 修改为 webapps：mv webapps.dist webapps 在本地主机/宿主机访问：ip:8080 即可访问 3.2.3 tomcat 安装指南 默认安装最新存在 404 的问题，而且最新版是 tomcat10，开发中实际用到的重要为 8。 所以可以安装指定版本（8）的 tomcat 解决 404 问题: 拉取：docker pull billygoo/tomcat8-jdk8 运行：docker run -d -p 8080:8080 --name mytomcat8 billygoo/tomcat8-jdk8 3.3 安装 mysql 3.3.1 操作过程 安装 5.7 版本： 搜索镜像：docker search mysql:5.7 拉取镜像：docker pull mysql:5.7 查看镜像：docker images mysql:5.7 启动镜像：由于数据的重要性，一般设置容器卷映射进行宿主机备份 123456docker run -p 3306:3306 --privileged=true --name mymysql:5.7 \\-v /docker/mysql/log:/var/log/mysql \\ # 映射到docker/mysql/log-v /docker/mysql/data:/var/lib/mysql \\ # 映射到docker/mysql/data-v /docker/mysql/conf:/etc/mysql \\ # 映射到docker/mysql/conf-e MYSQL_ROOT_PASSWORD=root \\-d mysql:5.7 -e：mysql 环境设置 停止容器：docker stop mymysql 移除容器：docker rm mymysql 3.3.2 docker 容器中的中文乱码 mysql5.7 的字符集默认为拉丁，输入中文会有乱码问题，所以需要修改字符集。 前面启动 mysql5.7 的镜像时，设置了容器中 mysql 配置文件（/etc/mysql）的映射目录为宿主机（/docker/mysql/conf)，创建并编辑my.cnf 12345678910111213[client]default-character-set=utf8[mysql]default-character-set=utf8[mysqld]init_connect=&#x27;SET collation_connection = utf8_unicode_ci&#x27;init_connect=&#x27;SET NAMES utf8&#x27;character-set-server=utf8collation-server=utf8_unicode_ciskip-character-set-client-handshakeskip-name-resolve 注意：这里 utf8 不能写成 utf-8，否则在命令行连接 mysql 会报错 由于修改了配置文件，所以需要重启 mysql docker 里面的 mysql 容器实例查看字符集设置，内容如下：SHOW VARIABLES LIKE 'character%' 3.4 安装 Redis 3.4.1 操作过程 安装 5.7 版本： 搜索镜像：docker search redis:6.0.8 拉取镜像：docker pull redis:6.0.8 查看镜像：docker images redis:6.0.8 启动镜像： 指定本地配置文件启动参见 3.4.2 3.4.2 redis 配置文件指定运行的设置——使用自己的配置文件 由于数据的重要性，一般设置容器卷映射进行宿主机备份，同时指定本地（-d redis redis-server /etc/redis/redis.conf）配置文件启动. 新装的 redis 时没有 redis.conf 文件的，这里以出厂默认为演示 +++ 展开/收起 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044104510461047104810491050105110521053105410551056105710581059106010611062106310641065106610671068106910701071107210731074107510761077107810791080108110821083108410851086108710881089109010911092109310941095109610971098109911001101110211031104110511061107110811091110111111121113111411151116111711181119112011211122112311241125112611271128112911301131113211331134113511361137113811391140114111421143114411451146114711481149115011511152115311541155115611571158115911601161116211631164116511661167116811691170117111721173117411751176117711781179118011811182118311841185118611871188118911901191119211931194119511961197119811991200120112021203120412051206120712081209121012111212121312141215121612171218121912201221122212231224122512261227122812291230123112321233123412351236123712381239124012411242124312441245124612471248124912501251125212531254125512561257125812591260126112621263126412651266126712681269127012711272127312741275127612771278127912801281128212831284128512861287128812891290129112921293129412951296129712981299130013011302130313041305130613071308130913101311131213131314131513161317131813191320132113221323132413251326132713281329133013311332133313341335133613371338133913401341134213431344134513461347134813491350135113521353135413551356135713581359136013611362136313641365136613671368136913701371# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option &quot;include&quot; won&#x27;t be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you&#x27;d better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 loopback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#bind 127.0.0.1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.protected-mode no# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# GENERAL ###################################### By default Redis does not run as a daemon. Use &#x27;yes&#x27; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;&quot;# To enable logging to the system logger, just set &#x27;syslog-enabled&#x27; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &#x27;databases&#x27;-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all &quot;save&quot; lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that&#x27;s set to &#x27;yes&#x27; as it&#x27;s almost always a win.# If you want to save some CPU in the saving child set it to &#x27;no&#x27; but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &#x27;dbfilename&#x27; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of replicas.# 2) Redis replicas are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition replicas automatically try to reconnect to masters# and resynchronize with them.## replicaof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the replica to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the replica request.## masterauth &lt;master-password&gt;# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to &#x27;yes&#x27; (the default) the replica will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if replica-serve-stale-data is set to &#x27;no&#x27; the replica will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,# SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,# COMMAND, POST, HOST: and LATENCY.#replica-serve-stale-data yes# You can configure a replica instance to accept writes or not. Writing against# a replica instance may be useful to store some ephemeral data (because data# written on a replica will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default replicas are read-only.## Note: read only replicas are not designed to be exposed to untrusted clients# on the internet. It&#x27;s just a protection layer against misuse of the instance.# Still a read only replica exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only replicas using &#x27;rename-command&#x27; to shadow all the# administrative / dangerous commands.replica-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New replicas and reconnecting replicas that are not able to continue the replication# process just receiving differences, need to do what is called a &quot;full# synchronization&quot;. An RDB file is transmitted from the master to the replicas.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the replicas incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to replica sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more replicas# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new replicas arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple replicas# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the replicas.## This is important since once the transfer starts, it is not possible to serve# new replicas arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more replicas arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# Replicas send PINGs to server in a predefined interval. It&#x27;s possible to change# this interval with the repl_ping_replica_period option. The default value is 10# seconds.## repl-ping-replica-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of replica.# 2) Master timeout from the point of view of replicas (data, pings).# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-replica-period otherwise a timeout will be detected# every time there is low traffic between the master and the replica.## repl-timeout 60# Disable TCP_NODELAY on the replica socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# replica data when replicas are disconnected for some time, so that when a replica# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the replica missed while# disconnected.## The bigger the replication backlog, the longer the time the replica can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a replica connected.## repl-backlog-size 1mb# After a master has no longer connected replicas for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last replica disconnected, for# the backlog buffer to be freed.## Note that replicas never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly &quot;partially# resynchronize&quot; with the replicas: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The replica priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a replica to promote into a# master if the master is no longer working correctly.## A replica with a low priority number is considered better for promotion, so# for instance if there are three replicas with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the replica as not able to perform the# role of master, so a replica with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.replica-priority 100# It is possible for a master to stop accepting writes if there are less than# N replicas connected, having a lag less or equal than M seconds.## The N replicas need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the replica, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough replicas# are available, to the specified number of seconds.## For example to require at least 3 replicas with a lag &lt;= 10 seconds use:## min-replicas-to-write 3# min-replicas-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-replicas-to-write is set to 0 (feature disabled) and# min-replicas-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# replicas in different ways. For example the &quot;INFO replication&quot; section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover replica instances.# Another place where this info is available is in the output of the# &quot;ROLE&quot; command of a master.## The listed IP and address normally reported by a replica is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the replica to connect with the master.## Port: The port is communicated by the replica during the replication# handshake, and is normally the port that the replica is using to# listen for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the replica may be actually reachable via different IP and port# pairs. The following two options can be used by a replica in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## replica-announce-ip 5.5.5.5# replica-announce-port 1234################################## SECURITY #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands. This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to replicas may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error &#x27;max number of clients reached&#x27;.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can&#x27;t remove keys according to the policy, or if the policy is# set to &#x27;noeviction&#x27;, Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the &#x27;noeviction&#x27; policy).## WARNING: If you have replicas attached to an instance with maxmemory on,# the size of the output buffers needed to feed the replicas are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of replicas is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have replicas attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for replica# output buffers (but this is not needed if the policy is &#x27;noeviction&#x27;).## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key among the ones with an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don&#x27;t evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5# Starting from Redis 5, by default a replica will ignore its maxmemory setting# (unless it is promoted to master after a failover or manually). It means# that the eviction of keys will be just handled by the master, sending the# DEL commands to the replica as keys evict in the master side.## This behavior ensures that masters and replicas stay consistent, and is usually# what you want, however if your replica is writable, or you want the replica to have# a different memory setting, and you are sure all the writes performed to the# replica are idempotent, then you may change this default (but be sure to understand# what you are doing).## Note that since the replica by default does not evict, it may end using more# memory than the one set via maxmemory (there are certain buffers that may# be larger on the replica, or data structures may sometimes take more memory and so# forth). So make sure you monitor your replicas and make sure they have enough# memory to never hit a real out-of-memory condition before the master hits# the configured maxmemory setting.## replica-ignore-maxmemory yes############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It&#x27;s up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a replica performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives:lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don&#x27;t fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that&#x27;s usually the right compromise between# speed and data safety. It&#x27;s up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that&#x27;s snapshotting),# or on the contrary, use &quot;always&quot; that&#x27;s very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&#x27;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can&#x27;t happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;# string and loads the prefixed RDB file, and continues loading the AOF# tail.aof-use-rdb-preamble yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn&#x27;t want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################ Normal Redis instances can&#x27;t be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A replica of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a replica to actually have an exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple replicas able to failover, they exchange messages# in order to try to give an advantage to the replica with the best# replication offset (more data from the master processed).# Replicas will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single replica computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the &quot;connected&quot; state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the replica will not try to failover# at all.## The point &quot;2&quot; can be tuned by user. Specifically a replica will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * replica-validity-factor) + repl-ping-replica-period## So for example if node-timeout is 30 seconds, and the replica-validity-factor# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the# replica will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large replica-validity-factor may allow replicas with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a replica at all.## For maximum availability, it is possible to set the replica-validity-factor# to a value of 0, which means, that replicas will always try to failover the# master regardless of the last time they interacted with the master.# (However they&#x27;ll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-replica-validity-factor 10# Cluster replicas are able to migrate to orphaned masters, that are masters# that are left without working replicas. This improves the cluster ability# to resist to failures as otherwise an orphaned master can&#x27;t be failed over# in case of failure if it has no working replicas.## Replicas migrate to orphaned masters only if there are still at least a# given number of other working replicas for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica# will migrate only if there is at least 1 other working replica for its master# and so forth. It usually reflects the number of replicas you want for every# master in your cluster.## Default is 1 (replicas migrate only if their masters remain with at least# one replica). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents replicas from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-replica-no-failover no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don&#x27;t have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# A Alias for g$lshzxe, so that the &quot;AKE&quot; string means all the events.## The &quot;notify-keyspace-events&quot; takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:# notify-keyspace-events Ex## By default all notifications are disabled because most users don&#x27;t need# this feature and the feature has some overhead. Note that if you don&#x27;t# specify at least one of K or E, no events will be delivered.#notify-keyspace-events &quot;&quot;############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don&#x27;t start compressing until after 1 node into the list,# going from either the head or tail&quot;# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don&#x27;t compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entires limit by setting max-bytes to 0 and max-entries to the desired# value.stream-node-max-bytes 4096stream-node-max-entries 100# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don&#x27;t have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can&#x27;t consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# replica -&gt; replica clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don&#x27;t receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and replica clients, since# subscribers and replicas receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here.## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporary raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used as# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# When redis saves RDB file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it&#x27;s maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested# even in production and manually tested by multiple engineers for some# time.## What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an &quot;hot&quot; way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don&#x27;t have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command &quot;CONFIG SET activedefrag yes&quot;.## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag yes# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage# active-defrag-cycle-min 5# Maximal effort for defrag in CPU percentage# active-defrag-cycle-max 75# Maximum number of set/hash/zset/list fields that will be processed from# the main dictionary scan# active-defrag-max-scan-fields 1000 +++ 主要修改内容如下： 启动 redis： 1234docker run -p 6379:6379 --privileged=true --name myredis \\-v /mydata/redis/data:/data \\-v /mydata/redis/conf:/etc/redis/redis.conf \\-d redis:6.0.8 redis-server /etc/redis/redis.conf 连接 redis：docker exec -it redisID /bin/bash 使用 redis：redis-cli 停止容器：docker stop myredis 移除容器：docker rm myredis 第 4 章 安装实战（集群） 4.1 MySql 主从复制 4.2 Redis 集群 第 5 章 Dockerfile 5.1 简介 5.1.1 基本概念 Dockerfile 是用来构建 Docker 镜像的文本文件，是由一条条构建镜像所需的指令和参数构成的脚本。 构建步骤： 编写 Dockerfile 文件 docker build 命令构建镜像 docker run 依镜像运行容器实例 5.1.2 构建过程解析 docker 从基础镜像运行一个容器 执行一条指令并对容器作出修改 执行类似 docker commit 的操作提交一个新的镜像层 docker 再基于刚提交的镜像运行一个新容器 执行 dockerfile 中的下一条指令直到所有指令都执行完成 5.1.3 比较 Dockerfile、Docker 镜像与 Docker 容器 Dockerfile 定义了进程需要的一切东西。Dockerfile 涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计 namespace 的权限控制)等等。 Docker 镜像，在用 Dockerfile 定义一个文件之后，docker build 时会产生一个 Docker 镜像，当运行 Docker 镜像时会真正开始提供服务。 Docker 容器，容器是直接提供服务的。 从应用软件的角度来看，Dockerfile、Docker 镜像与 Docker 容器分别代表软件的三个不同阶段： Dockerfile 是软件的原材料 Docker 镜像是软件的交付品 Docker 容器则可以认为是软件镜像的运行态，也即依照镜像运行的容器实例 Dockerfile 面向开发，Docker 镜像成为交付标准，Docker 容器则涉及部署与运维，三者缺一不可，合力充当 Docker 体系的基石。 5.2 保留字指令 5.2.1 指令介绍 FROM：基础镜像，当前新镜像是基于哪个镜像的，指定一个已经存在的镜像作为模板，第一条必须是 from。 MAINTAINER：镜像维护者的姓名和邮箱地址 RUN：容器构建时（docker build）需要运行的命令 shell 格式：RUN yum -y install vim exec 格式： EXPOSE：当前容器对外暴露出的端口 WORKDIR：从宿主机登录容器后的目录 USER：指定该镜像以什么样的用户去执行，如果不指定，默认是 root ENV：用来在构建镜像过程中设置环境变量 ENV MY_PATH /usr/mytest：定义了名为 MY_PATH 的变量 ADD：将宿主机目录下的文件拷贝进镜像且会自动处理 URL 和解压 tar 压缩包 COPY：拷贝文件和目录到镜像中。 COPY src dest： COPY [“src”, “dest”] &lt;dest 目标路径&gt;：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。 VOLUME：容器数据卷，用于数据保存和持久化工作 CMD：指定容器启动后的要干的事情，支持 shell 和 exec 两种格式 Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换 CMD 是在 docker run 时运行。 RUN 是在 docker build 时运行。 ENTRYPOINT：指定一个容器启动时要运行的命令 类似于 CMD 指令，但是 ENTRYPOINT 不会被 docker run 后面的命令覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序。 实际运行：nginx -c /etc/nginx/new.conf 优点：在执行 docker run 的时候可以指定 ENTRYPOINT 运行所需的参数。 注意：如果 Dockerfile 中如果存在多个 ENTRYPOINT 指令，仅最后一个生效。 5.2.2 Dockerfile 文件解析 每条保留字指令都必须为大写字母且后面要跟随至少一个参数 指令按照从上到下，顺序执行 #表示注释 每条指令都会创建一个新的镜像层并对镜像进行提交 5.3 案例 5.3.1 打包应用 5.3.2 打包微服务 使用 maven 工具执行 mvn-package 命令，将微服务打成 jar 包 编写 Dockerfile 文件 12345678910111213# 基础镜像使用javaFROM java:8# 作者MAINTAINER iceriver# VOLUME 在主机/docker/目录下创建了一个临时文件并链接到容器的/tmpVOLUME /tmp# 将jar包添加到容器中并更名为zzyy_docker.jarADD docker_boot-0.0.1-SNAPSHOT.jar my-test.jar# 运行jar包RUN bash -c &#x27;touch /my-test.jar&#x27;ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/my-test.jar&quot;]#暴露6001端口作为微服务EXPOSE 6001 第 6 章 Docker 网络 6.1 介绍 第 7 章 Docker-compose 容器编排","tags":[{"name":"Docker","slug":"Docker","permalink":"https://sk370.github.io/tags/Docker/"}]},{"title":"Dubbo","date":"2022-08-17T23:58:21.000Z","path":"2022/08/18/dubbo/Dubbo/","text":"《分布式系统原理与范型》定义：“分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像单个相关系统”。分布式系统（distributed system）是建立在网络之上的软件系统。分布式系统是一个大规模网站的治理系统。 第 1 章 分布式基础 1.1 分布式系统 《分布式系统原理与范型》定义：“分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像单个相关系统”。 分布式系统（distributed system）是建立在网络之上的软件系统。 分布式系统是一个大规模网站的治理系统。 1.2 发展演变 1.2.1 单一架构应用 网站流量小，一个应用将所有功能都部署在一起，减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 缺点： 性能扩展比较难 协同开发问题 不利于升级维护 1.2.2 垂直架构应用 将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的 Web 框架(MVC)是关键。通过切分业务来实现各个模块独立部署，降低了维护和部署的难度，团队各司其职更易管理，性能扩展也更方便，更有针对性。 缺点：公用模块无法重复利用，开发性的浪费 1.2.3 分布式服务架构 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的 分布式服务框架(RPC) 是关键。 1.2.4 流动计算架构 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心[SOA]( Service Oriented Architecture)是关键。 1.3 RPC 1.3.1 RPC 思想 RPC【Remote Procedure Call】是指远程过程调用，是一种进程间通信方式，他是一种技术的思想，而不是规范。 它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。 RPC 两个核心模块：通讯，序列化。 RPC 框架有很多：dubbo、gRPC、Thrift、HSF（High Speed Service Framework) 1.3.2 基本原理 1.3.3 举例 第 2 章 dubbo 2.1 dubbo 核心概念 2.1.1 简介 Apache Dubbo (incubating) |ˈdʌbəʊ| 是一款高性能、轻量级的开源 Java RPC 框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 官网：https://dubbo.apache.org/zh/index.html Dubbo 架构具有以下几个特点，分别是连通性、健壮性、伸缩性、以及向未来架构的升级性。 2.1.2 基本概念 节点角色说明： 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 2.2 dubbo 环境搭建 2.2.1 安装 zookeeper 注册中心 也可以采用其他注册中心。 解决报错问题：进入 conf 文件夹，复制一份 zoo_sample.cfg，改名为 zoo.cfg 修改 zookeeper 配置文件，本地保存临时文件：新建 data 文件夹，修改配置文件中 dataDir 指向的路径 zkServer.cmd：启动注册中心 zkCli.cmd：连接注册中心（作为管理客户端使用） ls /services：查看有多少服务连接到 zookeeper 2.2.2 安装 dubbo-admin 管理控制台 dubbo 本身并不是一个服务软件。它其实就是一个 jar 包能够帮你的 java 程序连接到 zookeeper，并利用 zookeeper 消费、提供服务。 但是为了让用户更好的管理监控众多的 dubbo 服务，官方提供了一个可视化的监控程序，不过这个监控即使不装也不影响使用。 进入 dubbo-admin 的 application.properties 文件中，查看 zookeeper 地址是否正确。 在 dubbo-admin 文件下执行 mvn clean package -Dmaven.test.skip=true ，生成 jar 包 为了便于操作：进入 target 目录，拷贝一份打包好的 dubbo-admin-0.0.1-SNAPSHOT.jar 放到喜欢的位置，在命令行终端输入java -jar dubbo-admin-0.0.1-SNAPSHOT.jar，账户名密码均为 root。 注意：启动 dubbo-admin 时，需要启动 zookeeper 2.2.3 安装 dubbo-monitor-simple 监控中心 主要用来统计服务的调用次数和调用时间，服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心，监控中心则使用数据绘制图表来显示。 打开 target 目录下的 dubbo-monitor-simple-2.0.0-assembly.tar.gz 压缩包，检查注册中心地址 启动监控中心，访问http://localhost:8080/ 2.3 使用案例 2.3.1 需求描述 某个电商系统，订单服务需要调用用户服务获取某个用户的所有地址； 模块 功能 订单服务 web 模块 创建订单等 用户服务 service 模块 查询用户地址等 测试预期结果： 订单服务 web 模块在 A 服务器，用户服务模块在 B 服务器，A 可以远程调用 B 的功能。 2.3.2 监控中心配置 安装了 dubbo-monitor-simple 监控中心后，要想让服务提供者和消费者能够加入到监控中心，需要进行相关配置。 2.3.3 SpringBoot 整合 dubbo 第 3 章 Dubbo 配置 3.1 配置文件 3.1.1 标签配置 配置官方文档： 3.1.2 属性配置 dubbo:consumer dubbo:reference 3.1.3 属性配置覆盖策略 方法级优先，接口级次之，全局配置再次之。 如果级别一样，则消费方优先，提供方次之。 3.1.4 超时与重试 场景模拟：消费服务调用提供者服务超时后，可以通过retries属性指定重试次数，比如设置为 3，则重试机制如下： 第 1 次连接默认提供者服务，超时后进行第 2 次连接。 两次连接失败后，更换提供者服务进行第 3 次连接。 三次连接失败后，继续更换提供者服务进行第 4 次连接。 幂等（多次操作结果一样，如查询）一般需要设置重试 不幂等（多次操作结果不一样，如插入），一般不设置重试。 3.1.5 多版本配置 场景模拟：提供者服务有多个版本（使用dubbo:service version指定），消费者服务可以通过dubbo:reference version指定与提供者服务相同的版本进行连接。 适合生产环境下服务版本升级。 3.1.6 （本地）服务存根 远程服务后，客户端通常只剩下接口，而实现全在服务器端，但提供方有些时候想在客户端也执行部分逻辑。 一般约定 Stub 类配置在接口服务中，xml 设置配置在提供者服务中。具体如下： 存根类：设置在接口服务中，创建 Stub 实现。 123456789101112131415161718public class BarServiceStub implements BarService &#123; private final BarService barService; // 构造函数传入真正的远程代理对象 public BarServiceStub(BarService barService)&#123; this.barService = barService; &#125; public String sayHello(String name) &#123; // 此代码在客户端执行, 你可以在客户端做ThreadLocal本地缓存，或预先验证参数是否合法，等等 try &#123; return barService.sayHello(name); &#125; catch (Exception e) &#123; // 你可以容错，可以做任何AOP拦截事项 return &quot;容错数据&quot;; &#125; &#125;&#125; xml 配置：&lt;dubbo:consumer interface=&quot;com.foo.BarService&quot; stub=&quot;true&quot;/&gt;或&lt;dubbo:consumer interface=&quot;com.foo.BarService&quot; stub=&quot;com.foo.BarServiceStub&quot;/&gt; 3.2 dubbo:properties 属性加载顺序 优先级从上到下依次降低：虚拟机参数 →springboot 配置文件参数（xml 和 properties）→ 公共配置（dubbo.properties） 3.3 springboot 与 dubbo 整合的 3 中方式 3.3.1 方式一 导入 dubbo-start 在 application.properties 文件中配置属性 springboot 主程序使用@EanbleDubbo 注解开启 dubbo 注解功能 @Service 注解及参数暴露服务 @Reference 注解及参数引用服务 3.3.2 方式二 导入 dubbo-start 在原始 xml 文件中配置属性 xml 文件中暴露服务及引用服务 springboot 主程序使用@ImportResource(locations=“classpath:xxx.xml”)引入配置文件 3.3.3 方式三 导入 dubbo-start 创建 spring 配置类 配置类中设置相关的配置属性 springboot 主程序使用@EanbleDubbo 注解或@DubboComponentScan 开启 dubbo 注解功能，同时指定扫描配置类的路径 @Service 注解及参数暴露服务 @Reference 注解及参数引用服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Configurationpublic class MyDubboConfig &#123; @Bean public ApplicationConfig applicationConfig() &#123; ApplicationConfig applicationConfig = new ApplicationConfig(); applicationConfig.setName(&quot;boot-user-service-provider&quot;); return applicationConfig; &#125; //&lt;dubbo:registry protocol=&quot;zookeeper&quot; address=&quot;127.0.0.1:2181&quot;&gt;&lt;/dubbo:registry&gt; @Bean public RegistryConfig registryConfig() &#123; RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setProtocol(&quot;zookeeper&quot;); registryConfig.setAddress(&quot;127.0.0.1:2181&quot;); return registryConfig; &#125; //&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20882&quot;&gt;&lt;/dubbo:protocol&gt; @Bean public ProtocolConfig protocolConfig() &#123; ProtocolConfig protocolConfig = new ProtocolConfig(); protocolConfig.setName(&quot;dubbo&quot;); protocolConfig.setPort(20882); return protocolConfig; &#125; /** *&lt;dubbo:service interface=&quot;com.atguigu.gmall.service.UserService&quot; ref=&quot;userServiceImpl01&quot; timeout=&quot;1000&quot; version=&quot;1.0.0&quot;&gt; &lt;dubbo:method name=&quot;getUserAddressList&quot; timeout=&quot;1000&quot;&gt;&lt;/dubbo:method&gt; &lt;/dubbo:service&gt; */ @Bean public ServiceConfig&lt;UserService&gt; userServiceConfig(UserService userService)&#123; ServiceConfig&lt;UserService&gt; serviceConfig = new ServiceConfig&lt;&gt;(); serviceConfig.setInterface(UserService.class); serviceConfig.setRef(userService); serviceConfig.setVersion(&quot;1.0.0&quot;); //配置每一个method的信息 MethodConfig methodConfig = new MethodConfig(); methodConfig.setName(&quot;getUserAddressList&quot;); methodConfig.setTimeout(1000); //将method的设置关联到service配置中 List&lt;MethodConfig&gt; methods = new ArrayList&lt;&gt;(); methods.add(methodConfig); serviceConfig.setMethods(methods); //ProviderConfig //MonitorConfig return serviceConfig; &#125;&#125; 第 4 章 高可用（场景处理） 4.1 zookeeper 宕机与 dubbo 直连 现象：zookeeper 注册中心宕机，还可以消费 dubbo 暴露的服务。 原因：健壮性 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 高可用：通过设计，减少系统不能提供服务的时间。 dubbo 直连：@Reference(url=&quot;ip&quot;) 4.2 集群下 dubbo 负载均衡配置 在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。 4.2.1 Random LoadBalance 策略 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 4.2.2 RoundRobin LoadBalance 轮循，按公约后的权重设置轮循比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 4.2.3 LeastActive LoadBalance 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 4.2.4 ConsistentHash LoadBalance 一致性 Hash，相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 算法参见：http://en.wikipedia.org/wiki/Consistent_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.arguments&quot; value=&quot;0,1&quot; /&gt; 缺省用 160 份虚拟节点，如果要修改，请配置&lt;dubbo:parameter key=&quot;hash.nodes&quot; value=&quot;320&quot; /&gt; 4.3 服务降级 当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作。 可以通过服务降级功能临时屏蔽某个出错的非关键服务，并定义降级后的返回策略。 dubbo 支持两种降级策略： 123RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf(&quot;zookeeper://10.20.153.10:2181&quot;));registry.register(URL.valueOf(&quot;override://0.0.0.0/com.foo.BarService?category=configurators&amp;dynamic=false&amp;application=foo&amp;mock=force:return+null&quot;)); mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 或在 dubbo-admin 管理控制台将消费者进行屏蔽 还可以改为 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。 或在 dubbo-admin 管理控制台将消费者进行容错设置 4.4 集群容错 4.4.1 集群容错概念 各节点关系： 这里的 Invoker是 Provider的一个可调用 Service的抽象，Invoker封装了 Provider地址及 Service 接口信息 Directory代表多个 Invoker，可以把它看成List&lt;Invoker&gt; ，但与 List不同的是，它的值可能是动态变化的，比如注册中心推送变更 Cluster将 Directory中的多个 Invoker伪装成一个 Invoker，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个 Router负责从多个 Invoker中按路由规则选出子集，比如读写分离，应用隔离等 LoadBalance负责从多个 Invoker中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选。 4.4.2 集群容错模式 在集群调用失败时，Dubbo 提供了多种容错方案，缺省为 failover 重试。 Failover Cluster 失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=“2” 来设置重试次数(不含第一次)。 重试次数配置如下：&lt;dubbo:service retries=&quot;2&quot; /&gt;或&lt;dubbo:reference retries=&quot;2&quot; /&gt;或 Failfast Cluster 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe Cluster 失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback Cluster 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking Cluster 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=“2” 来设置最大并行数。 Broadcast Cluster 广播调用所有提供者，逐个调用，任意一台报错则报错 [2]。通常用于通知所有提供者更新缓存或日志等本地资源信息。 4.4.3 集群模式配置 按照以下示例在服务提供方和消费方配置集群模式&lt;dubbo:service cluster=&quot;failsafe&quot; /&gt;或&lt;dubbo:reference cluster=&quot;failsafe&quot; /&gt; 4.4.4 整合 hystrix 容错 Hystrix 旨在通过控制那些访问远程系统、服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。 Hystrix 具备拥有回退机制和断路器功能的线程和信号隔离，请求缓存和请求打包，以及监控和配置等功能。 提供者和消费者：导入依赖：spring-cloud-starter-netflix-hystrix alt+/可以在 eclipse 中快速导入： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;version&gt;1.4.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; 提供者和消费者：配置 spring-cloud-starter-netflix-hystrix 然后在 Application 类上增加@EnableHystrix 来启用 hystrix starter： 123@SpringBootApplication@EnableHystrixpublic class ProviderApplication &#123; 配置 Provider 端：在 Dubbo 的 Provider 上增加@HystrixCommand 配置，这样子调用就会经过 Hystrix 代理 123456789101112@Service(version = &quot;1.0.0&quot;)public class HelloServiceImpl implements HelloService &#123; @HystrixCommand(commandProperties = &#123; @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;), @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value = &quot;2000&quot;) &#125;) @Override public String sayHello(String name) &#123; // System.out.println(&quot;async provider received: &quot; + name); // return &quot;annotation: hello, &quot; + name; throw new RuntimeException(&quot;Exception to show hystrix enabled.&quot;); &#125;&#125; 配置 Consumer 端：对于 Consumer 端，则可以增加一层 method 调用，并在 method 上配置@HystrixCommand。当调用出错时，会走到 fallbackMethod = &quot;reliable&quot;的调用里。 12345678910@Reference(version = &quot;1.0.0&quot;)private HelloService demoService;@HystrixCommand(fallbackMethod = &quot;reliable&quot;)public String doSayHello(String name) &#123; return demoService.sayHello(name);&#125;public String reliable(String name) &#123; return &quot;hystrix fallback value&quot;;&#125; 第 5 章 dubbo 原理 5.1 RPC 原理 一次完整的 RPC 调用流程（同步调用，异步另说）如下： 服务消费方（client）调用以本地调用方式调用服务； client stub 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； client stub 找到服务地址，并将消息发送到服务端； server stub 收到消息后进行解码； server stub 根据解码结果调用本地的服务； 本地服务执行并将结果返回给 server stub； server stub 将返回结果打包成消息并发送至消费方； client stub 接收到消息，并进行解码； 服务消费方得到最终结果。 RPC 框架的目标就是要 2~8 这些步骤都封装起来，这些细节对用户来说是透明的，不可见的。实际操作 dubbo 是第 1 步和第 9 步。 5.2 netty 通信原理 Netty 是一个异步事件驱动的网络应用程序框架，用于快速开发可维护的高性能协议服务器和客户端。它极大地简化并简化了 TCP 和 UDP 套接字服务器等网络编程。 BIO(Blocking IO)：服务器接到一个请求即开启一个线程，不能处理大量请求。 NIO (Non-Blocking IO)：Selector 一般称为选择器，也可以翻译为多路复用器，Connect（连接就绪）、Accept（接受就绪）、Read（读就绪）、Write（写就绪）。Selector 监听多个通道，事件就绪后才开启线程。 Netty 基本原理： 5.3 Dubbo 原理 5.3.1 dubbo 框架设计 图例说明： 图中左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。 图中从下至上分为十层，各层均为单向依赖，右边的黑色箭头代表层之间的依赖关系，每一层都可以剥离上层被复用，其中，Service 和 Config 层为 API，其它各层均为 SPI。 图中绿色小块的为扩展接口，蓝色小块为实现类，图中只显示用于关联各层的实现类。 图中蓝色虚线为初始化过程，即启动时组装链，红色实线为方法调用过程，即运行时调时链，紫色三角箭头为继承，可以把子类看作父类的同一个节点，线上的文字为调用的方法。 各层说明： config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 5.3.2 启动解析、加载配置信息流程 dubbo 的 xml 配置文件相当于是 spring 的一个配置类，因此可以找到 spring 中的标签解析器 BeanDefinitionParser 的实现子类DubboBeanDefinitionParser 查看如何解析 dubbo 的配置文件。 在 DubboBeanDefinitionParser 类中 parse()方法具体实现标签、属性、值的解析。 parse()方法中每个标签都有一个对应的 bean.class，在 DubboBeanDefinitionParser 构造器方法中对 bean.class 进行了初始化，在调用过程中，执行了 DubboNamespaceHandler 类中的 init()方法，对各个标签进行创建： 其中，除 service 和 reference 标签，其他标签创建的是配置类，这两个创建的是 bean。总体执行流程如下： 5.3.3 服务暴露过程 即 servicebean 的创建过程： 5.3.4 服务引用流程 即 referencebean 的创建过程： 5.3.5 服务调用流程 总结 dubbo 主要解决了 rpc（远程过程调用问题），不能解决分布式的所有问题，在 springcloud 中，集成了各种分布式问题的解决方案。 dubbo 的分布式原理与其他分布式框架底层思想一致、方式也基本一致（万变不离其宗）。 本文介绍了 dubbo 的主要使用特性/配置/功能，不常见的没有介绍。","tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://sk370.github.io/tags/Dubbo/"},{"name":"分布式框架","slug":"分布式框架","permalink":"https://sk370.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%A1%86%E6%9E%B6/"}]},{"title":"Redis","date":"2022-08-16T07:55:31.000Z","path":"2022/08/16/redis/Redis/","text":"Redis（Remote Dictionary Server )，即远程字典服务，是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 尚硅谷_Redis6 课件.docx 在这个虚拟机中 第 1 章 数据库 1.1 技术发展 1、解决功能性的问题：Java、Jsp、RDBMS、Tomcat、HTML、Linux、JDBC、SVN 2、解决扩展性的问题：Struts、Spring、SpringMVC、Hibernate、Mybatis 3、解决性能的问题：NoSQL、Java 线程、Hadoop、Nginx、MQ、ElasticSearch - NoSQL：解决 cpu 及内存压力、解决 IO 压力 1.2 NoSQL 数据库 1.2.1 概述 概念：NoSQL(NoSQL = Not Only SQL )，意即“不仅仅是 SQL”，泛指非关系型的数据库。 存储方式：NoSQL 不依赖业务逻辑方式存储，而以简单的 key-value 模式存储。因此大大的增加了数据库的扩展能力。 特点： 不遵循 SQL 标准。 不支持 ACID。 远超于 SQL 的性能。 适用场景： 对数据高并发的读写 海量数据的读写 对数据高可扩展性 不适用场景： 需要事务支持 基于 sql 的结构化查询存储，处理复杂的关系,需要即席查询——自定义查询。 1.2.2 各类 NoSQL 数据库对比 Memcache： 很早出现的 NoSql 数据库 数据都在内存中，一般不持久化 支持简单的 key-value 模式，支持类型单一 一般是作为缓存数据库辅助持久化的数据库 Redis： 几乎覆盖了 Memcached 的绝大部分功能 数据都在内存中，支持持久化，主要用作备份恢复 除了支持简单的 key-value 模式，还支持多种数据结构的存储，比如 list、set、hash、zset 等。 一般是作为缓存数据库辅助持久化的数据库 MongoDB： 高性能、开源、模式自由(schema free)的文档型数据库 数据都在内存中， 如果内存不足，把不常用的数据保存到硬盘 虽然是 key-value 模式，但是对 value（尤其是json）提供了丰富的查询功能 支持二进制数据及大型对象 可以根据数据的特点替代 RDBMS，成为独立的数据库。或者配合 RDBMS，存储特定的数据。 1.3 行式数据库 1.4 列式数据库 1.4.1 HBase HBase 是 Hadoop 项目中的数据库。它用于需要对大量的数据进行随机、实时的读写操作的场景中。 HBase 的目标就是处理数据量非常庞大的表，可以用普通的计算机处理超过 10 亿行数据，还可处理有数百万列元素的数据表。 1.4.2 Cassandra[kəˈsændrə] Apache Cassandra 是一款免费的开源 NoSQL 数据库，其设计目的在于管理由大量商用服务器构建起来的庞大集群上的海量数据集(数据量通常达到 PB 级别)。 在众多显著特性当中，Cassandra 最为卓越的长处是对写入及读取操作进行规模调整，而且其不强调主集群的设计思路能够以相对直观的方式简化各集群的创建与扩展流程。 1.5 图关系数据库 1.5.1 Neo4j 第 2 章 Redis 数据库 2.1 Redis 数据库概述 Redis 是一个开源的 key-value 存储系统。 和 Memcached 类似，它支持存储的 value 类型相对更多，包括 string(字符串)、list(链表)、set(集合)、zset(sorted set --有序集合)和 hash（哈希类型）。 这些数据类型都支持 push/pop、add/remove 及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。 在此基础上，Redis 支持各种不同方式的排序。 与 memcached 一样，为了保证效率，数据都是缓存在内存中。 区别的是 Redis 会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件。 并且在此基础上实现了 master-slave(主从)同步。 2.2 应用场景 2.2.1 配合关系型数据库做高速缓存 高频次，热门访问的数据，降低数据库 IO 分布式架构，做 session 共享 2.2.2 持久化数据（多样化） 2.3 Redis 安装 2.3.1 安装 C 语言环境（gcc 编译器） yum install centos-release-scl scl-utils-build yum install -y devtoolset-8-toolchain scl enable devtoolset-8 bash gcc --version 2.3.2 解压 redis 并安装 tar -zxvf redis-6.2.1.tar.gz cd redis-6.2.1 编译：make 安装：make install 默认安装至：/usr/local/bin目录下： redis-benchmark：性能测试工具，可以在自己本子运行，看看自己本子性能如何 redis-check-aof：修复有问题的 AOF 文件，rdb 和 aof 后面讲 redis-check-dump：修复有问题的 dump.rdb 文件 redis-sentinel：Redis 集群使用 redis-server：Redis 服务器启动命令 redis-cli：客户端，操作入口 2.4 启动 Redis 服务 2.4.1 前台启动 在任意目录下执行redis-server，此时窗口不能关闭 2.4.2 后台启动 进入 redis 解压后的目录 将目录中的 redis-conf 文件拷贝至其他位置，如 etc 目录cp redis.conf /etc/redis.conf【也可以不复制，直接改】 修改/etc/redis.conf(128 行)文件将里面的 daemonize no 改成 yes，让服务在后台启动 进入/usr/local/bin目录下，执行redis-server /etc/redis.conf命令，表示按指定的配置文件启动： 查看 redis 进程：ps -ef | grep redis，此时还未启动 2.4.3 进入 redis 操作 在任意目录下执行redis-server /etc/redis.conf，并使用ps -ef | grep redis查看进程： 127.0.0.1:6379 表示只允许本地连接 在任意目录下执行redis-cli即可完成连接 2.5 关闭 redis 2.5.1 方式一 查看 redis 进程：ps -ef | grep redis 杀死进程：kill -9 11337 2.5.2 方式二 未进入实例时：redis-cli shutdown 进入实例时：shutdown或者exit（不会停止服务） 2.6 Redis 相关知识 端口号 6379 是 merz 的 9 键输入 redis 默认默认 16 个数据库，类似数组下标从 0 开始，初始默认使用 0 号库 使用命令 select 来切换数据库。如: select 8 所有库同样密码。 dbsize 查看当前数据库的 key 的数量 flushdb 清空当前库 flushall 通杀全部库 Redis 是单线程+多路 IO 复用技术 多路复用是指使用一个线程来检查多个文件描述符（Socket）的就绪状态，比如调用 select 和 poll 函数，传入多个文件描述符，如果有一个文件描述符就绪，则返回，否则阻塞直到超时。得到就绪状态后进行真正的操作可以在同一个线程里执行，也可以启动线程执行（比如使用线程池） 串行 vs 多线程+锁（memcached） vs 单线程+多路 IO 复用(Redis) （与 Memcache 三点不同: 支持多数据类型，支持持久化，单线程+多路 IO 复用） 第 3 章 常用五大数据类型 3.0 Redis 常见数据类型操作命令 keys * 查看当前库所有 key (匹配：keys *1) exists key 判断某个 key 是否存在 type key 查看你的 key 是什么类型 del key 删除指定的 key 数据 unlink key 根据 value 选择非阻塞删除 仅将 keys 从 keyspace 元数据中删除，真正的删除会在后续异步操作。 expire key 10 10 秒钟：为给定的 key 设置过期时间 ttl key 查看还有多少秒过期，-1 表示永不过期，-2 表示已过期 select n 命令切换数据库 dbsize 查看当前数据库的 key 的数量 flushdb 清空当前库 flushall 通杀全部库 3.2 String 字符串 3.2.1 简介 String 是 Redis 最基本的类型，一个 key 对应一个 value。 String 类型是二进制安全的。意味着 Redis 的 string 可以包含任何数据。比如 jpg 图片或者序列化的对象。 String 类型是 Redis 最基本的数据类型，一个 Redis 中字符串 value 最多可以是 512M。 3.2.2 常用命令 set &lt;key&gt;&lt;value&gt; 添加键值对 给已有 key 设置 value 会发生数据覆盖 setnx &lt;key&gt;&lt;value&gt; 只有在 key 不存在时，设置 key 的值 *NX：当数据库中 key 不存在时，可以将 key-value 添加数据库 *XX：当数据库中 key 存在时，可以将 key-value 添加数据库，与 NX 参数互斥 *EX：key 的超时秒数 *PX：key 的超时毫秒数，与 EX 互斥 get &lt;key&gt; 查询对应键的值 append &lt;key&gt;&lt;value&gt; 将给定的 追加到原值的末尾 strlen &lt;key&gt; 获得值的长度 incr &lt;key&gt; 将 key 中储存的数字值增 1，只能对数字值操作，如果为空，新增值为 1。 decr &lt;key&gt; 将 key 中储存的数字值减 1，只能对数字值操作，如果为空，新增值为-1 incrby/decrby &lt;key&gt;&lt;步长&gt;将 key 中储存的数字值按步长增减。 mset &lt;key1&gt;&lt;value1&gt;&lt;key2&gt;&lt;value2&gt; 同时设置一个或多 key-value 对 mget &lt;key1&gt;&lt;key2&gt;&lt;key3&gt;同时获取一个或多个 value msetnx &lt;key1&gt;&lt;value1&gt;&lt;key2&gt;&lt;value2&gt; 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在时成功。 原子性：有一个失败则全部失败 etrange &lt;key&gt;&lt;起始位置&gt;&lt;结束位置&gt;获得值的范围，前后均为闭区间 setrange &lt;key&gt;&lt;起始位置&gt;&lt;value&gt;用 覆写所储存的字符串值，从&lt;起始位置&gt;的下标插入 **setex &lt;key&gt;&lt;过期时间&gt;&lt;value&gt;**设置键值的同时，设置过期时间，单位秒。 getset &lt;key&gt;&lt;value&gt;以新换旧，设置了新值同时返回旧值。 3.2.3 原子操作 所谓原子操作是指不会被线程调度机制打断的操作，这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切换到另一个线程）。 （1）在单线程中，能够在单条指令中完成的操作都可以认为是&quot;原子操作&quot;，因为中断只能发生于指令之间。 （2）在多线程中，不能被其它进程（线程）打断的操作就叫原子操作。 Redis 单命令的原子性主要得益于 Redis 的单线程。 案例： java 中的 i是否是原子操作？不是 redis 中 i=0;两个线程分别对 i 进行100 次，最后的可能值是多少？** 2~200200** 3.2.4 数据结构 String 的数据结构为简单动态字符串(Simple Dynamic String,缩写 SDS)。是可以修改的字符串，内部结构实现上类似于 Java 的 ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配。 如图中所示，内部为当前字符串实际分配的空间 capacity 一般要高于实际字符串长度 len。当字符串长度小于 1M 时，扩容都是加倍现有的空间，如果超过 1M，扩容时一次只会多扩 1M 的空间。需要注意的是字符串最大长度为 512M。 3.3 列表 List 3.3.1 简介 单键多值 Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。 3.3.2 常用命令 lpush/rpush &lt;key&gt;&lt;value1&gt;&lt;value2&gt;&lt;value3&gt;从左边/右边插入一个或多个值。 lpop/rpop &lt;key&gt;从左边/右边吐出一个值。值在键在，值光键亡。 rpoplpush &lt;key1&gt;&lt;key2&gt;从列表右边吐出一个值，插到列表左边。 lrange &lt;key&gt;&lt;start&gt;&lt;stop&gt;按照索引下标获得元素(从左到右) lrange &lt;key&gt; 0 -1 0 左边第一个，-1 右边第一个，（0 -1 表示获取所有） lindex &lt;key&gt;&lt;index&gt;按照索引下标获得元素(从左到右) llen &lt;key&gt;获得列表长度 linsert &lt;key&gt; before/after &lt;value&gt;&lt;newvalue&gt;在的前/后面插入插入值 lrem &lt;key&gt;&lt;n&gt;&lt;value&gt;从左边删除 n 个值为 value(从左到右) lset &lt;key&gt;&lt;index&gt;&lt;value&gt;将列表 key 下标为 index 的值替换成 value 3.3.3 数据结构 List 的数据结构为快速链表 quickList： 首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。 当数据量比较多的时候才会改成 quicklist。 因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是 int 类型的数据，结构上还需要两个额外的指针 prev 和 next。 Redis 将链表和 ziplist 结合起来组成了 quicklist。也就是将多个 ziplist 使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。 3.4 集合 set 3.4.1 简介 Redis set 对外提供的功能与 list 类似是一个列表的功能，特殊之处在于 set 是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set 是一个很好的选择，并且 set 提供了判断某个成员是否在一个 set 集合内的重要接口，这个也是 list 所不能提供的。 3.4.2 常用命令 sadd &lt;key&gt;&lt;value1&gt;&lt;value2&gt; 将一个或多个 member 元素加入到集合 key 中，已经存在的 member 元素将被忽略 smembers &lt;key&gt;取出该集合的所有值。 sismember &lt;key&gt;&lt;value&gt;判断集合是否为含有该值，有 1，没有 0 scard &lt;key&gt;返回该集合的元素个数。 srem &lt;key&gt;&lt;value1&gt;&lt;value2&gt; 删除集合中的某个元素。 spop &lt;key&gt;随机从该集合中吐出一个值，所有值取出则 key 也不存在。 srandmember &lt;key&gt;&lt;n&gt;随机从该集合中取出 n 个值。不会从集合中删除。 smove &lt;sourcekey&gt;&lt;destinationkey&gt;&lt;value&gt;把集合中一个值从一个集合移动到另一个集合 sinter &lt;key1&gt;&lt;key2&gt;返回两个集合的交集元素。 sunion &lt;key1&gt;&lt;key2&gt;返回两个集合的并集元素。 sdiff &lt;key1&gt;&lt;key2&gt;返回两个集合的差集元素(key1 中的特有的) 3.4.3 数据结构 Set 数据结构是 dict 字典，字典是用哈希表实现的。 Java 中 HashSet 的内部实现使用的是 HashMap，只不过所有的 value 都指向同一个对象。Redis 的 set 结构也是一样，它的内部也使用 hash 结构，所有的 value 都指向同一个内部值。 Redis 的 Set 是 string 类型的无序集合。它底层其实是一个 value 为 null 的 hash 表，所以添加，删除，查找的复杂度都是 O(1)。 3.5 哈希 Hash 3.5.1 简介 Redis hash 是一个键值对集合。 Redis hash 的 value 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。类似 Java 里面的 Map&lt;String,Object&gt;。 用户 ID 为查找的 key，存储的 value 用户对象包含姓名，年龄，生日等信息，如果用普通的 key/value 结构来存储 3.5.2 常用命令 hset &lt;key&gt;&lt;field&gt;&lt;value&gt;给集合中的 键赋值 hget &lt;key1&gt;&lt;field&gt;从&lt;key1&gt;集合取出 value hmset &lt;key1&gt;&lt;field1&gt;&lt;value1&gt;&lt;field2&gt;&lt;value2&gt;批量设置 hash 的值 hexists&lt;key1&gt;&lt;field&gt;查看哈希表 key 中，给定域 field 是否存在。 hkeys &lt;key&gt;列出该 hash 集合的所有 field hvals &lt;key&gt;列出该 hash 集合的所有 value hincrby &lt;key&gt;&lt;field&gt;&lt;increment&gt;为哈希表 key 中的域 field 的值加上增量 1 hsetnx &lt;key&gt;&lt;field&gt;&lt;value&gt;将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在时能够加入 3.5.3 数据结构 Hash 类型对应的数据结构是两种：ziplist（压缩列表），hashtable（哈希表）。当 field-value 长度较短且个数较少时，使用 ziplist，否则使用 hashtable。 3.6 有序集合 zset 3.6.1 简介 Redis 有序集合 zset 与普通集合 set 非常相似，是一个没有重复元素的字符串集合。 不同之处是有序集合的每个成员都关联了一个评分（score）,这个评分（score）被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的，但是评分可以是重复了。 因为元素是有序的, 所以你也可以很快的根据评分（score）或者次序（position）来获取一个范围的元素。 访问有序集合的中间元素也是非常快的,因此你能够使用有序集合作为一个没有重复成员的智能列表。 3.6.2 常用命令 zadd &lt;key&gt;&lt;score1&gt;&lt;value1&gt;&lt;score2&gt;&lt;value2&gt;将一个或多个 member 元素及其 score 值加入到有序集 key 当中。 **zrange &lt;key&gt;&lt;start&gt;&lt;stop&gt; [WITHSCORES]**** **返回有序集 key 中，下标在之间的元素 带 WITHSCORES，可以让分数一起和值返回到结果集。 zrangebyscore key minmax [withscores] [limit offset count]返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列。 zrevrangebyscore key maxmin [withscores] [limit offset count] 同上，改为从大到小排列。 zincrby &lt;key&gt;&lt;increment&gt;&lt;value&gt; 为元素的 score 加上增量 zrem &lt;key&gt;&lt;value&gt;删除该集合下指定值的元素 zcount &lt;key&gt;&lt;min&gt;&lt;max&gt;统计该集合，分数区间内的元素个数 zrank &lt;key&gt;&lt;value&gt;返回该值在集合中的排名，从 0 开始。 3.6.3 使用案例 实现一个文章访问量的排行榜。 3.6.4 数据结构 SortedSet(zset)是 Redis 提供的一个非常特别的数据结构，一方面它等价于 Java 的数据结构 Map&lt;String, Double&gt;，可以给每一个元素 value 赋予一个权重 score，另一方面它又类似于 TreeSet，内部的元素会按照权重 score 进行排序，可以得到每个元素的名次，还可以通过 score 的范围来获取元素的列表。 zset 底层使用了两个数据结构 （1）hash，hash 的作用就是关联元素 value 和权重 score，保障元素 value 的唯一性，可以通过元素 value 找到相应的 score 值。 （2）跳跃表，跳跃表的目的在于给元素 value 排序，根据 score 的范围获取元素列表。 3.6.5 跳跃表 1、简介 有序集合在生活中比较常见，例如根据成绩对学生排名，根据得分对玩家排名等。对于有序集合的底层实现，可以用数组、平衡树、链表等。数组不便元素的插入、删除；平衡树或红黑树虽然效率高但结构复杂；链表查询需要遍历所有效率低。Redis 采用的是跳跃表。跳跃表效率堪比红黑树，实现远比红黑树简单。 2、实例 对比有序链表和跳跃表，从链表中查询出 51 （1） 有序链表 要查找值为 51 的元素，需要从第一个元素开始依次查找、比较才能找到。共需要 6 次比较。 （2） 跳跃表 从第 2 层开始，1 节点比 51 节点小，向后比较。 21 节点比 51 节点小，继续向后比较，后面就是 NULL 了，所以从 21 节点向下到第 1 层 在第 1 层，41 节点比 51 节点小，继续向后，61 节点比 51 节点大，所以从 41 向下 在第 0 层，51 节点为要查找的节点，节点被找到，共查找 4 次。 从此可以看出跳跃表比有序链表效率要高 3.7 Bitmaps（特殊的字符串） 3.7.1 简介 现代计算机用二进制（位）作为信息的基础单位， 1 个字节等于 8 位，例如“abc”字符串是由 3 个字节组成，但实际在计算机存储时将其用二进制表示，“abc”分别对应的 ASCII 码分别是 97、98、99，对应的二进制分别是 01100001、 01100010 和 01100011，如下图： 合理地使用操作位能够有效地提高内存使用率和开发效率。 Redis 提供了 Bitmaps 这个“数据类型”可以实现对位的操作： Bitmaps 本身不是一种数据类型，实际上它就是字符串（key-value），但是它可以对字符串的位进行操作。 Bitmaps 单独提供了一套命令，所以在 Redis 中使用 Bitmaps 和使用字符串的方法不太相同。可以把 Bitmaps 想象成一个以位为单位的数组，数组的每个单元只能存储 0 和 1，数组的下标在 Bitmaps 中叫做偏移量。 3.7.2 常用命令 setbit （1）格式：setbit&lt;key&gt;&lt;offset&gt;&lt;value&gt;设置 Bitmaps 中某个偏移量的值 （2）实例 每个独立用户是否访问过网站存放在 Bitmaps 中，将访问的用户记做 1，没有访问的用户记做 0，用偏移量作为用户的 id。 设置键的第 offset 个位的值（从 0 算起），假设现在有 20 个用户，userid=1， 6， 11， 15， 19 的用户对网站进行了访问，那么当前 Bitmaps 初始化结果如图 注： 很多应用的用户 id 以一个指定数字（例如 10000）开头，直接将用户 id 和 Bitmaps 的偏移量对应势必会造成一定的浪费，通常的做法是每次做 setbit 操作时将用户 id 减去这个指定数字。 在第一次初始化 Bitmaps 时，假如偏移量非常大，那么整个初始化过程执行会比较慢，可能会造成 Redis 的阻塞。 getbit （1）格式：getbit&lt;key&gt;&lt;offset&gt;获取 Bitmaps 中某个偏移量的 value 值。即获取键的第 offset 位的值（从 0 开始算） （2）实例 获取 id=8 的用户是否在 2020-11-06 这天访问过，返回 0 说明没有访问过： 注：因为 100 根本不存在，所以也是返回 0 bitcount 统计字符串被设置为 1 的 bit 数。一般情况下，给定的整个字符串都会被进行计数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行。start 和 end 参数的设置，都可以使用负数值：比如 -1 表示最后一个位，而 -2 表示倒数第二个位，start、end 是指 bit 组的字节的下标数，二者皆包含。 （1）格式：bitcount&lt;key&gt;[start end]统计字符串从 start 字节到 end 字节比特值为 1 的数量 举例： K1 【01000001 01000000 00000000 00100001】，对应【0，1，2，3】 bitcount K1 1 2：统计下标 1、2 字节组中 bit=1 的个数，即 01000000 00000000–》bitcount K1 1 2 --》1 bitcount K1 1 3：统计下标 1、2 字节组中 bit=1 的个数，即 01000000 00000000 00100001–》bitcount K1 1 3–》3 bitcount K1 0 -2：统计下标 0 到下标倒数第 2，字节组中 bit=1 的个数，即 01000001 01000000 00000000–》bitcount K1 0 -2–》3 （2）实例 计算 2022-11-06 这天的独立访问用户数量 start 和 end 代表起始和结束字节数，下面操作计算用户 id 在第 1 个字节到第 3 个字节之间的独立访问用户数，对应的用户 id 是 11， 15， 19。 注意：redis 的 setbit 设置或清除的是 bit 位置，而 bitcount 计算的是 byte 位置。 4、bitop (1)格式：bitop and(or/not/xor) &lt;destkey&gt; [key…]：bitop 是一个复合操作，它可以做多个 Bitmaps 的 and（交集）、or（并集）、not（非）、xor（异或）操作并将结果保存在 destkey 中。 (2)实例 2020-11-04 日访问网站的 userid=1,2,5,9。 _setbit unique:users:20201104 1 1_ _setbit unique:users:20201104 2 1_ _setbit unique:users:20201104 5 1_ _setbit unique:users:20201104 9 1_ 2020-11-03 日访问网站的 userid=0,1,4,9。 _setbit unique:users:20201103 0 1_ _setbit unique:users:20201103 1 1_ _setbit unique:users:20201103 4 1_ _setbit unique:users:20201103 9 1_ 计算出两天都访问过网站的用户数量 _bitop and unique:users:and:20201104_03 unique:users:20201103 unique:users:20201104_ 计算出任意一天都访问过网站的用户数量（例如月活跃就是类似这种），可以使用 or 求并集 3.7.3 Bitmaps 与 set 对比 假设网站有 1 亿用户，每天独立访问的用户有 5 千万，如果每天用集合类型和 Bitmaps 分别存储活跃用户可以得到表 很明显，这种情况下使用 Bitmaps 能节省很多的内存空间，尤其是随着时间推移节省的内存还是非常可观的 但 Bitmaps 并不是万金油，假如该网站每天的独立访问用户很少，例如只有 10 万（大量的僵尸用户），那么两者的对比如下表所示，很显然，这时候使用 Bitmaps 就不太合适了，因为基本上大部分位都是 0。 3.8 HyperLogLog 3.8.1 简介 在工作当中，我们经常会遇到与统计相关的功能需求，比如统计网站 PV（PageView 页面访问量）,可以使用 Redis 的 incr、incrby 轻松实现。 但像 UV（UniqueVisitor，独立访客）、独立 IP 数、搜索记录数等需要去重和计数的问题如何解决？这种求集合中不重复元素个数的问题称为基数问题。 基数：比如数据集 {1, 3, 5, 7, 5, 7, 8}，那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为 5。基数估计就是在误差可接受的范围内，快速计算基数。 解决基数问题有很多种方案： （1）数据存储在 MySQL 表中，使用 distinct count 计算不重复个数 （2）使用 Redis 提供的 hash、set、bitmaps 等数据结构来处理 以上的方案结果精确，但随着数据不断增加，导致占用空间越来越大，对于非常大的数据集是不切实际的。 能否能够降低一定的精度来平衡存储空间？Redis 推出了 HyperLogLog Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 3.8.2 常用命令 1、pfadd （1）格式：pfadd &lt;key&gt;&lt; element&gt; [element ...] 添加指定元素到 HyperLogLog 中 （2）实例 将所有元素添加到指定 HyperLogLog 数据结构中。如果执行命令后 HLL 估计的近似基数发生变化，则返回 1，否则返回 0。 2、pfcount （1）格式：pfcount&lt;key&gt; [key ...] 计算 HLL 的近似基数，可以计算多个 HLL，比如用 HLL 存储每天的 UV，计算一周的 UV 可以使用 7 天的 UV 合并计算即可 （2）实例 3、pfmerge （1）格式：pfmerge&lt;destkey&gt;&lt;sourcekey&gt; [sourcekey ...] 将一个或多个 HLL 合并后的结果存储在另一个 HLL 中，比如每月活跃用户可以使用每天的活跃用户来合并计算可得 （2）实例 3.9 Geospatial 3.9.1 简介 Redis 3.2 中增加了对 GEO 类型的支持。GEO，Geographic，地理信息的缩写。该类型，就是元素的 2 维坐标，在地图上就是经纬度。redis 基于该类型，提供了经纬度设置，查询，范围查询，距离查询，经纬度 Hash 等常见操作。 3.6.2 常用命令 1、geoadd （1）格式：geoadd&lt;key&gt;&lt; longitude&gt;&lt;latitude&gt;&lt;member&gt; [longitude latitude member...] 添加地理位置（经度，纬度，名称） （2）实例： geoadd china:city 121.47 31.23 shanghai geoadd china:city 106.50 29.53 chongqing 114.05 22.52 shenzhen 116.38 39.90 beijing 两极无法直接添加，一般会下载城市数据，直接通过 Java 程序一次性导入。 有效的经度从 -180 度到 180 度。有效的纬度从 -85.05112878 度到 85.05112878 度。 当坐标位置超出指定范围时，该命令将会返回一个错误。 已经添加的数据，是无法再次往里面添加的。 2、geopos （1）格式：geopos &lt;key&gt;&lt;member&gt; [member...] 获得指定地区的坐标值 （2）实例 3、geodist （1）格式：geodist&lt;key&gt;&lt;member1&gt;&lt;member2&gt; [m|km|ft|mi ] 获取两个位置之间的直线距离 （2）实例 单位： m 表示单位为米[默认值]。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 如果用户没有显式地指定单位参数，那么 GEODIST 默认使用米作为单位 4、georadius （1）格式：georadius&lt;key&gt;&lt; longitude&gt;&lt;latitude&gt;radius m|km|ft|mi 以给定的经纬度为中心，找出某一半径内的元素经度纬度距离单位 （2）实例 第 4 章 配置文件 4.1 Units 单位 配置大小单位,开头定义了一些基本的度量单位，只支持 bytes，不支持 bit。大小写不敏感。 4.2 INCLUDES 包含 类似 jsp 中的 include，多实例的情况可以把公用的配置文件提取出来。 4.3 NETWORK 网络 4.3.1 bind 默认情况 bind=127.0.0.1 只能接受本机的访问请求。 不写的情况下，无限制接受任何 ip 地址的访问。 生产环境肯定要写你应用服务器的地址；服务器是需要远程访问的，所以需要将其注释掉。 4.3.2** protected-mode** 如果开启了 protected-mode，那么在没有设定 bind ip 且没有设密码的情况下，Redis 只允许接受本机的响应。需要设置为 no 满足学习要求。 此时重启 redis，查看进程： *:6379 表示连接不受限制 4.3.3 port 端口，默认 6379 4.3.4** tcp-backlog** 设置 tcp 的 backlog，backlog 其实是一个连接队列，backlog 队列总和=未完成三次握手队列+ 已经完成三次握手队列。 在高并发环境下你需要一个高 backlog 值来避免慢客户端连接问题。 注意 Linux 内核会将这个值减小到/proc/sys/net/core/somaxconn 的值（128），所以需要确认增大/proc/sys/net/core/somaxconn 和/proc/sys/net/ipv4/tcp_max_syn_backlog（128）两个值来达到想要的效果 4.3.5 timeout 一个空闲的客户端维持多少秒会关闭，0 表示关闭该功能。即永不关闭。 4.3.6 tcp-keeplive 对访问客户端的一种心跳检测，每个 n 秒检测一次。 单位为秒，如果设置为 0，则不会进行 Keepalive 检测，建议设置成 60 4.4 GENERAL 通用 4.4.1 daemonize 是否为后台进程，设置为 yes。守护进程，后台启动。 4.4.2 pidfile 存放 pid 文件的位置，每个实例会产生一个不同的 pid 文件。保存每次操作 redis 的进程号。 4.4.3 loglevel 指定日志记录级别，Redis 总共支持四个级别：debug、verbose、notice、warning，默认为notice。 四个级别根据使用阶段来选择，生产环境选择 notice 或者 warning 4.4.4 logfile 日志文件名称 4.4.5 **databases ** 设定库的数量默认 16，默认数据库为 0，可以使用 SELECT 命令在连接上指定数据库 id 4.5 SECURITY 在命令中设置密码，只是临时的。重启 redis 服务器，密码就还原了。 访问密码的查看、设置和取消。配置文件默认不需要密码。永久设置，需要再配置文件中进行设置。 4.6 LIMITS 限制 4.6.1 CLIENTS - maxclients 设置 redis 同时可以与多少个客户端进行连接。默认情况下为 10000 个客户端。如果达到了此限制，redis 则会拒绝新的连接请求，并且向这些连接请求方发出“max number of clients reached”以作回应。 4.6.2 MEMORY MANAGEMENT - **maxmemory ** 建议必须设置，否则，将内存占满，造成服务器宕机 设置 redis 可以使用的内存量。一旦到达内存使用上限，redis 将会试图移除内部数据，移除规则可以通过 maxmemory-policy 来指定。 如果 redis 无法根据移除规则来移除内存中的数据，或者设置了“不允许移除”，那么 redis 则会针对那些需要申请内存的指令返回错误信息，比如 SET、LPUSH 等。 但是对于无内存申请的指令，仍然会正常响应，比如 GET 等。如果你的 redis 是主 redis（说明你的 redis 有从 redis），那么在设置内存使用上限时，需要在系统中留出一些内存空间给同步队列缓存，只有在你设置的是“不移除”的情况下，才不用考虑这个因素。 4.6.3 MEMORY MANAGEMENT - maxmemory-policy volatile-lru：使用 LRU 算法移除 key，只对设置了过期时间的键；（最近最少使用） allkeys-lru：在所有集合 key 中，使用 LRU 算法移除 key volatile-random：在过期集合中移除随机的 key，只对设置了过期时间的键 allkeys-random：在所有集合 key 中，移除随机的 key volatile-ttl：移除那些 TTL 值最小的 key，即那些最近要过期的 key noeviction：不进行移除。针对写操作，只是返回错误信息 4.6.4 MEMORY MANAGEMENT - maxmemory-samples 设置样本数量，LRU 算法和最小 TTL 算法都并非是精确的算法，而是估算值，所以你可以设置样本的大小，redis 默认会检查这么多个 key 并选择其中 LRU 的那个。 一般设置 3 到 7 的数字，数值越小样本越不准确，但性能消耗越小。 4.7 SNAPSHOTTING 4.7.1 持久化 RDB 配置 4.7.2 AOF 配置 第 5 章 发布和订阅 5.1 发布和订阅概念 Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。 5.2 图示 客户端可以订阅频道如下图： 当给这个频道发布消息后，消息就会发送给订阅的客户端 5.3 命令行实现 1、打开一个客户端订阅 channel1 SUBSCRIBE channel1 2、打开另一个客户端，给 channel1 发布消息 hello publish channel1 hello 返回的 1 是订阅者数量 3、打开第一个客户端可以看到发送的消息 注：发布的消息没有持久化，如果在订阅的客户端收不到 hello，只能收到订阅后发布的消息 第 6 章 Jedis 工具 6.1 介绍 java 操作 redis 的客户端工具。 类似操作 mysql 的 jdbc 6.2 使用 6.2.1 环境搭建 引入依赖： 创建测试类： 执行测试：【注意 Redis 的配置文件设置需要满足 2 个条件】 注释掉了本地访问地址 127.0.0.1 protected-mode 设置为 no 连接 Linux 虚拟机中的 redis 如果出现连接超时，检查是否关闭防火墙 查看防火墙状态：systemctl status firewalld 关闭防火墙：systemctl stop firewalld 6.2.2 API 6.2.3 Jedis 连接池 123456789101112131415161718192021222324252627282930313233343536package iceriver.redis;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;public class JedisPoolUtil &#123; private static volatile JedisPool jedisPool = null; private JedisPoolUtil() &#123; &#125; public static JedisPool getJedisPoolInstance() &#123; if (null == jedisPool) &#123; synchronized (JedisPoolUtil.class) &#123; if (null == jedisPool) &#123; JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(200); poolConfig.setMaxIdle(32); poolConfig.setMaxWaitMillis(100*1000); poolConfig.setBlockWhenExhausted(true); poolConfig.setTestOnBorrow(true); // ping PONG jedisPool = new JedisPool(poolConfig, &quot;192.168.150.133&quot;, 6379, 60000 ); &#125; &#125; &#125; return jedisPool; &#125; public static void release(JedisPool jedisPool, Jedis jedis) &#123; if (null != jedis) &#123; jedisPool.returnResource(jedis); &#125; &#125;&#125; 节省每次连接 redis 服务带来的消耗，把连接好的实例反复利用。 MaxTotal：控制一个 pool 可分配多少个 jedis 实例，通过 pool.getResource()来获取；如果赋值为-1，则表示不限制；如果 pool 已经分配了 MaxTotal 个 jedis 实例，则此时 pool 的状态为 exhausted。 maxIdle：控制一个 pool 最多有多少个状态为 idle(空闲)的 jedis 实例； MaxWaitMillis：表示当 borrow 一个 jedis 实例时，最大的等待毫秒数，如果超过等待时间，则直接抛 JedisConnectionException； testOnBorrow：获得一个 jedis 实例的时候是否检查连接可用性（ping()）；如果为 true，则得到的 jedis 实例均是可用的； 6.3 手机验证码案例 要求： 1、输入手机号，点击发送后随机生成 6 位数字码，2 分钟有效 2、输入验证码，点击验证，返回成功或失败 3、每个手机号每天只能输入 3 次 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package iceriver.redis.jedis;import redis.clients.jedis.Jedis;import java.util.Random;/** * TODO * 1、输入手机号，点击发送后随机生成6位数字码，2分钟有效 * 2、输入验证码，点击验证，返回成功或失败 * 3、每个手机号每天只能输入3次 * @author zhuyuqi * @version v0.0.1 * @className PhoneCode * @description https://developer.aliyun.com/profile/sagwrxp2ua66w * @date 2022/08/23 20:12 */public class PhoneCode &#123; public static void main(String[] args) &#123; //模拟发送验证码 verifyCode(&quot;123456&quot;); //校验// getRedisCode(&quot;123456&quot;,&quot;061171&quot;); &#125; /** * 每次生成1个[0, 9]的数，循环6次进行拼接 * @return */ public static String getCode()&#123; Random random = new Random(); String code = &quot;&quot;; for (int i = 0; i &lt; 6; i++) &#123; int rand = random.nextInt(10); code += rand; &#125; return code; &#125; /** * 每个手机号每天只能发3次、每次的验证码放到redis中设置过期时间 * @param phone */ public static void verifyCode(String phone)&#123; Jedis jedis = new Jedis(&quot;192.168.150.132&quot;, 6379); String countKey = &quot;VerifyCode&quot; + phone + &quot;count&quot;;//保存验证码使用次数 String codeKey = &quot;VerifyCode&quot; + phone + &quot;code&quot;;//保存验证码 // 每个手机发送3次 String count = jedis.get(countKey); if(count == null)&#123; jedis.setex(countKey, 24*60*60, &quot;1&quot;); &#125;else if(Integer.parseInt(count) &lt; 3)&#123; jedis.incr(countKey); &#125;else &#123; System.out.println(&quot;今天的验证机会（3次）已经用完&quot;); jedis.close(); return; &#125; String vCode = getCode(); jedis.setex(codeKey, 120, vCode); jedis.close(); &#125; /** * 验证码校验 * @param phone * @param code */ public static void getRedisCode(String phone, String code)&#123; Jedis jedis = new Jedis(&quot;192.168.150.132&quot;, 6379); // 手机发送次数的String设计，保证key的唯一性 String codeKey = &quot;VerifyCode&quot; + phone + &quot;code&quot;; String codeValue = jedis.get(codeKey); if(codeValue.equals(code))&#123; System.out.println(&quot;成功&quot;); &#125;else&#123; System.out.println(&quot;失败&quot;); &#125; jedis.close(); &#125;&#125; 第 7 章 Redis 整合 Springboot 创建 springboot 项目 引入 redis 依赖 123456789101112&lt;!-- redis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- spring2.X集成redis所需common-pool2--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt;&lt;/dependency&gt; springboot 核心配置文件中添加 redis 配置信息 1234567891011121314151617#Redis服务器地址spring.redis.host=192.168.150.132#Redis服务器连接端口spring.redis.port=6379#Redis数据库索引（默认为0）spring.redis.database= 0#连接超时时间（毫秒）spring.redis.timeout=1800000#连接池最大连接数（使用负值表示没有限制）spring.redis.lettuce.pool.max-active=20#最大阻塞等待时间(负数表示没限制)spring.redis.lettuce.pool.max-wait=-1#连接池中的最大空闲连接spring.redis.lettuce.pool.max-idle=5#连接池中的最小空闲连接spring.redis.lettuce.pool.min-idle=0 创建 redis 配置类 1234567891011121314151617181920212223242526272829303132333435363738394041public class RedisConfig extends CachingConfigurerSupport &#123; @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory) &#123; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;(); RedisSerializer&lt;String&gt; redisSerializer = new StringRedisSerializer(); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); template.setConnectionFactory(factory);//key序列化方式 template.setKeySerializer(redisSerializer);//value序列化 template.setValueSerializer(jackson2JsonRedisSerializer);//value hashmap序列化 template.setHashValueSerializer(jackson2JsonRedisSerializer); return template; &#125; @Bean public CacheManager cacheManager(RedisConnectionFactory factory) &#123; RedisSerializer&lt;String&gt; redisSerializer = new StringRedisSerializer(); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);//解决查询缓存转换异常的问题 ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om);// 配置序列化（解决乱码的问题）,过期时间600秒 RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofSeconds(600)) .serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(redisSerializer)) .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(jackson2JsonRedisSerializer)) .disableCachingNullValues(); RedisCacheManager cacheManager = RedisCacheManager.builder(factory) .cacheDefaults(config) .build(); return cacheManager; &#125;&#125; 第 8 章 事务操作 8.1 Redis 事务 8.1.1 事务定义 Redis 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 Redis 事务的主要作用就是串联多个命令防止别的命令插队。 8.1.2 Multi、Exec、discard 从输入 Multi 命令开始，输入的命令都会依次进入命令队列中，但不会执行，直到输入 Exec 后，Redis 会将之前的命令队列中的命令依次执行。 组队的过程中可以通过 discard 来放弃组队。 8.1.3 事务错误处理 组队情况下：组队中某个命令出现错误，执行时整个队列都会被取消。 执行情况下：出错的命令不会被执行，其他的命令会被执行，不会回滚。 8.1.4 乐观锁与悲观锁 乐观锁：每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。Redis 就是利用这种 check-and-set 机制实现事务的。 悲观锁：每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会 block 直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 8.1.5 WATCH KEY 和 UNWATCH 在执行 multi 之前，先执行 watch key1 [key2],可以监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 UNWATCH 取消 WATCH 命令对所有 key 的监视。 如果在执行 WATCH 命令之后，EXEC 命令或 DISCARD 命令先被执行了的话，那么就不需要再执行 UNWATCH 了。 8.1.6 事务三特性 单独的隔离操作 事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 没有隔离级别的概念 队列中的命令没有提交之前都不会实际被执行，因为事务提交前任何指令都不会被实际执行 不保证原子性 事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚 8.2 秒杀案例 8.2.1 解决计数器和人员记录的事务操作 秒杀成功：库存数量减少、秒杀成功的人员增加数量 库存使用 Sting 保存，用户使用 set 保存。 8.2.2 思路 8.2.3 代码实现 极简版（存在超卖、连接超时问题） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SecKill_redis &#123; //秒杀过程 public static boolean doSecKill(String uid,String prodid) throws IOException &#123; //1 uid和prodid非空判断 if(uid == null || prodid == null) &#123; return false; &#125; //2 连接redis Jedis jedis = new Jedis(&quot;192.168.150.133&quot;,6379); //3 拼接key // 3.1 库存key String kcKey = &quot;sk:&quot;+prodid+&quot;:qt&quot;; // 3.2 秒杀成功用户key String userKey = &quot;sk:&quot;+prodid+&quot;:user&quot;; //4 获取库存，如果库存null，秒杀还没有开始 String kc = jedis.get(kcKey); if(kc == null) &#123; System.out.println(&quot;秒杀还没有开始，请等待&quot;); jedis.close(); return false; &#125; // 5 判断用户是否重复秒杀操作 if(jedis.sismember(userKey, uid)) &#123; System.out.println(&quot;已经秒杀成功了，不能重复秒杀&quot;); jedis.close(); return false; &#125; //6 判断如果商品数量，库存数量小于1，秒杀结束 if(Integer.parseInt(kc)&lt;=0) &#123; System.out.println(&quot;秒杀已经结束了&quot;); jedis.close(); return false; &#125; //7 秒杀过程 //7.1 库存-1 jedis.decr(kcKey); //7.2 把秒杀成功用户添加清单里面 jedis.sadd(userKey,uid); System.out.println(&quot;秒杀成功了..&quot;); jedis.close(); return true; &#125;&#125; 连接池版（解决连接超时问题） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class SecKill_redis &#123; //秒杀过程 public static boolean doSecKill(String uid,String prodid) throws IOException &#123; //1 uid和prodid非空判断 if(uid == null || prodid == null) &#123; return false; &#125; //2 连接redis JedisPool jedisPoolInstance = JedisPoolUtil.getJedisPoolInstance(); Jedis jedis = jedisPoolInstance.getResource(); //3 拼接key // 3.1 库存key String kcKey = &quot;sk:&quot;+prodid+&quot;:qt&quot;; // 3.2 秒杀成功用户key String userKey = &quot;sk:&quot;+prodid+&quot;:user&quot;; //4 获取库存，如果库存null，秒杀还没有开始 String kc = jedis.get(kcKey); if(kc == null) &#123; System.out.println(&quot;秒杀还没有开始，请等待&quot;); jedis.close(); return false; &#125; // 5 判断用户是否重复秒杀操作 if(jedis.sismember(userKey, uid)) &#123; System.out.println(&quot;已经秒杀成功了，不能重复秒杀&quot;); jedis.close(); return false; &#125; //6 判断如果商品数量，库存数量小于1，秒杀结束 if(Integer.parseInt(kc)&lt;=0) &#123; System.out.println(&quot;秒杀已经结束了&quot;); jedis.close(); return false; &#125; //7 秒杀过程 //7.1 库存-1 jedis.decr(kcKey); //7.2 把秒杀成功用户添加清单里面 jedis.sadd(userKey,uid); System.out.println(&quot;秒杀成功了..&quot;); jedis.close(); return true; &#125;&#125; 事务版——乐观锁（解决超卖问题） 在获取商品数量之前，就给商品加锁（watch) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class SecKill_redis &#123; //秒杀过程 public static boolean doSecKill(String uid,String prodid) throws IOException &#123; //1 uid和prodid非空判断 if(uid == null || prodid == null) &#123; return false; &#125; //2 连接redis JedisPool jedisPoolInstance = JedisPoolUtil.getJedisPoolInstance(); Jedis jedis = jedisPoolInstance.getResource(); //3 拼接key // 3.1 库存key String kcKey = &quot;sk:&quot;+prodid+&quot;:qt&quot;; // 3.2 秒杀成功用户key String userKey = &quot;sk:&quot;+prodid+&quot;:user&quot;; //7.1 监视库存 jedis.watch(kcKey); //4 获取库存，如果库存null，秒杀还没有开始 String kc = jedis.get(kcKey); if(kc == null) &#123; System.out.println(&quot;秒杀还没有开始，请等待&quot;); jedis.close(); return false; &#125; // 5 判断用户是否重复秒杀操作 if(jedis.sismember(userKey, uid)) &#123; System.out.println(&quot;已经秒杀成功了，不能重复秒杀&quot;); jedis.close(); return false; &#125; //6 判断如果商品数量，库存数量小于1，秒杀结束 if(Integer.parseInt(kc)&lt;=0) &#123; System.out.println(&quot;秒杀已经结束了&quot;); jedis.close(); return false; &#125; //7 秒杀过程 //7.2 使用事务 Transaction multi = jedis.multi(); multi.decr(kcKey);//库存-1 multi.sadd(userKey, uid);//添加到秒杀成功的用户清单 List&lt;Object&gt; list = multi.exec(); if(list == null || list.size() == 0)&#123; System.out.println(&quot;秒杀失败&quot;); jedis.close(); return false; &#125; System.out.println(&quot;秒杀成功了..&quot;); jedis.close(); return true; &#125;&#125; 商品数量较多时，由于乐观锁会比较版本号，如果持有的版本号已经发生变化，即使还有商品，仍然不会去执行，会导致库存遗留。解决该问题可以使用悲观锁，但 Redis 不能直接使用悲观锁，需要借助 LUA 脚本语言。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package iceriver.redis;import java.io.IOException;import java.util.HashSet;import java.util.List;import java.util.Set;import org.apache.commons.pool2.impl.GenericObjectPoolConfig;import org.slf4j.LoggerFactory;import ch.qos.logback.core.joran.conditional.ElseAction;import redis.clients.jedis.HostAndPort;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisCluster;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;import redis.clients.jedis.ShardedJedisPool;import redis.clients.jedis.Transaction;public class SecKill_redisByScript &#123; static String secKillScript =&quot;local userid=KEYS[1];\\r\\n&quot; + &quot;local prodid=KEYS[2];\\r\\n&quot; + &quot;local qtkey=&#x27;sk:&#x27;..prodid..\\&quot;:qt\\&quot;;\\r\\n&quot; + &quot;local usersKey=&#x27;sk:&#x27;..prodid..\\&quot;:usr\\&quot;;\\r\\n&quot; + &quot;local userExists=redis.call(\\&quot;sismember\\&quot;,usersKey,userid);\\r\\n&quot; + &quot;if tonumber(userExists)==1 then \\r\\n&quot; + &quot; return 2;\\r\\n&quot; + &quot;end\\r\\n&quot; + &quot;local num= redis.call(\\&quot;get\\&quot; ,qtkey);\\r\\n&quot; + &quot;if tonumber(num)&lt;=0 then \\r\\n&quot; + &quot; return 0;\\r\\n&quot; + &quot;else \\r\\n&quot; + &quot; redis.call(\\&quot;decr\\&quot;,qtkey);\\r\\n&quot; + &quot; redis.call(\\&quot;sadd\\&quot;,usersKey,userid);\\r\\n&quot; + &quot;end\\r\\n&quot; + &quot;return 1&quot; ; public static boolean doSecKill(String uid,String prodid) throws IOException &#123; JedisPool jedispool = JedisPoolUtil.getJedisPoolInstance(); Jedis jedis=jedispool.getResource(); String sha1= jedis.scriptLoad(secKillScript);//加载脚本 Object result= jedis.evalsha(sha1, 2, uid,prodid);//执行脚本，传入脚本、参数数量、参数名称——固定写法 String reString=String.valueOf(result); if (&quot;0&quot;.equals( reString ) ) &#123; System.err.println(&quot;已抢空！！&quot;); &#125;else if(&quot;1&quot;.equals( reString ) ) &#123; System.out.println(&quot;抢购成功！！！！&quot;); &#125;else if(&quot;2&quot;.equals( reString ) ) &#123; System.err.println(&quot;该用户已抢过！！&quot;); &#125;else&#123; System.err.println(&quot;抢购异常！！&quot;); &#125; jedis.close(); return true; &#125;&#125; 8.2.4 模拟并发 使用 linux 中的 ab 工具，进行模拟测试。 centos6 默认安装了，centos7 需要手动安装 安装指令：yum install httpd-tools ab -n 2000 -c 200 -p /root/apps/profile -T application/x-www-form-urlencoded http://192.168.101.28:8080/pro03/doseckill -n：共 2000 个请求 -c：同一时间发起 200 个请求 -p：创建文件，写入prodid=0101&amp;，作为请求参数 在 linux 终端执行上述命令，会发现极简版的秒杀 redis 中的数据出现负数： 同时有可能出现连接超时问题 8.3 LUA 脚本语言 8.3.1 介绍 Lua 是一个小巧的脚本语言，Lua 脚本可以很容易的被 C/C++ 代码调用，也可以反过来调用 C/C++的函数，Lua 并没有提供强大的库，一个完整的 Lua 解释器不过 200k，所以 Lua 不适合作为开发独立应用程序的语言，而是作为嵌入式脚本语言。 很多应用程序、游戏使用 LUA 作为自己的嵌入式脚本语言，以此来实现可配置性、可扩展性。 8.3.1 LUA 在 Redis 中的优势——悲观锁操作 将复杂的或者多步的 redis 操作，写为一个脚本，一次提交给 redis 执行，减少反复连接 redis 的次数。提升性能。 LUA 脚本是类似 redis 事务，有一定的原子性，不会被其他命令插队，可以完成一些 redis 事务性的操作。 但是注意 redis 的 lua 脚本功能，只有在 Redis 2.6 以上的版本才可以使用。 利用 lua 脚本淘汰用户，解决超卖问题。 redis 2.6 版本以后，通过 lua 脚本解决争抢问题，实际上是redis 利用其单线程的特性，用任务队列的方式解决多任务并发问题。 第 9 章 Redis 持久化 9.1 RDB（Redis Database） 9.1.1 介绍 在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的 Snapshot 快照，它恢复时是将快照文件直接读到内存里。 Redis 中缺省配置时，持久化即采用这种机制。 Redis 会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。 整个过程中，主进程不进行任何 IO 操作的，确保了极高的性能如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那 RDB 方式要比 AOF 方式更加的高效。RDB 的缺点是最后一次持久化后的数据可能丢失。 在命令行中可以使用save和bgsave指令进行当前的持久化操作： save：只管保存，在主线程中执行，会导致阻塞，所以请慎用； bgsave：当执行 bgsave 命令时，redis 会 fork 出一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。 lastsave：获取最后一次成功执行快照的时间。 flushall：产生 dump.rdb 文件，但里面是空的，无意义。 shutdown：会触发一次持久化操作，即使持久化操作在配置文件中没有打开。 特点： 适合大规模的数据恢复 对数据完整性和一致性要求不高更适合使用 节省磁盘空间 恢复速度快 Fork 的时候，内存中的数据被克隆了一份，大致 2 倍的膨胀性需要考虑 虽然 Redis 在 fork 时使用了写时拷贝技术,但是如果数据庞大时还是比较消耗性能。 在备份周期在一定间隔时间做一次备份，所以如果 Redis 意外 down 掉的话，就会丢失最后一次快照后的所有修改。 9.1.2 Fork Fork 的作用是复制一个与当前进程一样的进程。新进程的所有数据（变量、环境变量、程序计数器等）数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程。 在 Linux 程序中，fork()会产生一个和父进程完全相同的子进程，但子进程在此后多会 exec 系统调用，出于效率考虑，Linux 中引入了“写时复制技术”，即一般情况父进程和子进程会共用同一段物理内存，只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。 9.1.3 RDB 持久化流程 9.1.4 RDB 文件的备份及还原 config get dir：查询 rdb 文件的目录 备份：使用 Linux 的cp，命令拷贝至其他地方。 还原：将 dup.rdb 文件移动至工作目录下，备份数据会自动加载。 9.1.5 禁用 RDB 动态停止：命令行执行：redis-cli config set save &quot;&quot; 配置文件中关闭：修改配置文件，打开save &quot;&quot; 9.2 AOF（Append Of File） 9.2.1 介绍 以日志的形式来记录每个写操作（增量保存），将 Redis 执行过的所有写指令记录下来(读操作不记录)，只许追加文件但不可以改写文件，redis 启动之初会读取该文件重新构建数据，换言之，redis 重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 即记录之前执行过的写操作，redis 重启就加载该文件，自动执行一边当时的写操作进行数据恢复。 AOF 默认不开启，可以在配置文件中修改，参见【4.7.2】。 AOF 和 RDB 同时开启时，系统默认读取 AOF 的数据。 特点： 备份机制更稳健，丢失数据概率更低。 可读的日志文本，通过操作 AOF 稳健，可以处理误操作 比起 RDB 占用更多的磁盘空间。 恢复备份速度要慢。 每次读写都同步的话，有一定的性能压力。 存在个别 Bug，造成恢复不能。 9.2.2 持久化流程 客户端的请求写命令会被 append 追加到 AOF 缓冲区内； AOF 缓冲区根据 AOF 持久化策略[always,everysec,no]将操作 sync 同步到磁盘的 AOF 文件中； AOF 文件大小超过重写策略或手动重写时，会对 AOF 文件 rewrite 重写，压缩 AOF 文件容量； Redis 服务重启时，会重新 load 加载 AOF 文件中的写操作达到数据恢复的目的。 9.2.3 AOF 启动、修复和恢复 AOF 的备份机制和性能虽然和 RDB 不同, 但是备份和恢复的操作同 RDB 一样，都是拷贝备份文件，需要恢复时再拷贝到 Redis 工作目录下，启动系统即加载。 如遇到 AOF 文件损坏，则在启动 redis 过程中会发生失败的情况。而在 redis 的安装目录（默认安装目录：/usr/local/bin/）下有 redis-check-aof–fix 工具，通过执行/usr/local/bin/redis-check-aof--fix appendonly.aof可以对 aof 的持久化文件进行恢复。 9.2.4 Rewrite 压缩 是什么： AOF 采用文件追加方式，文件会越来越大。为避免出现此种情况，新增了重写机制, 当 AOF 文件的大小超过所设定的阈值时，Redis 就会启动 AOF 文件的内容压缩，只保留可以恢复数据的最小指令集。 命令bgrewriteaof可以手动触发一个体积优化的 aof 文件。 重写原理： AOF 文件持续增长而过大时，会 fork 出一条新进程来将文件重写(也是先写临时文件最后再 rename)，redis4.0 版本后的重写，是指上就是把 rdb 的快照，以二级制的形式附在新的 aof 头部，作为已有的历史数据，替换掉原来的流水账操作。 触发机制： Redis 会记录上次重写时的 AOF 大小，默认配置是当 AOF 文件大小是上次 rewrite 后大小的一倍且文件大于 64M 时触发。 配置文件中：auto-aof-rewrite-percentage、auto-aof-rewrite-min-size 指定触发机制，参见4.7.2。 重写流程： bgrewriteaof 触发重写，判断是否当前有 bgsave 或 bgrewriteaof 在运行，如果有，则等待该命令结束后再继续执行。 主进程 fork 出子进程执行重写操作，保证主进程不会阻塞。 子进程遍历 redis 内存中数据到临时文件，客户端的写请求同时写入 aof_buf 缓冲区和 aof_rewrite_buf 重写缓冲区保证原 AOF 文件完整以及新 AOF 文件生成期间的新的数据修改动作不会丢失。 进程写完新的 AOF 文件后，向主进程发信号，父进程更新统计信息。 主进程把 aof_rewrite_buf 中的数据写入到新的 AOF 文件。 使用新的 AOF 文件覆盖旧的 AOF 文件，完成 AOF 重写。 9.3 RDB 和 AOF 的选择 官方推荐两个都启用。 如果对数据不敏感，可以选单独用 RDB。 不建议单独用 AOF，因为可能会出现 Bug。 如果只是做纯内存缓存，可以都不用。 性能建议： 因为 RDB 文件只用作后备用途，建议只在 Slave 上持久化 RDB 文件，而且只要 15 分钟备份一次就够了，只保留 save 900 1 这条规则。 启用 AOF 的情况下由于会持续 IO，造成新文件的阻塞不可避免，应当尽量把基础大小 64M 改到 5G 以上。 第 10 章 主从复制 10.1 介绍 主机数据更新后根据配置和策略，自动同步到备机的 master/slaver 机制，Master 以写为主，Slave 以读为主。（主服务器进行写操作，写完后复制到从服务器，读在从服务器进行）。一主多从。 作用：读写分离，性能扩展；容灾快速恢复。 10.2 模拟一主多从 【原本默认启动时不需要 redis.conf，本机为了按照指定的配置启动，将安装包中的 redis.conf 拷贝了一份，放到了/etc/redis.conf，现在在此基础上进一步进行优化】 新建/root/apps/data/目录，拷贝一份 redis.conf，并做好配置。 /root/apps/data/目录下创建三份 redis_xxx.conf 每个 redis_xxx.conf 文件中写入下列内容： | include /root/apps/redis/redis.conf pidfile /var/run/redis_6379.pid port 6379 dbfilename dump6379.rdb | include /root/apps/redis/redis.conf pidfile /var/run/redis_6380.pid port 6380 dbfilename dump6380.rdb | include /root/apps/redis/redis.conf pidfile /var/run/redis_6381.pid port 6381 dbfilename dump6381.rdb | | — | — | — | 按照指定配置文件的方式启动： 查看进程 Redis-cli -p 指定端口号连接 3 台 redis 服务，并用 info replicaction 查看主机状态，可以地看到 3 台都是主服务器 设置从服务器：在 6380 和 6381 的从服务器中执行 slaveof 127.0.0.1 6379，再执行 info replication 查看状态 10.3 特点 6379 主机：可以写入数据、也可以读取数据 6380、6381 从机：只能读取数据，不能写入数据，写入数据报错。 6379 主机挂掉，重启即可，数据仍然有效。 10.4 常用技巧 10.4.1 一主二仆 从服务挂掉，重启后是主服务器，不是从服务器。重新连接到主服务器，仍然可以获取主服务器的所有数据。 这里如果有新加入的从服务器应该是同样的效果。 主服务器挂掉，从服务器不会上位。主服务器重启，仍然能连接到两个从服务器。 原理： 从服务器成功连接到主服务器后发送一个 sync 命令给 master（主动方 slave） master 接到命令后启动后台存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master 将传送整个数据文件到 slave，slave 完成数据的同步。 全量复制：slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。 后续每次 master 进行写操作，就会和从服务器进行数据同步。（主动方 master） 增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave,完成同步 10.4.2 薪火相传 上一个 Slave 可以是下一个 slave 的 Master，Slave 同样可以接收其他 slaves 的连接和同步请求，那么该 slave 作为了链条中下一个的 master, 可以有效减轻 master 的写压力,去中心化降低风险。 祖先服务器是主服务器，其他都是从服务器。 特点与一主二仆相同。 10.4.3 反客为主 当一个 master 宕机后，后面的 slave 可以立刻升为 master，其后面的 slave 不用做任何修改。 用 slaveof no one 将从机变为主机。需要手动完成，且原先已经建立了主从关系。 10.4.4 哨兵模式 反客为主的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。 在/root/apps/data/目录下创建 sentinel.conf 文件中写入：sentinel monitor mymaster 127.0.0.1 6379 1 mymaster 为监控对象起的服务器名称， 1 为至少有多少个哨兵同意迁移的数量。 启动哨兵：redis-sentinel /路径/sentinel.conf 默认情况下，哨兵会监视主机状态，监控到机挂掉，会将一台从机升级为主机，将挂掉的主机当为从机。 选举策略： 从机的 Redis.conf 中 replica-priority 值设置最小的。 偏移量最大的：偏移量值获得原主机数据最完整的。 选择 runid 最小的：runid 是 redis 启动时随机生成的一个 40 位 runid 缺点：存在复制延时。由于所有的写操作都是先在 Master 上操作，然后同步更新到 Slave 上，所以从 Master 同步到 Slave 机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave 机器数量的增加也会使这个问题更加严重。 java 代码实现哨兵模式，参考如下： 123456789101112131415161718192021private static JedisSentinelPool jedisSentinelPool=null;public static Jedis getJedisFromSentinel()&#123;if(jedisSentinelPool==null)&#123; Set&lt;String&gt; sentinelSet=new HashSet&lt;&gt;(); sentinelSet.add(&quot;192.168.11.103:26379&quot;);//哨兵的地址 JedisPoolConfig jedisPoolConfig =new JedisPoolConfig(); jedisPoolConfig.setMaxTotal(10); //最大可用连接数 jedisPoolConfig.setMaxIdle(5); //最大闲置连接数 jedisPoolConfig.setMinIdle(5); //最小闲置连接数 jedisPoolConfig.setBlockWhenExhausted(true); //连接耗尽是否等待 jedisPoolConfig.setMaxWaitMillis(2000); //等待时间 jedisPoolConfig.setTestOnBorrow(true); //取连接的时候进行一下测试 ping pong jedisSentinelPool=new JedisSentinelPool(&quot;mymaster&quot;,sentinelSet,jedisPoolConfig);//mymaster是sentinel.conf文件中指定的原主机名称 return jedisSentinelPool.getResource(); &#125;else&#123;return jedisSentinelPool.getResource(); &#125;&#125; 第 11 章 Redis 集群 11.1 集群介绍 11.1.1 设置集群的意义 解决容量问题 解决并发写操作，分摊压力。 主从模式，薪火相传模式，主机宕机，导致 ip 地址发生变化，应用程序中配置需要修改对应的主机地址、端口等信息。 不足： 多键操作是不被支持的 多键的 Redis 事务是不被支持的。lua 脚本不被支持 由于集群方案出现较晚，很多公司已经采用了其他的集群方案，而代理或者客户端分片的方案想要迁移至 redis cluster，需要整体迁移而不是逐步过渡，复杂度较大。 11.1.2 集群概念 redis30 之前，通过代理主机处理上述问题，3.0 时，redis 提供了无中心化集群配置。 Redis 集群实现了对 Redis 的水平扩容，即启动 N 个 redis 节点，将整个数据库分布存储在这 N 个节点中，每个节点存储总数据的 1/N。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）：即使集群中有一部分节点失效或者无法进行通讯，集群也可以继续处理命令请求。 11.2 集群操作 11.2.1 搭建集群 新建/root/apps/data/目录，拷贝一份 redis.conf，并做好配置。 /root/apps/data/目录下创建 6 份 redis_xxx.conf，每个 redis_xxx.conf 文件中写入下列内容： | include /root/apps/redis/redis.conf pidfile /var/run/redis_6379.pid port 6379 dbfilename dump6379.rdb cluster-enabled yes cluster-config-file nodes-6379.conf cluster-node-timeout 15000 | include /root/apps/redis/redis.conf pidfile /var/run/redis_6380.pid port 6380 dbfilename dump6380.rdb cluster-enabled yes cluster-config-file nodes-6380.conf cluster-node-timeout 15000 | include /root/apps/redis/redis.conf pidfile /var/run/redis_6381.pid port 6381 dbfilename dump6381.rdb cluster-enabled yes cluster-config-file nodes-6381.conf cluster-node-timeout 15000 | | — | — | — | | include /root/apps/redis/redis.conf pidfile /var/run/redis_6389.pid port 6389 dbfilename dump6389.rdb cluster-enabled yes cluster-config-file nodes-6389.conf cluster-node-timeout 15000 | include /root/apps/redis/redis.conf pidfile /var/run/redis_6390.pid port 6390 dbfilename dump6390.rdb cluster-enabled yes cluster-config-file nodes-6390.conf cluster-node-timeout 15000 | include /root/apps/redis/redis.conf pidfile /var/run/redis_6391.pid port 6391 dbfilename dump6391.rdb cluster-enabled yes cluster-config-file nodes-6391.conf cluster-node-timeout 15000 | 按照指定配置文件的方式启动并查看进程： 在启动 redis-server 的目录下，查看 nodes-xxxx.conf 节点文件是否生成 redis 安装后，会在/usr/loca 下存放一份 redis-cli 文件，平时连接 redis 时使用的就是这里的 redis-cli。但进行节点合体，需要找到 redis 的安装包，使用其 src 目录中原有的 redis-cli 文件：在改目录下执行下述命令： redis-cli --cluster create --cluster-replicas 1 192.168.150.135:6379 192.168.150.135:6380 192.168.150.135:6381 192.168.150.135:6389 192.168.150.135:6390 192.168.150.135:6391 这里需要写真实 ip，不能写 127.0.0.1 -replicas 1 采用最简单的方式配置集群，一台主机，一台从机，正好三组。 连接集群：在任意路径下执行redis-cli -c -p 6379，即可完成集群的连接，这里的端口任意，因为是集群，连接到哪一台都一样。 查看集群信息：cluster nodes。根据 id 确认主从关系。 11.2.2 集群分配原理 一个集群至少要有三个主节点。 选项--cluster-replicas 1 表示为集群中的每个主节点创建一个从节点。 分配原则尽量保证每个主数据库运行在不同的 IP 地址，每个从库和主库不在一个 IP 地址上。 一个 Redis 集群包含 16384 个插槽（hash slot），数据库中的每个键都属于这 16384 个插槽的其中一个，集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽，其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和。 CRC：循环校验码 校验和：传输位数的累加。 集群中的每个节点负责处理一部分插槽。上述例子中： 节点 A 负责处理 0 号至 5460 号插槽。 节点 B 负责处理 5461 号至 10922 号插槽。 节点 C 负责处理 10923 号至 16383 号插槽。 11.2.3 数据保存、查询机制 插入数据时，首先根据 key 计算出 CRC16 校验和，根据校验和所在范围，连接到相应的主服务器。同时，要查询改 key 所对应的值，需要到对应的主机去。 如果一条语句插入多个数据，由于 key 计算的校验和不同，会导致数据分布到不同的的主机去，此时 redis 会报错，可以通过&#123;&#125;定义组的概念，这样计算校验和会根据&#123;&#125;中的名称去计算。 查询集群中的值： cluster keyslot &lt;key&gt;：查询 key 所在的插槽值 cluster countkeysinslot 插槽值：查询插槽值对应的数，要求这个插槽在当前主机插槽范围内， cluster getkeysinslot 插槽值 数量：返回改插槽值中对应数量的数据的 key 11.2.4 集群故障恢复 主节点下线，从节点会在 15s 内上升位主节点。 主节点重新上线后，变为从节点。 如果某一段插槽的主从都挂掉，而配置文件的cluster-require-full-coverage为 yes ，那么，整个集群都挂掉 如果某一段插槽的主从都挂掉，而配置文件的cluster-require-full-coverage 为 no ，那么，该插槽数据全都不能使用，也无法存储。其他插槽的数据不受影响。 11.3 集群的 jedis 开发 即使连接的不是主机，集群会自动切换主机存储。主机写，从机读。 无中心化主从集群。无论从哪台主机写的数据，其他主机上都能读到数据。 123456789public class JedisClusterTest &#123; public static void main(String[] args) &#123; Set&lt;HostAndPort&gt;set =new HashSet&lt;HostAndPort&gt;(); set.add(new HostAndPort(&quot;192.168.31.211&quot;,6379));//端口号只要是在集群内，就能连接上集群。 JedisCluster jedisCluster=new JedisCluster(set); jedisCluster.set(&quot;k1&quot;, &quot;v1&quot;); System.out.println(jedisCluster.get(&quot;k1&quot;)); &#125;&#125; 11.4 Redis 集群的应用 11.4.1 缓存穿透——不存在的 key 问题描述： 一般情况下，如果有用户请求过来，先查缓存，如果缓存中存在数据，则直接返回。如果缓存中不存在，则再查数据库，如果数据库中存在，则将数据放入缓存，然后返回。如果数据库中也不存在，则直接返回失败。 但如果出现以下这两种特殊情况：用户请求的 id 在缓存中不存在、恶意用户伪造不存在的 id 发起请求。这样的用户请求导致的结果是：每次从缓存中都查不到数据，而需要查询数据库，同时数据库中也没有查到该数据，也没法放入缓存。也就是说，每次这个用户请求过来的时候，都要查询一次数据库。 很显然，缓存根本没起作用，好像被穿透了一样，每次都会去访问数据库。如果此时穿透了缓存，而直接数据库的请求数量非常多，数据库可能因为扛不住压力而挂掉。 解决方案： **对空值缓存：**如果一个查询返回的数据为空（不管是数据是否不存在），我们仍然把这个空结果（null）进行缓存，设置空结果的过期时间会很短，最长不超过五分钟 **设置可访问的名单（白名单）：**使用 bitmaps 类型定义一个可以访问的名单，名单 id 作为 bitmaps 的偏移量，每次访问和 bitmap 里面的 id 进行比较，如果访问 id 不在 bitmaps 里面，进行拦截，不允许访问。 采用布隆过滤器：(布隆过滤器（Bloom Filter）是 1970 年由布隆提出的。它实际上是一个很长的二进制向量(位图)和一系列随机映射函数（哈希函数）。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。)将所有可能存在的数据哈希到一个足够大的 bitmaps 中，一个一定不存在的数据会被这个 bitmaps 拦截掉，从而避免了对底层存储系统的查询压力。 **进行实时监控：**当发现 Redis 的命中率开始急速降低，需要排查访问对象和访问的数据，和运维人员配合，可以设置黑名单限制服务。 11.4.2 缓存击穿——热门 key 过期 问题描述： 缓存击穿是一个热点的 Key，有大并发集中对其进行访问，突然间这个 Key 失效了，导致大并发全部打在数据库上，导致数据库压力剧增。这种现象就叫做缓存击穿。 解决方案： **预先设置热门数据：**在 redis 高峰访问之前，把一些热门数据提前存入到 redis 里面，加大这些热门数据 key 的有效时长，甚至永不过期。 **实时调整：**现场监控哪些数据热门，实时调整 key 的过期时长。 使用锁： 就是在缓存失效的时候（判断拿出来的值为空），不是立即去 load db。 先使用缓存工具的某些带成功操作返回值的操作（比如 Redis 的 SETNX）去 set 一个 mutex key 当操作返回成功时，再进行 load db 的操作，并回设缓存,最后删除 mutex key； 当操作返回失败，证明有线程在 load db，当前线程睡眠一段时间再重试整个 get 缓存的方法。 11.4.3 缓存雪崩——大量 key 过期 问题描述： 当某一个时刻出现大规模的缓存失效的情况，那么就会导致大量的请求直接打在数据库上面，导致数据库压力巨大，如果在高并发的情况下，可能瞬间就会导致数据库宕机。这时候如果运维马上又重启数据库，马上又会有新的流量把数据库打死。这就是缓存雪崩。 解决方案： 构建多级缓存架构：nginx 缓存 + redis 缓存 +其他缓存（ehcache 等） 使用锁或队列：用加锁或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。不适用高并发情况 设置过期标志更新缓存：记录缓存数据是否过期（设置提前量），如果过期会触发通知另外的线程在后台去更新实际 key 的缓存。 将缓存失效时间分散开：比如我们可以在原有的失效时间基础上增加一个随机值，比如 1-5 分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 第 12 章 分布式锁 12.1 问题描述 随着业务发展的需要，原单体单机部署的系统被演化成分布式集群系统，由于分布式系统多线程、多进程并且分布在不同机器上，这将使原单机部署情况下的并发控制锁策略失效，单纯的 Java API 并不能提供分布式锁的能力。为了解决这个问题就需要一种跨 JVM 的互斥机制来控制共享资源的访问，这就是分布式锁要解决的问题！ 分布式锁主流的实现方案： 基于数据库实现分布式锁 基于缓存（Redis 等） 基于 Zookeeper 每一种分布式锁解决方案都有各自的优缺点： 性能：redis 最高 可靠性：zookeeper 最高 为了确保分布式锁可用，至少要确保锁的实现同时满足以下四个条件： 互斥性。在任意时刻，只有一个客户端能持有锁。 不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。 解铃还须系铃人。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。 加锁和解锁必须具有原子性。 12.2 Redis 实现分布式锁——指令操作 setnx &lt;key&gt; &lt;value&gt;：使用 setnx 指令，对 key 加锁，只能再在删除 key 后设置新值。 释放锁指令：del &lt;key&gt; 也可以通过设置过期时间到期释放：expire &lt;key&gt; 时间值 为了避免设置完锁后，无法设置过期时间的问题（如设置完锁后服务器挂掉），可以在设置锁的同时，指定过期时间：set &lt;key&gt; &lt;value&gt; nx ex 时间值 12.3 Redis 实现分布式锁——Jedis 操作 12.3.1 简洁版（redis 指令版的转化，并做一点优化：设置锁的时候指定过期时间） 12345678910111213141516171819202122232425262728@GetMapping(&quot;testLock&quot;)public void testLock()&#123; //1获取锁，setne Boolean lock = redisTemplate.opsForValue().setIfAbsent(&quot;lock&quot;, &quot;111&quot;, 3, TimeUnit.SECOND);//等价于setnx lock 111 ex 3 nx //2获取锁成功、查询num的值 if(lock)&#123; Object value = redisTemplate.opsForValue().get(&quot;num&quot;); //2.1判断num为空return if(StringUtils.isEmpty(value))&#123; return; &#125; //2.2有值就转成成int int num = Integer.parseInt(value+&quot;&quot;); //2.3把redis的num加1 redisTemplate.opsForValue().set(&quot;num&quot;, ++num); //2.4释放锁，del redisTemplate.delete(&quot;lock&quot;); &#125;else&#123; //3获取锁失败、每隔0.1秒再获取 try &#123; Thread.sleep(100); testLock(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 12.3.2 优化 1：设置锁唯一 id 上述方式中分布式锁存在的问题（可能会释放其他服务器的锁）： 场景：如果业务逻辑的执行时间是 7s。执行流程如下 index1 业务逻辑没执行完（服务器卡顿），3 秒后锁被自动释放。 index2 获取到锁，执行业务逻辑，3 秒后锁被自动释放。 index3 获取到锁，执行业务逻辑 index1 业务逻辑执行完成，开始调用 del 释放锁，这时释放的是 index3 的锁，导致 index3 的业务只执行 1s 就被别人释放。 最终等于没锁的情况。 解决：setnx 获取锁时，设置一个指定的唯一值（例如：uuid）；释放前获取这个值，判断是否自己的锁。 UUID 防止误删（将前面设置给 lock 的无意义的 111，修改为 uuid 值，确保不重复、唯一）： 12345678910111213141516171819202122232425262728293031@GetMapping(&quot;testLock&quot;)public void testLock()&#123; //1获取锁，setnx String uuid = UUID.randomUUID.toString(); Boolean lock = redisTemplate.opsForValue().setIfAbsent(&quot;lock&quot;, uuid, 3, TimeUnit.SECOND);//等价于setnx lock uuid ex 3 nx //2获取锁成功、查询num的值 if(lock)&#123; Object value = redisTemplate.opsForValue().get(&quot;num&quot;); //2.1判断num为空return if(StringUtils.isEmpty(value))&#123; return; &#125; //2.2有值就转成成int int num = Integer.parseInt(value+&quot;&quot;); //2.3把redis的num加1 redisTemplate.opsForValue().set(&quot;num&quot;, ++num); //2.4释放锁，del if(uuid.equals((String)redisTemplate.opsForValue().get(&quot;lock&quot;)))&#123; redisTemplate.delete(&quot;lock&quot;); &#125; &#125;else&#123; //3获取锁失败、每隔0.1秒再获取 try &#123; Thread.sleep(100); testLock(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 12.3.3 优化 2：LUA 脚本保证原子性 上述操作中还存在的问题（删除缺乏原子性）： 问题：删除操作缺乏原子性。 场景： index1 执行删除时，查询到的 lock 值确实和 uuid 相等。 index1 执行删除前，lock 刚好过期时间已到，被 redis 自动释放。 index2 获取了 lock，index2 线程获取到了 cpu 的资源，开始执行方法。 index1 执行删除，此时会把 index2 的 lock 删除。 利用 LUA 脚本的原子性，进行代码优化 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@GetMapping(&quot;testLockLua&quot;)public void testLockLua() &#123; //1 声明一个uuid ,将做为一个value 放入我们的key所对应的值中 String uuid = UUID.randomUUID().toString(); //2 定义一个锁：lua 脚本可以使用同一把锁，来实现删除！ String skuId = &quot;25&quot;; // 访问skuId 为25号的商品 100008348542 String locKey = &quot;lock:&quot; + skuId; // 锁住的是每个商品的数据 // 3 获取锁 Boolean lock = redisTemplate.opsForValue().setIfAbsent(locKey, uuid, 3, TimeUnit.SECONDS); // 第一种： lock 与过期时间中间不写任何的代码。 // redisTemplate.expire(&quot;lock&quot;,10, TimeUnit.SECONDS);//设置过期时间 // 如果true if (lock) &#123; // 执行的业务逻辑开始 // 获取缓存中的num 数据 Object value = redisTemplate.opsForValue().get(&quot;num&quot;); // 如果是空直接返回 if (StringUtils.isEmpty(value)) &#123; return; &#125; // 不是空 如果说在这出现了异常！ 那么delete 就删除失败！ 也就是说锁永远存在！ int num = Integer.parseInt(value + &quot;&quot;); // 使num 每次+1 放入缓存 redisTemplate.opsForValue().set(&quot;num&quot;, String.valueOf(++num)); /*使用lua脚本来锁*/ // 定义lua 脚本 String script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;; // 使用redis执行lua执行 DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;(); redisScript.setScriptText(script); // 设置一下返回值类型 为Long // 因为删除判断的时候，返回的0,给其封装为数据类型。如果不封装那么默认返回String 类型， // 那么返回字符串与0 会有发生错误。 redisScript.setResultType(Long.class); // 第一个要是script 脚本 ，第二个需要判断的key，第三个就是key所对应的值。 redisTemplate.execute(redisScript, Arrays.asList(locKey), uuid); &#125; else &#123; // 其他线程等待 try &#123; // 睡眠 Thread.sleep(1000); // 睡醒了之后，调用方法。 testLockLua(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 12.3.4 项目中使用 定义 key 时应该是为每个 sku 定义的，也就是每个 sku 有一把锁。 String locKey =“lock:”+skuId; // 锁住的是每个商品的数据 Boolean lock = redisTemplate.opsForValue().setIfAbsent(locKey, uuid,3,TimeUnit.SECONDS); 第 13 章 Redis6.0 新特性 13.1 ACL 13.1.1 简介 Redis ACL 是 Access Control List（访问控制列表）的缩写，该功能允许根据可以执行的命令和可以访问的键来限制某些连接。 在 Redis 5 版本之前，Redis 安全规则只有密码控制还有通过 rename 来调整高危命令比如 flushdb ， KEYS* ， shutdown 等。Redis 6 则提供 ACL 的功能对用户进行更细粒度的权限控制： 接入权限:用户名和密码 可以执行的命令 可以操作的 KEY 13.1.2 常用命令 acl list：查看所有用户的权限列表 acl cat：查看添加权限指令类别 加参数类型名可以查看类型下具体命令 acl whoami：查看当前用户 acl setuser user1：创建新用户，赋予默认权限 首次设置权限为 off，表示未启用。需要设置为 on acl setuser user2 on &gt;password ~cached:* +get：设置有用户名、密码、ACL 权限、并启用的用户 用户名：user2 密码：password 操作 key 要求：带 cached:操作 操作权限：只能操作 get 指令 auth 用户名 密码：切换用户 13.2 IO 多线程 13.2.1 简介 IO 多线程其实指客户端交互部分的网络 IO 交互处理模块多线程，而非执行命令多线程。Redis6 执行命令依然是单线程。 13.2.2 原理架构 Redis 6 加入多线程,但跟 Memcached 这种从 IO 处理到数据访问多线程的实现模式有些差异。Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程。之所以这么设计是不想因为多线程而变得复杂，需要去控制 key、lua、事务，LPUSH/LPOP 等等的并发问题。整体的设计大体如下： 另外，多线程 IO 默认也是不开启的，需要再配置文件中配置 io-threads-do-reads yes io-threads 4 13.2.3 工具支持 Cluster 之前老版 Redis 想要搭集群需要单独安装 ruby 环境，Redis 5 将 redis-trib.rb 的功能集成到 redis-cli 。另外官方 redis-benchmark 工具开始支持 cluster 模式了，通过多线程的方式对多个分片进行压测。 13.3 其他新功能 RESP3 新的 Redis 通信协议：优化服务端与客户端之间通信 Client side caching 客户端缓存：基于 RESP3 协议实现的客户端缓存功能。为了进一步提升缓存的性能，将客户端经常访问的数据 cache 到客户端。减少 TCP 网络交互。 Proxy 集群代理模式：Proxy 功能，让 Cluster 拥有像单实例一样的接入方式，降低大家使用 cluster 的门槛。不过需要注意的是代理不改变 Cluster 的功能限制，不支持的命令还是不会支持，比如跨 slot 的多 Key 操作。 Modules API：Redis 6 中模块 API 开发进展非常大，因为 Redis Labs 为了开发复杂的功能，从一开始就用上 Redis 模块。Redis 可以变成一个框架，利用 Modules 来构建不同系统，而不需要从头开始写然后还要 BSD 许可。Redis 一开始就是一个向编写各种系统开放的平台。","tags":[{"name":"Redis","slug":"Redis","permalink":"https://sk370.github.io/tags/Redis/"}]},{"title":"Srping Boot2","date":"2022-08-02T12:29:36.000Z","path":"2022/08/02/springboot2/SpringBoot2/","text":"SpringBoot 是 Spring 官方整合的 Spring 框架，能够快速创建生产级别的 Spring 应用。 Spring 基础 熟悉 Maven 版本概览： 2.7.2 CURRENT GA（学习版本为：2.3.4.RELEASE） 3.0.0-SNAPSHOT SNAPSHOT 3.0.0-M4 PRE 环境要求（基于 Spring Boot 2.7.2）： Java：≥8 Spring Framework：≥5.3.22 Maven：≥3.5 Spring Boot 2.7.2 支持的 Servlet 容器版本： Tomacat 9.0：4.0 Jetty 9.4：3.1 Jetty 10.0：4.0 Undertow 2.0：4.0 1. Spring Boot 介绍 1.1 Spring Boot 用途 SpringBoot 是 Spring 官方整合的 Spring 框架，能够快速创建生产级别的 Spring 应用。 一句话介绍： SpringBoot 是整合 Spring 技术栈的一站式框架【spring boot 是对 spring 框架整合的整合】 SpringBoot 是简化 Spring 技术栈的快速开发脚手架 优点： 创建独立 Spring 应用 内嵌 web 服务器 自动配置 Spring 以及第三方功能 提供生产级别的监控、健康检查及外部化配置 无代码生成、无需编写 XML 1.2 spring boot 出现背景 1.2.1 微服务（2014 年） 微服务是一种架构风格 一个应用拆分为一组小型服务 每个服务运行在自己的进程内，也就是可独立部署和升级 服务之间使用轻量级 HTTP 交互 服务围绕业务功能拆分 可以由全自动部署机制独立部署 去中心化，服务自治。服务可以使用不同的语言、不同的存储技术 1.2.2 分布式 分布式的难点 远程调用 服务发现 负载均衡 服务容错 配置管理 服务监控 链路追踪 日志管理 任务调度 分布式的解决 spring boot + spring cloud 1.2.3 云原生 1.3 创建 Spring Boot2 项目（以 web 工程为例） 1.3.1 手工搭建 创建 maven 工程 pom.xml 文件中引入依赖 123456789101112&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; spring-boot-starter ：核⼼模块，包括⾃动配置⽀持、⽇志和 YAML 添加 test 模块 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; spring-boot-starter-test ：测试模块，包括 JUnit、 Hamcrest、 Mockito 添加 web 模块 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency 从上图可以看到 web 模块依赖于 boot-starter 模块。 web 模块比 boot-starter 模块多引入了 json、tomcat、web、webmvc 四个模块 总结：只需要引入 web 模块即可，boot-starter 模块会依赖传递加载。 src/main/java/iceriver.boot 包下创建主程序，名称任意 1234567891011121314package iceriver.boot;import org.springframework.boot.SpringApplication;/** * @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/8/3 20:10 */@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MainApplication.class,args); &#125;&#125; src/main/java/iceriver.boot.controller 包下创建处理请求的控制器 12345678910111213141516package iceriver.boot.controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/8/3 20:12 */@RestControllerpublic class HelloController &#123; @RequestMapping(&quot;/hello&quot;) public String handle01()&#123; return &quot;Hello, Spring Boot 2!&quot;; &#125;&#125; 运行 src/main/java/iceriver.boot 包下的主程序 main 方法 浏览器访问http://localhost:8080/hello 修改端口号：在 resources 目录下创建 application.properties 文件【文件名称固定，称为核心配置文件】 1server.port=8888 部署： pom.xml 文件中添加插件 12345678 &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 执行 maven 打包命令： 执行java -jar jar包名命令 1.3.2 spring initializr【spring 初始化向导】 创建工程/模块 选择需要的依赖 2. Spring Boot 的底层注解 2.1 第一组：@Configuration 和@Bean 2.1.1 @Configuration 将类声明为配置类，【参见 Spring5 4.5 完全注解开发02.Spring5】 @Configuration(proxyBeanMethods = true)默认为 true，此时 Spring Boot 总会检查这个组件是否在容器中存在，存在时不会再创建。 此时不论是通过 MainApplication.java 主程序从容器获取组件 还是从配置类 MyConfig.java 的方法获取组件，都是同一个对象（单实例） 123456789101112131415161718192021222324252627282930313233343536package iceriver.boot;import iceriver.boot.bean.Pet;import iceriver.boot.bean.User;import iceriver.boot.config.MyConfig;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ConfigurableApplicationContext;/** * @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/8/3 20:10 *///@SpringBootApplication(scanBasePackages = &quot;iceriver.boot&quot;)@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; //1、返回IOC容器 ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); //2、查看容器里面的组件 String[] names = run.getBeanDefinitionNames(); for (String name : names) &#123; System.out.println(name); &#125; //3、从容器获取组件 Pet tom01 = run.getBean(&quot;tom&quot;, Pet.class); Pet tom02 = run.getBean(&quot;tom&quot;, Pet.class); System.out.println(tom01 == tom02);//true MyConfig bean = run.getBean(MyConfig.class); //System.out.println(bean);//iceriver.boot.config.MyConfig$$EnhancerBySpringCGLIB$$2bce5da6@585ac855 User user = bean.user01(); User user1 = bean.user01(); System.out.println(user == user1);//true &#125;&#125; @Configuration(proxyBeanMethods = false)时： 通过 MainApplication.java 主程序从容器获取组件是同一个对象 从配置类 MyConfig.java 的方法获取组件，不是同一个对象 ⭐ 该配置为 true 时，解决了组件依赖问题：比如 User 类中有 Pet 类的对象，因为在创建 User 时，执行到创建 Pet 对象时，使用的是配置类调用方法创建 Pet 对象： 设置为 true：跳过了调用方法创建 Pet 对象，直接在容器中调用，保证了 User 对象也是唯一的。 设置为 false：调用方法创建 Pet 对象，由于 Pet 对象不一致，导致 User 对象不一致，创建 User 组件在容器中出现了两个 最佳实战 配置类组件之间无依赖关系用 Lite 模式加速容器启动过程，减少判断，设置为 false 配置类组件之间有依赖关系，方法会被调用得到之前单实例组件，用 Full 模式，设置为 true 2.1.2 @Bean 给容器中添加组件，用在方法前，方法名为组件 id，返回类型为组件类型，返回值为组件在容器中的实例。 可以使用如下的方式@Bean(&quot;tom&quot;)使用自定义名称（不以方法名作为组件名） 通过 MainApplication.java 主程序从容器获取组件： 默认情况容器中的组件是单实例的 12345678910111213141516171819202122package iceriver.boot.config;import iceriver.boot.bean.Pet;import iceriver.boot.bean.User;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/8/3 23:00 */@Configurationpublic class MyConfig &#123; @Bean public User user01()&#123; return new User(&quot;张三&quot;, 18); &#125; @Bean(&quot;tom&quot;) public Pet tomPet()&#123; return new Pet(&quot;tom01&quot;); &#125;&#125; 1234567891011121314151617181920212223242526272829package iceriver.boot;import iceriver.boot.bean.Pet;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ConfigurableApplicationContext;/** * @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/8/3 20:10 *///@SpringBootApplication(scanBasePackages = &quot;iceriver.boot&quot;)@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; //1、返回IOC容器 ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); //2、查看容器里面的组件 String[] names = run.getBeanDefinitionNames(); for (String name : names) &#123; System.out.println(name); &#125; //3、从容器获取组件 Pet tom01 = run.getBean(&quot;tom&quot;, Pet.class); Pet tom02 = run.getBean(&quot;tom&quot;, Pet.class); System.out.println(tom01 == tom02);//true &#125;&#125; 通过 MainApplication.java 主程序从容器获取配置类组件： 配置类是被 EnhancerBySpringCGLIB 增强了的代理类 1234567891011121314151617181920212223242526package iceriver.boot;import iceriver.boot.config.MyConfig;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ConfigurableApplicationContext;/** * @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/8/3 20:10 *///@SpringBootApplication(scanBasePackages = &quot;iceriver.boot&quot;)@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; //1、返回IOC容器 ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); //2、查看容器里面的组件 String[] names = run.getBeanDefinitionNames(); for (String name : names) &#123; System.out.println(name); &#125; //3、从容器获取组件（配置类） System.out.println(run.getBean(MyConfig.class));//iceriver.boot.config.MyConfig$$EnhancerBySpringCGLIB$$2bce5da6@585ac855 &#125;&#125; 2.2 第二组：@Component、@Controller、@Service、@Repository 【参见 Spring5 2.3.102.Spring5】 2.3 第三组：@ComponentScan 指定包扫描的路径，【参见 2.3.402.Spring5】 扫描到的组件如果有第二组的注解，则注册为组件 2.4 第四组：@Import 可以用在任何组件上，配合任何组件注解使用。 导入任意指定类型的组件： @Import(&#123;User.class, DBHelper.class&#125;) 给容器中自动创建出这两个类型的组件、默认组件的名字就是全类名（调用其无参构造器创建） 2.5 第五组：@Conditional 条件装配：满足指定条件，则进行注入 可以用在方法前，表示满足该条件，则对该方法进行 bea 注入 可以用在类前，表示满足该条件，才对该类中的方法进行 ben 注入。 2.6 第六组：@ImportSource 2.7 第七组：绑定配置文件 2.7.1 方式一：@**ConfigurationProperties+@**Component 用在 JavaBean 前，声明当前文件为一个配置文件类。 将 JavaBean 属性的值写在配置文件中，通过 spring boot 读取 properties 配置文件，并把它封装到 JavaBean 中。 @ConfigurationProperties(prefix = &quot;mycar&quot;) 需要与@Component 配合使用，将 JavaBean 通过@Component 声明为组件。 或者不使用@Component，在 MyConfig.java 中使用@Bean 创建组件，方法返回无参构造对象 2.7.2 方式二：@EnableConfigurationProperties +@ConfigurationProperties 对于无法对 JavaBean 使用@Component 的组件（如别人的 JavaBean），在 MyConfig.java 配置类前使用@EnableConfigurationProperties(Car.Class）——只能用在配置类上 作用 1：开启 Car 的配置绑定功能 作用 2：将 Car 这个组件自动注册到容器中 2.7.3 @ConfigurationProperties 将当前类与配置文件中的值绑定。绑定的只能是核心配置文件，只会在核心配置文件中查找值。 2.8 第八组：@SpringBootApplication 2.8.0 介绍 将某个类指定为主程序类/主配置类。他自动扫描的路径是同级的其他路径，同下@ComponentScan(“iceriver.boot”) 它是三个组件的合成： @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(“iceriver.boot”) 2.8.1 @SpringBootConfiguration 代表当前是一个配置类。 2.8.2 @EnableAutoConfiguration @AutoConfigurationPackage：导入主程序类所在位置的包下一系列组件。指定了默认的包规则。 12345@Import(AutoConfigurationPackages.Registrar.class) //给容器中导入一个组件public @interface AutoConfigurationPackage &#123;&#125;//利用Registrar给容器中导入一系列组件//将指定的一个包下的所有组件导入进来？MainApplication 所在包下。 @Import(AutoConfigurationImportSelector.class)：导入 META-INF/spring.factories 位置的文件中配置的组件（共 127 个，加载时全部加载，但使用时由于使用了@ConditionalOnClass 注解，可以实现按需配置，用户导入相关的场景启动器后才会激活该注解） 123456789101、利用getAutoConfigurationEntry(annotationMetadata);给容器中批量导入一些组件2、调用List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes)获取到所有需要导入到容器中的配置类3、利用工厂加载 Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader)；得到所有的组件4、从META-INF/spring.factories位置来加载一个文件。 默认扫描我们当前系统里面所有META-INF/spring.factories位置的文件 spring-boot-autoconfigure-2.3.4.RELEASE.jar包里面也有META-INF/spring.factories 文件里面写死了spring-boot一启动就要给容器中加载的所有配置类 虽然我们127个场景的所有自动配置启动的时候默认全部加载。xxxxAutoConfiguration按照条件装配规则（@Conditional），最终会按需配置。 2.8.3 @ComponentScan(“iceriver.boot”) 组件扫描 扫描到的组件如果有第二组的注解，则自动注册为组件 2.9 9 组 @SpringBootTest、@RunWith 3. Spring Boot 的自动配置 3.1 自动配置实现 基于 1.3.1（https://www.yuque.com/zhuyuqi/zna9x5/gs0vv5#bAIMR）创建的 spring boot2 项目进行分析 3.1.1 依赖配置 用户项目使用了 spring-boot-starter-parent 作为父项目进行依赖管理。 spring-boot-starter-parent 使用了 spring-boot-dependencies 作为父项目进行依赖管理。而 spring-boot-dependencies 中声明了几乎开发中常用的依赖版本号——自动仲裁机制。 依赖中 spring-boot-starter-*称为场景启动器，是一组整合好的方便的依赖描述符。只要引入 starter，满足该场景的常规依赖都会自动引入。 spring-boot-starter-web 底层基础依赖是 spring-boot-starter，是所有场景启动最底层的依赖。 *-spring-boot-starter： 第三方提供的简化开发的场景启动器。 3.1.2 自动配置内容 自动配置好 tomcat spring-boot-starter-web 场景启动器引入了 tomcat 场景启动器 自动配置好 SpringMVC spring-boot-starter-web 场景启动器引入了 springmvc 场景启动器 自动配置好了 springmvc 的常用组件（依赖） 配置好了 SpringMVC DispatcherServlet 以及其他依赖，如 jdbc、日志、字符编码等。 创建了默认的包结构： 官方文档 2.1 节，说明了使用默认的包结构，会自动进行包扫描，而不需要手动设置 当然可以通过@SpringBootApplication(scanBasePackages = &quot;iceriver.boot&quot;)扫描指定包 或者使用@ComponentScan(scanBasePackages = &quot;iceriver.boot&quot;)、@EnableAutoConfiguration、@SpringBootConfiguration三个代替 各种配置拥有默认值。如文件上传设定了最大上传大小 默认配置最终都是映射到某个类上，如：MultipartProperties 修改配置时，application.properties配置文件的值最终会绑定每个类上，这个类会在容器中创建对象 按需加载所有自动配置项 springBoot 所有的自动配置功能都在 spring-boot-autoconfigure 包里面 引入了哪些场景这个场景的自动配置才会开启 3.2 自动配置原理 【参见2.8https://www.yuque.com/zhuyuqi/zna9x5/gs0vv5#uvJdT】 3.2 自动配置流程 SpringBoot 先加载所有的自动配置类 xxxxxAutoConfiguration 每个自动配置类按照条件进行生效，默认都会绑定配置文件指定的值。xxxxProperties 里面拿。xxxProperties 和配置文件进行了绑定 生效的配置类就会给容器中装配很多组件 只要容器中有这些组件，相当于这些功能就有了 定制化配置 用户直接自己@Bean 替换底层的组件 用户去看这个组件是获取的配置文件什么值就去修改。 xxxxxAutoConfiguration —&gt; 组件 —&gt; xxxxProperties 里面拿值 ----&gt; application.properties 3.4 【心得】spring boot 开发最佳实践流程 引入场景依赖（场景启动器） 查看自动配置了哪些（选做） 自己分析，引入场景对应的自动配置一般都生效了 application.properties 核心配置文件中 debug=true 开启自动配置报告。在控制台输出中查看：Negative（不生效）\\Positive（生效） 是否需要修改 参照文档修改配置项 自己分析。xxxxProperties 绑定了配置文件的哪些。 自定义加入或者替换组件 @Bean、@Component。。。 自定义器 XXXXXCustomizer. 3.5 【扩展】springboot 工作流程 springboot 启动，读取 spring-boot-autoconfigure-2.7.2.jar 包下的/META-INF/spirng.factories 文件。读取 org.springframework.boot.autoconfigure.EnableAutoConfiguration 属性的值加载自动配置类。 根据自动配置类中指定的 XxxProperties 类设置自动配置的属性值，程序员根据 XxxProperties 类中指定的属性在 yml 配置文件中修改自动配置。 springboot 通过@ConditionalXxx 注解指定特定组件加入 IOC 容器时所需要的特定条件，条件满足时才会加入 IOC 容器。 4. 开发小技巧 4.1 Lombok 简化 JavaBean 开发 pom.xml 中引入 lombok 1234&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt; 安装 Lombok 插件 使用： JavaBean 只编写属性，不写 getter 和 setter JavaBean 前使用@Data 注解，会在编译时生成 getter 和 setter javabean 前使用@ToString，会在编译时自动生成 toString() JavaBean 前使用@AllArgsConstructor 注解，会在编译时生成全部参数的有参构造器 JavaBean 前使用@NoArgsConstructor 注解，会在编译时生成无参构造器 JavaBean 前使用@EqualsAndHashCode 注解，会在编译时生成 equals()和 hashcode() JavaBean 前使用@Slf4j 注解，可以在代码中使用log.info(&quot;str&quot;)语法在控制台输出内容 4.2 Dev-tools 重启 restart 功能： 热更新 reload 需要收费 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 使用：ctrl+f9 本质：和 idea 重启效果一样 4.3 yml 读取自定义属性 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 123456789101112@Configuration@ConfigurationProperties(prefix = &quot;short.message&quot;)@Data@NoArgsConstructor@AllArgsConstructorpublic class ShortMessageProperties &#123; private String accessKeyId;//私有id private String accessKeySecret;//私有key private String endpoint;//短信服务域名 private String signName;//短信签名 private String templateCode;//短信模板&#125; 5. Spring Boot2 核心技术（功能） 5.1 配置文件 5.1.1 application.properties 同时配置了.properties和.yml，两个文件会同时生效，里面的内容.properties会覆盖.yml 5.1.2 application.yml 简介：YAML 是 “YAML Ain’t Markup Language”（YAML 不是一种标记语言）的递归缩写。在开发的这种语言时，YAML 的意思其实是：“Yet Another Markup Language”（仍是一种标记语言）。 基本语法： key: value；kv 之间有空格 大小写敏感 使用缩进表示层级关系 缩进不允许使用 tab，只允许空格 缩进的空格数不重要，只要相同层级的元素左对齐即可 '#'表示注释 字符串无需加引号，如果要加，''与&quot;&quot;表示字符串内容会原样输出字符串/将转义字符转换 数据类型 字面量（k: v）：单个的、不可再分的值。date、boolean、string、number、null 对象：键值对的集合。map、hash、set、object 1234567#方式一：k: &#123;k1:v1,k2:v2,k3:v3&#125;#方式二：k: k1: v1 k2: v2 k3: v3 数组：一组按次序排列的值。array、list、queue 1234567#方式一：k: [v1,v2,v3]#方式二：k: - v1 - v2 - v3 5.1.3 配置提示 给自定义的类和配置文件添加输入提示 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 打包时排除，减少资源占用（对开发没有影响） 12345678910111213141516&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 5.2 web 开发 5.2.1 创建 web 项目 5.2.2 静态资源访问 在 resources 路径下：静态资源放在/static (/public 、/resources、/META-INF/resources路径下，则可以直接通过当前项目根路径/+静态资源名的方式访问到。 原理：浏览器地址栏的请求首先经过 spring mvc 的 dispatcherServlet 处理，如果控制器 controller 中有对应的请求方法，则按该方法处理。如果没有对应的请求方法，则被 servlet 默认的控制器处理，直接去访问静态资源。 修改静态资源访问前缀 控制器中设定的拦截请求为/**所有请求，这样会导致静态资源的访问和控制器处理的请求都被拦截。 开发中一般希望将静态资源内的访问、和控制器处理前端请求分离开，所以一般会在核心配置文件中间通过spring.mvc.static-path-pattern=/res/**给静态资源的访问增加前缀（res 可以设定为其他名字） 123spring: mvc: static-path-pattern: /res/** 此时访问地址形式为：当前项目路径/res/静态资源名 修改静态资源的路径（自定义静态资源路径） 在核心配置文件中通过spring.resources.static-locations=[class:/hh/]进行配置 123456spring: mvc: static-path-pattern: /res/** resources: static-locations: [classpath:/haha/] 访问时，地址形式为：当前项目路径/res/静态资源名【访问路径没变化，变的只是静态资源的位置】 访问 webjar webjar 是人们把前端的 js 文件等制作成了 spring 依赖，引入相关依赖后，相当于引入了前端 js 文件 https://www.webjars.org/ 12345&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt;&lt;/dependency&gt; 此时，可以通过 http://localhost:8080/webjars/**jquery/3.5.1/jquery.js**访问该 js 文件 同时，该 js 文件也可以通过静态资源访问前缀获得，即http://localhost:8080/res/webjars/**jquery/3.5.1/jquery.js** 欢迎页（index.html 页面） 把 index.html 当作静态资源，放在相关路径下即可解析为首页，访问当前项目路径即可作为欢迎页。 注意：静态资源访问前缀会影响访问地址，访问时需要加上访问前缀 也可以通过控制器编写 index 方法，路径用/，映射到 index 页面 自定义 Favicon 放到静态资源路径下，起名为favicon.ico，则每个页面都会带上小图标 注意：静态资源访问前缀会影响，访问带有访问前缀的的页面才会显示。 小图标保存在 session 中共享 5.2.3 静态资源配置原理 spring boot 启动时，加载的 springmvc 的自动配置类 WebMvcAutoConfiguration 生效。 【25 集，没看下去】 5.3 请求处理 请求映射（restful 风格）：spring boot 启动时 WebMvcAutoConfiguration 自动配置了 OrderedHiddenHttpMethodFilter 类，该类继承自HiddenHttpMethodFilter，从而能够处理 restful 风格的请求。 但默认情况下，spring.mvc.hiddenmethod.filter 值是 false，即使设置了隐藏域，发送 put、delete 请求时还是 post 请求。 只有在核心配置文件中将 spring.mvc.hiddenmethod.filter=true 后，才可以正常发送 put、delete 请求。 这个配置只针对于 form 表单使用 对于 postman 等这些能直接发送 put、delete 请求的不用配置 自定义请求方法时，在配置类中重新定义（此时隐藏域的 name 属性要设置为_m（原始为_method））： 请求映射处理原理（怎么知道是哪种请求）： 【参见 SpringMVC10.303.Spring MVC】 servlet 处理请求的方法是 doService()，doService()经过一大堆封装、重写之后变成了 DispatcherServlet 的 doDispatcher()，该方法中调用了 getHandler()得到所有 handlermappings，去看哪个 handlerMapping 能处理得到的请求 RequestMappingHandlerMapping：保存了所有@RequestMapping 和 handler 的映射规则。 WelcomePageHandlerMapping：是 springBoot 自动配置的，处理 index 页面（欢迎页）的请求，默认配置好了。 自定义 HandlerMapping【没讲】 注解： 请求映射注解： @RequestMapping @GetMapping、@PostMapping、@PutMapping、@DeleteMapping 请求传参注解： @PathVariable：路径占位符，将请求路径解析为参数（restful 请求） @RequestHeader：03.Spring MVC @ModelAttribute @RequestParam： @MatrixVariable：矩阵变量 请求参数以分号分割，如/cars/&#123;low=34;brand=byd,audi&#125; springBoot 默认仅用了矩阵变量，需要在核心配置文件中手动开启： urlpathhelper 需要单独写一章 @CookieValue @RequestBody： @RequestAttribute：从请求域获取参数 6. springboot 整合案例 6.0 整合 web 6.0.1 引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 只引入这一个依赖即可 6.1 整合 mybatis 6.1.1 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; spring 官方没有提供 mybtis 的 starer，这是 mybatis 官方提供的，在 spring-boot-dependencies 里就没有管理的依赖版本，所以要引入版本号。 操作 mybatis 需要数据库，为了方便使用再引入 druid 连接池（同样，spring-boot-dependencies 里没有管理 druid 的依赖） 12345678910&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.21&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.5&lt;/version&gt;&lt;/dependency&gt; 6.1.2 配置 mysql 数据源 在 application.properties 文件中配置 mysql 数据源 1234spring.datasource.url=jdbc:mysql://localhost:3306/testspring.datasource.username=rootspring.datasource.password=123456spring.datasource.driver-class-name=com.mysql.jdbc.Driver 数据库驱动使用 com.mysql.cj.jdbc.Driver 时，则 datasource.url 必须带时区，即spring.datasource.url=jdbc:mysql://localhost:3306/test?serverTimezone=UTC 6.1.3 使用 mybatis 建立好数据库，创建好对应的 Javabean。 编写 XxxMapper 类，制定好方法。 在 resources/mybatis/mapper 下创建好对应的 XxxMapper.xml 映射文件，写好 sql 语句。 在 application.properties 文件中配置 mybatis 的映射文件路径： 1mybtis.mapper.location:classpath*./mybatis/mapper/*Mapper.xml 为了能在控制台打印 sql 语句，再在 application.properties 文件中配置 mapper 的日志打印级别。 1logging.level.XxxMapper的全包名.debug 主启动类使用@MapperScan(“com.atguigu.crowd.mysql.mapper”)注解扫描所有类，或者使用@Mapper 标记单个类 【扩展：精简写法】 如果不创建*Mapper.xml 的映射文件（也就没在 application.properties 文件中声明映射文件路径），则需要在 XxxMapper 类中使用注解指定 sql 语句，如： 1234567@Mapperpublic interface UserMapper &#123; @Select(&quot;SELECT * FROM USER WHERE NAME = #&#123;name&#125;&quot;) User findByName(@Param(&quot;name&quot;) String name); @Insert(&quot;INSERT INTO USER(NAME, AGE) VALUES(#&#123;name&#125;, #&#123;age&#125;)&quot;) int insert(@Param(&quot;name&quot;) String name, @Param(&quot;age&quot;) Integer age);&#125; 6.2 整合 redis 6.2.1 引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt;&lt;/dependency&gt; 6.2.2 配置 redis 123456spring.redis.host=localhostspring.redis.port=6379spring.redis.pool.max-idle=8spring.redis.pool.min-idle=0spring.redis.pool.max-active=8spring.redis.pool.max-wait=-1 6.3 整合 Thymeleaf 6.3.1 引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 7. 扩展功能","tags":[{"name":"Srping Boot2","slug":"Srping-Boot2","permalink":"https://sk370.github.io/tags/Srping-Boot2/"}]},{"title":"MyBatis-Plus","date":"2022-08-02T10:28:22.000Z","path":"2022/08/02/mybatisplus/MyBatisPlus/","text":"MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。 最好学了 SpringBoot 之后【SpringBoot 使用时更简单】 也可以学习了 ssm 之后 代码发布地址: Github: https://github.com/baomidou/mybatis-plus Gitee: https://gitee.com/baomidou/mybatis-plus 文档发布地址:https://baomidou.com/pages/24112f 1. MyBatis-Plus 介绍 1.1 简介 MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。 1.2 特性 无侵入：只做增强不做改变，引入它不会对现有工程产生影响 损耗小：启动即会自动注入基本 CURD，性能基本无损耗，直接面向对象操作 强大的 CRUD 操作：内置通用 Mapper、通用 Service，仅仅通过少量配置即可实现单表大部分 CRUD 操作，更有强大的条件构造器，满足各类使用需求 支持 Lambda 形式调用：通过 Lambda 表达式，方便的编写各类查询条件，无需再担心字段写错 支持主键自动生成：支持多达 4 种主键策略（内含分布式唯一 ID 生成器 - Sequence），可自由配置，完美解决主键问题 支持 ActiveRecord 模式：支持 ActiveRecord 形式调用，实体类只需继承 Model 类即可进行强大的 CRUD 操作 支持自定义全局通用操作：支持全局通用方法注入（ Write once, use anywhere ） 内置代码生成器：采用代码或者 Maven 插件可快速生成 Mapper 、 Model 、 Service 、Controller 层代码，支持模板引擎等超多自定义配置 内置分页插件：基于 MyBatis 物理分页，开发者无需关心具体操作，配置好插件之后，写分页等同于普通 List 查询 分页插件支持多种数据库：支持 MySQL、MariaDB、Oracle、DB2、H2、HSQL、SQLite、Postgre、SQLServer 等多种数据库 内置性能分析插件：可输出 SQL 语句以及其执行时间，建议开发测试时启用该功能，能快速揪出慢查询 内置全局拦截插件：提供全表 delete 、 update 操作智能分析阻断，也可自定义拦截规则，预防误操作 1.3 框架结构 1.4 入门案例 1.5.1 开发环境 jdk8 Maven：3.6.1 Spring：5.3.1 My-Batis-Plus：3.4.3.4（注意：引入了 mybatisplus，就不要再引入 mybatis 和 mybatsi-spring，避免版本冲突（官网提示） 123456&lt;!--mybatis-plus--&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt;&lt;/dependency&gt; 1.5.2 创建数据表 123456789101112131415CREATE DATABASE `mybatis_plus` /*!40100 DEFAULT CHARACTER SET utf8mb4 */;use `mybatis_plus`;CREATE TABLE `user` (`id` bigint(20) NOT NULL COMMENT &#x27;主键ID&#x27;,`name` varchar(30) DEFAULT NULL COMMENT &#x27;姓名&#x27;,`age` int(11) DEFAULT NULL COMMENT &#x27;年龄&#x27;,`email` varchar(50) DEFAULT NULL COMMENT &#x27;邮箱&#x27;,PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO user (id, name, age, email) VALUES(1, &#x27;Jone&#x27;, 18, &#x27;test1@baomidou.com&#x27;),(2, &#x27;Jack&#x27;, 20, &#x27;test2@baomidou.com&#x27;),(3, &#x27;Tom&#x27;, 28, &#x27;test3@baomidou.com&#x27;),(4, &#x27;Sandy&#x27;, 21, &#x27;test4@baomidou.com&#x27;),(5, &#x27;Billie&#x27;, 24, &#x27;test5@baomidou.com&#x27;); 1.5.3 创建 Maven 工程（基于 Spring Boot） 新建项目： 不选择依赖，通过手动编辑 pom.xml 文件的形式添加 在 pom.xml 文件中添加依赖 空项目默认有 spring-boot-starter 和 spring-boot-starter-test 依赖 额外再引入 mybatis-plus-boot-starter、lombok（同时须下载该插件）、mysql-connector-java 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 配置数据源及日志输出（该日志用于在控制台查看 mybatis-plus 生成了什么 sql 语句） 在 resources 路径下新建 application.yml 核心配置文件 1234567891011121314spring:# 配置数据源信息 datasource: # 配置数据源类型 type: com.zaxxer.hikari.HikariDataSource # 配置连接数据库信息 driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:13306/mybatis_plus?characterEncoding=utf-8&amp;useSSL=false username: root password: dimitre123# 配置MyBatis日志mybatis-plus: configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl spring boot 2.0 内置 jdbc5 驱动 驱动类使用 com.mysql.jdbc.Driver url 使用：jdbc:mysql://localhost:13306/mybatis_plus?characterEncoding=utf-8&amp;useSSL=false spring boot 2.1 以上版本内置 jdbc8 驱动 驱动类使用 com.mysql.cj.jdbc.Driver url 使用：jdbc:mysql://localhost:13306/mybatis_plus? serverTimezone=GMT%2B8&amp;characterEncoding=utf-8&amp;useSSL=false 2. mybatis-plus 使用 2.1 Crud 功能 2.1.1 基本使用 创建 User JavaBean，使用@Data 注解生成无参构造器、getter、setter、equals、tosting、hashcode，没有生成带参的构造器。 注意：数据表中 id 使用的数据类型为 bigint，所以 User 类中的 id 使用了 long 类型 之所以这样做是因为 mybatis 插入数据时，自增的 id 使用的雪花算法，比较长。 创建 UserMapper 接口，继承自 BaseMapper，范型使用 User spring boot 的启动程序类，使用@MapperScan(“”)指定扫描的 mapper 包 扫描会把扫描到的类交给 ioc 容器管理，所以不是必须要在 UserMapper 类前使用@Repository 进行声明 但是 idea 工具在自动装配 usermapper 时，会报红线，如果要消除红线，可以添加@Repository 注解标记成持久层组件 在 test/iceriver.mybatisplus 目录下创建测试类 UserTests.java，使用@SpringBootTest 注解进行标记，实验增删改查方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public interface BaseMapper&lt;T&gt; extends Mapper&lt;T&gt; &#123; int insert(T entity); int deleteById(Serializable id); int deleteById(T entity); int deleteByMap(@Param(&quot;cm&quot;) Map&lt;String, Object&gt; columnMap); int delete(@Param(&quot;ew&quot;) Wrapper&lt;T&gt; queryWrapper); int deleteBatchIds(@Param(&quot;coll&quot;) Collection&lt;?&gt; idList); int updateById(@Param(&quot;et&quot;) T entity); int update(@Param(&quot;et&quot;) T entity, @Param(&quot;ew&quot;) Wrapper&lt;T&gt; updateWrapper); T selectById(Serializable id); List&lt;T&gt; selectBatchIds(@Param(&quot;coll&quot;) Collection&lt;? extends Serializable&gt; idList); List&lt;T&gt; selectByMap(@Param(&quot;cm&quot;) Map&lt;String, Object&gt; columnMap); default T selectOne(@Param(&quot;ew&quot;) Wrapper&lt;T&gt; queryWrapper) &#123; List&lt;T&gt; ts = this.selectList(queryWrapper); if (CollectionUtils.isNotEmpty(ts)) &#123; if (ts.size() != 1) &#123; throw ExceptionUtils.mpe(&quot;One record is expected, but the query result is multiple records&quot;, new Object[0]); &#125; else &#123; return ts.get(0); &#125; &#125; else &#123; return null; &#125; &#125; default boolean exists(Wrapper&lt;T&gt; queryWrapper) &#123; Long count = this.selectCount(queryWrapper); return null != count &amp;&amp; count &gt; 0L; &#125; Long selectCount(@Param(&quot;ew&quot;) Wrapper&lt;T&gt; queryWrapper); List&lt;T&gt; selectList(@Param(&quot;ew&quot;) Wrapper&lt;T&gt; queryWrapper); List&lt;Map&lt;String, Object&gt;&gt; selectMaps(@Param(&quot;ew&quot;) Wrapper&lt;T&gt; queryWrapper); List&lt;Object&gt; selectObjs(@Param(&quot;ew&quot;) Wrapper&lt;T&gt; queryWrapper); &lt;P extends IPage&lt;T&gt;&gt; P selectPage(P page, @Param(&quot;ew&quot;) Wrapper&lt;T&gt; queryWrapper); &lt;P extends IPage&lt;Map&lt;String, Object&gt;&gt;&gt; P selectMapsPage(P page, @Param(&quot;ew&quot;) Wrapper&lt;T&gt; queryWrapper);&#125; 2.1.2 自定义增删改查方法 使用自定义增删改查方法的流程与 mybatis 中操作一致 在 mapper 接口中定义 crud 方法 在 reosources 目录下创建 mapper 目录，创建对应的 mapper.xml 映射文件~~（是不是反过来了，默认不需要创建对应包名的 mapper 路径，如果是纯 mapper 路径需要指定？？需要测试下）~~ 如果映射文件不在 mapper 目录下，则需要在 application.yml 文件中进行配置，指定位置。 mybatis 中正好相反，需要在 resources 文件目录下创建 mapper.java 对应报名的 mapper 路径 在 mapper.xml 文件中编写 sql 语句 调用 mapper 接口中的方法进行使用 编写 crud 方法 12345678public interface UserMapper extends BaseMapper&lt;User&gt; &#123; /** * 自定义crud方法：根据id查询用户信息为map集合 * @param id * @return */ Map&lt;String, Object&gt; selectMapById(Long id);&#125; 编写映射文件 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;iceriver.mybatisplus.mapper.UserMapper&quot;&gt; &lt;!--Map&lt;String, Object&gt; selectMapById(Long id);--&gt; &lt;select id=&quot;selectMapById&quot; resultType=&quot;map&quot;&gt; select id,name,age,email from user where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 这里 resultType 之所以可以直接写 map，是因为 mybatisplus 定义的默认的类型别名。 执行测试 12345678910@SpringBootTestpublic class UserTests &#123; @Autowired private UserMapper userMapper; @Test public void testSelectMapById()&#123; Map&lt;String, Object&gt; map = userMapper.selectMapById(1L); System.out.println(map); &#125;&#125; 2.2 Service 接口 MyBatis-Plus 中有一个接口 IService 和其实现类 ServiceImpl，封装了常见的业务层逻辑。 由于实际开发中，service 层的方法非常复杂，mybatis 提供的通用 service 方法不能满足需求，为了既使用 mybatis 提供的 sevice 方法，又能满足实际开发的需要，可参照创建 usermapper 的方式，让自定义 service 继承自 ISservice，自定义 ServiceImpl 既实现自定义 service，又继承 mybatisplus 提供的 ServiceImpl service： 123public interface UserService extends IService&lt;User&gt; &#123;&#125; serviceImpl 123@Servicepublic class UserServiceImpl extends ServiceImpl&lt;UserMapper, User&gt; implements UserService &#123;&#125; - 需要使用@Service注解创建bean，交给ioc容器管理 - 或者使用配置类，使用@ConponentScan进行扫描 - 或者在springboot 启动程序，使用@ConponentScan进行扫描 mybatis-plus 中批量添加在 service 接口的方法中，底层实现批量添加的原理是循环。 2.3 雪花算法 2.3.1 出现背景 需要选择合适的方案去应对数据规模的增长，以应对逐渐增长的访问压力和数据量。数据库的扩展方式主要包括：业务分库、主从复制，数据库分表。 实现主从表分离. 2.3.2 数据库分表 将不同业务数据分散存储到不同的数据库服务器，能够支撑百万甚至千万用户规模的业务，但如果业务继续发展，同一业务的单表数据也会达到单台数据库服务器的处理瓶颈。例如，淘宝的几亿用户数据，如果全部存放在一台数据库服务器的一张表中，肯定是无法满足性能要求的，此时就需要对单表数据进行拆分 。 单表数据拆分有两种方式：垂直分表和水平分表。 垂直分表适合将表中某些不常用且占了大量空间的列拆分出去。 水平分表根据表中数据的行数和实际的业务情况，对标进行拆分。 水平分表的处置方式： 主键自增：以最常见的用户 ID 为例，可以按照 1000000 的范围大小进行分段，1~999999 放到表 1 中，1000000~1999999 放到表 2 中，以此类推。 复杂点：分段大小的选取。分段太小会导致切分后子表数量过多，增加维护复杂度；分段太大可能会导致单表依然存在性能问题，一般建议分段大小在 100 万至 2000 万之间。 优点：可以随着数据的增加平滑地扩充新的表。例如，现在的用户是 100 万，如果增加到 1000 万，只需要增加新的表就可以了，原有的数据不需要动。 缺点：分布不均匀。假如按照 1000 万来进行分表，有可能某个分段实际存储的数据量只有 1 条，而另外一个分段实际存储的数据量有 1000 万条。 取模：同样以用户 ID 为例，假如我们一开始就规划了 10 个数据库表，可以简单地用 user_id%10 的值来表示数据所属的数据库表编号，ID 为 985 的用户放到编号为 5 的子表中，ID 为 10086 的用户放到编号为 6 的子表中。 复杂点：初始表数量的确定。表数量太多维护比较麻烦，表数量太少又可能导致单表性能存在问题。 优点：表分布比较均匀。 缺点：扩充新的表很麻烦，所有数据都要重分布。 雪花算法：水平分表的处置方式。Twitter 公布的分布式主键生成算法，它能够保证不同表的主键的不重复性，以及相同表的主键的有序性。 核心思想：长度共 64bit（一个 long 型）。首先是一个符号位，1bit 标识，由于 long 基本类型在 Java 中是带符号的，最高位是符号位，正数是 0，负数是 1，所以 id 一般是正数，最高位是 0。41bit 时间截(毫秒级)，存储的是时间截的差值（当前时间截 - 开始时间截)，结果约等于 69.73 年。10bit 作为机器的 ID（5 个 bit 是数据中心，5 个 bit 的机器 ID，可以部署在 1024 个节点）。12bit 作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID）。 优点：整体上按照时间自增排序，并且整个分布式系统内不会产生 ID 碰撞，并且效率较高。 3. 注解 3.1 @MapperScan 配置在 spring boot 启动程序类上，指定扫描的 mapper 包 可以创建配置类，加在配置类上 3.2 @TableName 用于实体类 pojo 前，指定数据库中表名与实体类的对应关系。 未指定改属性时，由 basemapper 的范型决定表名与实体类的对应关系，对应不上就会报错。 也可以在 application.yml 核心配置文件中使用mybatis-plus.global-config.db-config.table-prefix: t_进行全局设置 3.3 @TableId mybatis-plus 在 crud 时，默认将字段名为 id 的列作为主键。 如果表中主键的列名不是 id，则会把主键列识别为普通列。 此时可以在实体类 pojo 的主键列属性前使用改注解，将该属性标记为主键。 value 属性：解决实体类 pojo 中的主键名与表中主键名不一致问题。可以使用 value 属性设置实体类 pojo 属性与表中主键名的对应关系。当只设置 value 时，value 可以省略，直接写值。 type 属性：设置主键生成策略。默认为 IdType.ASSIGN_ID，为雪花算法，IdType.AUTO 表示使用数据库设置的自增策略，但必须保证数据库进行了设置，否则报错。 全局配置主键生成策略：在 application.yml 核心配置文件中使用mybatis-plus.global.db-config.id-type: auto 3.4 @TableField 设置非主键字段实体类 pojo 中的属性名与数据库表中字段名的对应关系。 指定表中非主键的字段名（属性可以在表中有，也可以没有，没有时使用 exist=false）。 3.5 @TableLogic 逻辑删除：在表中设置一个字段，标记字段的状态。 逻辑删除会将删除功能变为修改功能，修改了标记为逻辑删除字段的值。 逻辑删除后，查询不会显示。 方式一：在数据库表中，给字段设置默认值，根据表中设置的默认值判断，如果默认值为 0，则表示处于未删除状态，为其他值，表示删除状态。（好像也不用） 方式二：在核心配置文件中使用 mybatis-plus.global-config.db-config.logic-delete-value 设置逻辑已删除值和 mybatis-plus.global-config.db-config.logic-not-delete-value 设置逻辑未删除值（可以不配置） 使用场景：数据恢复。 @TableLogic 标记逻辑删除的字段。 3.6 @Version 标识乐观锁本版好号字段 需要在配置类中开启乐观锁插件 3.7@EnumValue 根据实体类中取值为枚举类型的属性，创建枚举类 在枚举类中，将对应在实体类中的属性指定为枚举 解释：如 User 实体类的属性 sex，创建 SexEunm 的枚举类，在 SexEnum 中设定属性 sex，给 SexEnum 中 sex 指定注解@EunmValue 需要在核心配置文件中开启通用枚举扫描：mybatis-plus:.typeEnumsPackage: iceriver.mybatisplus.enums 3.8 @DS 指定数据源，参数名称在核心配置文件中设定 可以用在类上，也可以用有在方法上，同时用遵循就近原则 4. 条件构造器 4.1 wapper 介绍 BaseMapper 中 crud 等方法传入的条件。 Wrapper ： 条件构造抽象类，最顶端父类 AbstractWrapper ： 用于查询条件封装，生成 sql 的 where 条件 QueryWrapper ： 查询条件封装 UpdateWrapper ： Update 条件封装 AbstractLambdaWrapper ： 使用 Lambda 语法 LambdaQueryWrapper ：用于 Lambda 语法使用的查询 Wrapper LambdaUpdateWrapper ： Lambda 更新封装 Wrapper 4.2 QueryWapper 组装查询条件 1：ge、gt、le、lt 1234567@Testpublic void testSelect() &#123; QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.ge(&quot;age&quot;, 28); List&lt;User&gt; users = userMapper.selectList(queryWrapper); System.out.println(users);&#125; 组装查询条件 2：like、likeLeft（%在左边）、likeRight（%在右边）、between 12345678910111213141516@SpringBootTestpublic class QueryWapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test01()&#123;// 查询用户名包含a，年龄在20到30之间的所有用户信息// SELECT id,name,age,email FROM user WHERE (name LIKE ? AND age BETWEEN ? AND ? AND email IS NOT NULL) QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.like(&quot;name&quot;, &quot;a&quot;) .between(&quot;age&quot;,20,30) .isNotNull(&quot;email&quot;); List&lt;User&gt; users = userMapper.selectList(queryWrapper); users.forEach(user -&gt; System.out.print(user)); &#125;&#125; like()、between()等这些方法中传入的是表中的列名，不是 pojo 中的属性名。 between()可以使用 gt()、lt()代替 组装排序条件：orderByDesc、orderByAsc 123456789101112131415@SpringBootTestpublic class QueryWapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test02()&#123;// 按年龄降序查询用户，如果年龄相同则按id升序排列// SELECT id,name,age,email FROM user ORDER BY age DESC,id ASC QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.orderByDesc(&quot;age&quot;) .orderByAsc(&quot;id&quot;); List&lt;User&gt; users = userMapper.selectList(queryWrapper); users.forEach(user -&gt; System.out.print(user)); &#125;&#125; 组装删除功能 1234567891011121314@SpringBootTestpublic class QueryWapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test03()&#123;// 删除邮箱地址为null的用户// DELETE FROM user WHERE (email IS NULL) QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.isNull(&quot;email&quot;); int result = userMapper.delete(queryWrapper); System.out.println(&quot;result&quot; + result); &#125;&#125; 如果启用了逻辑删除功能，对应的删除 sql 语句会变为修改语句。 组装修改功能 1234567891011121314151617181920@SpringBootTestpublic class QueryWapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test04()&#123;// 将（年龄大于20并且用户名包含a）或者邮箱为null的用户信息修改// UPDATE user SET name=?, email=? WHERE (age &gt; ? AND name LIKE ? OR email IS NULL) QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.gt(&quot;age&quot;, 20) .like(&quot;name&quot;, &quot;a&quot;) .or() .isNull(&quot;email&quot;); User user1 = new User(); user1.setName(&quot;xiaoming&quot;); user1.setEmail(&quot;123@qq.com&quot;); int result = userMapper.update(user1, queryWrapper); System.out.println(&quot;result&quot; + result); &#125;&#125; 如果开启了逻辑删除，则逻辑删除后的不会被修改 注意 OR 查询条件在 mybatis-plus 中的用法（不传参，起拼接作用） 组装查询优先级 主要是 mybatis-plus 的条件构造器中 lambda 中的条件优先执行 为了能够写 lambda，并且这个查询条件不再用.代替，而是用and() 12345678910111213141516171819@SpringBootTestpublic class QueryWapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test05()&#123;// 将用户名包含a并且（年龄大于20或者邮箱为null）的用户信息修改// mybatis-plus的条件构造器中lambda中的条件优先执行// UPDATE user SET name=?, email=? WHERE (name LIKE ? AND (age &gt; ? AND email IS NULL)) QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.like(&quot;name&quot;, &quot;a&quot;) .and(i -&gt; i.gt(&quot;age&quot;, 20).isNull(&quot;email&quot;)); User user1 = new User(); user1.setName(&quot;dazhuang&quot;); user1.setEmail(&quot;312@qq.com&quot;); int result = userMapper.update(user1, queryWrapper); System.out.println(&quot;result&quot; + result); &#125;&#125; 如果开启了逻辑删除，则逻辑删除后的不会被修改 组装查询指定列 1234567891011121314@SpringBootTestpublic class QueryWapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test06()&#123;// 查询用户名、年龄、邮箱// SELECT name,age,email FROM user QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.select(&quot;name&quot;, &quot;age&quot;, &quot;email&quot;); List&lt;Map&lt;String, Object&gt;&gt; users = userMapper.selectMaps(queryWrapper); users.forEach(user -&gt; System.out.println(user)); &#125;&#125; 组装子查询 1234567891011121314@SpringBootTestpublic class QueryWapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test07()&#123;// 查询id小于等于100的用户// SELECT id,name,age,email FROM user WHERE (id IN (select id from user where id&lt;=100)) QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.inSql(&quot;id&quot;, &quot;select id from user where id&lt;=100&quot;); List&lt;User&gt; users = userMapper.selectList(queryWrapper); users.forEach(user -&gt; System.out.println(user)); &#125;&#125; 4.3 UpdateWapper 组装修改条件（按照上文5 项修改） 12345678910111213141516@SpringBootTestpublic class UpdateWrapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test01()&#123;// 将用户名包含a并且（年龄大于20或者邮箱为null）的用户信息修改// UPDATE user SET name=?,email=? WHERE (name LIKE ? AND (age &gt; ? AND email IS NULL)) UpdateWrapper&lt;User&gt; updateWrapper = new UpdateWrapper&lt;&gt;(); updateWrapper.like(&quot;name&quot;, &quot;i&quot;) .and(i -&gt; i.gt(&quot;age&quot;, 20).isNull(&quot;email&quot;)); updateWrapper.set(&quot;name&quot;,&quot;大王&quot;).set(&quot;email&quot;, &quot;newd@sina.com&quot;); int result = userMapper.update(null, updateWrapper); System.out.println(&quot;result&quot; + result); &#125;&#125; 4.4 模拟开发中根据提交数据组装条件 手动判断组装——利用 springframework 提供的工具类 1234567891011121314151617181920212223242526@SpringBootTestpublic class ConditionTests &#123; @Autowired private UserMapper userMapper; @Test public void test01() &#123;// SELECT id,name,age,email FROM user WHERE (age &gt;= ? AND age &lt;= ?) String name = &quot;&quot;; Integer ageBegin = 20; Integer ageEnd = 30; QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); if (StringUtils.isNotBlank(name)) &#123; //isNotBlank判断某个字符创是否不为空字符串、不为null、不为空白符 queryWrapper.like(&quot;user_name&quot;, name); &#125; if (ageBegin != null) &#123; queryWrapper.ge(&quot;age&quot;, ageBegin); &#125; if (ageEnd != null) &#123; queryWrapper.le(&quot;age&quot;, ageEnd); &#125; List&lt;User&gt; list = userMapper.selectList(queryWrapper); list.forEach(System.out::println); &#125;&#125; 使用带 condition 参数的重载方法构建查询条件 12345678910111213141516171819@SpringBootTestpublic class ConditionTests &#123; @Autowired private UserMapper userMapper; @Test public void test02() &#123;// SELECT id,name,age,email FROM user WHERE (age &gt;= ? AND age &lt;= ?) String name = &quot;&quot;; Integer ageBegin = 20; Integer ageEnd = 30; QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.like(StringUtils.isNotBlank(name), &quot;user_name&quot;, name) .ge(ageBegin != null, &quot;age&quot;, ageBegin) .le(ageEnd != null, &quot;age&quot;, ageEnd); List&lt;User&gt; list = userMapper.selectList(queryWrapper); list.forEach(System.out::println); &#125;&#125; 4.4 LambdaQueryWrapper 解决查询时不小心写错字段名的问题 123456789101112131415161718@SpringBootTestpublic class LambdaQueryWrapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test01() &#123;// SELECT id,name,age,email FROM user WHERE (age &gt;= ? AND age &lt;= ?) String name = &quot;&quot;; Integer ageBegin = 20; Integer ageEnd = 30; LambdaQueryWrapper&lt;User&gt; lambdaQueryWrapper = new LambdaQueryWrapper&lt;&gt;(); lambdaQueryWrapper.like(StringUtils.isNotBlank(name), User::getName, name) .ge(ageBegin != null, User::getAge, ageBegin) .le(ageEnd != null, User::getAge, ageEnd); List&lt;User&gt; list = userMapper.selectList(lambdaQueryWrapper); list.forEach(System.out::println); &#125; 4.5 LambdaUpdateWrapper 解决修改时不小心写错字段名的问题 12345678910111213141516@SpringBootTestpublic class LambdaUpdateWrapperTests &#123; @Autowired private UserMapper userMapper; @Test public void test01()&#123;// 将用户名包含a并且（年龄大于20或者邮箱为null）的用户信息修改// UPDATE user SET name=?,email=? WHERE (name LIKE ? AND (age &gt; ? AND email IS NULL)) LambdaUpdateWrapper&lt;User&gt; lambdaUpdateWrapper = new LambdaUpdateWrapper&lt;&gt;(); lambdaUpdateWrapper.like(User::getName, &quot;i&quot;) .and(i -&gt; i.gt(User::getAge, 20).isNull(User::getEmail)); lambdaUpdateWrapper.set(User::getName,&quot;大王&quot;).set(User::getEmail, &quot;newd@sina.com&quot;); int result = userMapper.update(null, lambdaUpdateWrapper); System.out.println(&quot;result&quot; + result); &#125;&#125; 5. 插件 5.1 分页插件 5.1.1 使用 mybatisplus 的分页方法 mybatis-plus 自带分页插件，但使用前需要进行配置：创建分页配置类 config.MyBatisPlusConfig.java 使用@Configuration 指定为配置类 由于在 springboot 的启动类上已经使用了@MapperScan 注解，所以配置类可以不使用@MapperScan 注解 但是为了程序规范化、语义化，建议将@MapperScan 注解添加到配置类上. 总结：主启动类和配置类都要加上@MapperScan 注解 编写分页器方法 12345678910111213@Configurationpublic class MyBatisPlusConfig &#123; /** * 定义了名为mybatisPlusInterceptor，返回类型为MybatisPlusInterceptor的方法 * @return */ @Bean public MybatisPlusInterceptor mybatisPlusInterceptor()&#123; MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); interceptor.addInnerInterceptor(new PaginationInnerInterceptor(DbType.MYSQL)); return interceptor; &#125;&#125; 测试： 12345678910111213@SpringBootTestpublic class MyBatisPlusPluginsTests &#123; @Autowired private UserMapper userMapper; @Test public void test()&#123; Page&lt;User&gt; page = new Page&lt;&gt;(1, 3); Page&lt;User&gt; page1 = userMapper.selectPage(page, null); System.out.println(page); System.out.println(&quot;-------------------&quot;); System.out.println(page1==page);//true &#125;&#125; 5.1.2 自定义分页方法 使用自定义分页方法的流程与 mybatis 中操作一致 在 mapper 接口中定义分页方法 在 reosources 目录下创建 mapper 目录，创建对应的 mapper.xml 映射文件 如果映射文件不在 mapper 目录下，则需要在 application.yml 文件中进行配置使用 mybatis-plus.mapper-location，指定位置。 在 mapper.xml 文件中编写 sql 语句 调用 mapper 接口中的方法进行使用 编写分页方法 1234567891011121314/*** @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w* @date: 2022/8/4 21:01*/public interface UserMapper extends BaseMapper&lt;User&gt; &#123; /** * 自定义分页查询方法：根据年龄查询用户信息并分页 * @param page mybatis-plus提供的分页对象，必须位于第一个参数的位置 * @param age * @return */ Page&lt;User&gt; selectPageVo(@Param(&quot;page&quot;) Page&lt;User&gt; page, @Param(&quot;age&quot;) Integer age);&#125; 自定义方法中，Page对象必须作为第一个参数 编写 sql 映射文件 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;iceriver.mybatisplus.mapper.UserMapper&quot;&gt; &lt;!-- Page&lt;User&gt; selectPageVo(@Param(&quot;page&quot;) Page&lt;User&gt; page, @Param(&quot;age&quot;) Integer age);--&gt; &lt;select id=&quot;selectPageVo&quot; resultType=&quot;User&quot;&gt; select uid,user_name,age,email from t_user where age &gt; #&#123;age&#125; &lt;/select&gt;&lt;/mapper&gt; 这里 resultType 实用类型别名，指定类型别名要在 springboot 核心配置文件中进行设定：mybatis-plus.type-aliases-package: iceriver.mybatisplus.pojo 测试方法： 1234567891011121314@SpringBootTestpublic class UserTests &#123; @Autowired private UserMapper userMapper; @Test public void testPageVo()&#123; Page&lt;User&gt; page = new Page&lt;&gt;(1,3); Page&lt;User&gt; page1 = userMapper.selectPageVo(page, 20); System.out.println(page); System.out.println(page1); System.out.println(page==page1); &#125;&#125; 5.2 乐观锁 5.2.1 乐观锁悲观锁介绍 场景： 一件商品，成本价是 80 元，售价是 100 元。老板先是通知小李，说你去把商品价格增加 50 元。小李正在玩游戏，耽搁了一个小时。正好一个小时后，老板觉得商品价格增加到 150 元，价格太高，可能会影响销量。又通知小王，你把商品价格降低 30 元。此时，小李和小王同时操作商品后台系统。小李操作的时候，系统先取出商品价格 100 元；小王也在操作，取出的商品价格也是 100 元。小李将价格加了 50 元，并将 100+50=150 元存入了数据库；小王将商品减了 30 元，并将 100-30=70 元存入了数据库。是的，如果没有锁，小李的操作就完全被小王的覆盖了。现在商品价格是 70 元，比成本价低 10 元。几分钟后，这个商品很快出售了 1 千多件商品，老板亏 1 万多。 乐观锁与悲观锁 上面的故事，如果是乐观锁，小王保存价格前，会检查下价格是否被人修改过了。如果被修改过了，则重新取出的被修改后的价格，150 元，这样他会将 120 元存入数据库。如果是悲观锁，小李取出数据后，小王只能等小李操作完之后，才能对价格进行操作，也会保证最终的价格是 120 元 乐观锁要求数据表有表示数据版本的字段，在修改前，乐观锁会检查一下最新的数据版本版与自己持有的版本号是否一致。 5.2.2 模拟修改冲突 创建商品表 123456789CREATE TABLE t_product( id BIGINT(20) NOT NULL COMMENT &#x27;主键ID&#x27;, NAME VARCHAR(30) NULL DEFAULT NULL COMMENT &#x27;商品名称&#x27;, price INT(11) DEFAULT 0 COMMENT &#x27;价格&#x27;, VERSION INT(11) DEFAULT 0 COMMENT &#x27;乐观锁版本号&#x27;, PRIMARY KEY (id));INSERT INTO t_product (id, NAME, price) VALUES (1, &#x27;外星人笔记本&#x27;, 100); 创建实体类 1234567@Datapublic class Product &#123; private Long id; private String name; private Integer price; private Integer version;&#125; 创建 mapper 接口 12public interface ProductMapper extends BaseMapper&lt;Product&gt; &#123;&#125; 测试 1234567891011121314151617181920212223242526@SpringBootTestpublic class ProductMapperTests &#123; @Autowired private ProductMapper productMapper; @Test public void test()&#123; //1. 小李查询商品价格 Product productL = productMapper.selectById(1); System.out.println(&quot;小李查询的商品价格&quot; + productL.getPrice()); //2. 小王查询商品价格 Product productW = productMapper.selectById(1); System.out.println(&quot;小王查询的商品价格&quot; + productW.getPrice()); System.out.println(&quot;----------修改价格前------------&quot;); //3. 小李对商品价格+50 productL.setPrice(productL.getPrice() + 50); productMapper.updateById(productL); //4. 小王对商品价格-30 System.out.println(&quot;小王查询的商品价格&quot; + productW.getPrice()); productW.setPrice(productW.getPrice() - 30); productMapper.updateById(productL); System.out.println(&quot;----------修改价格后------------&quot;); //5. 老板检查价格 Product productB = productMapper.selectById(1); System.out.println(&quot;老板查询的商品价格&quot; + productB.getPrice()); &#125;&#125; 5.2.3 mybatis-plus 乐观锁插件 实体类中添加乐观锁版本号注解@Version 配置类中开启乐观锁插件 123456789@Configurationpublic class MyConfig &#123; @Bean public MybatisPlusInterceptor mybatisPlusInterceptor()&#123; MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); interceptor.addInnerInterceptor(new OptimisticLockerInnerInterceptor()); return interceptor; &#125;&#125; 此时老板获得的价格为 150，因为小李的修改会执行，小王的修改不会执行。 小王如果想要修改成功，可以进行判断，修改不成功重新获取最新版本的数据进行修改 12345678910111213141516171819202122232425262728293031323334@SpringBootTestpublic class ProductMapperTests &#123; @Autowired private ProductMapper productMapper; @Test public void test02()&#123; //1. 小李查询商品价格 Product productL = productMapper.selectById(1); System.out.println(&quot;小李查询的商品价格&quot; + productL.getPrice()); //2. 小王查询商品价格 Product productW = productMapper.selectById(1); System.out.println(&quot;小王查询的商品价格&quot; + productW.getPrice()); System.out.println(&quot;----------修改价格前------------&quot;); //3. 小李对商品价格+50 productL.setPrice(productL.getPrice() + 50); productMapper.updateById(productL); //4. 小王对商品价格-30 System.out.println(&quot;小王查询的商品价格&quot; + productW.getPrice()); productW.setPrice(productW.getPrice() - 30); int result = productMapper.updateById(productW); if(result == 0)&#123; Product product = productMapper.selectById(1); product.setPrice(product.getPrice() - 30); productMapper.updateById(product); &#125; productMapper.updateById(productL); System.out.println(&quot;----------修改价格后------------&quot;); //5. 老板检查价格 Product productB = productMapper.selectById(1); System.out.println(&quot;老板查询的商品价格&quot; + productB.getPrice()); &#125;&#125; 6. 扩展 6.1 通用枚举 user 数据表添加 sex 字段，用整数 1、2 表示男、女 User 实体类定义 SexEnum 类型的属性 sex 创建 SexEunm 的枚举类，定义属性 sex 和 sexNmae，并用@EnumValue 注解标识 sex 属性 在 springboot 核心配置文件中开启通用枚举扫描：mybatis-plus:.typeEnumsPackage: iceriver.mybatisplus.enums 6.2 代码生成器 与 mybatis 中的逆向工程插件类似，不过 mybatis 的逆向工程只能生成实体类、dao 层 mappper 和 mapper 映射文件。 而 mybatis-plus 可以生成上述文件外，还能生成 controller 和 service 及 serviceimpl 创建新模块 添加依赖：mybatis-plus-generator 和 freemarker 1234567891011&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-generator&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 模板引擎 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.31&lt;/version&gt;&lt;/dependency&gt; 创建测试类，执行 main 方法 1234567891011121314151617181920212223public class FastAutoGeneratorTest &#123; public static void main(String[] args) &#123; FastAutoGenerator.create(&quot;jdbc:mysql://127.0.0.1:13306/mybatis_plus?characterEncoding=utf-8&amp;userSSL=false&quot;, &quot;root&quot;, &quot;dimitre123&quot;) .globalConfig(builder -&gt; &#123; builder.author(&quot;zhuyuqi&quot;) // 设置作者 //.enableSwagger() // 开启 swagger 模式 .fileOverride() // 覆盖已生成文件 .outputDir(&quot;D:\\\\PracticalExercise\\\\16.MyBatis-Plus\\\\plus04-generator\\\\mybatis_plus_generate_code&quot;); // 指定输出目录 &#125;) .packageConfig(builder -&gt; &#123; builder.parent(&quot;iceriver&quot;) // 设置父包名 .moduleName(&quot;mybatisplus&quot;) // 设置父包模块名 .pathInfo(Collections.singletonMap(OutputFile.mapperXml, &quot;D:\\\\PracticalExercise\\\\16.MyBatis-Plus\\\\plus04-generator\\\\mybatis_plus_generate_code&quot;)); // 设置mapperXml生成路径 &#125;) .strategyConfig(builder -&gt; &#123; builder.addInclude(&quot;user&quot;) // 设置需要生成的表名 //.addTablePrefix(&quot;t_&quot;, &quot;c_&quot;)// 设置过滤表前缀 ; &#125;) .templateEngine(new FreemarkerTemplateEngine()) // 使用Freemarker引擎模板，默认的是Velocity引擎模板 .execute(); &#125;&#125; 6.3 多数据源 使用场景：表分散在多库、读写分离、一主多从、混合模式 创建数据库及表： 第一个使用 mybatis_plus 库、user 表 第二个使用 mybatis_plus_01 库、product 表 新建模块 引入依赖：mybaitis-plus、lombok、mysql、dynamic-datasource-spring-boot-starter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;iceriver.mybatisplus&lt;/groupId&gt; &lt;artifactId&gt;plus05-datasources&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;plus05-datasources&lt;/name&gt; &lt;description&gt;plus05-datasources&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;dynamic-datasource-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 在 springboot 核心配置文件中添加多数据源 12345678910111213141516171819spring: # 配置数据源信息 datasource: dynamic: # 设置默认的数据源或者数据源组,默认值即为master primary: master # 严格匹配数据源,默认false.true未匹配到指定数据源时抛异常,false使用默认数据源 strict: false datasource: master: url: jdbc:mysql://localhost:13306/mybatis_plus?characterEncoding=utf-8&amp;useSSL=false driver-class-name: com.mysql.cj.jdbc.Driver username: root password: dimitre123 slave_1: url: jdbc:mysql://localhost:13306/mybatis_plus_01?characterEncoding=utf-8&amp;useSSL=false driver-class-name: com.mysql.cj.jdbc.Driver username: root password: dimitre123 创建相关的实体类、service、mapper UserServiceImpl 使用@DS(“master”)指定为主数据源 ProductServiceImpl 使用@DS(“master”)指定为主数据源 执行测试 123456789101112@SpringBootTestclass Plus05DatasourcesApplicationTests &#123; @Autowired private UserService userService; @Autowired private ProductService productService; @Test public void test()&#123; System.out.println(userService.getById(1)); System.out.println(productService.getById(1)); &#125;&#125; 6.4 MyBatisX 插件 6.4.1 基本介绍 MyBatis-Plus 提供了强大的 mapper 和 service 模板，能够大大的提高开发效率但是在真正开发过程中，MyBatis-Plus 并不能为我们解决所有问题，例如一些复杂的 SQL，多表联查，我们就需要自己去编写代码和 SQL 语句，我们该如何快速的解决这个问题呢，这个时候可以使用 MyBatisX 插件 MyBatisX 一款基于 IDEA 的快速开发插件，为效率而生 安装插件 功能： 可以快速定位 mapper 类和映射文件 代替本文 6.2 代码生成器https://www.yuque.com/zhuyuqi/zna9x5/hientp#W0v7A，并可视化操作生成过程。 快速生成自定义 curd 方法 mapper 映射文件的路径设置参见本文 2.1.2 1 点设置 mapper 文件路径https://www.yuque.com/zhuyuqi/zna9x5/hientp#JebDR 6.4.2 mybatisx 快速代码生成 创建模块： 引入依赖：mybaitis-plus、lombok、mysql 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;iceriver.mybatisplus&lt;/groupId&gt; &lt;artifactId&gt;plus06-mybatisx&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;plus06-mybatisx&lt;/name&gt; &lt;description&gt;plus06-mybatisx&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; applicaiton.yml 核心配置文件中配置数据源 12345678spring: # 配置数据源信息 datasource: type: com.zaxxer.hikari.HikariDataSource url: jdbc:mysql://localhost:13306/mybatis_plus?characterEncoding=utf-8&amp;useSSL=false driver-class-name: com.mysql.cj.jdbc.Driver username: root password: dimitre123 在 IDEA 中连接数据库： 6.4.3 自定义 CURD（快速生成 xml 中 sql） 以插入为例，输入 insert，选择 insertSelective 按下 alt+enter，选择第二个，会快速生成 mapper 方法和 sql","tags":[{"name":"MyBatis-Plus","slug":"MyBatis-Plus","permalink":"https://sk370.github.io/tags/MyBatis-Plus/"}]},{"title":"Spring Cloud","date":"2022-07-27T19:44:39.000Z","path":"2022/07/28/springcloud/SpringCloud/","text":"Spring Cloud 是分布式微服务架构的一站式解决方案，它提供了一套简单易用的编程模型，使我们能在 Spring Boot 的基础上轻松地实现微服务系统的构建。 springcloud 基于 http 协议，这是与 Dubbo 最本质的区别。 dubbo 的核心是基于 RPC 1. Spring Cloud 1.1 Spring Cloud 环境 Spring Cloud：Hoxton.SR1 Spring Boot：2.2.2.RELEASE Spring Cloud Alibaba：2.1.0.RELEASE Java：Java8 Maven：3.5+ MySQL：5.7+ 常用组件（pom） 1.2 Spring Boot 从 1.5 升级为 2.0 时，时代发生的变化： 以前学习内容： 现在（2020）学习内容： 1.3 学习内容 注册中心：Eureka 负载均衡：Ribbon 声明式调用远程方法：Feign 熔断、降级、监控：Hystrix 网关：Zuul 2. 微服务架构编码构建 2.1 核心思想 2.1.1 消费和提供者设计 约定&gt;配置&gt;编程 实体类抽取成一个公共的包，要使用实体类的包引入该依赖。 2.1.2 CAP 理论 C:Consistency（强一致性） A:Availability（可用性） P:Partition tolerance（分区容错性） CAP 理论关注粒度是数据，而不是整体系统设计的策略 经典 CAP 图： 最多只能同时较好的满足两个。 CAP 理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求， 因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类： CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。 CP - 满足一致性，分区容忍必的系统，通常性能不是特别高。 AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。 2.2 父工程管理依赖 2.2.1 创建父工程 选择 Maven 构建工程，暂不选择 spring 初始化 修改 maven 版本、仓库及设置 检查项目编码格式： 注解生效激活（之前从来没设置过） Java 编译版本选择 8（之前从来没设置过） 设置文件 File Type 过滤，即是否在 idea 文件窗口显示指定格式的文件（可不设置） 2.2.2 配置父工程组件（pom 文件） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;!-- 统一管理jar包版本 --&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;lombok.version&gt;1.16.18&lt;/lombok.version&gt; &lt;mysql.version&gt;5.1.47&lt;/mysql.version&gt; &lt;druid.version&gt;1.1.16&lt;/druid.version&gt; &lt;mybatis.spring.boot.version&gt;1.3.0&lt;/mybatis.spring.boot.version&gt;&lt;/properties&gt;&lt;!-- 子模块继承之后，提供作用：锁定版本+子modlue不用写groupId和version --&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!--spring boot 2.2.2--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!--spring cloud Hoxton.SR1--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!--spring cloud alibaba 2.1.0.RELEASE--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;$&#123;druid.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.spring.boot.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;$&#123;lombok.version&#125;&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;addResources&gt;true&lt;/addResources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 在未使用 spring 初始化工具创建项目的情况下，该父模块没有父模块，所以依赖引入了 spring-boot-dependencies 和 spring-cloud-dependencies，而且导入形式为 import。 在使用 spring 初始化工具创建项目的情况下，该模块的父模块会是 springboot，则引入依赖时需要引入 spring-cloud-dependencies。 2.2.3 热部署 Devtools 子工程引入依赖 工程添加开启 maven 插件 开启 idea 自动编译 注册自动编译（下面是老版本，2021 版本不需要设置，可参考https://www.yuque.com/zhuyuqi/zna9x5/gs0vv5#eEIgn，使用 ctrl+f9） ctrl+shift + alt +／开启注册表维护界面 勾选相关配置 2.3 REST 微服务工程 2.3.1 RestTemplate 在微服务开发中，一般约定：消费者的请求端口 80，则对应微服务的端口为 8001。 为了能够使消费者服务的请求更方便的发给服务提供者，产生了很对请求模板技术。RestTemplate 提供了多种便捷访问远程 Http 服务的方法， 是一种简单便捷的访问 restful 服务模板类，是 Spring 提供的用于访问 Rest 服务的客户端模板工具集。 官网地址： https://docs.spring.io/spring-framework/docs/5.2.2.RELEASE/javadoc-api/org/springframework/web/client/RestTemplate.html 可以这样理解，通常情况下，给后台服务发送请求是通过浏览器发送 http 请求，在后台处理业务时，如果需要使用第三方服务，则需要在后台给第三方服务发送请求，此时就需要后台服务封装一个 http 请求，httpclient 和 resttemplate 都起到了这样的作用。 相当于 http 协议的实现方式中：浏览器是前端的、httpclient 是后端的。 getForObject 方法/getForEntity 方法： 返回对象为响应体中数据转化成的对象，基本上可以理解为 Json 返回对象为 ResponseEntity 对象，包含了响应中的一些重要信息，比如响应头、响应状态码、响应体等 postForObject 方法/postForEntity 方法 2.3.2 创建 cloud-provider-payment8001 子模块 创建模块，使用父模块管理依赖版本 resources 目录下创建 application.properties 或 application.yml 核心配置文件 习惯：在 application.yml 核心配置文件中指定服务端口号及服务名称 12345678910111213141516server: port: 8001spring: application: name: cloud-provider-payment8001 datasource: type: com.alibaba.druid.pool.DruidDataSource # 当前数据源操作类型 driver-class-name: org.gjt.mm.mysql.Driver # mysql驱动包 url: jdbc:mysql://localhost:13306/springcloud?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: dimitre123mybatis: mapperLocations: classpath:mapper/*.xml type-aliases-package: com.atguigu.springcloud.entities # 所有Entity别名类所在包 com.mysql.jdbc.Driver 和 mysql-connector-java 5 一起用。 com.mysql.cj.jdbc.Driver 和 mysql-connector-java 6 一起用。 com.mysql.cj.jdbc.Driver 是 mysql-connector-java 6 中的特性，相比 mysql-connector-java 5 多了一个时区：serverTimezone，把数据源配置的驱动改一下就好了 org.gjt.mm.mysql.Driver 是当时最好的 MySQL JDBC，但不是 MySQL 公司的，然后 MySQL 将 MM 的 JDBC 驱动 收为官方的 JDBC 驱动，所以将驱动的 package 也改了，但还保留了 org.gjt.mm.mysql.Driver 这个路径的引用，也就是你使用新版的 JDBC 驱动时还可以通过这个来引用，打开下载的新版 JDBC 驱动的 jar 文件可以看到，只有一个文件的目录是 org.gjt.mm.mysql，就是为了兼容而设计的。 创建主启动类（如果使用 spring 初始化创建的项目不需要这个步骤。 2.3.3 创建 cloud-consumer-order80 模块 创建模块，使用父模块管理依赖版本 resources 目录下创建 application.properties 或 application.yml 核心配置文件 习惯：在 application.yml 核心配置文件中指定服务端口号及服务名称 使用 restTemplate 对象向提供者请求数据： 消费者服务中，创建配置类，在配置类中引入 RestTemplate，并使用@Bean 注入 在消费者服务中注入 restTemplate 对象， 在消费者服务调用提供者服务（给提供者服务发送请求时），使用 restTemplate 对象的方法。 postForObject(url, requestMap, ResponseBean.class) REST 请求地址、请求参数、HTTP 响应转换被转换成的对象类型。 getForObject(url, ResponseBean.class) 2.3.4 工程重构——创建 cloud-api-commons 由于消费者、提供者都需要实体类、异常、service 接口、返回数据等，所以抽取至新的公共模块中，让消费者、提供者依赖 cloud-api-commons 公共模块。 3. 服务注册中心 3.1 Eurake 3.1.1 服务注册与发现 Eureka 采用了 CS 的设计架构，Eureka Server 作为服务注册功能的服务器，它是服务注册中心。 在任何 rpc 远程框架中，都会有一个注册中心(存放服务地址相关信息(接口地址))。当服务器启动的时候，会把当前自己服务器的信息，比如服务地址、通讯地址等以别名方式注册到注册中心上。另一方（消费者|服务提供者），以该别名的方式去注册中心上获取到实际的服务通讯地址，然后再实现本地 RPC 调用。 RPC 远程调用框架核心设计思想：在于注册中心，因为使用注册中心管理每个服务与服务之间的一个依赖关系(服务治理概念)。 系统中的其他微服务，使用 Eureka 的客户端连接到 Eureka Server 并维持心跳连接，通过 Eureka Server 就可以监控系统中各个微服务是否正常运行。 3.1.2 服务治理 在传统的 rpc 远程调用框架中，管理每个服务与服务之间依赖关系比较复杂，管理比较复杂，所以需要使用服务治理，管理服务于服务之间依赖关系，可以实现服务调用、负载均衡、容错等，实现服务发现与注册。 Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务治理。 微服务 RPC 远程服务调用最核心的是什么？ 高可用，试想你的注册中心只有一个，故障时会导致整个为服务环境不可用，所以： 搭建 Eureka 注册中心集群 ，实现负载均衡+故障容错。 3.1.3 Eurake 的组成 Eureka Server 提供服务注册服务：各个微服务节点通过配置启动后，会在 EurekaServer 中进行注册，这样 EurekaServer 中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观看到。 EurekaClient 通过注册中心进行访问：是一个 Java 客户端，用于简化 Eureka Server 的交互，客户端同时也具备一个内置的、使用轮询(round-robin)负载算法的负载均衡器。EurekaClient 在应用启动后，将会向 Eureka Server 发送心跳(默认周期为 30 秒)。如果 Eureka Server 在多个心跳周期内没有接收到某个节点的心跳，EurekaServer 将会从服务注册表中把这个服务节点移除（默认 90 秒） 3.1.4 单机 Eureka 构建步骤——一个 Eurake 注册中心 创建 eurekaServer 服务注册中心： 创建 cloud-eureka-server7001 模块 修改 pom.xml——引入 eureka-server 依赖 编写 application.yml 配置文件 创建主启动类，使用@EnableEurekaServer 注解标记为服务注册中心 启动主程序类 将服务提供者注册到 eurekaServer 服务注册中心 修改 pom.xml——引入 eureka-client 依赖 修改 application.yml 配置文件，添加 eurake-client 的相关配置 修改主启动类，使用@EnableEurekaClient 注解注册到服务中心 将服务消费者注册到 eurekaServer 服务注册中心 修改 pom.xml——引入 eureka-client 依赖 修改 application.yml 配置文件，添加 eurake-client 的相关配置 修改主启动类，使用@EnableEurekaClient 注解注册到服务中心 3.1.5 集群 Eureka 构建步骤——多个 Eurake 注册中心 原理：Eurekaserver 相互注册 创建 eurekaServer 服务注册中心： 创建 cloud-eureka-server7002 模块 修改 pom.xml——引入 eureka-server 依赖 编写 application.yml 配置文件 创建主启动类，使用@EnableEurekaServer 注解标记为服务注册中心 启动主程序类 为了在本地一台主机上模拟多个注册中心（能够看到多台的效果），需要修改 hosts 文件 因为 spring 核心配置文件中，注册中心的实例名称（eureka-instance-hostname)使用了 localhost，当多个注册中心使用同一个名称时，会导致看不到注册中心的相互守望 所以可以采用域名映射 ip 的方式，对 hosts 文件进行修改 【测试不对 hosts 文件修改：不可行】看不到注册中心相互守望： 将服务提供者和消费者注册到 Eureka 集群中 修改 springboot 核心配置文件 执行测试【注意启动顺序】 启动注册中心 启动提供者服务 启动消费者服务 正常启动后，两个注册中心都能看到消费者和提供者服务 3.1.6 服务提供者的集群配置（消费者还是一个，不需要集群） 创建 cloud-provider-payment8002 模块 修改 pom.xml，引入spring-cloud-starter-netflix-eureka-client 编写 application.yml 配置文件——注意修改端口号为 8002 创建主启动类，使用@EnableEurekaClient 注解标记为服务注册中心 较低版本需要使用@EnableEurekaClient 注解。稍高版本也可以使用@EnableDiscoveryClient 注解。当前版本可以省略。 @EnableDiscoveryClient 是通用的，不仅仅可以用来发现 eureka 的注册中心。 3.1.7 开启负载均衡功能 （这里使用的是 org.springframework.cloud.client.loadbalancer 中的负载均衡功能？） 前面 3.1.6 虽然配置了多个服务提供者，还是存在一个问题： 服务消费者中的请求地址写死了，指向了 8001 端口的服务。 为了解决上述问题，需要对消费者的 controller 类进行修改，不能再对请求地址写死，修改方法如下（指向注册中心的服务提供者名字）： 同时，为了能够实现 eurake 注册中心对服务提供者的自动管理（负载均衡），需要在服务消费者的配置类类中，对 RestTemplate 方法使用@LoadBalanaced 开启负载均衡功能. 为了能够观察到负载均衡效果，可以修改消费者和提供者 controller 类，其中提供者添加端口号、消费者将 ip 地址修改为服务名称，通过启动服务看调用的哪个服务 通过观察，可以看到注册中心的默认负载均衡机制为：轮询——每人一次。 上述过程较为麻烦，后续可采用 Ribbon 的负载均衡功能，此时不再需要配置地址和端口号。 3.1.8 actuator 微服务信息完善 问题：注册中心鼠标指向集群注册中心的域名，不显示地址，同时，多个服务提供者的显示名称为 ip 地址，对于维护容易造成混淆，不能定位哪一台服务器。 引入spring-boot-starter-actuator依赖 修改核心配置文件，给服务指定名称：名称任意 3.1.9 服务发现 Discovery 对于注册进 eureka 里面的微服务，可以通过服务发现来获得该服务的信息。 是 spring cloud 的新功能。 修改 cloud-provider-payment8001 的 Controller 8001 服务提供者的主启动类使用@@EnableDiscoveryClient 3.1.10 Eurake 的自我保护 作用：保护模式主要用于一组客户端和 Eureka Server 之间存在网络分区场景下的保护。一旦进入保护模式，Eureka Server 将会尝试保护其服务注册表中的信息，不再删除服务注册表中的数据，也就是不会注销任何微服务。 即：某时刻某一个微服务不可用了，Eureka 不会立刻清理，依旧会对该微服务的信息进行保存。属于 CAP 里面的 AP 分支。 在自我保护模式中，Eureka Server 会保护服务注册表中的信息，不再注销任何服务实例。 现象：如果在 Eureka Server 的首页看到以下这段提示，则说明 Eureka 进入了保护模式： 出现原因：为了防止 EurekaClient 可以正常运行但是 与 EurekaServer 网络不通情况下，EurekaServer 不会立刻将 EurekaClient 服务剔除。 自我保护模式详解： 默认情况下，如果 EurekaServer 在一定时间内没有接收到某个微服务实例的心跳，EurekaServer 将会注销该实例（默认 90 秒）。但是当网络分区故障发生(延时、卡顿、拥挤)时，微服务与 EurekaServer 之间无法正常通信，以上行为可能变得非常危险了——因为微服务本身其实是健康的，此时本不应该注销这个微服务。Eureka 通过“自我保护模式”来解决这个问题——当 EurekaServer 节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。 设计思想：它的设计哲学就是宁可保留错误的服务注册信息，也不盲目注销任何可能健康的服务实例。一句话讲解：好死不如赖活着 总结：自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留所有微服务（健康的微服务和不健康的微服务都会保留）也不盲目注销任何健康的微服务。使用自我保护模式，可以让 Eureka 集群更加的健壮、稳定。 禁用自我保护： 修改注册中心（如 7001 的核心配置文件），设置 eureka.server.enable-self-preservation = false。 扩展：设置心跳检测周期：eureka-server-eviction-interval-timer-in-ms: 2000 修改服务提供者（eureka-client）（如 8001）： eureka.instance.lease-renewal-interval-in-seconds=1，eureka-client 默认 30s 向 eureka-server 发送一次心跳，修改后变为 1s eureka.instance.lease-expiration-duration-in-seconds=2，eureka-server 收到最后一次心跳后等待时间上限，单位为秒(默认是 90 秒)，超时将剔除服务 3.1.11 Eureka 停更说明 3.2 Zookeeper 前提要求：掌握 zookeeper，并在 centos7 上成功配置 这里学习在本地主机安装了：参考01.分布式框架—Dubbo 3.2.1 基本使用 zookeeper 是一个分布式协调工具，可以实现注册中心功能 关闭 Linux 服务器防火墙后启动 zookeeper 服务器 systemctl stop firewalld status firewalld zookeeper 服务器取代 Eureka 服务器，zk 作为服务注册中心 zookeeper 保存的服务节点是临时节点，也就是说如果服务断开，马上就会失去服务的注册信息，不会像 Eureka 一样有自我保护机制。 搭建 zookeeper 注册中心集群时，在配置文件中修改 3.2.1 创建 cloud-provider-payment8004 服务提供者 创建创建 cloud-provider-payment8002 模块 修改 pom.xml 引入 zookeeper 依赖 编写 application.yml 配置文件——注意修改端口号为 8004 创建主启动类，使用@EnableDiscoveryClient 注解标记，用于向使用 consul 或者 zookeeper 作为注册中心时注册服务 集群搭建时，只需要在核心配置文件中，指定多个 zookeeper 地址，使用逗号分割。 3.2.3 处理服务提供者启动报错问题 报错现象： 原因：jar 包冲突，版本不一致。引入的 zookeeper 依赖版本默认为 3.5.3-beta，本机（服务器）zookeeper 版本为 3.4.11 解决方案：修改本地依赖的 zookeeper 版本 3.2.4 创建 cloud-consumerzk-order80 服务消费者 创建创建 cloud-provider-payment8002 模块 修改 pom.xml 引入 zookeeper 依赖 编写 application.yml 配置文件——注意修改端口号为 82 创建主启动类，使用@EnableDiscoveryClient 注解标记，用于向使用 consul 或者 zookeeper 作为注册中心时注册服务 业务类：创建 ApplicationContextConfig 类，使用@Configuration 注解标记为配置类；并创建 getRestTemplate()方法，使用@Bean 和@LoadBalanced 进行管理。 测试： 3.3 Consul 3.3.1 Consul 简介 Consul 是一套开源的分布式服务发现和配置管理系统，由 HashiCorp 公司用 Go 语言开发。 提供了微服务系统中的服务治理、配置中心、控制总线等功能。这些功能中的每一个都可以根据需要单独使用，也可以一起使用以构建全方位的服务网格，总之 Consul 提供了一种完整的服务网格解决方案。 它具有很多优点。包括： 基于 raft 协议，比较简洁； 支持健康检查, 同时支持 HTTP 和 DNS 协议 支持跨数据中心的 WAN 集群 提供图形界面 跨平台，支持 Linux、Mac、Windows 主要作用： 服务发现：提供 HTTP 和 DNS 两种发现方式 健康监测：支持多种方式，HTTP、TCP、Docker、Shell 脚本定制化监控 KV 存储：Key、Value 的存储方式 多数据中心：Consul 支持多数据中心 可视化 web 管理界面 3.3.2 安装 Consul 下载完成后解压只有一个 consul.exe 文件，双击会一闪而退，需要使用开发者模式打开。 打开命令行终端，执行./consul --version可以查看版本信息 执行consul agent -dev进行启动，并在http://localhost:8500地址下查看可视化界面。 3.3.3 创建 cloud-providerconsul-payment8006 提供者服务 创建模块 修改 pom，引入 consul-discovery 依赖 创建核心配置文件 创建主启动类，并用@EnableDiscoveryClient 注解 编写业务类 3.3.4 创建 cloud-consumerconsul-order80 消费者服务 创建模块 修改 pom，引入 consul-discovery 依赖 创建核心配置文件 创建主启动类，并用@EnableDiscoveryClient 注解 创建配置类，并用@Configuration 注解，getRestTemplate()方法使用@Bean 和@LoadBalanced 注解 3.4 三种注册中心的对比 AP(Eureka) CP(Zookeeper/Consul) 4. 服务调用 使用 Eureka 的集群进行测试，服务调用主要指负载均衡 4.0 负载均衡 LB 负载均衡(Load Balance)简单的说就是将用户的请求平摊的分配到多个服务上，从而达到系统的 HA（高可用）。 常见的负载均衡有软件 Nginx，LVS，硬件 F5 等。 Nginx 是服务器负载均衡，客户端所有请求都会交给 nginx，然后由 nginx 实现转发请求。即负载均衡是由服务端实现的。 Ribbon 本地负载均衡，在调用微服务接口时候，会在注册中心上获取注册信息服务列表之后缓存到 JVM 本地，从而在本地实现 RPC 远程服务调用技术。 分类： 集中式 LB：即在服务的消费方和提供方之间使用独立的 LB 设施(可以是硬件，如 F5, 也可以是软件，如 nginx), 由该设施负责把访问请求通过某种策略转发至服务的提供方； 进程内 LB：将 LB 逻辑集成到消费方，消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器。 4.1 Ribbon 4.1.1 Ribbon 介绍 Spring Cloud Ribbon 是基于 Netflix Ribbon 实现的一套客户端负载均衡的工具。 简单的说，Ribbon 是 Netflix 发布的开源项目，主要功能是提供客户端的软件负载均衡算法和服务调用。 Ribbon 客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出 Load Balancer（简称 LB）后面所有的机器，Ribbon 会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器。我们很容易使用 Ribbon 实现自定义的负载均衡算法。 spring 想用自己的 LoadBalancer 替代 Netflix Ribbon，但目前无法实现。 Ribbon 就属于进程内 LB，它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址。 一句话介绍：负载均衡+RestTemplate 调用 RestTemplate 实现远程远程调用。 4.1.2 架构说明 Ribbon 在工作时分成两步 第一步先选择 EurekaServer ,它优先选择在同一个区域内负载较少的 server。 第二步再根据用户指定的策略，在从 server 取到的服务注册列表中选择一个地址。 其中 Ribbon 提供了多种策略：比如轮询、随机和根据响应时间加权。 总结：Ribbon 其实就是一个软负载均衡的客户端组件，他可以和其他所需请求的客户端结合使用，和 eureka 结合只是其中的一个实例。 4.1.3 使用 之前写样例时候没有引入 spring-cloud-starter-ribbon 也可以使用 ribbon, 但是 spring-cloud-starter-netflix-eureka-client 自带了 spring-cloud-starter-netflix-ribbon 引用 所以在使用 Eureka 时，已经赋予了客户端服务负载均衡的功能。【有待验证，前面应该是使用了 springcloud 的 loadbalancer，而不是 ribbon——2022.08.23 确认，使用的 ribbon 的】 4.1.4 核心组件——IRule 介绍 Ribbon 如何实现负载均衡算法， 其核心组件为 IRule IRule：根据特定算法中从服务列表中选取一个要访问的服务 自带负载均衡算法为： com.netflix.loadbalancer.RoundRobinRule：轮询 com.netflix.loadbalancer.RandomRule：随机 com.netflix.loadbalancer.RetryRule：先按照 RoundRobinRule 的策略获取服务，如果获取服务失败则在指定时间内会进行重试，获取可用的服务 WeightedResponseTimeRule：对 RoundRobinRule 的扩展，响应速度越快的实例选择权重越大，越容易被选择 BestAvailableRule：会先过滤掉由于多次访问故障而处于断路器跳闸状态的服务，然后选择一个并发量最小的服务 AvailabilityFilteringRule：先过滤掉故障实例，再选择并发较小的实例 ZoneAvoidanceRule：默认规则,复合判断 server 所在区域的性能和 server 的可用性选择服务器 4.1.5 替换负载均衡算法——给某个服务消费者指定特殊的算法 要修改负载均衡算法，就是修改服务消费者的请求策略。 官方文档明确给出了警告： 替换负载均衡算法的自定义配置类不能放在@ComponentScan 所扫描的当前包下以及子包下，否则我们自定义的这个配置类就会被所有的 Ribbon 客户端所共享，达不到特殊化定制的目的了。 修改流程： 新建包 myrule 新建 MySelfRule 规则类，并使用@Configuration 注解标记 主启动类使用@RibbonClient(name =&quot;**CLOUD-PROVIDER-PAYMENT**&quot;,configuration=MySelfRule.class) 注意：这里的 name 要跟 Eurkea 中注册的服务提供者名字一致 4.1.6 默认轮询算法原理 负载均衡算法：rest 接口第几次请求数 % 服务器集群总数量 = 实际调用服务器位置下标 ，每次服务重启动后 rest 接口计数从 1 开始。 List instances = discoveryClient.getInstances(“CLOUD-PAYMENT-SERVICE”); 如： List [0] instances = 127.0.0.1:8002 List [1] instances = 127.0.0.1:8001 8001+ 8002 组合成为集群，它们共计 2 台机器，集群总数为 2， 按照轮询算法原理： 当总请求数为 1 时： 1 % 2 =1 对应下标位置为 1 ，则获得服务地址为 127.0.0.1:8001 当总请求数位 2 时： 2 % 2 =0 对应下标位置为 0 ，则获得服务地址为 127.0.0.1:8002 当总请求数位 3 时： 3 % 2 =1 对应下标位置为 1 ，则获得服务地址为 127.0.0.1:8001 当总请求数位 4 时： 4 % 2 =0 对应下标位置为 0 ，则获得服务地址为 127.0.0.1:8002 如此类推… 4.1.7 源码分析 没理解 4.1.8 手写轮询算法 启动 7001、7002 集群服务注册中心 在服务提供者 8001、8002 的控制器类中，新增返回接口的方法 修改服务消费者： ApplicationContextConfig 配置类中，取消掉@LoadBalanced 创建自定义 lb 包，并创建自定义 LoadBalancer 接口，定义方法名为 instances，返回值类型为 ServiceInstance 的抽象方法 创建 LoadBalancer 的实现类 MyLb，并用@Componetn 注解标记为组件（这里要求看得懂源码，因为在参照源码写） 123456789101112131415161718192021222324252627282930313233343536package iceriver.springcloud.lb;import java.util.List;import java.util.concurrent.atomic.AtomicInteger;import org.springframework.cloud.client.ServiceInstance;import org.springframework.stereotype.Component;/*** @author zhuyuqi* @version v0.0.1* @className MyLb* @description https://developer.aliyun.com/profile/sagwrxp2ua66w* @date 2022/08/23 11:44*/@Componentpublic class MyLb implements LoadBalancer &#123; private AtomicInteger atomicInteger = new AtomicInteger(0);// 原子类 public final int getAndIncrement() &#123; int current; int next; do &#123; current = this.atomicInteger.get(); next = current &gt;= 2147483647 ? 0 : current + 1; &#125; while (!this.atomicInteger.compareAndSet(current, next));// 自旋锁 System.out.println(&quot;*********next:&quot; + next); return next; &#125; @Override public ServiceInstance instances(List&lt;ServiceInstance&gt; serviceInstances) &#123; int index = getAndIncrement() % serviceInstances.size(); return serviceInstances.get(index); &#125;&#125; 编写 controller 类 4.2 OpenFeign 4.2.1 Feign 的介绍 Feign 是一个声明式的 Web 服务客户端，让编写 Web 服务客户端变得非常容易，只需创建一个接口并在接口上添加注解即可。 Feign 也支持可拔插式的编码器和解码器。Feign 可以与 Eureka 和 Ribbon 组合使用以支持负载均衡。 OpenFeign 是 spring 官方在 Feign 的基础上对 Feign 做的增强，使其支持了 Spring MVC 标准注解和 HttpMessageConverters。 Feign 旨在使编写 Java Http 客户端变得更容易。 前面在使用 Ribbon+RestTemplate 时，利用 RestTemplate 对 http 请求的封装处理，形成了一套模版化的调用方法。但是在实际开发中，由于对服务依赖的调用可能不止一处，往往一个接口会被多处调用，所以通常都会针对每个微服务自行封装一些客户端类来包装这些依赖服务的调用。所以，Feign 在此基础上做了进一步封装，由他来帮助我们定义和实现依赖服务接口的定义。在 Feign 的实现下，我们只需创建一个接口并使用注解的方式来配置它(以前是 Dao 接口上面标注 Mapper 注解,现在是一个微服务接口上面标注一个 Feign 注解即可)，即可完成对服务提供方的接口绑定，简化了使用 Spring cloud Ribbon 时，自动封装服务调用客户端的开发量。 Feign 集成了 Ribbon： 利用 Ribbon 维护了 Payment 的服务列表信息，并且通过轮询实现了客户端的负载均衡。而与 Ribbon 不同的是，通过 feign 只需要定义服务绑定接口且以声明式的方法，优雅而简单的实现了服务调用 4.2.2 Feign 与 OpenFeign Feign OpenFeign | Feign 是 Spring Cloud 组件中的一个轻量级 RESTful 的 HTTP 服务客户端。 Feign 内置了 Ribbon，用来做客户端负载均衡，去调用服务注册中心的服务。Feign 的使用方式是：使用 Feign 的注解定义接口，调用这个接口，就可以调用服务注册中心的服务 | OpenFeign 是 Spring Cloud 在 Feign 的基础上支持了 SpringMVC 的注解，如@RequesMapping 等等。 OpenFeign 的@FeignClient 可以解析 SpringMVC 的@RequestMapping 注解下的接口，并通过动态代理的方式产生实现类，实现类中做负载均衡并调用其他服务。 | | org.springframework.cloud spring-cloud-starter-feign | org.springframework.cloud spring-cloud-starter-openfeign | 4.2.3 使用步骤","tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://sk370.github.io/tags/Spring-Cloud/"}]},{"title":"MyBatis","date":"2022-07-25T10:36:46.000Z","path":"2022/07/25/mybatis/MyBatis/","text":"MyBatis 是一款优秀的持久层框架，它支持自定义 SQL、存储过程以及高级映射。MyBatis 免除了几乎所有的 JDBC 代码以及设置参数和获取结果集的工作。MyBatis 可以通过简单的 XML 或注解来配置和映射原始类型、接口和 Java POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。 与 Spring、SpringMVC 无先后学习关系 需要 JDBC 基础 1. MyBatis 入门 1.1 MyBatis 简介 1.1.1 MyBatis 历史 MyBatis 最初是 Apache 的一个开源项目 iBatis, 2010 年 6 月这个项目由 Apache Software Foundation 迁移到了 Google Code。随着开发团队转投 Google Code 旗下， iBatis3.x 正式更名为 MyBatis。代码于 2013 年 11 月迁移到 Github。 iBatis 一词来源于“internet”和“abatis”的组合，是一个基于 Java 的持久层框架。 iBatis 提供的持久层框架包括 SQL Maps 和 Data Access Objects（DAO）。 1.1.2 MyBatis 特性 MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。 MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。 MyBatis 可以使用简单的 XML 或注解用于配置和原始映射，将接口和 Java 的 POJO（Plain Old Java Objects，普通的 Java 对象）映射成数据库中的记录。 MyBatis 是一个 半自动的ORM（Object Relation Mapping）框架。 MyBatis 的配置文件分为核心配置文件和映射配置文件，核心配置文件写项目数据库连接和 MyBatis 的全局配置，映射配置写 SQL 语句。 1.1.3 MyBatis 下载 MyBatis 下载地址：https://github.com/mybatis/mybatis-3/releases 1.1.4 和其它持久化层技术对比 JDBC： SQL 夹杂在 Java 代码中耦合度高，导致硬编码内伤 维护不易且实际开发需求中 SQL 有变化，频繁修改的情况多见 代码冗长，开发效率低 Hibernate 和 JPA 操作简便，开发效率高 程序中的长难复杂 SQL 需要绕过框架 内部自动生产的 SQL，不容易做特殊优化 基于全映射的全自动框架，大量字段的 POJO 进行部分映射时比较困难。 反射操作太多，导致数据库性能下降 。 MyBatis 轻量级，性能出色 SQL 和 Java 编码分开，功能边界清晰。Java 代码专注业务、SQL 语句专注数据 开发效率稍逊于 HIbernate，但是完全能够接受 1.2 使用 MyBatis 1.2.1 使用 Maven 构建环境 软件版本： IDEA：2021.2.4【与 3.8.6 版本的 Maven 版本不兼容】 Maven：3.5.4 Mysql：5.7 MyBatis：3.5.7 创建 maven 工程：参见【Maven 4.Maven】 项目结构： 修改pro01-mybatis打包方式：需要为 jar 包形式 &lt;packaging&gt;jar&lt;/packaging&gt; 在父工程12.MyBits中添加项目依赖： 123456789101112131415161718192021&lt;dependencies&gt; &lt;!-- Mybatis核心 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.7&lt;/version&gt; &lt;/dependency&gt; &lt;!-- junit测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- MySQL驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.3&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.2.2 配置 mybatis 创建 MyBatis 的核心配置文件： 在main/resources目录下创建配置文件，习惯命名为mybatis-config.xml（名称无要求），整合 Spring 后，该文件可省略。 官方文档Getting Started提供了配置文件的模板【正文3 页】 根据本地 MySQL 的配置，修改配置文件的内容。 创建 mapper 接口： 创建数据表： 123456789CREATE TABLE `t_user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(20) DEFAULT NULL, `password` varchar(20) DEFAULT NULL, `age` int(11) DEFAULT NULL, `sex` char(1) DEFAULT NULL, `email` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 创建 pojo 实体： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class User &#123; private Integer id; private String username; private String password; private Integer age; private String sex;//数据库中文为char类型 private String email; public User() &#123; &#125; public User(Integer id, String username, String password, Integer age, String sex, String email) &#123; this.id = id; this.username = username; this.password = password; this.age = age; this.sex = sex; this.email = email; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125;&#125; 创建 mapper 接口： mapper 接口相当于 service 接口，但不需要提供实现类 ，实现类由 MyBatis 生成。 1234567public interface UserMapper &#123; /** * 添加用户 * @return 受影响的行数 */ int insertUser();&#125; 创建 MyBatis 映射文件： ORM：Object Relationship Mapping对象关系映射。 对象：Java 的实体类对象。 关系：关系型数据库。 映射：二者之间的对应关系 。 Java 与数据库的对应关系： 类&lt;—&gt;表 属性&lt;—&gt;字段/列 对象&lt;—&gt;记录/行 映射文件命名规则： 表所对应的实体类的类名+Mapper.xml 例如：表 t_user，映射的实体类为 User，所对应的映射文件为UserMapper.xml 一个映射文件对应一个实体类，对应一张表的操作 MyBatis 映射文件用于编写 SQL，访问以及操作表中的数据 MyBatis 映射文件存放的位置是src/main/resources/mappers目录下 官方文档Getting Started提供了映射文件的模板【正文4 页】 映射文件须与 mapper 接口文件保持两个一致： 映射文件的命名空间（namespace）和 mapper 接口的全类名和保持一致 映射文件中编写 SQL 的标签的 id 属性和 mapper 接口中方法的方法名保持一致 核心配置文件中引入映射文件： 以路径的形式引入： 执行测试： 在src/test/java下创建测试目录及测试文件 编写测试方法： 1234567891011121314151617181920public class UserMapperTest &#123; @Test public void testInsertUser() throws IOException &#123; // 1. 加载核心配置文件 InputStream istream = Resources.getResourceAsStream(&quot;mybatis-config.xml&quot;); // 2. 获取SqlSessionFactoryBuilder对象 SqlSessionFactoryBuilder sqlSessionFactoryBuilder = new SqlSessionFactoryBuilder(); // 3. 获取SqlSessionFactory对象 SqlSessionFactory sqlSessionFactory = sqlSessionFactoryBuilder.build(istream); // 4. 获取SqlSession对象 SqlSession sqlSession = sqlSessionFactory.openSession(); // 5. 获取mapper接口（实现类的）对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class);//底层使用了代理模式创建实现类 // 6. 调用mapper接口方法，执行测试 int result = userMapper.insertUser(); // 7. 提交事务 sqlSession.commit();//由于核心配置文件开启了事务：&lt;transactionManager type=&quot;JDBC&quot;/&gt;，所以这里需要提交 System.out.println(result); &#125;&#125; 获取sqlsession时传入参数true，即可设置为自动提交事务 SqlSession：代表Java程序和数据库之间的会话。（HttpSession是Java程序和浏览器之间的会话） SqlSessionFactory：是“生产”SqlSession的“工厂”。 工厂模式：如果创建某一个对象，使用的过程基本固定，那么我们就可以把创建这个对象的相关代码封装到一个“工厂类”中，以后都使用这个工厂类来“生产”我们需要的对象。 1.2.3 引入 log4j 日志功能 在父工程pom.xml中加入依赖： 在子工程src/main/resources目录下创建log4j.xml 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE log4j:configuration SYSTEM &quot;log4j.dtd&quot;&gt;&lt;log4j:configuration xmlns:log4j=&quot;http://jakarta.apache.org/log4j/&quot;&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;org.apache.log4j.ConsoleAppender&quot;&gt; &lt;param name=&quot;Encoding&quot; value=&quot;UTF-8&quot; /&gt; &lt;layout class=&quot;org.apache.log4j.PatternLayout&quot;&gt; &lt;param name=&quot;ConversionPattern&quot; value=&quot;%-5p %d&#123;MM-dd HH:mm:ss,SSS&#125;%m (%F:%L) \\n&quot; /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;logger name=&quot;java.sql&quot;&gt; &lt;level value=&quot;debug&quot; /&gt; &lt;/logger&gt; &lt;logger name=&quot;org.apache.ibatis&quot;&gt; &lt;level value=&quot;info&quot; /&gt; &lt;/logger&gt; &lt;root&gt; &lt;level value=&quot;debug&quot; /&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;/root&gt;&lt;/log4j:configuration&gt; 放到父工程src/main/resources目录下报错，不知道为什么。 &lt;level value=&quot;debug&quot; /&gt;中debug代表输出信息级别的高低 FATAL(致命)&gt;ERROR(错误)&gt;WARN(警告)&gt;INFO(信息)&gt;DEBUG(调试) 输出的信息内容详细程度从右至左提高 1.3 MyBatis 的增删改查 1.3.1 添加 1234&lt;!-- int insertUser(); --&gt;&lt;insert id=&quot;insertUser&quot;&gt; insert into t_user values(null,&#x27;admin&#x27;,&#x27;123456&#x27;,23,&#x27;女&#x27;,&#x27;12315415@qq.com&#x27;)&lt;/insert&gt; 1.3.2 修改 1234&lt;!-- void updateUser(); --&gt;&lt;update id=&quot;updateUser&quot;&gt; update t_user set username = &#x27;guest&#x27; where id = 6&lt;/update&gt; 1.3.3 删除 1234&lt;!-- void deleteUser(); --&gt;&lt;delete id=&quot;deleteUser&quot;&gt; delete from t_user where id = 8&lt;/delete&gt; 1.3.4 查询 123456789&lt;!-- User getUserById(); --&gt;&lt;select id=&quot;getUserById&quot; resultType=&quot;iceriver.mybatis.pojo.User&quot;&gt; select * form t_user where id = 7&lt;/select&gt;&lt;!-- List&lt;User&gt; getAllUser(); --&gt;&lt;select id=&quot;getAllUser&quot; resultType=&quot;iceriver.mybatis.pojo.User&quot;&gt; select * from t_user&lt;/select&gt; 查询的标签 select 必须设置属性resultType或resultMap，用于设置实体类和数据库表的映射关系 resultType：自动映射，用于属性名和表中字段名一致的情况 resultMap：自定义映射，用于一对多或多对一或字段名和属性名不一致的情况 当查询的数据为多条时，不能使用实体类作为返回值，只能使用集合，否则会抛出异常TooManyResultsException；但是若查询的数据只有一条，可以使用实体类或集合作为返回值 1.4 核心配置文件详解 1.4.1 文件标签 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//MyBatis.org//DTD Config 3.0//EN&quot; &quot;http://MyBatis.org/dtd/MyBatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!--引入properties文件，此时就可以$&#123;属性名&#125;的方式访问属性值--&gt; &lt;properties resource=&quot;jdbc.properties&quot;/&gt; &lt;typeAliases&gt; &lt;!-- typeAlias：设置某个具体的类型的别名 属性： type：需要设置别名的类型的全类名 alias：设置此类型的别名， （1）若设置了type，但没设置alias，则UserMapper.xml中，该类型拥有默认的别名，即类名且不区分大小写。 （2）若设置此属性，则UserMapper.xml中，此时该类型的别名只能使用alias所设置的值。 （3）若没有设置type，也没设置alias，则UserMapper.xml中只能使用对应类型的全类名 --&gt; &lt;!--&lt;typeAlias type=&quot;iceriver.mybatis.pojo.User&quot;/&gt;--&gt; &lt;!--&lt;typeAlias type=&quot;iceriver.mybatis.pojo.User&quot; alias=&quot;abc&quot;&gt;&lt;/typeAlias&gt;--&gt; &lt;!--以包为单位，设置改包下所有的类型都拥有默认的别名，即类名且不区分大小写--&gt; &lt;package name=&quot;iceriver.mybatis.pojo&quot;/&gt; &lt;/typeAliases&gt; &lt;!-- environments：设置多个连接数据库的环境 属性：default：设置默认使用的环境的id --&gt; &lt;environments default=&quot;mysql_test&quot;&gt; &lt;!-- environment：设置具体的连接数据库的环境信息 属性：id：设置环境的唯一标识，可通过environments标签中的default设置某一个环境的id，表示默认使用的环境 --&gt; &lt;environment id=&quot;mysql_test&quot;&gt; &lt;!-- transactionManager：设置事务管理方式 属性：type：设置事务管理方式，type=&quot;JDBC|MANAGED&quot; type=&quot;JDBC&quot;：设置当前环境的事务管理都必须手动处理 type=&quot;MANAGED&quot;：设置事务被管理，例如spring中的AOP --&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;!-- dataSource：设置数据源 属性：type：设置数据源的类型，type=&quot;POOLED|UNPOOLED|JNDI&quot; （1）type=&quot;POOLED&quot;：使用数据库连接池，即会将创建的连接进行缓存，下次使用可以从缓存中直接获取，不需要重新创建 （2）type=&quot;UNPOOLED&quot;：不使用数据库连接池，即每次使用连接都需要重新创建 （3）type=&quot;JNDI&quot;：调用上下文中的数据源 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;!--设置驱动类的全类名--&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;!--设置连接数据库的连接地址--&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;!--设置连接数据库的用户名--&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;!--设置连接数据库的密码--&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!--引入映射文件--&gt; &lt;mappers&gt; &lt;!--&lt;mapper resource=&quot;mappers/UserMapper.xml&quot;/&gt;--&gt; &lt;!-- 以包为单位，将包下所有的映射文件引入核心配置文件 要求：1. mapper接口所在的包和映射文件所在的包一致 2. mapper接口的名字和映射文件的名字一致 --&gt; &lt;package name=&quot;iceriver.mybatis.mapper&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 核心配置文件中&lt;configuration&gt;的子标签必须按照固定的顺序书写： properties?,settings?,typeAliases?,typeHandlers?,objectFactory?,objectWrapperFactory?,reflectorFactory?,plugins?,environments?,databaseIdProvider?,mappers? 1.4.2 IDEA 中设置配置文件模板 核心配置文件模板： 映射文件模板： 2. MyBatis 获取参数值 2.1 获取参数值的两种方式 2.1.1 $&#123;&#125;字符串拼接 本质是字符串拼接 参数为字符串类型或日期类型的字段进行赋值时，需要手动加单引号 2.1.2 #&#123;&#125;占位符【优先使用】 本质是占位符赋值 参数为字符串类型或日期类型的字段进行赋值时，可以自动添加单引号 2.2 获取参数值的多种情况 2.2.1 单个字面量类型的参数 #&#123;param&#125;：param 名称任意，建议见明知意 '$&#123;param&#125;'：param 名称任意，建议见明知意；注意单引号''。 2.2.2 多个字面量类型的参数 若 mapper 接口中的方法参数为多个时，此时 MyBatis 会自动将这些参数放在一个 map 集合中，以arg0,arg1.…为键，以参数为值；以param1,param2…为键，以参数为值； 注意arg从 0 开始，param从 1 开始 因此只需要通过$&#123;&#125;和#&#123;&#125;访问 map 集合的键就可以获取相对应的值。 注意$&#123;&#125;需要手动加单引号。 arg0和param1可以混用，但要注意先后顺序。 2.2.3 map 集合类型的参数 若 mapper 接口中的方法需要的参数为多个时，此时可以手动创建 map 集合，将这些数据放在 map 中，只需要通过$&#123;&#125;和#&#123;&#125;访问 map 集合的键就可以获取相对应的值，注意$&#123;&#125;需要手动加单引号。 这里的参数名称是自定义 map 集合时放入的名称。 2.2.4 实体类类型的参数【建议使用】 同 2.2.3 的访问方式，传入的参数名是实体类的属性名。——不需要使用对象名.属性名的方式调用，直接使用属性名。 2.2.5 使用@param注解标识参数【建议使用】——daoceng 使用@param 语句格式：User checkLoginByParams(@Param(&quot;username&quot;) String username, @Param(&quot;password&quot;) String password); 相当于给参数起别名。传参时别名起的啥，mapper 映射文件就传啥。 或者以param1,param2…为参数 源码：集合中会放 2 组参数： 以@Param注解的value属性值为键，以参数为值； 以param1,param2…为键，以参数为值。 3. MyBatis 的各种查询功能 建议查询语句不要写;以免将其作为通用查询，与其他查询组合时出现错误。 3.1 查询返回不同类型、数量结果 3.1.1 查询返回一个实体类对象 返回一个数据可以以实体类对象或集合接收。 123456/*** 根据用户id查询用户信息* @param id* @return*/User getUserById(@Param(&quot;id&quot;) int id); 1234&lt;!--User getUserById(@Param(&quot;id&quot;) int id);--&gt;&lt;select id=&quot;getUserById&quot; resultType=&quot;User&quot;&gt; select * from t_user where id = #&#123;id&#125;&lt;/select&gt; 3.1.2 查询返回一个 List 集合 返回多个数据只能通过集合接收。 12345/*** 查询所有用户信息* @return*/List&lt;User&gt; getUserList(); 1234&lt;!--List&lt;User&gt; getUserList();--&gt;&lt;select id=&quot;getUserList&quot; resultType=&quot;User&quot;&gt; select * from t_user&lt;/select&gt; 3.1.3 查询返回单个数据 12345/*** 查询用户的总记录数* @return*/int getCount(); 12345678910&lt;!-- 在MyBatis映射文件的resultType中，对于Java中常用的类型都设置了类型别名 例如：java.lang.Integer--&gt;int|integer 例如：int--&gt;_int|_integer 例如：Map--&gt;map,List--&gt;list--&gt;&lt;!--int getCount();--&gt;&lt;select id=&quot;getCount&quot; resultType=&quot;_integer&quot;&gt; select count(id) from t_user&lt;/select&gt; 别名参见官方文档15-16 页 3.1.4 查询一条数据为 map 集合 将实体类对象转换为 map 集合，或实体类对象的部分数据转换为集合。 123456/*** 根据用户id查询用户信息为map集合* @param id* @return*/Map&lt;String, Object&gt; getUserToMap(@Param(&quot;id&quot;) int id); 12345&lt;!--Map&lt;String, Object&gt; getUserToMap(@Param(&quot;id&quot;) int id);--&gt;&lt;select id=&quot;getUserToMap&quot; resultType=&quot;map&quot;&gt; select * from t_user where id = #&#123;id&#125;&lt;/select&gt;&lt;!--结果：&#123;password=123456, sex=男, id=1, age=23, username=admin&#125;--&gt; 3.1.5 查询多条数据为 map 集合 方式一： 12345/*** 查询所有用户信息为map集合* 将表中的数据以map集合的方式查询，一条数据对应一个map；若有多条数据，就会产生多个map集合，此时可以将这些map放在一个list集合中获取*/List&lt;Map&lt;String, Object&gt;&gt; getAllUserToMap(); 1234&lt;!--List&lt;Map&lt;String, Object&gt;&gt; getAllUserToMap();--&gt;&lt;select id=&quot;getAllUserToMap&quot; resultType=&quot;map&quot;&gt; select * from t_user&lt;/select&gt; 方式二： 使用 map 集合接收，可以用@MapKey(&quot;str&quot;)指定某个字段为 map 集合的键，注意指定为键的属性必须不能重复。 1234567/*** 查询所有用户信息为map集合* @return* 将表中的数据以map集合的方式查询，一条数据对应一个map；若有多条数据，就会产生多个map集合，并且最终要以一个map的方式返回数据，此时需要通过@MapKey注解设置map集合的键，值是每条数据所对应的map集合*/@MapKey(&quot;id&quot;)Map&lt;String, Object&gt; getAllUserToMap(); 123456789101112&lt;!--Map&lt;String, Object&gt; getAllUserToMap();--&gt;&lt;select id=&quot;getAllUserToMap&quot; resultType=&quot;map&quot;&gt; select * from t_user&lt;/select&gt;结果：&lt;!-- &#123; 1=&#123;password=123456, sex=男, id=1, age=23, username=admin&#125;, 2=&#123;password=123456, sex=男, id=2, age=23, username=张三&#125;, 3=&#123;password=123456, sex=男, id=3, age=23, username=张三&#125; &#125;--&gt; 3.2 特殊 SQL 查询 3.2.1 模糊查询 123456/*** 测试模糊查询* @param mohu* @return*/List&lt;User&gt; testMohu(@Param(&quot;mohu&quot;) String mohu); 123456&lt;!--List&lt;User&gt; testMohu(@Param(&quot;mohu&quot;) String mohu);--&gt;&lt;select id=&quot;testMohu&quot; resultType=&quot;User&quot;&gt; &lt;!--select * from t_user where username like &#x27;%$&#123;mohu&#125;%&#x27;--&gt; &lt;!--select * from t_user where username like concat(&#x27;%&#x27;,#&#123;mohu&#125;,&#x27;%&#x27;)--&gt; select * from t_user where username like &quot;%&quot;#&#123;mohu&#125;&quot;%&quot;&lt;/select&gt; 注意$&#123;&#125;单引号与#&#123;&#125;双引号的区别。 3.2.2 批量删除 123456/*** 批量删除* @param ids* @return*/int deleteMore(@Param(&quot;ids&quot;) String ids); 1234&lt;!--int deleteMore(@Param(&quot;ids&quot;) String ids);--&gt;&lt;delete id=&quot;deleteMore&quot;&gt; delete from t_user where id in ($&#123;ids&#125;)&lt;/delete&gt; 不能使用#&#123;&#125;，因为 SQL 语句中，in的括号中不能写引号，而#&#123;&#125;会自动添加单引号 in (1,2,3) 3.2.3 动态设置表名 123456/*** 动态设置表名，查询所有的用户信息* @param tableName* @return*/List&lt;User&gt; getAllUser(@Param(&quot;tableName&quot;) String tableName); 1234&lt;!--List&lt;User&gt; getAllUser(@Param(&quot;tableName&quot;) String tableName);--&gt;&lt;select id=&quot;getAllUser&quot; resultType=&quot;User&quot;&gt; select * from $&#123;tableName&#125;&lt;/select&gt; 不能使用#&#123;&#125;，因为 SQL 语句中，表名不能写引号，而#&#123;&#125;会自动添加单引号 3.2.4 添加功能获取自增的主键 123456789/*** 添加用户信息* @param user* @return* useGeneratedKeys：设置使用自增的主键* keyProperty：因为增删改有统一的返回值是受影响的行数，因此只能将获取的自增的主键放在传输的参数user对象的某个属性中*/int insertUser(User user); 1234&lt;!--int insertUser(User user);--&gt;&lt;insert id=&quot;insertUser&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot;&gt; insert into t_user values(null,#&#123;username&#125;,#&#123;password&#125;,#&#123;age&#125;,#&#123;sex&#125;)&lt;/insert&gt; MySQL 添加数据后，返回的是受影响的行数，所以无法直接返回新增的主键值，但可以通过返回添加的对象，通过对象去调用主键值。 原生 JDBC 获取主键的方式： 遍历结果集可以获得主键 12345678@Testpublic void testJDBC() throws Exception &#123; Class.forName(&quot;&quot;); Connection connection = DriverManager.getConnection(&quot;&quot;, &quot;&quot;, &quot;&quot;); PreparedStatement ps = connection.prepareStatement(&quot;insert&quot;, Statement.RETURN_GENERATED_KEYS); ps.executeUpdate(); ResultSet resultSet = ps.getGeneratedKeys();&#125; 4. 自定义映射 resultMap 4.1 resultMap 处理 sql 字段与 pojo 属性不一致问题 4.1.1 sql 语句使用别名方式与 pojo 属性对应 1234&lt;!--List&lt;Emp&gt; getAllEmp();--&gt;&lt;select id=&quot;getAllEmp&quot; resultType=&quot;emp&quot;&gt; select eid,emp_name empName,age,sex gender,email from t_emp&lt;/select&gt; 4.1.2 设置全局配置将_自动映射为驼峰 mybatis-config.xml文件中开启resultMap全局配置 官方文档正文12 页 12345&lt;!--设置MyBatis的全局配置--&gt;&lt;settings&gt; &lt;!--将 _ 自动映射为驼峰，emp_name:empName--&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; 1234&lt;!--List&lt;Emp&gt; getAllEmp();--&gt;&lt;select id=&quot;getAllEmp&quot; resultType=&quot;emp&quot;&gt; select * from t_emp&lt;/select&gt; 4.1.3 当前 mapper 映射文件中进行自定义设置 可以不将表的所有字段都设置，如字段本身与属性名一致时可不设置，但建议全部进行设置。 不需要开启全局映射。 可以设置别名映射，比如将字段 sex 映射为属性 gender。 注意：&lt;select&gt;的resultMap属性值，必须与&lt;resultMap&gt;的id属性值对应 12345678910111213141516171819202122&lt;!-- resultMap：设置自定义映射关系 id：唯一标识，不能重复 type：设置映射关系中的实体类类型 子标签： id：设置主键的映射关系(主键的映射无论是否一致，都必须写) result：设置普通字段的映射关系 property：设置映射关系中的属性名，必须是type属性所设置的实体类类型中的属性名 column：设置映射关系中的字段名，必须是sql语句查询出的字段名--&gt;&lt;resultMap id=&quot;empResultMap&quot; type=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt;&lt;/resultMap&gt;&lt;!--List&lt;Emp&gt; getAllEmp();--&gt;&lt;select id=&quot;getAllEmp&quot; resultMap=&quot;empResultMap&quot;&gt; select * from t_emp&lt;/select&gt; 4.2 处理表中多对一的映射关系 多对一：实体类中有个属性是实体类对象。 4.2.1 级联赋值 1234567891011121314 &lt;!--处理多对一映射关系方式一：级联属性赋值--&gt;&lt;resultMap id=&quot;empAndDeptResultMapOne&quot; type=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt; &lt;result property=&quot;dept.did&quot; column=&quot;did&quot;&gt;&lt;/result&gt; &lt;result property=&quot;dept.deptName&quot; column=&quot;dept_name&quot;&gt;&lt;/result&gt;&lt;/resultMap&gt;&lt;!--Emp getEmpAndDept(@Param(&quot;eid&quot;) Integer eid);--&gt;&lt;select id=&quot;getEmpAndDept&quot; resultMap=&quot;empAndDeptResultMapOne&quot;&gt; select * from t_emp left join t_dept on t_emp.did = t_dept .did where t_emp.eid = #&#123;eid&#125;&lt;/select&gt; ! 4.2.2 使用&lt;association&gt;标签及其子标签 123456789101112131415161718192021&lt;!--处理多对一映射关系方式二：association--&gt;&lt;resultMap id=&quot;empAndDeptResultMapTwo&quot; type=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt; &lt;!-- association:处理多对一的映射关系 property:需要处理多对的映射关系的属性名 javaType:该属性的类型 --&gt; &lt;association property=&quot;dept&quot; javaType=&quot;Dept&quot;&gt; &lt;id property=&quot;did&quot; column=&quot;did&quot;&gt;&lt;/id&gt; &lt;result property=&quot;deptName&quot; column=&quot;dept_name&quot;&gt;&lt;/result&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;!--Emp getEmpAndDept(@Param(&quot;eid&quot;) Integer eid);--&gt;&lt;select id=&quot;getEmpAndDept&quot; resultMap=&quot;empAndDeptResultMapTwo&quot;&gt; select * from t_emp left join t_dept on t_emp.did = t_dept .did where t_emp.eid = #&#123;eid&#125;&lt;/select&gt; 4.2.3 分步查询 分步一： 123456/*** 查询指定员工的信息及所在部门，分步一：查询所在部门did* @param eid* @return*/Emp getEmpAndDeptByStepOne(@Param(&quot;eid&quot;) Integer eid); 1234567891011121314151617181920&lt;!--处理多对一映射关系方式三：分步查询、分步一--&gt;&lt;resultMap id=&quot;empAndDeptByStepResultMap&quot; type=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt; &lt;!-- select:设置分步查询的sql的唯一标识（namespace.SQLId或mapper接口的全类名.方法名） column:设置分步查询的条件--&gt; &lt;association property=&quot;dept&quot; select=&quot;iceriver.mybatis.mapper.DeptMapper.getEmpAndDeptByStepTwo&quot; column=&quot;did&quot;&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;!--Emp getEmpAndDeptByStepOne(@Param(&quot;eid&quot;) Integer eid);--&gt;&lt;select id=&quot;getEmpAndDeptByStepOne&quot; resultMap=&quot;empAndDeptByStepResultMap&quot;&gt; select * from t_emp where eid = #&#123;eid&#125;&lt;/select&gt; 分步二： 123456/*** 查询指定员工的信息及所在部门，分步二：根据did查询部门* @param did* @return*/Dept getEmpAndDeptByStepTwo(@Param(&quot;did&quot;) Integer did); 12345678910&lt;!--处理多对一映射关系方式三：分步查询、分步二--&gt;&lt;resultMap id=&quot;empAndDeptByStepResultMap&quot; type=&quot;Dept&quot;&gt; &lt;id property=&quot;did&quot; column=&quot;did&quot;&gt;&lt;/id&gt; &lt;result property=&quot;deptName&quot; column=&quot;dept_name&quot;&gt;&lt;/result&gt;&lt;/resultMap&gt;&lt;!--Dept getEmpAndDeptByStepTwo(@Param(&quot;did&quot;) Integer did);--&gt;&lt;select id=&quot;getEmpAndDeptByStepTwo&quot; resultMap=&quot;empAndDeptByStepResultMap&quot;&gt; select * from t_dept where did = #&#123;did&#125;&lt;/select&gt; 4.3 处理表中一对多的映射关系 一对多：实体类中有个属性是集合 4.3.1 使用&lt;connection&gt;标签及其子标签 1234567891011121314151617181920&lt;!--处理一对多映射关系方式一：--&gt;&lt;resultMap id=&quot;deptAndEmpResultMap&quot; type=&quot;Dept&quot;&gt; &lt;id property=&quot;did&quot; column=&quot;did&quot;&gt;&lt;/id&gt; &lt;result property=&quot;deptName&quot; column=&quot;dept_name&quot;&gt;&lt;/result&gt; &lt;!-- collection：处理一对多的映射关系 ofType：表示该属性所对应的集合中存储数据的类型 --&gt; &lt;collection property=&quot;emps&quot; ofType=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt; &lt;/collection&gt;&lt;/resultMap&gt;&lt;!--Dept getDeptAndEmp(@Param(&quot;did&quot;) Integer did);--&gt;&lt;select id=&quot;getDeptAndEmp&quot; resultMap=&quot;deptAndEmpResultMap&quot;&gt; select * from t_dept left join t_emp on t_dept.did = t_emp.did where t_dept.did = #&#123;did&#125;&lt;/select&gt; 4.3.2 分步查询 1234567891011121314&lt;!--处理一对多映射关系方式二：分步查询、分步一--&gt;&lt;resultMap id=&quot;deptAndEmpByStepResultMap&quot; type=&quot;Dept&quot;&gt; &lt;id property=&quot;did&quot; column=&quot;did&quot;&gt;&lt;/id&gt; &lt;result property=&quot;deptName&quot; column=&quot;dept_name&quot;&gt;&lt;/result&gt; &lt;collection property=&quot;emps&quot; select=&quot;iceriver.mybatis.mapper.EmpMapper.getDeptAndEmpByStepTwo&quot; column=&quot;did&quot; fetchType=&quot;eager&quot;&gt; &lt;/collection&gt;&lt;/resultMap&gt;&lt;!--Dept getDeptAndEmpByStepOne(@Param(&quot;did&quot;) Integer did);--&gt;&lt;select id=&quot;getDeptAndEmpByStepOne&quot; resultMap=&quot;deptAndEmpByStepResultMap&quot;&gt; select * from t_dept where did = #&#123;did&#125;&lt;/select&gt; 1234567891011&lt;!--List&lt;Emp&gt; getDeptAndEmpByStepTwo(@Param(&quot;did&quot;) Integer did);--&gt;&lt;resultMap id=&quot;empAndDeptResultMapOne&quot; type=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt;&lt;/resultMap&gt;&lt;select id=&quot;getDeptAndEmpByStepTwo&quot; resultMap=&quot;empAndDeptResultMapOne&quot;&gt; select * from t_emp where did = #&#123;did&#125;&lt;/select&gt; 4.4 分步查询的优点 分步查询的优点：可以实现延迟加载，但是必须在核心配置文件中设置全局配置信息： 1234&lt;!--开启延迟加载--&gt;&lt;settings&gt; &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; lazyLoadingEnabled：延迟加载的全局开关。当开启时，所有关联对象都会延迟加载 aggressiveLazyLoading：当开启时，任何方法的调用都会加载该对象的所有属性【3.7.5 版本默认开启】。 否则，每个属性会按需加载 核心配置文件开启延迟加载后，所有的分步查询都会延迟加载，如果需要控制当前映射文件中的分步查询，可以通过association和collection中的fetchType属性设置当前的分步查询是否使用延迟加载。 fetchType=&quot;lazy&quot;(延迟加载)，开启全局延迟加载时的默认值。 fetchType=&quot;eager&quot;(立即加载) ，未全局延迟加载时的默认值。 5. 动态 SQL 解决拼接 SQL 语句字符串时的痛点问题。 5.1 &lt;if&gt; &lt;if&gt;标签可通过test属性的表达式进行判断，若表达式的结果为true，则标签中的内容会执行；反之标签中的内容不会执行 123456789101112131415161718192021222324&lt;resultMap id=&quot;empResultMap&quot; type=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt;&lt;/resultMap&gt;&lt;!--List&lt;Emp&gt; getEmpByCondition(Emp emp);--&gt;&lt;select id=&quot;getEmpByCondition&quot; resultMap=&quot;empResultMap&quot;&gt; select * from t_emp where 1=1 &lt;if test=&quot;empName != null and empName != &#x27;&#x27;&quot;&gt; and emp_name = #&#123;empName&#125; &lt;/if&gt; &lt;if test=&quot;age != null and age != &#x27;&#x27;&quot;&gt; and age = #&#123;age&#125; &lt;/if&gt; &lt;if test=&quot;gender != null and gender != &#x27;&#x27;&quot;&gt; or sex = #&#123;gender&#125; &lt;/if&gt; &lt;if test=&quot;email != null and email != &#x27;&#x27;&quot;&gt; and email = #&#123;email&#125; &lt;/if&gt;&lt;/select&gt; if标签中test属性的值是实体类的属性名，或者别名。 if语句中的查询条件=左侧字段名是数据表中的字段名，右侧是实体类属性名。 这里的实体类中的属性名也相当于前端表单提交的属性名【表单提交的属性名与实体类的属性名一致】 这里1=1是为了解决各项条件都不符合，或emp_name不符合条件，导致where后面没有语句或where后面直接出现and或or的情况。 1=1这种用法可以在正常的 SQL 语句中使用，且不会影响执行效果。 5.2 &lt;where&gt; 1234567891011121314151617181920212223242526&lt;resultMap id=&quot;empResultMap&quot; type=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt;&lt;/resultMap&gt;&lt;!--List&lt;Emp&gt; getEmpByCondition(Emp emp);--&gt;&lt;select id=&quot;getEmpByCondition&quot; resultMap=&quot;empResultMap&quot;&gt; select * from t_emp &lt;where&gt; &lt;if test=&quot;empName != null and empName != &#x27;&#x27;&quot;&gt; and emp_name = #&#123;empName&#125; &lt;/if&gt; &lt;if test=&quot;age != null and age != &#x27;&#x27;&quot;&gt; and age = #&#123;age&#125; &lt;/if&gt; &lt;if test=&quot;gender != null and gender != &#x27;&#x27;&quot;&gt; or sex = #&#123;gender&#125; &lt;/if&gt; &lt;if test=&quot;email != null and email != &#x27;&#x27;&quot;&gt; and email = #&#123;email&#125; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; where和if一般结合使用： 若where标签中的 if 条件都不满足，则where标签没有任何功能，即不会添加where关键字 若where标签中的if条件满足，则where标签会自动添加where关键字，并将条件最前方多余的and/or去掉 注意：where标签不能去掉条件最后多余的and/or 5.3 &lt;trim&gt; 1234567891011121314151617181920212223242526&lt;resultMap id=&quot;empResultMap&quot; type=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt;&lt;/resultMap&gt;&lt;!--List&lt;Emp&gt; getEmpByCondition(Emp emp);--&gt;&lt;select id=&quot;getEmpByCondition&quot; resultMap=&quot;empResultMap&quot;&gt; select * from t_emp &lt;trim prefix=&quot;where&quot; prefixOverrides=&quot;and|or&quot;&gt; &lt;if test=&quot;empName != null and empName != &#x27;&#x27;&quot;&gt; and emp_name = #&#123;empName&#125; &lt;/if&gt; &lt;if test=&quot;age != null and age != &#x27;&#x27;&quot;&gt; and age = #&#123;age&#125; &lt;/if&gt; &lt;if test=&quot;gender != null and gender != &#x27;&#x27;&quot;&gt; or sex = #&#123;gender&#125; &lt;/if&gt; &lt;if test=&quot;email != null and email != &#x27;&#x27;&quot;&gt; and email = #&#123;email&#125; &lt;/if&gt; &lt;/trim&gt;&lt;/select&gt; trim用于去掉或添加标签中的内容： prefix：在 trim 标签中的内容的前面添加指定内容 prefixOverrides：在trim标签中的内容的前面去掉指定内容 suffix：在trim标签中的内容的后面添加指定内容 suffixOverrides：在trim标签中的内容的后面去掉指定内容 5.4 &lt;choose&gt;、&lt;when&gt;、&lt;otherwise&gt; 相当于 if…else if…if when至少要有一个 otherwise最多只能有一个 12345678910111213141516171819202122232425262728293031&lt;resultMap id=&quot;empResultMap&quot; type=&quot;Emp&quot;&gt; &lt;id property=&quot;eid&quot; column=&quot;eid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;empName&quot; column=&quot;emp_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;age&quot; column=&quot;age&quot;&gt;&lt;/result&gt; &lt;result property=&quot;gender&quot; column=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;&gt;&lt;/result&gt;&lt;/resultMap&gt;&lt;!--List&lt;Emp&gt; getEmpByChoose(Emp emp);--&gt;&lt;select id=&quot;getEmpByChoose&quot; resultMap=&quot;empResultMap&quot;&gt; select * from t_emp &lt;where&gt; &lt;choose&gt; &lt;when test=&quot;empName != null and empName != &#x27;&#x27;&quot;&gt; emp_name = #&#123;empName&#125; &lt;/when&gt; &lt;when test=&quot;age != null and age != &#x27;&#x27;&quot;&gt; age = #&#123;age&#125; &lt;/when&gt; &lt;when test=&quot;gender != null and gender != &#x27;&#x27;&quot;&gt; sex = #&#123;gender&#125; &lt;/when&gt; &lt;when test=&quot;email != null and email != &#x27;&#x27;&quot;&gt; email = #&#123;email&#125; &lt;/when&gt; &lt;otherwise&gt; did=1 &lt;/otherwise&gt; &lt;/choose&gt; &lt;/where&gt;&lt;/select&gt; 5.5 &lt;foreach&gt; 5.5.1 批量删除 123456789101112131415161718192021222324252627&lt;!--int deleteMoreByArray(@Param(&quot;eids&quot;) Integer[] eids);--&gt;&lt;!--&lt;delete id=&quot;deleteMoreByArray&quot;&gt; delete from t_emp where eid in ( &lt;foreach collection=&quot;eids&quot; item=&quot;eid&quot; separator=&quot;,&quot;&gt; #&#123;eid&#125; &lt;/foreach&gt; )&lt;/delete&gt;--&gt;&lt;!--&lt;delete id=&quot;deleteMoreByArray&quot;&gt; delete from t_emp where eid in &lt;foreach collection=&quot;eids&quot; item=&quot;eid&quot; separator=&quot;,&quot; open=&quot;(&quot; close=&quot;)&quot;&gt; #&#123;eid&#125; &lt;/foreach&gt;&lt;/delete&gt;--&gt;&lt;delete id=&quot;deleteMoreByArray&quot;&gt; delete from t_emp where &lt;foreach collection=&quot;eids&quot; item=&quot;eid&quot; separator=&quot;or&quot;&gt; eid=#&#123;eid&#125; &lt;/foreach&gt;&lt;/delete&gt; collection：设置要循环的数组或集合 item：表示集合或数组中的每一个数据 separator：设置循环体之间的分隔符 open：设置foreach标签中的内容的开始符 close：设置foreach标签中的内容的结束符 5.5.2 批量添加 1234567&lt;!--int insertMoreByList(@Param(&quot;emps&quot;) List&lt;Emp&gt; emps);--&gt;&lt;insert id=&quot;insertMoreByList&quot;&gt; insert into t_emp values &lt;foreach collection=&quot;emps&quot; item=&quot;emp&quot; separator=&quot;,&quot;&gt; (null,#&#123;emp.empName&#125;,#&#123;emp.age&#125;,#&#123;emp.gender&#125;,#&#123;emp.email&#125;,null) &lt;/foreach&gt;&lt;/insert&gt; 注意这里gender是实体类中的属性名，sex是表中字段名。由于前文设置了gender的映射关系，所以这里可以使用gender 5.6 &lt;sql&gt;片段 设置sql片段，记录一段数据，在使用的地方使用&lt;include&gt;标签引入，插入到sql语句中。 1234&lt;sql id=&quot;empColumns&quot;&gt; eid,ename,age,sex,did&lt;/sql&gt;select &lt;include refid=&quot;empColumns&quot;&gt;&lt;/include&gt; from t_emp 6. MyBatis 的缓存 只针对查询功能有效 6.1 一级缓存（不同查询在同一个sqlSession下） 一级缓存是SqlSession级别的，通过同一个 SqlSession 查询的数据会被缓存，下次查询相同的数据，就会从缓存中直接获取，不会从数据库重新访问【不会执行 sql 查询】 不论SqlSession对象是否关闭或提交，都会保存一级缓存 sqlSession.commit()或sqlSession.close() 使一级缓存失效的四种情况： 不同的SqlSession对应不同的一级缓存 同一个SqlSession但是查询条件不同 同一个SqlSession两次查询期间执行了任何一次增删改操作 同一个SqlSession两次查询期间手动清空了缓存 sqlSession.clearCache() 6.2 二级缓存（不同查询在同一个sqlSessionFactory下） 二级缓存级别大于一级缓存。 二级缓存是SqlSessionFactory级别，通过同一个SqlSessionFactory创建的SqlSession查询的结果会被缓存；此后若再次执行相同的查询语句，结果就会从缓存中获取。 二级缓存开启的条件（同时满足）： 在核心配置文件中，设置全局配置属性cacheEnabled=&quot;true&quot; 默认即为true，不需要设置。 在映射文件中设置标签&lt;cache/&gt; 二级缓存必须在SqlSession对象关闭或提交之后有效 sqlSession.commit()或sqlSession.close() 查询的数据所转换的实体类类型必须实现序列化Serializable接口 使二级缓存失效的情况： 两次查询之间执行了任意的增删改，会使一级和二级缓存同时失效 6.3 二级缓存的相关配置 通过在mapper映射文件中设置&lt;cache/&gt; 标签的属性配置： eviction属性：缓存回收策略 LRU（Least Recently Used） – 最近最少使用的：移除最长时间不被使用的对象。默认值。 FIFO（First in First out） – 先进先出：按对象进入缓存的顺序来移除它们。 SOFT – 软引用：移除基于垃圾回收器状态和软引用规则的对象。 WEAK – 弱引用：更积极地移除基于垃圾收集器状态和弱引用规则的对象。 flushInterval属性：刷新间隔，单位毫秒 默认情况是不设置，也就是没有刷新间隔，缓存仅仅调用语句时刷新 size属性：引用数目，正整数。代表缓存最多可以存储多少个对象，太大容易导致内存溢出 readOnly属性：只读，true/false true：只读缓存；会给所有调用者返回缓存对象的相同实例。因此这些对象不能被修改。这提供了很重要的性能优势。 false：读写缓存；会返回缓存对象的拷贝（通过序列化）。这会慢一些，但是安全，因此默认是false。 6.4 MyBatis 缓存查询的顺序 先查询二级缓存，因为二级缓存中可能会有其他程序已经查出来的数据，可以拿来直接使用。【先搜索大范围，再搜索小范围】。 如果二级缓存没有命中，再查询一级缓存。 如果一级缓存也没有命中，则查询数据库。 SqlSession关闭之后，一级缓存中的数据会写入二级缓存。 6.5 整合第三方缓存 EHCache 只能代替二级缓存，没法代替一级缓存。【为什么？】 6.5.1 添加依赖 123456789101112&lt;!-- Mybatis EHCache整合包 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.caches&lt;/groupId&gt; &lt;artifactId&gt;mybatis-ehcache&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- slf4j日志门面的一个具体实现 --&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt; 6.5.2 各 jar 包功能 jar 包名称 作用 mybatis-ehcache Mybatis 和 EHCache 的整合包 ehcache EHCache 核心包 slf4j-api SLF4J 日志门面包 logback-classic 支持 SLF4J 门面接口的一个具体实现 日志门面：提供了规范、接口 6.5.3 创建配置文件 在resources目录下创建ehcache.xml 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;&lt;ehcache xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:noNamespaceSchemaLocation=&quot;../config/ehcache.xsd&quot;&gt; &lt;!-- 磁盘保存路径 --&gt; &lt;diskStore path=&quot;D:\\iceriver\\ehcache&quot;/&gt; &lt;defaultCache maxElementsInMemory=&quot;1000&quot; maxElementsOnDisk=&quot;10000000&quot; eternal=&quot;false&quot; overflowToDisk=&quot;true&quot; timeToIdleSeconds=&quot;120&quot; timeToLiveSeconds=&quot;120&quot; diskExpiryThreadIntervalSeconds=&quot;120&quot; memoryStoreEvictionPolicy=&quot;LRU&quot;&gt; &lt;/defaultCache&gt;&lt;/ehcache&gt; 属性名 是否必须 作用 maxElementsInMemory 是 在内存中缓存的 element 的最大数目 maxElementsOnDisk 是 在磁盘上缓存的 element 的最大数目，若是 0 表示无穷大 eternal 是 设定缓存的 elements 是否永远不过期。 如果为 true，则缓存的数据始终有效， 如果为 false 那么还要根据 timeToIdleSeconds、timeToLiveSeconds 判断 overflowToDisk 是 设定当内存缓存溢出的时候是否将过期的 element 缓存到磁盘上 timeToIdleSeconds 否 当缓存在 EhCache 中的数据前后两次访问的时间超过 timeToIdleSeconds 的属性取值时， 这些数据便会删除，默认值是 0,也就是可闲置时间无穷大 timeToLiveSeconds 否 缓存 element 的有效生命期，默认是 0.,也就是 element 存活时间无穷大 diskSpoolBufferSizeMB 否 DiskStore(磁盘缓存)的缓存区大小。默认是 30MB。每个 Cache 都应该有自己的一个缓冲区 diskPersistent 否 在 VM 重启的时候是否启用磁盘保存 EhCache 中的数据，默认是 false。 diskExpiryThreadIntervalSeconds 否 磁盘缓存的清理线程运行间隔，默认是 120 秒。每个 120s， 相应的线程会进行一次 EhCache 中数据的清理工作 memoryStoreEvictionPolicy 否 当内存缓存达到最大，有新的 element 加入的时候， 移除缓存中 element 的策略。 默认是 LRU（最近最少使用），可选的有 LFU（最不常使用）和 FIFO（先进先出） 在resources目录下创建logback.xml 存在 SLF4J 时，作为简易日志的 log4j 将失效，此时我们需要借助 SLF4J 的具体实现 logback 来打印日志 问题 1：那可以删了log4j吗？ 问题 2：可以不使用logback吗？ 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration debug=&quot;true&quot;&gt; &lt;!-- 指定日志输出的位置 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;!-- 日志输出的格式 --&gt; &lt;!-- 按照顺序分别是：时间、日志级别、线程名称、打印日志的类、日志主体内容、换行 --&gt; &lt;pattern&gt;[%d&#123;HH:mm:ss.SSS&#125;] [%-5level] [%thread] [%logger][%msg]%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 设置全局日志级别。日志级别按顺序分别是：DEBUG、INFO、WARN、ERROR --&gt; &lt;!-- 指定任何一个日志级别都只打印当前级别和后面级别的日志。 --&gt; &lt;root level=&quot;DEBUG&quot;&gt; &lt;!-- 指定打印日志的appender，这里通过“STDOUT”引用了前面配置的appender --&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;/root&gt; &lt;!-- 根据特殊需求指定局部日志级别 --&gt; &lt;logger name=&quot;com.atguigu.crowd.mapper&quot; level=&quot;DEBUG&quot;/&gt;&lt;/configuration&gt; 6.5.4 设置使用EHCache 在 mapper 映射文件的&lt;cache/&gt;标签中指定type属性值为EhcacheCache： &lt;cache type=&quot;org.mybatis.caches.ehcache.EhcacheCache&quot;/&gt; 7. MyBatis 的逆向工程 7.1 正向工程与逆向工程 正向工程：先创建 Java 实体类，由框架负责根据实体类生成数据库表。Hibernate是支持正向工程的。 逆向工程：先创建数据库表，由框架负责根据数据库表，反向生成如下资源： Java 实体类 Mapper 接口（持久层） Mapper 映射文件 7.2 创建 MyBatis 的逆向工程 7.2.1 添加依赖和插件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;12.MyBatis&lt;/artifactId&gt; &lt;groupId&gt;iceriver.mybatis&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;pr04-mybatis-MBG&lt;/artifactId&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;!-- 依赖MyBatis核心包 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.7&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 控制Maven在构建过程中相关配置 --&gt; &lt;build&gt; &lt;!-- 构建过程中用到的插件 --&gt; &lt;plugins&gt; &lt;!-- 具体插件，逆向工程的操作是以构建过程中插件形式出现的 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;!-- 插件的依赖 --&gt; &lt;dependencies&gt; &lt;!-- 逆向工程的核心依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySQL驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.8&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 7.2.2 创建逆向工程配置文件 在resources目录下创建配置文件generatorConfig.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt;&lt;generatorConfiguration&gt; &lt;!-- targetRuntime: 执行生成的逆向工程的版本 MyBatis3Simple: 生成基本的CRUD（清新简洁版） MyBatis3: 生成带条件的CRUD（奢华尊享版） --&gt; &lt;context id=&quot;DB2Tables&quot; targetRuntime=&quot;MyBatis3Simple&quot;&gt; &lt;!-- 数据库的连接信息 --&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql://localhost:13306/mybatis_db&quot; userId=&quot;root&quot; password=&quot;dimitre123&quot;&gt; &lt;/jdbcConnection&gt; &lt;!-- javaBean的生成策略--&gt; &lt;javaModelGenerator targetPackage=&quot;iceriver.mybatis.bean&quot; targetProject=&quot;.\\src\\main\\java&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot; /&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot; /&gt; &lt;/javaModelGenerator&gt; &lt;!-- SQL映射文件的生成策略 --&gt; &lt;sqlMapGenerator targetPackage=&quot;iceriver.mybatis.mapper&quot; targetProject=&quot;.\\src\\main\\resources&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot; /&gt; &lt;/sqlMapGenerator&gt; &lt;!-- Mapper接口的生成策略 --&gt; &lt;javaClientGenerator type=&quot;XMLMAPPER&quot; targetPackage=&quot;iceriver.mybatis.mapper&quot; targetProject=&quot;.\\src\\main\\java&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot; /&gt; &lt;/javaClientGenerator&gt; &lt;!-- 逆向分析的表 --&gt; &lt;!-- tableName设置为*号，可以对应所有表，此时不写domainObjectName --&gt; &lt;!-- domainObjectName属性指定生成出来的实体类的类名 --&gt; &lt;table tableName=&quot;t_emp&quot; domainObjectName=&quot;Emp&quot;/&gt; &lt;table tableName=&quot;t_dept&quot; domainObjectName=&quot;Dept&quot;/&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 文件名称是固定的吗？ 7.2.3 执行 MBG 插件 简洁版生成的 mapper 文件： 奢华版生成的 mapper 文件： 带Selective的方法与不带的区别是：如果传入的参数有null，带Selective的方法不会修改原值，不带Selective的方法会将原值改为null 7.2.4 QBC 查询 123456789101112131415161718192021222324252627public class MBGTest &#123; @Test public void testMBG()&#123; try&#123; InputStream inputStream = Resources.getResourceAsStream(&quot;mybatis-config.xml&quot;); SqlSessionFactory build = new SqlSessionFactoryBuilder().build(inputStream); SqlSession sqlSession = build.openSession(true); EmpMapper mapper = sqlSession.getMapper(EmpMapper.class); //1. 查询所有员工 List&lt;Emp&gt; emps = mapper.selectByExample(null); emps.forEach(emp -&gt; System.out.println(emp)); //2. 根据条件查询 EmpExample empExample = new EmpExample(); empExample.createCriteria().andEmpNameEqualTo(&quot;a1&quot;).andAgeGreaterThan(20); empExample.or().andDidIsNotNull(); List&lt;Emp&gt; emps = mapper.selectByExample(empExample); emps.forEach(emp -&gt; System.out.println(emp)); //3. 修改 mapper.updateByPrimaryKey(new Emp(1, &quot;wuxie&quot;, null, null, null,null)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 8. 分页插件 8.1 添加依赖 123456&lt;!-- https://mvnrepository.com/artifact/com.github.pagehelper/pagehelper --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.2.0&lt;/version&gt;&lt;/dependency&gt; 8.2 配置分页插件 在mybatic-config.xml核心配置文件中配置插件： 1234&lt;plugins&gt; &lt;!--设置分页插件--&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageInterceptor&quot;&gt;&lt;/plugin&gt;&lt;/plugins&gt; 尚筹网中的配置，使用的 4.0 版本的，不知道是不是版本的变化： 1234567891011121314151617&lt;bean id=&quot;sqlSessionFactoryBean&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;!--设置分页插件--&gt; &lt;property name=&quot;plugins&quot;&gt; &lt;array&gt; &lt;bean class=&quot;com.github.pagehelper.PageHelper&quot;&gt; &lt;property name=&quot;properties&quot;&gt; &lt;props&gt; &lt;!-- 配置数据库方言：告诉pagehelper使用的什么数据库 --&gt; &lt;prop key=&quot;dialect&quot;&gt;mysql&lt;/prop&gt; &lt;!-- 配置页码的合理化修i正，在1~总页数之间修正页码 --&gt; &lt;prop key=&quot;reasonable&quot;&gt;true&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/array&gt; &lt;/property&gt;&lt;/bean&gt; 8.3 使用分页查询 在查询功能之前使用PageHelper.startPage(int pageNum, int pageSize)开启分页功能 pageNum：当前页的页码 pageSize：每页显示的条数 在查询获取到 list 集合之后，使用PageInfo&lt;T&gt; pageInfo = new PageInfo&lt;&gt;(List&lt;T&gt; list, int navigatePages)获取分页相关数据 list：分页之后的数据 navigatePages：导航分页的页码数 分页pageInfo相关数据： pageNum：当前页的页码 pageSize：每页显示的条数 size：当前页显示的真实条数 total：总记录数 pages：总页数 prePage：上一页的页码 nextPage：下一页的页码 isFirstPage/isLastPage：是否为第一页/最后一页 hasPreviousPage/hasNextPage：是否存在上一页/下一页 navigatePages：导航分页的页码数 navigatepageNums：导航分页的页码，[1,2,3,4,5]","tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://sk370.github.io/tags/MyBatis/"}]},{"title":"Spring MVC","date":"2022-07-19T03:16:27.000Z","path":"2022/07/19/springmvc/SpringMVC/","text":"通常我们认为，Spring MVC 是 Spring 的子项目，其实，它谈不上是一个独立的子项目，因为连一个正规的名字都没有。Spring Web MVC 是基于 Servlet API 构建的原始 Web 框架，从一开始就包含在 Spring 框架中。正式名称“Spring Web MVC”来自其源模块（spring-webmvc）的名称，但更常见的名称是“Spring MVC”。 1. SpringMVC 简介 1.1 Spring MVC 介绍 1.1.1 MVC MVC是一种软件架构的思想，将软件按照模型、视图、控制器来划分 M：Model，模型层，指工程中的JavaBean，作用是处理数据 JavaBean分为两类： 一类称为实体类Bean：专门存储业务数据的，如 Student、User等 一类称为业务处理 Bean：指 Service或 Dao对象，专门用于处理业务逻辑和数据访问。 V：View，视图层，指工程中的 html 或 jsp 等页面，作用是与用户进行交互，展示数据 C：Controller，控制层，指工程中的servlet，作用是接收请求和响应浏览器 MVC的工作流程：用户通过视图层发送请求到服务器，在服务器中请求被Controller接收，Controller调用相应的 Model 层处理请求，处理完毕将结果返回到Controller，Controller再根据请求处理的结果找到相应的 View 视图，渲染数据后最终响应给浏览器 1.1.2 SpringMVC SpringMVC是Spring的一个后续产品，是 Spring 的一个子项目 SpringMVC 是 Spring 为表述层开发提供的一整套完备的解决方案。在表述层框架历经 Strust、WebWork、Strust2 等诸多产品的历代更迭之后，目前业界普遍选择了 SpringMVC 作为 JavaEE 项目表述层开发的首选方案。 注：三层架构分为表述层（或表示层）、业务逻辑层、数据访问层，表述层表示前台页面和后台servlet 1.1.3 SpringMVC 的特点 **Spring **家族原生产品，与IOC容器等基础设施无缝对接 基于原生的**Servlet**，通过了功能强大的前端控制器**DispatcherServlet**，对请求和响应进行统一处理 表述层各细分领域需要解决的问题全方位覆盖，提供全面解决方案 代码清新简洁，大幅度提升开发效率 内部组件化程度高，可插拔式组件即插即用，想要什么功能配置相应组件即可 性能卓著，尤其适合现代大型、超大型互联网项目要求 1.2 Spring MVC 使用 1.2.1 开发环境 IDEA2021.2.4 maven3.5.4【3.8.6 版本与 idea2021 使用会找不到依赖】 tomcat8 Spring5.3.1 1.2.2 创建 maven 工程 创建项目： 修改本地 maven 仓库： 添加框架支持，因为为 web 项目。 这里使用框架支持还是手动建立 web 目录都没有区别，配置好工件路径即可。【参见 JavaWeb4.1】 1.2.3 添加项目依赖 必须的依赖： Spring MVC 日志：logback servlet api thymeleaf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;iceriver.springmvc&lt;/groupId&gt; &lt;artifactId&gt;pro01-demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;dependencies&gt; &lt;!-- SpringMVC --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- ServletAPI --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring5和Thymeleaf整合包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.thymeleaf&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-spring5&lt;/artifactId&gt; &lt;version&gt;3.0.12.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.3.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 1.2.4 配置web.xml 配置SpringMVC的前端控制器DispatcheServlet，对浏览器发送的请求进行统一处理。 配置名为SpringMVC的servlet 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd&quot; version=&quot;4.0&quot;&gt; &lt;servlet&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; /所匹配的请求可以是/login或.html或.js或.css方式的请求路径，但是/不能匹配.jsp请求路径的请求 /*会匹配包括.jsp的请求 优化：加载springMVC.xml配置文件，并通过设置&lt;load-on-startup&gt;改变优先级，让在启动服务器时完成初始化，提高第一次请求的响应速度。 1.2.5 创建请求控制器 web.xml中声明了控制路径/，还需要定义控制 servlet 程序，如HelloControler.java，对不同的请求进行不同的处理。 通过@Controller注解的方式将该控制类交给IoC管理，声明为控制器文件。 1234567891011package iceriver.springmvc.controller;import org.springframework.stereotype.Controller;/*** @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w* @date: 2022/7/19 15:31*/@Controllerpublic class HelloController &#123;&#125; 1.2.6 创建springMVC.xml配置文件 1.2.4 中步骤二加载了springMVC.xml配置文件，但此时还没有，springMVC.xml的作用是 spring 框架的一个配置文件。这里需要建在resourses文件夹下。 springMVC 的配置文件具有默认的位置和名称 默认的位置：WEB-INF 默认的名称：-servlet.xml 若要为 springMVC 的配置文件设置自定义的位置和名称，需要在 servlet 标签中添加init-param标签 1.2.5 中使用了注解，因此需要在springMVC.xml文件中进行相应的配置以保证生效。 引入名称空间、开启组件扫描【参见 Spring2.3.2 部分】 这里没设置扫面指定注解 配置thymeleaf视图解析器。 采用了IoC属性注入的级联赋值 这里视图前缀部分规定了前端页面文件要放在WEB-INF/templates目录下 WEB-INF目录下的文件不能被前端请求直接访问，也不能被重定向访问，只能通过请求转发访问。 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 自动扫描包 --&gt; &lt;context:component-scan base-package=&quot;iceriver.springmvc.controller&quot;/&gt; &lt;!-- 配置Thymeleaf视图解析器 --&gt; &lt;bean id=&quot;viewResolver&quot; class=&quot;org.thymeleaf.spring5.view.ThymeleafViewResolver&quot;&gt; &lt;property name=&quot;order&quot; value=&quot;1&quot;/&gt; &lt;property name=&quot;characterEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;property name=&quot;templateEngine&quot;&gt; &lt;bean class=&quot;org.thymeleaf.spring5.SpringTemplateEngine&quot;&gt; &lt;property name=&quot;templateResolver&quot;&gt; &lt;bean class=&quot;org.thymeleaf.spring5.templateresolver.SpringResourceTemplateResolver&quot;&gt; &lt;!-- 视图前缀 --&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/templates/&quot;/&gt; &lt;!-- 视图后缀 --&gt; &lt;property name=&quot;suffix&quot; value=&quot;.html&quot;/&gt; &lt;property name=&quot;templateMode&quot; value=&quot;HTML5&quot;/&gt; &lt;property name=&quot;characterEncoding&quot; value=&quot;UTF-8&quot; /&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; WEB-INF/templates目录下新建index.html文件 IDEA 不配置情况下，新建的 html 文件不带&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;后面的 thymeleaf 命名空间。可以通过设置进行修改 1.2.7 启动工程 配置 tomcat 服务器【参见 JavaWeb4.2】 关于 404 错误： 1.2.8 前端访问指定页面 index.html下新建target.html，并在index.html添加访问链接：&lt;a th:href=&quot;@&#123;/target&#125;&quot;&gt;访问target&lt;/a&gt; 12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; hello &lt;a th:href=&quot;@&#123;/target&#125;&quot;&gt;访问target&lt;/a&gt; &lt;a th:href=&quot;@&#123;/testForward&#125;&quot;&gt;testForward&lt;/a&gt; &lt;a th:href=&quot;@&#123;/testRedirect&#125;&quot;&gt;testRedirect&lt;/a&gt; &lt;/body&gt;&lt;/html&gt; HelloController.java中按照index同样的方式配置targe.html的访问方法。 12345678910111213141516171819202122232425262728package iceriver.springmvc.controller;import org.springframework.stereotype.Controller;import org.springframework.ui.ModelMap;import org.springframework.web.bind.annotation.RequestMapping;@Controllerpublic class HelloController &#123; // 通过@RequestMapping注解，可以通过请求路径匹配要处理的具体的请求 // /表示的当前工程的上下文路径 @RequestMapping(&quot;/&quot;) public String index()&#123; return &quot;index&quot;; &#125; @RequestMapping(&quot;/target&quot;) public String toTarget()&#123; return &quot;target&quot;; &#125; @RequestMapping(&quot;/testForward&quot;) public String testForward()&#123; return &quot;forward:/target&quot;; &#125; @RequestMapping(&quot;/testRedirect&quot;) public String testRedirect()&#123; return &quot;redirect:/target&quot;; &#125;&#125; 1.2.9 总结 浏览器发送请求，若请求地址符合前端控制器的url-pattern（web.xml），该请求就会被前端控制器DispatcherServlet处理。前端控制器会读取SpringMVC的核心配置文件(springmvc.xml)，通过扫描组件找到控制器，将请求地址和控制器中@RequestMapping注解的value属性值进行匹配，若匹配成功，该注解所标识的控制器方法就是处理请求的方法。处理请求的方法需要返回一个字符串类型的视图名称，该视图名称会被视图解析器解析，加上前缀和后缀组成视图的路径，通过Thymeleaf对视图进行渲染，最终请求转发到视图所对应页面。 2. 控制器配置 2.1 @RequestMapping注解 2.1.1 使用 作用：用于请求和处理请求的控制器方法建立映射关系。 一个请求只能映射到一个控制器方法。一个请求映射多个控制器方法会报错。 使用位置：可以用于类（控制器）前，也可以用在方法前： 用在类前：设置请求路径的初始信息 用在方法前：设置请求路径的具体信息 同时使用时，相当于请求路径进行了拼接，要符合类前设置的路径后，再去匹配方法前的路径。 2.1.2 属性 value：配置请求地址 value设置多个值时，表示该请求映射能够匹配多个请求地址所对应的请求 value属性必须设置 不匹配时报 404 错误 1234&lt;a th:href=&quot;@&#123;/testRequestMapping&#125;&quot; &gt;测试@RequestMapping的value属性--&gt;/testRequestMapping&lt;/a&gt;&lt;a th:href=&quot;@&#123;/test&#125;&quot;&gt;测试@RequestMapping的value属性--&gt;/test&lt;/a&gt; 1234@RequestMapping(value = &#123;&quot;/testRequestMapping&quot;, &quot;/test&quot;&#125;)public String testRequestMapping()&#123; return &quot;success&quot;;&#125; mthod：通过请求的请求方式（get 或 post）匹配请求映射 RequestMethod.XXX设置多个时，表示该请求映射能够匹配多种请求方式的请求 处理 get 请求的映射：@GetMapping 处理 post 请求的映射：@PostMapping 处理 put 请求的映射：@PutMapping 处理 delete 请求的映射：@DeleteMapping 请求地址满足请求映射的 value 属性，但不满足 method 属性，浏览器报错 405 错误 目前浏览器只支持 get 和 post，在 form 表单提交时，设置了其他请求方式的字符串（put 或 delete），则按照默认的请求方式 get 处理 要发送 put 和 delete 请求，需要通过 spring 提供的过滤器HiddenHttpMethodFilter，详见 RESTful。 1234&lt;a th:href=&quot;@&#123;/test&#125;&quot;&gt;测试@RequestMapping的value属性--&gt;/test&lt;/a&gt;&lt;form th:action=&quot;@&#123;/test&#125;&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;submit&quot; /&gt;&lt;/form&gt; 1234567@RequestMapping( value = &#123;&quot;/testRequestMapping&quot;, &quot;/test&quot;&#125;, method = &#123;RequestMethod.GET, RequestMethod.POST&#125;)public String testRequestMapping()&#123; return &quot;success&quot;;&#125; params：通过请求的请求参数匹配请求映射。 params可以通过四种表达式设置请求参数和请求映射的匹配关系 &quot;param&quot;：要求请求映射所匹配的请求必须携带 param 请求参数 &quot;!param&quot;：要求请求映射所匹配的请求必须不能携带 param 请求参数 &quot;param=value&quot;：要求请求映射所匹配的请求必须携带 param 请求参数且param=value &quot;param!=value&quot;：要求请求映射所匹配的请求必须携带 param 请求参数但是param!=value 若当前请求满足@RequestMapping 注解的 value 和 method 属性，但是不满足 params 属性，此时页面回报错 400 123&lt;a th:href=&quot;@&#123;/test(username=&#x27;admin&#x27;,password=123456)&quot; &gt;测试@RequestMapping的params属性--&gt;/test&lt;/a&gt; 12345678@RequestMapping( value = &#123;&quot;/testRequestMapping&quot;, &quot;/test&quot;&#125; ,method = &#123;RequestMethod.GET, RequestMethod.POST&#125; ,params = &#123;&quot;username&quot;,&quot;password!=123456&quot;&#125;)public String testRequestMapping()&#123; return &quot;success&quot;;&#125; headers：通过请求的请求头信息匹配请求映射 headers可以通过四种表达式设置请求头信息和请求映射的匹配关系 &quot;header&quot;：要求请求映射所匹配的请求必须携带 header 请求头信息 &quot;!header&quot;：要求请求映射所匹配的请求必须不能携带 header 请求头信息 &quot;header=value&quot;：要求请求映射所匹配的请求必须携带 header 请求头信息且 header=value &quot;header!=value&quot;：要求请求映射所匹配的请求必须携带 header 请求头信息且 header!=value 若当前请求满足@RequestMapping注解的 value 和 method 属性，但是不满足 headers 属性，此时页面显示 404 错误，即资源未找到 2.1.3 ant 风格路径 @RequestMapping修饰的value属性代表的匹配路径可以使用 ant 风格，ant 风格的规则如下： ?：表示任意的单个字符 但不能表示/和?。 *：表示任意的 0 个或多个字符 **：表示任意的一层或多层目录 使用**时只能连用，中间有任何其他字符会当作单个* 2.1.4 路径占位符 SpringMVC支持占位符，常用于RESTful风格中。占位符表示路径中该位置实际上是个参数。 1&lt;a th:href=&quot;@&#123;/testRest/1/admin&#125;&quot;&gt;测试路径中的占位符--&gt;/testRest&lt;/a&gt; 1234567@RequestMapping(&quot;/testRest/&#123;id&#125;/&#123;username&#125;&quot;)public String testRest(@PathVariable(&quot;id&quot;) String id, @PathVariable(&quot;username&quot;) String username)&#123; System.out.println(&quot;id:&quot;+id+&quot;,username:&quot;+username); return &quot;success&quot;;&#125;//最终输出的内容为--&gt;id:1,username:admin 2.2 获取请求参数 2.2.1 通过 ServletAPI 获取 使用HttpServletRequest对象的表单数据获取方法【参见 Javaweb5.7.5】 1234567@RequestMapping(&quot;/testParam&quot;)public String testParam(HttpServletRequest request)&#123; String username = request.getParameter(&quot;username&quot;); String password = request.getParameter(&quot;password&quot;); System.out.println(&quot;username:&quot;+username+&quot;,password:&quot;+password); return &quot;success&quot;;&#125; 2.2.2 通过控制器方法的形参获取 12345@RequestMapping(&quot;/testParam&quot;)public String testParam(String username, String password)&#123; System.out.println(&quot;username:&quot;+username+&quot;,password:&quot;+password); return &quot;success&quot;;&#125; 123&lt;a th:href=&quot;@&#123;/testParam(username=&#x27;admin&#x27;,password=123456)&#125;&quot; &gt;测试获取请求参数--&gt;/testParam&lt;/a&gt; 多个同名参数（如复选框），可以使用字符串接收，此时多个参数值会以,分隔。如hobby:a,b,c 多个同名参数（如复选框），也可以使用字符串数组接收，此时多个参数值会组成数组。 支持get、post、超链接传参。 通过控制方法形参获取请求参数的方式要求形参和实参的名称一模一样。 2.2.3 @RequestParam 通过控制方法形参获取请求参数的方式要求形参和实参的名称一模一样。对于名称不一样的情况，可以使用@RequestParam建立参数映射关系。 @RequestParam的属性： value、name：二者等价，为形参和实参建立映射关系 defaultValue：为实参设置默认值，如果user_name没有传递或传递&quot;&quot;，则使用默认值。 不设置defaultValue，请求未传递参数时值为null required：该参数是否必须，默认为 true，表示必须。 设置为 true 时，同时没有设置defaultValue，请求未传递参数时报40错误——请求未传参。 设置为 false，同时没有设置defaultValue，请求未传递参数时值为null。 @PathVariable：spring 提供的，获取路径中的参数，配合 restful 风格使用，转换成实体类的属性。 不能转化为实体类对象 @RequestBody：spring 提供的，将前台提交的 json 数据转换为实体类对象 只能接收 post 请求，且提交数据为 json 格式 如果提交参数为路径拼接，使用了@RequestBody 获取参数会报 400 @param：mybatis 提供的，用于 Dao 层，确定 sql 语句的参数。 2.2.4 @PathVariable路径占位符 见本章【2.1.4https://www.yuque.com/zhuyuqi/zna9x5/zh889g#wG0bQ】 @PathVariable与@RequestParam的区别： @PathVariable只能用于请求路径上/的参数（RESTful 风格），一般用于get、delete请求 @RequestParam只能用于传统请求路径上?和&amp;风格的请求参数，一般用于post请求 @PathVariable没有defaultValue 2.2.5 @ReqquestHeader 将请求头信息和控制器方法的形参创建映射关系。 属性有三个value、required、defaultValue 用法同@RequestParam 2.2.6 @CookieValue 将 cookie 数据和控制器方法的形参创建映射关系 属性有三个value、required、defaultValue， 用法同@RequestParam 参数类型使用 Cookie 时，可以得到整个 cookie 对象 2.2.7 通过 POJO 获取请求参数 可以在控制器方法形参位置传入一个pojo类对象作为形参，当请求的参数名和pojo的属性名一致，会自动赋值，即创建对象。 123456@RequestMapping(&quot;/testpojo&quot;)public String testPOJO(User user)&#123; System.out.println(user); return &quot;success&quot;;&#125;//最终结果--&gt;User&#123;id=null, username=&#x27;张三&#x27;, password=&#x27;123&#x27;, age=23, sex=&#x27;男&#x27;, email=&#x27;123@qq.com&#x27;&#125; 123456789101112131415&lt;form th:action=&quot;@&#123;/testpojo&#125;&quot; method=&quot;post&quot;&gt; 用户名：&lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt; 密码：&lt;input type=&quot;password&quot; name=&quot;password&quot; /&gt; 性别：&lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;男&quot; /&gt;男&lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;女&quot; /&gt;女 年龄：&lt;input type=&quot;text&quot; name=&quot;age&quot; /&gt; 邮箱：&lt;input type=&quot;text&quot; name=&quot;email&quot; /&gt; &lt;input type=&quot;submit&quot; /&gt;&lt;/form&gt; 2.3 处理请求参数的乱码 GET请求乱码：跟 tomcat 版本有关，【参见 JavaWeb5.7.5】 POST请求乱码：在web.xml文件中，使用过滤器 1234567891011121314151617&lt;!--配置springMVC的编码过滤器--&gt;&lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceResponseEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 3. 域对象共享数据 3.1 向request域对象共享数据 3.1.1 共享request域对象共享数据的原理 SpringMVC 本质上是请求转发，这样从头到尾是一个请求，所以可以通过request域对象共享数据。 3.1.2 向request域对象共享数据的方式 使用ServletAPI向request域对象共享数据： 12345@RequestMapping(&quot;/testServletAPI&quot;)public String testServletAPI(HttpServletRequest request)&#123; request.setAttribute(&quot;testScope&quot;, &quot;hello,servletAPI&quot;); return &quot;success&quot;;&#125; 使用ModelAndView向request域对象共享数据： 1234567@RequestMapping(&quot;/testModelAndView&quot;)public ModelAndView testModelAndView()&#123; ModelAndView mav = new ModelAndView(); mav.addObject(&quot;testScope&quot;, &quot;hello,ModelAndView&quot;); mav.setViewName(&quot;success&quot;); return mav;&#125; 使用 Model 向 request 域对象共享数据 12345@RequestMapping(&quot;/testModel&quot;)public String testModel(Model model)&#123; model.addAttribute(&quot;testScope&quot;, &quot;hello,Model&quot;); return &quot;success&quot;;&#125; 使用 map 向 request 域对象共享数据 12345@RequestMapping(&quot;/testMap&quot;)public String testMap(Map&lt;String, Object&gt; map)&#123; map.put(&quot;testScope&quot;, &quot;hello,Map&quot;); return &quot;success&quot;;&#125; 使用 ModelMap 向 request 域对象共享数据 12345@RequestMapping(&quot;/testModelMap&quot;)public String testModelMap(ModelMap modelMap)&#123; modelMap.addAttribute(&quot;testScope&quot;, &quot;hello,ModelMap&quot;); return &quot;success&quot;;&#125; 五种方式对比： 类 参数类型 方法 返回值 HttpServletRequest HttpServletRequest setAttribute() 字符串页面名 ModelAndView 无参 addObject() ModelAndView的的对象 Model Model addAttribute() 字符串页面名 Map&lt;String, Object&gt; Map&lt;String, Object&gt; put() 字符串页面名 ModelMap ModelMap addAttribute() 字符串页面名 HttpServletRequest是servlet的原生对象。 Model、ModelMap、Map本质运行类型均为BindingAwareModelMap【使用了多态】，本质一样。 Model 转发时将数据放入到请求域中，重定向则是放到路径参数上。？？？？是这样吗， 3.1.3 使用request域对象属性 request域对象设置的属性，thymeleaf调用时可以直接使用属性名调用。 3.2 向session域对象共享数据 使用原生的HttpSession类对象进行设置，方法为setAttribute() 使用属性时，需要使用session.属性名 12345@RequestMapping(&quot;/testSession&quot;)public String testSession(HttpSession session)&#123; session.setAttribute(&quot;testSessionScope&quot;, &quot;hello,session&quot;); return &quot;success&quot;;&#125; 3.3 向application域对象共享数据 3.3.1 获取application域对象 application域对象是ServletContext的实例对象 获取ServletContext实例对象的方式有多种：【参见 Javaweb5.7.3】 通过request对象获取：request.getServletContext() 通过session对象获取：session.getServletContext() 通过init()方法中，通过ServletConfig对象获取：servletConfig.getServletContext()。 通过 jsp 页面的pageContext内置对象 123456@RequestMapping(&quot;/testApplication&quot;)public String testApplication(HttpSession session)&#123; ServletContext application = session.getServletContext(); application.setAttribute(&quot;testApplicationScope&quot;, &quot;hello,application&quot;); return &quot;success&quot;;&#125; 3.3.2 使用方式 设置即原生方式：调用ServletContext类对象的setAttribute()方法进行设置 使用即原生方式：通过application.属性名 3.4 重定向携带数据 3.4.1 RedirectAttributes addAttribute()：添加数据会添加到重定向的路径上，作为请求参数，使用@requestparam 注解获取。 addFlashAttribute()：将数据放到 session 里面，但是只能读取一次。 4. SpringMVC 的视图 4.0 SpringMVC 视图介绍 SpringMVC 中的视图是 View 接口，视图的作用渲染数据，将模型 Model 中的数据展示给用户 SpringMVC 视图的种类很多，默认有转发视图和重定向视图 如果 springmvc 配置文件没有配置视图解析器，应该样实现请求访问呢？ 2022.07.20 测试： 配置了InternalResourceView视图解析（未配置前后缀）、不配置视图解析器，使用 forward、不使用 forward，只要控制器地址写从 web 路径开始的文件全路径，就可以访问到 jsp 页面。 但不能访问 html 页面、配置了前后缀会访问不到 jsp 页面。 4.1 默认视图 4.1.1 转发视图 SpringMVC 中默认的转发视图是InternalResourceView 当控制器方法中所设置的视图名称以forward:为前缀时，此时的视图名称不会被 SpringMVC 配置文件中所配置的视图解析器解析，而是会将前缀forward:去掉，剩余部分作为最终路径通过转发的方式实现跳转 没有写forward:，且没有配置视图解析器时，默认的转发视图也是InternalResourceView 不能直接跳转WEB-INF下的静态资源，必须跳转到控制器中的其他跳转方法。 4.1.2 重定向视图 SpringMVC 中默认的重定向视图是RedirectView 当控制器方法中所设置的视图名称以redirect:为前缀时，此时的视图名称不会被 SpringMVC 配置文件中所配置的视图解析器解析，而是会将前缀redirect:去掉，剩余部分作为最终路径通过重定向的方式实现跳转 不能直接跳转WEB-INF下的静态资源，必须跳转到控制器中的其他跳转方法。 重定向视图和转发视图都会让浏览器地址发生变化，但转发视图最后的地址为请求的地址（forward后的部分），重定向视图最后的地址为转发后处理的地址（redirect给了谁，这个谁处理后的地址才是浏览器的地址） 从前端请求的角度看，重定向的地址跳到了不是他发起请求的一个地址。 4.2 其他视图 4.2.1 ThymeleafView 当控制器方法中所设置的视图名称没有任何前缀时，此时的视图名称会被 SpringMVC 配置文件中所配置的视图解析器解析，视图名称拼接视图前缀和视图后缀所得到的最终路径，会通过转发的方式实现跳转 4.2.2 InternalResourceView SpringMVC 默认的视图解析器，不配置视图解析器，或者配置了视图解析器，但转发以forward:开头，或者配置的视图解析器就是InternalResourceView（如 jsp 页面），则使用该视图的方式（前后缀）进行跳转。 4.3 视图控制器 view-controller 当控制器方法中，仅仅用来实现页面跳转，没有其他处理时，可以在SpringMVC.xml 配置文件中将处理器方法使用&lt;view-controller&gt;标签进行表示，在配置文件中完成跳转。 &lt;mvc:view-controller path=&quot;/testView&quot; view-name=&quot;success&quot;&gt;&lt;/mvc:view-controller&gt; path：设置处理的请求地址 view-name：设置请求地址所对应的视图名称 12&lt;mvc:view-controller path=&quot;/&quot; view-name=&quot;index&quot;/&gt;&lt;!--配置视图控制器--&gt;&lt;!--访问首页--&gt;&lt;mvc:annotation-driven /&gt;&lt;!--开启mvc注解驱动--&gt; 当 SpringMVC 配置文件中设置任何一个&lt;view-controller&gt;时，控制器中的其他请求映射方法将全部失效，此时需要在 SpringMVC 配置文件中开启 mvc 注解驱动： &lt;mvc:annotation-driven /&gt; &lt;mvc:annotation-driven /&gt;功能有很多，这里只是用到了它的一个很小的点。 5. RESTful 风格 5.1 RESTful 简介 REST：Representational State Transfer，表现层资源状态转移。 5.1.1 资源 资源是一种看待服务器的方式，即，将服务器看作是由很多离散的资源组成。每个资源是服务器上一个可命名的抽象概念。因为资源是一个抽象的概念，所以它不仅仅能代表服务器文件系统中的一个文件、数据库中的一张表等等具体的东西，可以将资源设计的要多抽象有多抽象，只要想象力允许而且客户端应用开发者能够理解。与面向对象设计类似，资源是以名词为核心来组织的，首先关注的是名词。一个资源可以由一个或多个 URI 来标识。URI 既是资源的名称，也是资源在 Web 上的地址。对某个资源感兴趣的客户端应用，可以通过资源的 URI 与其进行交互。 5.1.2 资源的表述 资源的表述是一段对于资源在某个特定时刻的状态的描述。可以在客户端-服务器端之间转移（交换）。资源的表述可以有多种格式，例如 HTML/XML/JSON/纯文本/图片/视频/音频等等。资源的表述格式可以通过协商机制来确定。请求-响应方向的表述通常使用不同的格式。 5.1.3 状态转移 状态转移说的是：在客户端和服务器端之间转移（transfer）代表资源状态的表述。通过转移和操作资源的表述，来间接实现操作资源的目的。 5.2 RESTful 的实现 5.2.1 路径与请求的关系 RESTful 中，将 HTTP 协议里面，四个表示操作方式：GET、POST、PUT、DELETE对应成四种基本操作： GET 用来获取资源 POST 用来新建资源 PUT 用来更新资源 DELETE 用来删除资源。 REST 风格提倡 URL 地址使用统一的风格设计，从前到后各个单词使用斜杠分开，不使用问号键值对方式携带请求参数，而是将要发送给服务器的数据作为 URL 地址的一部分，以保证整体风格的一致性。 操作 传统方式 REST 风格 查询操作 getUserById?id=1 user/1–&gt;get 请求方式 保存操作 saveUser user–&gt;post 请求方式 删除操作 deleteUser?id=1 user/1–&gt;delete 请求方式 更新操作 updateUser user–&gt;put 请求方式 5.2.2 HiddenHttpMethodFilter 浏览器只支持发送get和post方式的请求，SpringMVC 提供了HiddenHttpMethodFilter 帮助我们将 POST 请求转换为 DELETE 或 PUT 请求。 HiddenHttpMethodFilter 处理put和delete请求的条件： 当前请求的请求方式必须为post。 当前请求必须传输请求参数_method。 请求参数_method的值才是最终的请求方式。 put和delete请求的实现步骤： 在web.xml中注册HiddenHttpMethodFilter。 注意点：在web.xml中注册时，必须先注册CharacterEncodingFilter，再注册HiddenHttpMethodFilter，因为CharacterEncodingFilter内部有一个获取请求方式的操作，如果不提前设置好编码，会出现乱码情况： String paramValue = request.getParameter(this.methodParam); 5.3 模拟增删改查 功能 URL 地址 请求方式 访问首页 / GET 查询全部数据 /employee GET 删除 /employee/2 DELETE 跳转到添加数据页面 /toAdd GET 执行保存 /employee POST 跳转到更新数据页面 /employee/2 GET 执行更新 /employee PUT 5.3.1 查询功能 1&lt;a th:href=&quot;@&#123;/employee&#125;&quot;&gt;访问员工信息&lt;/a&gt; 123456@RequestMapping(value = &quot;/employee&quot;, method = RequestMethod.GET)public String getEmployeeList(Model model)&#123; Collection&lt;Employee&gt; employeeList = employeeDao.getAll(); model.addAttribute(&quot;employeeList&quot;, employeeList); return &quot;employee_list&quot;;&#125; 123456789101112131415161718192021222324252627282930313233&lt;table border=&quot;1&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;text-align: center;&quot; id=&quot;dataTable&quot;&gt; &lt;tr&gt; &lt;th colspan=&quot;5&quot;&gt;Employee Info&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;id&lt;/th&gt; &lt;th&gt;lastName&lt;/th&gt; &lt;th&gt;email&lt;/th&gt; &lt;th&gt;gender&lt;/th&gt; &lt;th&gt;options(&lt;a th:href=&quot;@&#123;/toAdd&#125;&quot;&gt;add&lt;/a&gt;)&lt;/th&gt; &lt;/tr&gt; &lt;tr th:each=&quot;employee : $&#123;employeeList&#125;&quot;&gt; &lt;td th:text=&quot;$&#123;employee.id&#125;&quot;&gt;&lt;/td&gt; &lt;td th:text=&quot;$&#123;employee.lastName&#125;&quot;&gt;&lt;/td&gt; &lt;td th:text=&quot;$&#123;employee.email&#125;&quot;&gt;&lt;/td&gt; &lt;td th:text=&quot;$&#123;employee.gender&#125;&quot;&gt;&lt;/td&gt; &lt;td&gt; &lt;a class=&quot;deleteA&quot; @click=&quot;deleteEmployee&quot; th:href=&quot;@&#123;&#x27;/employee/&#x27;+$&#123;employee.id&#125;&#125;&quot; &gt;delete&lt;/a &gt; &lt;a th:href=&quot;@&#123;&#x27;/employee/&#x27;+$&#123;employee.id&#125;&#125;&quot;&gt;update&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; 5.3.2 删除功能 123456789101112131415161718192021222324252627282930&lt;a class=&quot;deleteA&quot; @click=&quot;deleteEmployee&quot; th:href=&quot;@&#123;&#x27;/employee/&#x27;+$&#123;employee.id&#125;&#125;&quot; &gt;delete&lt;/a&gt;&lt;!-- 作用：通过超链接控制表单的提交，将post请求转换为delete请求 --&gt;&lt;form id=&quot;delete_form&quot; method=&quot;post&quot;&gt; &lt;!-- HiddenHttpMethodFilter要求：必须传输_method请求参数，并且值为最终的请求方式 --&gt; &lt;input type=&quot;hidden&quot; name=&quot;_method&quot; value=&quot;delete&quot; /&gt;&lt;/form&gt;&lt;script type=&quot;text/javascript&quot;&gt; var vue = new Vue(&#123; el: &quot;#dataTable&quot;, methods: &#123; //event表示当前事件 deleteEmployee: function (event) &#123; //通过id获取表单标签 var delete_form = document.getElementById(&quot;delete_form&quot;); //将触发事件的超链接的href属性为表单的action属性赋值 delete_form.action = event.target.href; //提交表单 delete_form.submit(); //阻止超链接的默认跳转行为 event.preventDefault(); &#125;, &#125;, &#125;);&lt;/script&gt; 这里用到了vue.js，由于vue.js不经过中央控制器处理，所以需要开启静态资源（本地 js、css、img）加载： springMVC的前端控制器不能处理静态资源 Servlet的默认前端控制器defaultServlet可以处理静态资源，这个defaultServlet定义在 tomcat 的web.xml——全局配置 在本地配置——项目的web.xml中使用&lt;mvc:default-servlet-handler/&gt;进行开启 静态资源处理顺序：springmvc→defaultservlet→404 12345@RequestMapping(value = &quot;/employee/&#123;id&#125;&quot;, method = RequestMethod.DELETE)public String deleteEmployee(@PathVariable(&quot;id&quot;) Integer id)&#123; employeeDao.delete(id); return &quot;redirect:/employee&quot;;&#125; 5.3.3 插入功能 12&lt;a th:href=&quot;@&#123;/toAdd&#125;&quot;&gt;add&lt;/a&gt;&lt;!-- 连接通过视图控制器进行转发--&gt; 1&lt;mvc:view-controller path=&quot;/toAdd&quot; view-name=&quot;employee_add&quot;/&gt; 1234567&lt;form th:action=&quot;@&#123;/employee&#125;&quot; method=&quot;post&quot;&gt; lastName:&lt;input type=&quot;text&quot; name=&quot;lastName&quot; /&gt;&lt;br /&gt; email:&lt;input type=&quot;text&quot; name=&quot;email&quot; /&gt;&lt;br /&gt; gender:&lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;1&quot; /&gt;male &lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;0&quot; /&gt;female&lt;br /&gt; &lt;input type=&quot;submit&quot; value=&quot;add&quot; /&gt;&lt;br /&gt;&lt;/form&gt; 12345@RequestMapping(value = &quot;/employee&quot;, method = RequestMethod.POST)public String addEmployee(Employee employee)&#123; employeeDao.save(employee); return &quot;redirect:/employee&quot;;&#125; 5.3.4 修改功能 1&lt;a th:href=&quot;@&#123;&#x27;/employee/&#x27; + $&#123;employee.id&#125;&#125;&quot;&gt;update&lt;/a&gt; 123456@RequestMapping(value = &quot;/employee/&#123;id&#125;&quot;, method = RequestMethod.GET)public String getEmployeeById(@PathVariable(&quot;id&quot;) Integer id, Model model)&#123; Employee employee = employeeDao.get(id); model.addAttribute(&quot;employee&quot;, employee); return &quot;employee_update&quot;;&#125; 1234567891011121314151617181920212223242526272829&lt;form th:action=&quot;@&#123;/employee&#125;&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;_method&quot; value=&quot;put&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;id&quot; th:value=&quot;$&#123;employee.id&#125;&quot; /&gt; &lt;label for=&quot;&quot;&gt; lastName：&lt;input type=&quot;text&quot; name=&quot;lastName&quot; th:value=&quot;$&#123;employee.lastName&#125;&quot; /&gt; &lt;/label&gt; &lt;label for=&quot;&quot;&gt; email：&lt;input type=&quot;text&quot; name=&quot;email&quot; th:value=&quot;$&#123;employee.email&#125;&quot; /&gt; &lt;/label&gt; &lt;label for=&quot;&quot;&gt; gender：&lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;1&quot; th:field=&quot;$&#123;employee.gender&#125;&quot; /&gt;male &lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;0&quot; th:field=&quot;$&#123;employee.gender&#125;&quot; /&gt;female &lt;/label&gt; &lt;input type=&quot;submit&quot; value=&quot;修改&quot; /&gt;&lt;/form&gt; 12345@RequestMapping(value = &quot;/employee&quot;, method = RequestMethod.PUT)public String updateEmployee(Employee employee)&#123; employeeDao.save(employee); return &quot;redirect:/employee&quot;;&#125; 6. HttpMessageConverter HttpMessageConverter，报文信息转换器，将请求报文转换为 Java 对象，或将 Java 对象转换为响应报文HttpMessageConverter提供了两个注解和两个类型： @RequestBody，@ResponseBody， RequestEntity，ResponseEntity 6.1 @RequestBody 标识控制器形参。 @RequestBody可以获取请求体，需要在控制器方法设置一个形参，使用@RequestBody进行标识，当前请求的请求体就会为当前注解所标识的形参赋值。 前端发送的数据格式需为 json 格式字符串。json 格式对象会报错，需要使用@@RequestParam 12345&lt;form th:action=&quot;@&#123;/testRequestBody&#125;&quot; method=&quot;post&quot;&gt; 用户名：&lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt;&lt;br /&gt; 密码：&lt;input type=&quot;password&quot; name=&quot;password&quot; /&gt;&lt;br /&gt; &lt;input type=&quot;submit&quot; /&gt;&lt;/form&gt; 12345@RequestMapping(&quot;/testRequestBody&quot;)public String testRequestBody(@RequestBody String requestBody)&#123; System.out.println(&quot;requestBody:&quot;+requestBody); return &quot;success&quot;;&#125; 6.2 RequestEntity 标识控制器形参类型。 RequestEntity封装请求报文的一种类型，需要在控制器方法的形参中设置该类型的形参，当前请求的请求报文就会赋值给该形参，可以通过getHeaders()获取请求头信息，通过getBody()获取请求体信息 12345&lt;form th:action=&quot;@&#123;/testRequestEntity&#125;&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt; &lt;input type=&quot;text&quot; name=&quot;password&quot; /&gt; &lt;input type=&quot;submit&quot; value=&quot;测试RequestEntity&quot; /&gt;&lt;/form&gt; 123456@RequestMapping(&quot;/testRequestEntity&quot;)public String testRequestEntity(RequestEntity&lt;String&gt; requestEntity)&#123; System.out.println(&quot;requestHeader:&quot;+requestEntity.getHeaders()); System.out.println(&quot;requestBody:&quot;+requestEntity.getBody()); return &quot;success&quot;;&#125; 6.3 @ResponseBody 标识控制器方法。 6.3.1 使用原生servletAPI的HttpServletResponse响应数据 1&lt;a th:href=&quot;@&#123;/testResponse&#125;&quot;&gt;通过servletAPI的response对象响应浏览器数据&lt;/a&gt; 1234@RequestMapping(&quot;/testResponse&quot;)public void testResponse(HttpServletResponse response) throws IOException &#123; response.getWriter().print(&quot;hello,response&quot;);&#125; 6.3.2 使用@ResponseBody返回文本信息 1&lt;a th:href=&quot;@&#123;/testResponse&#125;&quot;&gt;通过servletAPI的response对象响应浏览器数据&lt;/a&gt; 12345@RequestMapping(value = &quot;/testResponseBody&quot;, produces = &quot;text/html;charset=UTF-8&quot;)@ResponseBodypublic String testResponseBody()&#123; return &quot;成功&quot;;&#125; 这里返回值不再是视图名称，而是响应体。 6.3.3 使用@ResponseBody返回实体对象（springmvc 处理 json） 浏览器无法直接解析 java 语言的实体对象，可以通过转换为 json 格式的字符串进行输出。 12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt;&lt;/dependency&gt; 1&lt;a th:href=&quot;@&#123;/testResponseUser&#125;&quot;&gt;通过@ResponseBody响应浏览器User对象&lt;/a&gt; 12345@RequestMapping(value = &quot;/testResponseUser&quot;, produces = &quot;text/html;charset=UTF-8&quot;)@ResponseBodypublic User testResponseUser()&#123; return new User(1001, &quot;admin&quot;, &quot;123456&quot;, 23, &quot;男&quot;);&#125; springmvc处理json的必要条件： pom.xml添加了依赖 srpingMVC.xml开启了注解驱动 &lt;mvc:annotation-driven /&gt; 处理器方法使用@ResponseBody注解 处理器方法返回值类型为实体类，返回值为实体类对象 6.3.4 使用@ResponseBody与 springmvc 处理 ajax 12345678910111213141516171819202122232425&lt;div id=&quot;app&quot;&gt; &lt;a @click=&quot;testAxios&quot; th:href=&quot;@&#123;/testAxios&#125;&quot;&gt;SpringMVC处理ajax&lt;/a&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot; th:src=&quot;@&#123;/static/js/vue.js&#125;&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; th:src=&quot;@&#123;/static/js/axios.min.js&#125;&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; new Vue(&#123; el: &quot;#app&quot;, methods: &#123; testAxios: function (event) &#123; axios(&#123; method: &quot;post&quot;, url: event.target.href, params: &#123; username: &quot;admin&quot;, password: &quot;123456&quot;, &#125;, &#125;).then(function (response) &#123; alert(response.data); &#125;); event.preventDefault(); &#125;, &#125;, &#125;);&lt;/script&gt; 123456@RequestMapping(&quot;/testAxios&quot;)@ResponseBodypublic String testAxios(String username, String password)&#123; System.out.println(username+&quot;,&quot;+password); return &quot;hello,axios&quot;;&#125; 6.4 ResponseEntity 6.4.1 作用 标识控制器方法的返回值类型。 ResponseEntity用于控制器方法的返回值类型，该控制器方法的返回值就是响应到浏览器的响应报文 6.4.2 应用：文件下载 1&lt;a th:href=&quot;@&#123;testDown&#125;&quot;&gt;下载1.jpg&lt;/a&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package iceriver.springmvc.controller;import org.springframework.http.HttpHeaders;import org.springframework.http.HttpStatus;import org.springframework.http.ResponseEntity;import org.springframework.stereotype.Controller;import org.springframework.util.MultiValueMap;import org.springframework.web.bind.annotation.RequestMapping;import javax.servlet.ServletContext;import javax.servlet.http.HttpSession;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;/*** @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w* @date: 2022/7/28 15:05*/@Controllerpublic class FileUpAndFileDownload &#123; @RequestMapping(&quot;/testDown&quot;) public ResponseEntity&lt;byte[]&gt; testResponseEntity(HttpSession session) throws IOException &#123; //获取ServletContext对象 ServletContext servletContext = session.getServletContext(); //获取服务器中文件的真实路径 String realPath = servletContext.getRealPath(&quot;/static/img/1.jpg&quot;); //创建输入流 InputStream is = new FileInputStream(realPath); //创建字节数组 byte[] bytes = new byte[is.available()]; //将流读到字节数组中 is.read(bytes); //创建HttpHeaders对象设置响应头信息 MultiValueMap&lt;String, String&gt; headers = new HttpHeaders(); //设置要下载方式以及下载文件的名字 headers.add(&quot;Content-Disposition&quot;, &quot;attachment;filename=1.jpg&quot;); //设置响应状态码 HttpStatus statusCode = HttpStatus.OK; //创建ResponseEntity对象 ResponseEntity&lt;byte[]&gt; responseEntity = new ResponseEntity&lt;&gt;(bytes, headers, statusCode); //关闭输入流 is.close(); return responseEntity; &#125;&#125; 6.4.3 【补充】文件上传 pom.xml文件中，添加依赖： 123456&lt;!-- https://mvnrepository.com/artifact/commons-fileupload/commons-fileupload --&gt;&lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; springMVC.xml文件中添加配置——文件上传解析器： 12&lt;!--必须通过文件解析器的解析才能将文件转换为MultipartFile对象--&gt;&lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;/&gt; 实现上传： 1234&lt;form th:action=&quot;@&#123;/testUp&#125;&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 头像：&lt;input type=&quot;file&quot; name=&quot;photo&quot; /&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot; /&gt;&lt;/form&gt; 123456789101112131415161718@RequestMapping(&quot;/testUp&quot;)public String testUp(MultipartFile photo, HttpSession session) throws IOException &#123; //获取上传的文件的文件名 String fileName = photo.getOriginalFilename(); //通过ServletContext获取服务器中photo目录的路径 ServletContext servletContext = session.getServletContext(); String photoPath = servletContext.getRealPath(&quot;photo&quot;); File file = new File(photoPath); //判断photoPath所对应路径是否存在 if(!file.exists())&#123; //若不存在，则创建目录 file.mkdir(); &#125; String finalPath = photoPath + File.separator + fileName; //上传文件 photo.transferTo(new File(finalPath)); return &quot;success&quot;;&#125; 解决同名文件上传后“文件内容”发生覆盖的情况： 使用UUID 123456789101112131415161718192021222324@RequestMapping(&quot;/testUp&quot;)public String testUp(MultipartFile photo, HttpSession session) throws IOException &#123; //获取上传的文件的文件名 String fileName = photo.getOriginalFilename(); //获取上传的文件的后缀名 String suffixName = fileName.substring(fileName.lastIndexOf(&quot;.&quot;)); //将UUID作为文件名 String uuid = UUID.randomUUID().toString().replaceAll(&quot;-&quot;,&quot;&quot;); //将uuid和后缀名拼接后的结果作为最终的文件名 fileName = uuid + suffixName; //通过ServletContext获取服务器中photo目录的路径 ServletContext servletContext = session.getServletContext(); String photoPath = servletContext.getRealPath(&quot;photo&quot;); File file = new File(photoPath); //判断photoPath所对应路径是否存在 if(!file.exists())&#123; //若不存在，则创建目录 file.mkdir(); &#125; String finalPath = photoPath + File.separator + fileName; //上传文件 photo.transferTo(new File(finalPath)); return &quot;success&quot;;&#125; 6.5 @RestController注解 @RestController注解是springMVC提供的一个复合注解，标识在控制器的类上，就相当于为类添加了@Controller注解，并且为其中的每个方法添加了@ResponseBody注解。 7. 拦截器（Interceptor） 7.1 拦截器的介绍 过滤器作用于请求发送到 dispatcherservlet 之前 拦截器作用于请求由 dispatcherservlet 发送到其他 controllersevlet 拦截器共有 3 个抽象方法【默认方法】： preHandle()：控制器方法执行之前执行，其boolean类型的返回值表示是否拦截或放行，返回true为放行，即调用控制器方法；返回false表示拦截，即不调用控制器方法 postHandle()：控制器方法执行之后执行 afterComplation()：处理完视图和模型数据，渲染视图完毕之后执行 7.2 创建及配置拦截器 注册拦截器：在springMVC.xml中配置拦截器： 拦截所有请求： 方式一： 1234&lt;mvc:interceptor&gt; &lt;bean class=&quot;com.atguigu.interceptor.FirstInterceptor&quot;&gt;&lt;/bean&gt; &lt;ref bean=&quot;firstInterceptor&quot;&gt;&lt;/ref&gt;&lt;/mvc:interceptor&gt; - ~~方式二（外部bean方式）：~~ 1234&lt;bean class=&quot;com.atguigu.interceptor.FirstInterceptor&quot;&gt;&lt;/bean&gt;&lt;mvc:interceptor&gt; &lt;ref bean=&quot;firstInterceptor&quot;&gt;&lt;/ref&gt;&lt;/mvc:interceptor&gt; - ~~方式三（注解方式）：该方式需要对拦截器类~~`~~FirstInterceptor.java~~`~~使用~~`~~@Component~~`~~注解，将拦截器交给spring作为bean管理~~ 1234&lt;mvc:interceptor&gt; &lt;ref bean=&quot;firstInterceptor&quot;&gt;&lt;/ref&gt;&lt;/mvc:interceptor&gt; 拦截/放行指定请求： 1234567891011121314&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;!--处理的所有的请求进行拦截 --&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;mvc:exclude-mapping path=&quot;/testRequestEntity&quot;/&gt; &lt;ref bean=&quot;firstInterceptor&quot;&gt;&lt;/ref&gt; &lt;bean class=&quot;com.atguigu.interceptor.FirstInterceptor&quot;&gt;&lt;/bean&gt; &lt;/mvc:interceptor&gt;&lt;mvc:interceptors&gt;&lt;!-- 1. ref或bean标签设置拦截器，ref为外部bean方式 2. mvc:mapping设置需要拦截的请求（请求经过拦截器，拦截器中的方法会被执行），路径为ant风格 3. mvc:exclude-mapping设置需要放行的请求（请求不会经过拦截器，拦截器中的方法不会被执行）--&gt; 使用拦截器：编写Interceptor拦截器类，继承于HandlerInterceptor，使用ctr+o进行 preHandle()方法重写，令两个方法可以不用。 将拦截器交给 spring 管理： 方式一：使用@Component注解，将拦截器交给 spring 作为 spring 管理 方式二：在 springMVC.xml 中使用标签把拦截器交给 spring 管理 缺少设置白名单、设置管理配置类的内容，还可以再丰富 7.3 多个拦截器执行顺序 若每个拦截器的preHandle()都返回true preHandle()会按照配置的顺序执行，而postHandle()和afterComplation()会按照配置的反序执行 若某个拦截器的preHandle()返回了false preHandle()返回false和它之前的拦截器的preHandle()都会执行。 postHandle()都不执行。 返回false的拦截器之前的拦截器的afterComplation()会执行 8. 异常处理器 8.1 基于 xml 的异常处理 SpringMVC提供了一个处理控制器方法执行过程中所出现的异常的接口：HandlerExceptionResolver HandlerExceptionResolver接口的实现类有：DefaultHandlerExceptionResolver和SimpleMappingExceptionResolver SpringMVC提供了自定义的异常处理器SimpleMappingExceptionResolver 在springMVC.xml文件配置异常处理： 123456789101112&lt;bean class=&quot;org.springframework.web.servlet.handler.SimpleMappingExceptionResolver&quot;&gt; &lt;property name=&quot;exceptionMappings&quot;&gt; &lt;props&gt; &lt;!-- key：表示异常类型 error：发生异常要跳转到的页面， --&gt; &lt;prop key=&quot;java.lang.ArithmeticException&quot;&gt;error&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;!--保存异常信息，默认在请求域中，此处保存的异常信息key为ex --&gt; &lt;property name=&quot;exceptionAttribute&quot; value=&quot;ex&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 8.2 基于注解的异常处理 创建配置类，使用@ControllerAdive注解标记为异常处理类 123456789@ControllerAdvice//将当前类标识为异常处理的组件public class ExceptionController &#123; @ExceptionHandler(ArithmeticException.class)//设置所标识方法处理的异常，参数为异常类型.class //ex表示当前请求处理中出现的异常对象 public String handleArithmeticException(Exception ex, Model model)&#123; model.addAttribute(&quot;ex&quot;, ex); return &quot;error&quot;; &#125;&#125; 9. 注解配置 Spring MVC 9.1 创建初始化类，代替web.xml 在Servlet3.0环境中，容器会在类路径中查找实现javax.servlet.ServletContainerInitializer接口的类，如果找到的话就用它来配置Servlet容器。 Spring提供了这个接口的实现，名为SpringServletContainerInitializer，这个类反过来又会查找实现WebApplicationInitializer的类并将配置的任务交给它们来完成。 Spring3.2引入了一个便利的WebApplicationInitializer基础实现，名为AbstractAnnotationConfigDispatcherServletInitializer，当我们的类扩展了AbstractAnnotationConfigDispatcherServletInitializer并将其部署到Servlet3.0容器的时候，容器会自动发现它，并用它来配置Servlet上下文。 123456789101112131415161718192021222324252627282930313233343536373839404142public class WebInit extends AbstractAnnotationConfigDispatcherServletInitializer &#123; /** * 指定spring的配置类 * @return */ @Override protected Class&lt;?&gt;[] getRootConfigClasses() &#123; return new Class[]&#123;SpringConfig.class&#125;; &#125; /** * 指定SpringMVC的配置类 * @return */ @Override protected Class&lt;?&gt;[] getServletConfigClasses() &#123; return new Class[]&#123;SpringMVC.class&#125;; &#125; /** * 指定DispatcherServlet的映射规则，即url-pattern * @return */ @Override protected String[] getServletMappings() &#123; return new String[]&#123;&quot;/&quot;&#125;; &#125; /** * 添加过滤器 * @return */ @Override protected Filter[] getServletFilters() &#123; CharacterEncodingFilter encodingFilter = new CharacterEncodingFilter(); encodingFilter.setEncoding(&quot;UTF-8&quot;); encodingFilter.setForceRequestEncoding(true); HiddenHttpMethodFilter hiddenHttpMethodFilter = new HiddenHttpMethodFilter(); return new Filter[]&#123;encodingFilter, hiddenHttpMethodFilter&#125;; &#125;&#125; 9.3 创建 springmvc 配置类，代替 springmvc 配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Configuration@ComponentScan(&quot;com.atguigu.mvc.controller&quot;)//1. 扫描组件@EnableWebMvc//5. 开启MVC注解驱动public class SpringMVC implements WebMvcConfigurer &#123; //4. 使用默认的servlet处理静态资源 @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123; configurer.enable(); &#125; //6. 配置文件上传解析器 @Bean public MultipartResolver multipartResolver()&#123; return new CommonsMultipartResolver(); &#125; //8. 配置拦截器 @Override public void addInterceptors(InterceptorRegistry registry) &#123; FirstInterceptor firstInterceptor = new FirstInterceptor(); registry.addInterceptor(firstInterceptor).addPathPatterns(&quot;/**&quot;); &#125; //3. 配置视图控制器 @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(&quot;/success&quot;).setViewName(&quot;success&quot;); &#125; //7. 配置异常映射 @Override public void configureHandlerExceptionResolvers(List&lt;HandlerExceptionResolver&gt; resolvers) &#123; SimpleMappingExceptionResolver exceptionResolver = new SimpleMappingExceptionResolver(); Properties prop = new Properties(); prop.setProperty(&quot;java.lang.ArithmeticException&quot;, &quot;error&quot;); //设置异常映射 exceptionResolver.setExceptionMappings(prop); //设置共享异常信息的键 exceptionResolver.setExceptionAttribute(&quot;ex&quot;); resolvers.add(exceptionResolver); &#125; //2. 视图解析器 //配置生成模板解析器 @Bean public ITemplateResolver templateResolver() &#123; WebApplicationContext webApplicationContext = ContextLoader.getCurrentWebApplicationContext(); ServletContextTemplateResolver templateResolver = new ServletContextTemplateResolver(webApplicationContext.getServletContext()); templateResolver.setPrefix(&quot;/WEB-INF/templates/&quot;); templateResolver.setSuffix(&quot;.html&quot;); templateResolver.setCharacterEncoding(&quot;UTF-8&quot;); templateResolver.setTemplateMode(TemplateMode.HTML); return templateResolver; &#125; //生成模板引擎并为模板引擎注入模板解析器 @Bean public SpringTemplateEngine templateEngine(ITemplateResolver templateResolver) &#123; SpringTemplateEngine templateEngine = new SpringTemplateEngine(); templateEngine.setTemplateResolver(templateResolver); return templateEngine; &#125; //生成视图解析器并未解析器注入模板引擎 @Bean public ViewResolver viewResolver(SpringTemplateEngine templateEngine) &#123; ThymeleafViewResolver viewResolver = new ThymeleafViewResolver(); viewResolver.setCharacterEncoding(&quot;UTF-8&quot;); viewResolver.setTemplateEngine(templateEngine); return viewResolver; &#125;&#125; 10. SpringMVC 执行流程 10.1 SpringMVC 常用组件 DispatcherServlet：前端控制器，不需要工程师开发，由框架提供 作用：统一处理请求和响应，整个流程控制的中心，由它调用其它组件处理用户的请求 HandlerMapping：处理器映射器，不需要工程师开发，由框架提供 作用：根据请求的url、method等信息查找Handler，即控制器方法 HandlerAdapter：处理器适配器，不需要工程师开发，由框架提供 作用：通过HandlerAdapter对处理器（控制器方法）进行执行 Handler：处理器，需要工程师开发【即Controller】 作用：在DispatcherServlet的控制下Handler对具体的用户请求进行处理 ViewResolver：视图解析器，不需要工程师开发，由框架提供 作用：进行视图解析，得到相应的视图，例如：ThymeleafView、InternalResourceView、RedirectView View：视图 作用：将模型数据通过页面展示给用户 10.2 DispatcherServlet 初始化过程 DispatcherServlet 本质上是一个 Servlet，所以天然的遵循 Servlet 的生命周期，宏观上是 Servlet生命周期来进行调度。即，创建 web 服务时，从 Servlet 类的 init()方法开始执行。 从上图可见，Servlet类中init()执行到最后是在FrameworkServlet类中调用了initWebApplicationContext()方法，而在该方法中，调用了createWebApplicationContext()方法 createWebApplicationContext()方法使用反射创建了 IOC 容器。 initWebApplicationContext()方法将 IOC 容器在应用域中共享。 同时，在FrameworkServlet类的初始化initWebApplicationContext()方法中，调用了onRefresh(wac)方法，此方法在DispatcherServlet中进行了重写，调用了initStrategies(context)方法，初始化策略，即初始化DispatcherServlet的各个组件： 1234567891011protected void initStrategies(ApplicationContext context) &#123; initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context);&#125; 10.3 DispatcherServlet 调用组件处理请求过程 web 容器在处理前端请求时，请求由Servlet类中的service()方法从上到下处理，而Servlet中的service()被HttpServlet类重写了。 后FrameworkServlet重写了HttpServlet中的service()和doXxx()（doGet()等），这些方法中调用了FrameworkServlet类中的processRequest(request, response)。 processRequest(request, response)调用了FrameworkServlet中的doService()，而doService()在FrameworkServlet是一个抽象方法，在DispatcherServlet类中进行了重写。 DispatcherServlet中的doService()调用了DispatcherServlet中的doDispatch()，即该方法处理了最后的请求。请求处理后的视图处理等后续处理交给processDispatchResult()完成。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // Determine handler for the current request. /* mappedHandler：调用链 包含handler、interceptorList、interceptorIndex handler：浏览器发送的请求所匹配的控制器方法 interceptorList：处理控制器方法的所有拦截器集合 interceptorIndex：拦截器索引，控制拦截器afterCompletion()的执行 */ mappedHandler = getHandler(processedRequest); if (mappedHandler == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // Determine handler adapter for the current request. // 通过控制器方法创建相应的处理器适配器，调用所对应的控制器方法 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = &quot;GET&quot;.equals(method); if (isGet || &quot;HEAD&quot;.equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; // 调用拦截器的preHandle() if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // Actually invoke the handler. // 由处理器适配器调用具体的控制器方法，最终获得ModelAndView对象 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; applyDefaultViewName(processedRequest, mv); // 调用拦截器的postHandle() mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; catch (Throwable err) &#123; // As of 4.3, we&#x27;re processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(&quot;Handler dispatch failed&quot;, err); &#125; // 后续处理：处理模型数据和渲染视图 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125; catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(&quot;Handler processing failed&quot;, err)); &#125; finally &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125; else &#123; // Clean up any resources used by a multipart request. if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243private void processDispatchResult(HttpServletRequest request, HttpServletResponse response, @Nullable HandlerExecutionChain mappedHandler, @Nullable ModelAndView mv, @Nullable Exception exception) throws Exception &#123; boolean errorView = false; if (exception != null) &#123; if (exception instanceof ModelAndViewDefiningException) &#123; logger.debug(&quot;ModelAndViewDefiningException encountered&quot;, exception); mv = ((ModelAndViewDefiningException) exception).getModelAndView(); &#125; else &#123; Object handler = (mappedHandler != null ? mappedHandler.getHandler() : null); mv = processHandlerException(request, response, handler, exception); errorView = (mv != null); &#125; &#125; // Did the handler return a view to render? if (mv != null &amp;&amp; !mv.wasCleared()) &#123; // 处理模型数据和渲染视图 render(mv, request, response); if (errorView) &#123; WebUtils.clearErrorRequestAttributes(request); &#125; &#125; else &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;No view rendering, null ModelAndView returned.&quot;); &#125; &#125; if (WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Concurrent handling started during a forward return; &#125; if (mappedHandler != null) &#123; // Exception (if any) is already handled.. // 调用拦截器的afterCompletion() mappedHandler.triggerAfterCompletion(request, response, null); &#125;&#125; 10.4 SpringMVC 的执行流程 用户向服务器发送请求，经过过滤器后请求被SpringMVC的前端控制器 DispatcherServlet捕获。 DispatcherServlet对请求URL进行解析，得到请求资源标识符（URI），通过handlermapping判断请求URI对应的映射： 不存在 再判断是否配置了mvc:default-servlet-handler——默认的前端控制器，处理静态资源 如果没配置，则控制台报映射查找不到，客户端展示 404 错误 如果有配置，则访问目标资源（一般为静态资源，如：JS,CSS,HTML），找不到客户端也会展示 404 错误 存在：执行下面流程 根据该URI，调用HandlerMapping获得该Handler配置的所有相关的对象（包括Handler对象以及Handler对象对应的拦截器），最后以HandlerExecutionChain执行链对象的形式返回。 DispatcherServlet 根据获得的Handler，选择一个合适的HandlerAdapter。 如果成功获得HandlerAdapter，此时将开始执行拦截器的preHandler(…)方法【正向】 提取Request中的模型数据，填充Handler（控制器方法）入参，开始执行Handler（Controller)方法，处理请求。在填充Handler的入参过程中，根据你的配置，Spring将帮你做一些额外的工作： HttpMessageConveter： 将请求消息（如Json、xml等数据）转换成一个对象，将对象转换为指定的响应信息 数据转换：对请求消息进行数据转换。如String转换成Integer、Double等 数据格式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等 数据验证： 验证数据的有效性（长度、格式等），验证结果存储到 BindingResult 或 Error 中 Handler执行完成后，向DispatcherServlet 返回一个ModelAndView对象。 此时将开始执行拦截器的postHandle(...)方法【逆向】。 根据返回的ModelAndView（此时会判断是否存在异常：如果存在异常，则执行HandlerExceptionResolver进行异常处理）选择一个适合的ViewResolver进行视图解析，根据Model和View，来渲染视图。 渲染视图完毕执行拦截器的afterCompletion(…)方法【逆向】。 将渲染结果返回给客户端。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"https://sk370.github.io/tags/Spring-MVC/"}]},{"title":"JVM","date":"2022-07-17T23:51:00.000Z","path":"2022/07/18/jvm/JVM/","text":"Java Virtual Machine，Java 虚拟机的简称，是 Java 技术的核心。是专门为执行单个计算机程序而设计。本文详细描述了Java虚拟机的构成、发展以及使用调优。并对Java类在虚拟机的加载过程、保存、存放形式进行研究。 第 1 章 JVM 和 Java 体系结构 参考书 Java 虚拟机规范（Java SE 8 版） 深入理解 Java 虚拟机（JVM 高级特性与最佳实践）【第 2/3 版】 深入理解 JVM&amp;G1 GC 揭秘 Java 虚拟机（JVM 设计原理与实践） Java 虚拟机基础教程 实战 Java 虚拟机（JVM 故障诊断与性能优化） Java 虚拟机精讲 码出高效（Java 开发手册） 自己动手写 Java 虚拟机 1.1 JVM 介绍 1.1.1 认识 JVM JVM：Java Virtual Machine，Java 虚拟机的简称，是 Java 技术的核心。 虚拟机： 系统虚拟机：如 Visual Box，VMware，提供了一个可运行完整操作系统的软件平台 程序虚拟机：如 JVM，专门为执行单个计算机程序而设计。 作用：二进制字节码的运行环境。 特点： 一次编译，到处运行 自动内存管理 自动垃圾回收 JVM 拥有语言无关性，并未与 Java 语言绑定，只要该编程语言转换为字节码文件遵循了 Java 虚拟机的规范，它就能被 Java 虚拟机所识别并装在运行。 任何能够再 Java 虚拟机上执行的字节码格式均一样，统称为 jvm 字节码。 JVM 是遵循 JSR-292 规范的，该规范的一系列项目推动 JVM 从“Java 语言的虚拟机”向“多语言虚拟机”方向发展。 1.1.2 Java 和 JVM 的重大发展历程 2000 年，SUN 发布了 JDK1.3，同时发布了 Java HotSpot Virtual Machine，成为 Java 的默认虚拟机。 2006 年，SUN 将 Java 开源，称为 open JDK。 2008 年，Oracle 收购了 BEA，得到了 JRockit 虚拟机。 2010 年，Oracle 收购了 SUN，获得了 Java 商标和 HotSpot 虚拟机，并启动了将 JRockit 整合至 HotSpot 的计划。 2011 年，发布 JDK7，正式启用新垃圾回收器 G1。 2014 年，发布 JDK8，基本完成了 JRockit 向 HotSpot 的整合。【目前所用版本】 2017 年，发布 JDK9，G1 称为默认 GC。 2017 年，IBM 开源 J9（虚拟机） 2018 年，发布 JDK11，发布 ZGC——一款新的 GC 2019 年，JDK12 发布，open JDK 加入了 RedHat 开发的 Shenandoah GC——一款 GC 1.1.3 各类 JVM 1996 年，SUN 发布 Java1.0，发布了 Sun Classic VM。虚拟机内部只提供解释器，使用 JIT 编译器需要进行外挂，同时会接管虚拟机执行系统，二者不能配合工作。 Exact VM，jdk1.2 时发布，使得编译器和解释器可以混合工作。 HotSpot VM，jdk1.3 时发布，成为默认虚拟机。 JRockit，BEA 发布，JDK8 时，整合至 HotSpot VM J9，IBM 发布，全称 IBM Technology for Java Virtual Machine，简称 IT4J，J9 是内部代号，2017 年开源 J9 VM，称为 OpenJ9，由 Eclipse 基金会管理，又称 Eclipse OpenJ9 KVM 和 CDC/CLDC Hotspot，Oracle 发布的 Java ME 虚拟机 Azul VM，与特定硬件平台绑定、软硬件配合的专用虚拟机 Liquid VM，自身实现了一个专用操作系统的必要功能。 Apache Harmony Microsoft JVM，只能再 windows 平台下运行 Taobao JVM，深度定制且开源 1.2 JVM 体系 1.2.1 JVM 的整体结构 JVM 由类加载子系统、运行时数据区、执行引擎、本地方法接口、本地方法库组成，采用了解释器与编译器并行的架构。其结构简图如下： 详图如下： 1.2.2 Java 代码执行流程 1.2.3 JVM 的架构模型 Java 编译器输入指令流是基于栈的指令集架构【原因是跨平台设计】。 基于栈式架构的特点： 设计和实现更简单，适用于资源受限的系统 避开了寄存器的分配难题：使用零地址指令方式分配 指令流中的指令大部分是零地址指令，其执行过程依赖于操作栈。指令集更小，编译器容易实现 不需要硬件支持，可移植性更好，更好实现跨平台 基于寄存器架构的特点： 典型的应用是 x86 的二进制指令集：比如传统的 PC 以及 Android 的 Davlik 虚拟机。 指令集架构则完全依赖硬件，与硬件的耦合度高，可移植性差。 性能优秀和执行更高效 花费更少的指令去完成一项操作 在大部分情况下，基于寄存器架构的指令集往往都以一地址指令、二地址指令和三地址指令为主，而基于栈式架构的指令集却是以零地址指令为主 1.2.4 JVM 的生命周期 虚拟机的启动 Java 虚拟机的启动是通过引导类加载器（bootstrap class loader）创建一个初始类（initial class）来完成的，这个类是由虚拟机的具体实现指定的。 虚拟机的执行 一个运行中的 Java 虚拟机有着一个清晰的任务：执行 Java 程序 程序开始执行时他才运行，程序结束时他就停止。 执行一个所谓的 Java 程序的时候，真真正正在执行的是一个叫做 Java 虚拟机的进程。 虚拟机的退出 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止。 由于操作系统用现错误而导致 Java 虚拟机进程终止。 某线程调用 Runtime 类或 System 类的 exit( )方法，或 Runtime 类的 halt( )方法，并且 Java 安全管理器也允许这次 exit( )或 halt( )操作。 除此之外，JNI（Java Native Interface）规范描述了用 JNI Invocation API 来加载或卸载 Java 虚拟机时，Java 虚拟机的退出情况。 第 2 章 类加载子系统 2.1 作用 类加载器子系统负责从文件系统或者网络中加载 Class 文件，要求 class 文件在文件开头有特定的文件标识（符合某种语言规范——如 Java 规范）。 2.2 ClassLoader ClassLoader 只负责 class 文件的加载，至于 class 文件是否可以运行则由 Execution Engine（执行引擎）决定。 加载的类信息（称为 DNA 元数据模板），存放于一块称为方法区的内存空间。除了类信息外，方法区中还会存放运行时常量池的信息，可能还包括字符串字面量和数字常量（这部分常量信息是 Class 文件中常量池部分的内存映射）。 一句话理解：就是要给把.class文件加载到 JVM 的的搬运工，加载以流的形式加载。 2.2.1 ClassLoader 的常用方法和获取方法 常用方法： 获取方法： 获取当前类的 ClassLoader clazz.getClassLoader()，clazz 是类的 Class 对象【参考反射】 获取当前线程上下文的 ClassLoader Thread.currentThread().getContextClassLoader() 获取系统的 ClassLoader（扩展类加载器：ExtClassLoader） ClassLoader.getSystemClassLoader() 获取调用者的 ClassLoader DriverManager.getCallerClassLoader 2.3 加载流程 阶段一：加载。 通过一个类的全限定名获取定义此类的二进制字节流，将这个字节流代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区的这个类的各种数据的访问入口。 加载.class文件的方式： 从本地系统中直接加载。 通过网络获取，典型场景：Web Applet。 从 Zip 压缩包中获取，称为日后 jar、war 格式的基础。 运行时计算生成，使用最多的就是：动态代理技术（java.lang.reflact.Proxy)。 从其他文件生成，典型场景：JSP 应用。 从专有数据库中提取.class 文件，比较少见。 从加密文件中获取，典型的防 Class 文件被反编译的保护措施。 阶段二：链接。 验证 目的在于确保 Class 文件的字节流中包含信息符合当前的虚拟机要求，保证被加载类的正确性，不会危害虚拟机的自身安全。 可在 Java 虚拟机运行的二进制文件开头是 CA FE BA BE，称为魔数。 注意包括四种验证，文件格式验证，元数据验证，字节码验证，符号引用验证。 准备 为类变量（static 修饰的变量）分配内存并且设置该类变量的默认初始值。 这里不包含用 final 修饰的 static，因为 final 在编译的时候就会分配了~~，准备阶段会显示初始化~~。 这里不会为实例变量分配初始化，类变量会分配在方法区中，而实例变量是会随着对象一起分配到 Java 堆中。 解析 就常量池内的符号引用转化为直接引用的过程。 事实上，解析操作往往会随着 JVM 在执行完初始化之后再执行。 符号引用就是一组符号来描述所引用的目标。符号引用的字面量形式明确定义在《Java 虚拟机规范》的 Class 文件格式中，直接引用的就是直接指向目标的指针、相对偏移或者一个简介定位到目标的句柄。 解析动作主要针对类或者接口、字段、类方法、接口方法、方法类型。对应常量池中的 CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info 等。 阶段三：初始化。 初始化阶段就是执行类构造器方法()的过程。 此方法不需要定义，是 Javac 编译器自动收集类中所有的类变量的赋值动作和静态代码块中的语句合并而来。构造器方法中指令按语句在源文件中出现的顺序执行。 ()不同于类的构造器，只有代码中有静态变量或静态代码块，字节码文件才有此方法。 若该类有父类，JVM 会保证子类的()执行之前，父类的()已经执行完毕 虚拟机必须保证一个类的() 方法在多线程下被同步加锁。 2.4 类加载器分类 2.4.1 概述 JVM 支持两种类型的类加载器，分别是引导类加载器（Bootstrap ClassLoader）和自定义加载器（User-Defined ClassLoader）——Java 虚拟机规范将所有派生于抽象类 ClassLoader 的类加载器都称为自定义类加载器。 123456789101112131415161718192021222324252627282930313233public class ClassLoaderTest &#123; public static void main(String[] args) &#123; // 获取系统类加载器 ClassLoader systemClassLoader = ClassLoader.getSystemClassLoader(); // sun.misc.Launcher$AppClassLoader@18b4aac2 System.out.println(systemClassLoader); // 获取其上层：拓展类加载器 ClassLoader parent = systemClassLoader.getParent(); // sun.misc.Launcher$ExtClassLoader@1540e19d System.out.println(parent); // 获取顶层加载器 BootStrapClassLoader 获取不到 因为C/C++写的 ClassLoader parent1 = parent.getParent(); // null System.out.println(parent1); // 对于用户来说 默认使用的是系统类加载器 ClassLoader classLoader = ClassLoaderTest.class.getClassLoader(); // sun.misc.Launcher$AppClassLoader@18b4aac2 System.out.println(classLoader); // sun.misc.Launcher$ExtClassLoader@1540e19d ClassLoader parent2 = classLoader.getParent(); System.out.println(parent2); // 在上层 // null ClassLoader parent3 = parent2.getParent(); System.out.println(parent3); // Java的核心类库都是使用引导类加载的 // String 类是使用引导类加载器加载的 ClassLoader classLoader1 = String.class.getClassLoader(); System.out.println(classLoader1); &#125;&#125; 通过上述代码可以发现： 系统类加载器是 AppClassLoader，其上层为 ExtClassLoader，可以通过代码获取。 ExtClassLoader 的上层为 BootstrapClassLoader，但无法通过代码获取（获取到 null），因为 BootstrapClassLoader 是 C/C++语言实现的。 程序员使用的类加载器（默认）为 AppClassLoader。 Java 的核心类库使用的类加载器是 BootstrapClassLoader。 由此可见类加载器共分为三个等级： 顶级：BootstrapClassLoader 扩展级：ExtClassLoader 程序级（系统级）：AppClassLoader 2.4.2 BootstrapClassLoader 启动类加载器（引导类加载器 Bootstrap ClassLoader）是使用 C/C++语言实现的，嵌套在 JVM 内部。 它用来加载 Java 的核心库（JAVA_HOME/jre/lib/rt.jar、resources.jar 或者 sun.boot.class.path 路径下的内容），用于提供 JVM 自身需要的类。 并不继承自 java.lang.ClassLoader ，没有父加载器。 加载拓展类和应用程序类加载器，并指定为他们的父加载器。 处于安全考虑，Bootstrap 启动类只加载包名为 java、javax、sun 等开头的类。 123456789public class ClassLoaderTest1 &#123; public static void main(String[] args) &#123; System.out.println(&quot;启动类加载器&quot;); URL[] urLs = Launcher.getBootstrapClassPath().getURLs(); for (URL urL : urLs) &#123; System.out.println(urL.toString()); &#125; &#125;&#125; 123456789启动类加载器file:/D:/jdk/jdk1.8/jre/lib/resources.jarfile:/D:/jdk/jdk1.8/jre/lib/rt.jarfile:/D:/jdk/jdk1.8/jre/lib/sunrsasign.jarfile:/D:/jdk/jdk1.8/jre/lib/jsse.jarfile:/D:/jdk/jdk1.8/jre/lib/jce.jarfile:/D:/jdk/jdk1.8/jre/lib/charsets.jarfile:/D:/jdk/jdk1.8/jre/lib/jfr.jarfile:/D:/jdk/jdk1.8/jre/classes 2.4.3 ExtClassLoader 扩展类加载器（Extension ClassLoader）Java 语言编写，由 sun.misc.Launcher&amp;ExtClassLoader 实现。 派生于 ClassLoader 类 父类加载器为启动类加载器 BootstrapClassLoader 从 java.ext.dirs 系统属性所指定的目录中加载类库，或者从 JDK 的安装目录 jre/lib/ext 子目录（拓展目录）下加载类库。如果用户创建的 Jar 放在此目录下，也会自带由拓展类加载器加载。 123456789public class ClassLoaderTest1 &#123; public static void main(String[] args) &#123; System.out.println(&quot;拓展类加载器&quot;); String property = System.getProperty(&quot;java.ext.dirs&quot;); for (String path : property.split(&quot;;&quot;)) &#123; System.out.println(path); &#125; &#125;&#125; 123拓展类加载器D:\\jdk\\jdk1.8\\jre\\lib\\extC:\\WINDOWS\\Sun\\Java\\lib\\ext 2.4.4 AppClassLoader 应用程序类加载器（系统类加载器 AppClassLoader）Java 语言编写，由 sun.misc.Launcher&amp;AppClassLoader 实现。 派生于 ClassLoader 类。 父类加载器为拓展类加载器（ExtClassLoader）。 它负责加载的是环境变量 classpath 或者系统属性 java.class.path 指定路径下的类库。 该类加载时程序中默认的类加载器，一般来说，Java 应用程序都是由它来完成加载。 通过 ClassLoader#getSystemClassLoader()方法可以获取到该类加载器。 2.5 自定义类加载器 此处描述的自定义类加载器不同于 2.4.1 描述的自定义类加载器，此处自定义类加载器可以理解为高度定制的类加载器，有以下需求场景： 隔离加载类：中间件有自己依赖的 jar 包，应用程序也有自己依赖的 jar 包，有时存在类名冲突的情况。 修改类加载的方式：动态加载。 拓展加载源：上述代码中可以看到默认加载源是 jdk 目录，如果有其他来源，如网络则可以进行扩展。 防止源码泄漏：防止源码被反编译、篡改，发布源代码前进行加密，运行前进行解密。 用户自定义类加载器实现步骤： 继承抽象类 java.lang.ClassLoader 类的方法。 在 JDK1.2 之前，继承 ClassLoader 类并重写 loadClass()方法。 在 JDK1.2 之后，不再建议用户去覆盖 loadClass()方法，而是建议把自定义类加载逻辑写在 findClass()方法中。 在编写自定义类加载器的时候，如果没有太过于复杂的有要求，可以直接继承 URLClassLaoder 类，这样可以避免自己编写 findClass() 方法以及器获取字节码流的方式，使得自定义类加载器编写更加简洁。 1234567891011121314151617181920212223public class CustomClassLoader extends ClassLoader &#123; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; try &#123; byte[] result = getClassFromCustomPath(name); if(result == null)&#123; throw new FileNotFoundException(); &#125;else&#123; return defineClass(name,result,0,result.length); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; throw new ClassNotFoundException(name); &#125; private byte[] getClassFromCustomPath(String name)&#123; //从自定义路径中加载指定类:细节略 //如果指定路径的字节码文件进行了加密，则需要在此方法中进行解密操作。 return null; &#125;&#125; 2.7 双亲委派机制 2.7.1 概述 Java 虚拟机对 class 文件采用的是按需加载的方式，也就是说当需要使用该类的时候才会将它的 class 文件加载到内存生成 class 对象。而且加载某个类的 class 文件的时候，Java 虚拟机采用的是双亲委派机制，也就是把请求交给父类处理，它是一种任务委派机制。 它具有以下优势： 避免类的重复加载。class 文件只能被某一种类加载器加载一份。 保护程序安全，防止核心 API 被随意篡改。防止应用程序中定义与核心包相同的全类名类。 2.7.2 工作原理 如果一个类加载器收到了类加载的请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行。 如果父类加载器还存在器父类加载器，则进一步向上委托，一次递归请求最终到达顶层的启动类加载器。 如果父类加载器可以完成类加载任务，就成功返回，如果父类加载器无法完成此任务，子加载器才会尝试自己去加载。 接口是由引导类加载器加载，而接口的实现则是由系统类加载器（线程上下文加载器）加载。 2.7.3 沙箱安全机制 自定义 java.lang.String 类，但是在加载自定义 String 类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载 JDK 自带的文件(rt.jar 包中的 java\\lang\\String.class)，这样就可以保证对 Java 核心源代码的保护。 2.8 补充内容 2.8.1 同一个类的判断 在 JVM 中表示两个 class 对象是否为同一个类存在的两个必要条件： 全限定类名必须完全一致。 加载这个类的 ClassLoader(指的是 ClassLoader 实例对象)必须相同。 换句话说，JVM 中即使这两个类对象(class 对象)来源于同一个 class 文件，被同一个虚拟机加载，只要加载它们的 ClassLoader 对象不同，那么这两个类对象也是不相等。 JVM 必须知道一个类型是由启动加载器的还是由用户加载器加载的。如果一个类型是由用户类加载器加载的，那么 JVM 会将这个类加载器的一个引用作为类型信息的一部分保存在方法区内。当解析一个类型到另一个类型的引用的时候，JVM 需要保证这两个类型的加载器是相同的。 2.8.2 主动使用和被动使用 Java 程序对类的使用分为主动使用和被动使用（是否会导致类的初始化—是否执行了加载的初始化过程）： 主动使用： 创建类的实例（new）。 访问某个类的或者接口的静态变量，或者对该静态变量赋值。 调用类的静态方法。 反射(Class.forName(“xxx.xxx.xxx”))。 初始化一个类的子类。 Java 虚拟机启动的时候被标记为启动类的类。 Java7 开始支持动态的语言支持 java.lang.invoke.MethodHandle =实例的解析结果 REF_getStatic、REF_putStatic、REF_invokeStatic 对应的类没有初始化，则初始化。 被动使用：其他情况。 第 3 章 运行时数据区 3.1 概述 JVM 的内存布局规定了 Java 在运行过程中内存申请、分配、管理的策略，保证了 JVM 的高效稳定运行。不同的 JVM 对于内存的划分方式和管理机制存在着部分差异，经典的 JVM 内存布局如下： 以阿里的为例，详细布局如下： Java 虚拟机定义了若干种程序运行期间会使用到的运行时数据区，其中堆和方法区随着虚拟机的销毁而销毁（一个虚拟机一份）。其余三部分是与线程一一对应的，这些与线程对应的数据区域会随着线程开始和结束而创建和销毁。 下图中灰色的是单独线程私有的，红色的是多个线程共享的 每个线程：独立包括程序计数器、栈、本地栈 线程间共享：堆、堆外内存（永久代或者元空间、代码缓存） Class Runtime：位于 java.lang.Runtime 包下，每个 Java 应用程序都有一个类 Runtime 实例，它允许应用程序与运行应用程序的环境进行交互。 可以从 getRuntime 方法获得当前运行时。 3.2 线程（虚拟机角度） JVM 允许一个应用有多个线程并行执行，在 HotSpot JVM 里，每个线程都与操作系统的本地线程直接映射。线程是一个程序里面的运行单元。 当一个 Java 线程准备好执行以后，此时一个操作系统的本地线程也同时创建，Java 线程执行结束之后，本地线程也会回收。 操作系统负责所有线程的安排调度（安排线程到任何一个可用的 CPU 上），一旦本地线程初始化成功，它就会调用 Java 线程中的 run()方法。 JVM 系统线程（后台线程、守护线程，不包括调用 public static void main(String[] args)的 main 线程以及所有 main 线程创建的线程）主要包括： 虚拟机线程： 这种线程的操作是需要 JVM 达到安全点才会出现，执行类型包括 &quot;stop-the-world&quot;的垃圾收集器，线程栈收集，线程挂起以及偏向锁撤销。 周期任务线程：这种线程是时间周期事件的体现（比如中断），一般用于周期性操作的任务调度。 GC 线程：对在 JVM 里面不同种类的垃圾收集行为提供支持。 编译线程：运行时将字节码编译到本地代码。 信号调度线程：这种线程接受信号并发送给 JVM，在它内部通过调用适当的方法进行处理。 3.3 程序计数器 程序计数寄存器（Program Counter Register）又称 PC 寄存器。寄存器存储线程相关的指令信息。CPU 只有把数据装载到寄存器才能够运行。 JVM 中的 PC 寄存器是对物理 PC 寄存器的一种抽象模拟， 并非广义上的物理寄存器，将其翻译为 PC 计数器（或指令计数器）会更加的贴切（也称为程序钩子），并且也不容易引起一些不必要的误会。 作用：PC 寄存器用来存储指向下一条指令的地址，即将要执行的指令代码，由执行引擎读取下一条指令。（指令值存放在栈帧的方法返回地址区域） 特点： 是一块很小的内存空间，几乎可以忽略不计，也是运行速度最快的存储区域。 在 JVM 规范中，每个线程都有它自己的程序计数器，是线程私有的，生命周期与线程的生命周期保持一致。 任何时间一个线程都只有一个方法在执行，也就是所谓的当前方法，程序计数器会存储当前线程执行 Java 方法的 JVM 指令地址，或者，如果是在执行 native 方法，则是为指定值（underfed）。 它是程序控制流的指示器、分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 字节码解释器工作的时候就是通过改变这个计数器的值来选取下一条需要执行的字节码指令。 它是唯一的一个在 Java 虚拟机规范中没有规定任何 OutOfMemoryError 的区域。 如下图部分反编译代码截图，PC 寄存器保存反编译文件中的指令地址（由于是栈结构，记录偏移地址即可知道具体的指令地址），执行引擎会读取 PC 寄存器中的指令地址，得到地址对应的操纵指令，转换为计算机识别的机器指令发送给 CPU 执行。 3.3.1 常见面试题 使用 PC 寄存器存储字节码指令地址有什么用呢？为什么使用 PC 寄存器记录当前线程的执行地址呢？ 因为 CPU 需要不停的切换各个线程，这时候切换回来以后，就得知道接着从哪开始继续执行。 JVM 的字节码解释器（将字节码转换机器码，解释器逐行进行）就是需要通过改变 PC 寄存器的值来明确下一条具体应该执行那条字节码指令 PC 寄存器为什么设定为线程私有的？ 由于 CPU 时间片轮转限制，众多线程在并发执行的过程中，任何一个确定的时刻，一个处理器或者多核处理器中的一个内核，只会执行某个线程中的一条指令，CPU 会不停的做任务切换，为了能够准确记录各个线程正在执行的当前字节码指令的地址，最好的办法就是为每一个线程都分配一个 PC 寄存器，这样一来各个线程之间就可以独立计算，从而不会出现相互干扰的情况。 扩展：CPU 时间片就是 CPU 分给各个程序的执行时间，每个线程被分配一个时段，这就是时间片。 3.4 虚拟机栈（VMS) 3.4.1 概述 虚拟机栈出现的背景： 由于跨平台的设计，Java 指令都是根据栈来设计的，不同平台的 CPU 架构不同，所以不能设计为基于寄存器的。 优点是跨平台，指令集少，编译器容易实现，缺点是性能下降，实现同样的功能需要更多的指令。 内存中的栈和堆： 栈是运行时的单位，而堆是存储的单位。 即：栈是解决程序的运行问题，即程序如何执行，或者说如何处理数据。堆解决的问题是数据存储的问题，即数据怎么放，放在哪儿。 Java 虚拟机栈： Java 虚拟机栈（Java Virtual Machine Stack），早期也叫 Java 栈，每个线程在创建的时候都会创建一个虚拟机栈，其内部保存的一个个栈帧（Stack Frame）对应的一次次的方法调用。 是线程私有的。 生命周期： 生命周期和线程一致。 作用： 主管 Java 程序的运行，它保存方法的局部变量（8 种基本数据类型、对象的引用地址）、部分结果，并参与方法的调用和返回。 优点： 栈是一种快速有效的分配存储的方式，访问速度仅次于程序计数器。 JVM 直接对 Java 栈的操作只有 2 个： 每个方法执行，伴随着进栈（入栈、出栈）。 执行结束后的出栈工作。 对于栈来说不存在垃圾回收问题 无 GC 但有 OOM(Out Of Memmory)和 StackOverflowError 设置栈内存的大小： -Xss 以字节为单位 ，单位是可以调节的。 栈顶缓存技术： 基于栈式架构的虚拟机所使用的零地址指令（只有操作码，没有操作数的指令）更加紧凑，但是完成一项操作的时候必然需要使用更多的入栈的出栈指令，这同时也就意味着需要将更多的指令分派（instruction dispatch）次数和内存读/写次数。 由于操作数是存在内存中的，因此频繁的执行内存读/写操作必然会影响执行速度。为了解决这个问题，Hotspot JVM 的设计者们提供了栈顶缓存技术（ToS Top-of-Stack Cashing）将栈顶元素全部缓存在物理的 CPU 的寄存器中，以此降低对内存的读/写次数。提升执行引擎的执行效率。 各区是否存在 Error 和 GC： Sof oom GC 方法区 √ √ 堆空间 √ √ 程序计数器 × × × 本地方法栈 √ √ × 虚拟机栈 √ √ × 3.4.2 栈的存储单位和运行原理 栈的存储单位： 每个线程都有自己的栈，栈中的数据都是以栈帧（Stack Frame）的格式存在。 在这个线程上正在执行的每个方法都各自对应一个栈帧（Stack Frame）。 栈帧是一个内存区块，是一个数据集，维系着方法执行过程中的各种数据信息。 栈的运行原理： JVM 直接对 Java 栈的操作只有两个，就是对栈帧的压栈和出栈，遵循先进后出/后进先出原则。 在一条活动的线程中，一个时间点上，只会有一个活动的栈帧。即只有当前正在执行的方法的栈帧（栈顶栈帧）是有效的，这个栈帧被称为当前栈帧（Current Frame），与当前栈帧相对应的方法就是当前方法（Current Method），定义这个方法的类就是当前类（Current Class）。 执行引擎运行所有的字节码指令只针对当前栈帧进行操作。 如果在该方法中调用了其他方法，对应的新的栈帧会被创建出来，放在栈的顶端，成为新的当前栈帧。 不同的线程中所包含的栈帧是不允许存在相互引用的，即不可能在一个栈帧之中引用另外一个线程的栈帧。 如果当前方法调用了其他的方法，方法返回之际，当前栈帧会传回次方法的执行结果给前一个栈帧，接着虚拟机会丢弃当前的栈帧，使得前一个栈帧重新成为当前栈帧。 Java 方法有两种返回函数的方式，一种是正常的函数返回，使用 return 指令，另外一种是抛出异常，不管使用那种方式，都会导致栈帧被弹出。 12345678910111213141516171819202122public class InvokeMethodFrame &#123; public static void main(String[] args) &#123; System.out.println(&quot;InvokeSelf.main 开始执行...&quot;); method1(); System.out.println(&quot;InvokeSelf.main 执行结束...&quot;); &#125; public static void method1() &#123; method2(); System.out.println(&quot;InvokeSelf.method1&quot;); &#125; public static void method2() &#123; method3(); System.out.println(&quot;InvokeSelf.method2&quot;); &#125; public static void method3() &#123; method4(); System.out.println(&quot;InvokeSelf.method3&quot;); &#125; public static void method4() &#123; System.out.println(&quot;InvokeSelf.method4&quot;); &#125;&#125; 3.4.3 栈帧的内部结构 栈帧的内部结构： 局部变量表（Local Variables）。 操作数栈（Operand Stack）（或表达式栈）。 动态链接（Dynamic Linking）（或指向运行时常量池的方法引用）。 方法返回地址（Return Address）（或方法正常退出或者异常退出的定义）。 一些附加信息。 3.4.3.1 局部变量表（Local Variables） 局部变量表也被称为局部变量数组或本地变量表： 定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量，这些数据类型包括各类型的基本数据类型、对象引用（reference），以及 returnAddress 类型。 由于局部变量表是建立在线程的栈上，是线程的私有数据。因此不存在数据不安全的问题。 局部变量表所需的容量大小是编译器确定下来的，并保存在方法的 Code 属性的 maximum local variables 数据数据项中，在方法运行期间是不会改变局部变量表的大小的。 方法嵌套调用的次数由栈的大小决定，一般来说，栈越大，方法嵌套调用次数越多。而对一个函数而言，它的参数和局部变量越多，使得局部变量表膨胀，它的栈帧就越大，一满足方法调用所需传递的信息增加的需求。进而函数调用就会占用更多的栈空间，而导致其嵌套调用的次数就会减少。 局部变量表中的变量只在当前方法中调用有效，在方法执行的时候，虚拟机通过使用局部变量表完成参数值到参数列表的传递过程。当方法调用结束的时候，随着方法栈帧的销毁，局部变量表也会随之销毁。 在栈帧中，与性能调优关系最密切的就是前面提到的局部变量表，在方法执行时，虚拟机使用局部变量表完成方法的传递。 局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中直接或者间接引用的对象都不会被回收。 局部变量表的存储单元： 局部变量表最基本的存储单元是 Slot（变量槽），index 从 0 开始，到数组长度-1 的索引结束，通过这个索引即可成功访问到局部变量表中指定的局部变量值。 在局部变量表中，32 位以内的类型只占用一个 Slot（包括 returnAddress 类型），64 位的类型（long 和 double）占用 2 个 Slot（占用两个索引，访问时使用前一个索引）。 byte、short、char、float 在存储之前被转换为 int，boolean 也被转为 int，0 代表 false，非 0 代表 true。 long 和 double 则占据两个 Slot。 当一个实例方法被调用的时候，它的方法参数和方法体内部定义的局部变量将会按照顺序被复制到局部变量表中的每一个 Slot 上。 如果当前帧是由构造方法或者实例方法创建的，那么该对象引用 this 将会存放在 index 为 0 的 Slot 处，其余的参数按照参数表顺序继续排列。 所以静态方法中无法使用 this 调用，因为 this 变量不存在于当前方法的局部变量表中。 栈帧中的局部变量表中的槽位是可以重用的，如果一个局部变量过了其作用域，那么在其作用域之后申明的新的局部变量就很有可能会复用过期局部变量的槽位，从而达到节省资源的目的。 静态变量、实例变量与局部变量的对比： 类变量：linking 的 prepare 阶段 - 给类变量默认赋值 -&gt; initial 阶段 - 给类变量显示赋值 （静态代码块中） 实例变量：随着对象的创建，会在堆空间进行分配实例变量空间，并进行默认赋值。 局部变量：在使用之前必须显示赋值（否则编译不通过）。 3.4.3.2 操作数栈（Operand Stack） 一般而言”Java 虚拟机的解释引擎是基于栈的执行引擎“的描述中，栈指的就是操作数栈。 操作数栈，也可以称为表达式栈（Expression Stack），在方法执行的过程中，主要用于保存计算过程的中间结果，同时作为计算过程中变量临时的存储空间。 根据字节码指令，往栈中写入数据或者提取数据，即入栈（push）/出栈（pop） 某些字节码指令将值压入操作数栈，其余的字节码指令将操作数取出栈，使用它们后再把结果压入栈。 操作数栈就是 JVM 执行引擎的一个工作区，当一个方法刚开始执行的时候，一个新栈帧也随之被创建出来，这个方法的操作数栈是空的。 每一个操作数栈都会拥有一个明确的栈深度用于存储数值，其所需的最大深度在编译器就定义好了，保存在方法的 Code 属性中，为 max_stack 的值。 栈中的任何一个元素都可以是任意的 Java 数据类型。 32bit 的类型占用一个栈单位深度。 62bit 的类型占用两个栈单位深度。 操作数栈虽然使用数组结构实现，但不能通过索引的方式来进行数据访问，只能通过标准的入栈（push）和出栈（pop）操作来完成一次数据访问。 如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中，并更新 PC 寄存器的下一条需要执行的字节码指令。 操作数栈中元素的数据类型必须与字节码指令的序列严格匹配，这由编译器再编译期间进行验证，同时在类加载中的类检验阶段的数据流分析阶段要再次验证。 3.4.3.3 动态链接（Dynamic Linking） 有些文章中将动态链接、方法返回地址、其他附加信息总称为帧数据区。 每一个栈帧内部都包含一个指向运行时常量池中该栈帧所属方法的引用。包含这个引用的目的就是为了支持当前方法的代码能够实现动态链接（Dynamic Linking）。比如：involvedynamic 指令。 在 Java 源文件被编译到字节码文件的时候，所有的变量（包含类变量、成员变量、局部变量吗？）和方法引用都作为符号引用）（Symbolic Reference）保存在 class 文件的常量池中。比如：描述一个方法调用了另外的其他方法的时候。就是通过常量池中指向方法的符号引用来表示的，动态链接的作用就是将这些符号引用转换为调用方法的直接引用。 静态链接： 当一个字节码文件被装载到 JVM 内部的时候，如果被调用的 目标方法在编译器可知，且在运行期保持不变时，这种情况下将调用方法的符号引用转换为直接引用的过程叫做静态链接。 动态链接： 如果被调用的方法在编译期间无法被确定下来，也就是说，只能够在程序运行期将调用方法的符号引用转换为直接引用，由于这种引用转换过程具备动态性，因此也就称之为动态链接。 3.4.3.4 方法返回地址和附加信息的补充知识——方法调用 在 JVM 中，将符号引用转换为调用方法的直接引用与方法的绑定机制相关（符号引用、直接引用参看 3.4.3.3 动态链接）： 早期绑定 早期绑定就是指被调用的 目标方法如果在编译期间可知，且运行期保持不变时，即可将这个方法与所属的类型进行绑定，这样一来，由于明确了被调用的目标方法究竟是哪一个，因此可以使用静态链接的方式将符号引用转为直接引用。 晚期绑定 如果被调用的方法在编译期无法被确定下来，只能够在程序运行期间根据实际的类型绑定相关的方法，这种绑定叫做晚期绑定。 Java 中任意一个普通方法都具备虚函数（C++中）的特征，如果某个方法不想用于虚函数的特征的时候，可以使用 final 关键字来标记： 非虚方法：如果方法在编译期就确定了具体的调用版本，这个版本在运行时候是不可变的，这样的方法叫做非虚方法。 静态方法、私有方法、final 方法、实例构造器、父类方法都是非虚方法。 虚方法：除了非虚方法都是虚方法。 虚拟机中提供的方法调用的指令： 普通调用指令： invokestatic：调用静态方法。 invokespecial：调用私有方法、父类方法等非虚方法。 invokevirtual：调用所有的虚方法（final 方法在这调用，但是非虚方法）。 invokeinterface：调用接口方法。 动态调用指令 invokedynamic：动态解析出需要调用的方法，然后执行。 前四条指令固化在虚拟机内部，方法的执行调用不可人为干涉，而 invokedynamic 指令则支持用户确定方法版本。invokestatic 指令和 invokespecial 指令调用的方法称为非虚方法。其余的（final 修饰的除外）称为虚方法。 关于 involvedynamic 指令： JVM 字节码指令集一直比较稳定，一直到 Java7 中才增加了一个 invokedynamic 指令，这是 Java 为了实现[动态类型语言]支持而做的一种表达方式。 但是在 Java7 中并没有提供直接生成 involvedynamic 指令的方法，需要借助 ASM 这种底层字节码工具来生成 involvedynamic 指令。直到 Java8 的 Lambda 表达式的出现，involvedynamic 指令才有了直接的生成方式。 Java7 中增加的动态语言类型支持的本质是对 Java 虚拟机规范的修改，而不是对 Java 语言规则的修改，这里比较复杂，最直接的受益者就是运行在 Java 平台的动态语言的编译器。 动态类型语言和静态类型语言： 动态类型语言的静态类型语言的区别就是对类型的检查是在编译期还是在运行期，满足前者就是静态类型语言，满足后者就是动态类型语言。 静态类型语言就是判断变量自身的类型信息；动态类型语言是判断变量值的信息，变量没有类型信息，变量值才有类型信息。 Java 中方法重写的本质： 找到操作数栈顶的第一个元素随执行的对象的实际类型，记作 C。 如果在类型 C 中找到与常量池中描述符、简单名称（？）都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找过程结束；如果不通过，就返回 java.lang.IllegalAccessError 异常。 程序试图访问一个属性或者调用一个方法的时候，这个属性或者方法没有权限进行访问。 否则，按照继承关系从下往上一依此对 C 的各个父类进行第 2 步的搜索和验证。 如果始终没有找到，就抛出 java.lang.AbstractMethodError 异常。 虚方法表： 在面向对象的编程中，会很繁琐的使用到动态分派（invokevirtual），如果每次动态分派的过程中都要重新在类的方法元数据中搜索合适的目标的话，就可能影响到执行效率。因此，为了提高性能，JVM 采用在类的方法区建立一个虚方法表（virtual method table）（非虚方法不会出现在表中）来实现，使用索引表来代替查找。 每个类都有一个虚方法表，表中存放着各个方法的实际入口。 虚方法表在什么时候会被创建？ 在类加载的链接阶段（解析过程）被创建并开始初始化，类的变量初始值准备完成之后，JVM 会把该类的方法表 也初始化完毕。 3.4.3.5 方法返回地址（Return Address） 方法返回地址存放调用该方法的 PC 寄存器的值。 方法退出后都要返回到该方法被调用的位置，方法正常退出的时候，调用者的 PC 寄存器的值作为返回地址，即调用该方法的指令的下一条指令的地址。而通过异常退出的，返回的地址是要通过异常表来确定，栈帧中一般不会保持这部分的信息。 当一个方法开始执行之后，只有两种方式可以退出这个方法： 执行引擎遇到任何一个方法返回的字节码指令（return），就会有返回值传递给上层的方法调用者，简称正常完成出口。 一个方法在正常调用完成之后究竟需要使用哪一个返回指令还需要根据方法返回值的实际数据类型而定。 在字节码指令中，返回指令包含 ireturn （返回值是 boolean、byte、char、short 和 int 类型时使用），lreturn、freturn、dreturn、areturn（返回值引用类型）、return（为 void 方法、实例化初始方法（构造方法、静态代码块）、类和接口的初始方法使用）。 在方法执行的过程中遇到了异常（Exception），并且这个异常没有在方法内进行处理，也就是只要在本地方法异常表没有搜索到匹配的异常处理器，就会导致方法退出，简称异常完成出口。 方法执行过程中抛出异常时的异常处理，存储在一个异常处理表中，方便在发生异常的时候找到处理异常的代码。 本质上，方法的退出就是当前栈帧出栈的过程。因此，需要恢复上层方法的局部变量表、操作数栈、讲返回值压入调用者栈帧的操作数栈、设置 PC 寄存器值等，让调用者方法继续执行下去。 正常完成出口和异常完成出口的区别在于：通过异常出口完成退出的不会给它的上层调用者产生任何的返回值。 3.4.3.6 一些附加信息 栈帧中还允许携带与 Java 虚拟机实现相关的一些附加信息，例如：对程序调试支持的信息。 3.5 堆 3.5.1 概述 每个进程拥有一个 JVM 实例，一个 JVM 实例只存在一个堆内存，堆也是 Java 内存管理的核心区域： Java 堆区在 JVM 启动的时候即被创建。其空间大小也就确定了。是 JVM 管理的最大的一块内存空间 堆内存的大小可以使用参数进行调节（-Xms10m -Xmx10m 最小、最大内存设置，只能影响新生代、老年代）。 《Java 虚拟机规范》规定，堆可以处于物理上不连续的内存空间中，但是在逻辑上他应该是连续的。 所有的线程共享 Java 堆，在这里还可以划分为线程私有缓冲区（Thread Local Allocation Buffer ，TLAB） 《Java 虚拟机规范》中对 Java 堆的描述是：所有的对象实例以及数组都对应当在运行时分配在堆上 “几乎“所有的对象实例都在这里进行分配内存——从实际使用角度看。 数组和对象可能永远不会存储在栈上，因为栈帧中保存引用，这个引用指向对象或者数组在堆中的位置。 在方法结束后，堆中的对象不会马上被移除，仅仅在垃圾收集的时候，才会被移除。（如果马上移除，GC 线程会频繁调用，挤占用户线程，影响性能） 堆是 GC（Garbage Collection 垃圾收集器）执行垃圾回收的重点区域。 jdk 自带查看堆内存分析的工具。在 jdk 的安装目录下的 bin 下的 jvisualvm.exe（单独的 jre 没有这个工具） 3.5.2 内存划分 3.5.2.1 堆空间逻辑划分 现代的垃圾收集器大部分都基于分代收集理论设计。堆空间细分为： Java7 以及之前堆的内存逻辑分为三部分：新生区+养老区+永久区 Young Generation Space：新生区 Young/New 又被划分为 Eden 区和 Survivor 区 Tenure Generation Space：养老区 Old/Tenure Permanent Space：永久区 Perm Java8 及以后堆内存逻辑上分为三部分：新生区+养老区+元空间 Young Generation Space：新生区 Young/New 又被划分为 Eden 区和 Survivor 区 Tenure Generation Space：养老区 Old/Tenure Meta Space：元空间 Meta 约定称谓：新生代=新生代 =年轻代，养老区=老年区=老年代 ，永久代=永久区。 3.5.2.2 堆空间分代思想 其实不分代也是完全可以的，分代的唯一理由就是优化 GC 性能，如果没有分代，那么所有的对象都在一起。GC 的时候，就会对整堆进行全局扫描，然而很多对象都是朝生夕死的，如果分代的话，把这些对象聚集在一起，GC 先回收这部分，就会节省很多空间和资源。 经过研究，不同对象的生命周期不同，70%~99% 都是临时对象 新生代：由 Eden、两块大小相同的 Survivor（又称 from/to，s0/s1） 构成，to 总为空。 老年代：存放新生代中经历多次 GC 依然存活的对象。 3.5.2.3 堆内存分配策略（对象提升(promotion)规则） 如果对象在 Eden 出现并经过第一次 Minor GC 之后仍然存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 区中。每熬过一次 MinorGC，年龄就增加 1 岁，当年龄增加到一定的程度（默认为 15 岁）就会晋升到老年代。 对象晋升老年代的年龄，可以通过 ： -XX:MaxTenuringThreshold 来设置。 针对不同的年龄段的对象分配原则如下： 优先分配到 Eden。 大对象（通常指需要连续存储空间的对象）直接分配到老年代。 尽量避免程序中出现过多的大对象。 长期存活的对象分配到老年代。 动态对象年龄判断： 如果 Survivor 区中的相同年龄的所有的对象的总和大于 Survivor 空间的一半，年龄大于或者等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄 空间分配担保：-XX:HandlePromotionFailure 3.5.2.4 对象分配过程(TLAB） TLAB 指 Thread Local Allocation Buffer（线程本地分配缓存区）。由于堆区是线程共享区域，任何线程都可以访问到堆区中的共享数据。而对象实例的创建在 JVM 中非常频繁，因此在并发环境下从堆区中划分内存空间是线程不安全的。如果要避免多个线程操作同一个地址，需要使用加锁等机制，但会影响分配速度。因此将堆中 Eden 区继续进行划分，JVM 为每个线程分配了一个私有缓存区域，私有不足则再使用共享的 Eden 区。 私有缓存区域的划分时从内存模型角度而不是垃圾收集的角度。 多线程同时分配内存的时候，使用 TLAB 可以避免一系列的非线程安全问题。同时还能提升内存分配的吞吐量，称为快速分配策略。 几乎所有的 OpenJDK 衍生出来的 JVM 都提供了 TLAB 设计。 尽管不是所有的对象实例都能在 TLAB 中成功分配内存，但是 JVM 确实把 TLAB 作为内存分配的首选。 在程序中，开发人员可以通过选项-XX:UseTLAB设置是否开启 TLAB 空间（默认开启）。 默认情况下，TLAB 空间的内存非常小，仅仅占有整个 Eden 空间的 1%。可以通过选项-XX:TLABWasteTargetPercent设置 TLAB 空间所占用的 Eden 空间的百分比大小。 一旦对象在 TLAB 空间分配内存失败的时候，JVM 就会尝试使用加锁机制来确保数据操作的原子性，直接使用 Eden 区分配内存。 对象分配过程： 3.5.3 堆空间大小、设置与查看 Java 堆用来存储 Java 对象实例，那么堆的大小在 JVM 启动的时候就已经设定好了，可以通过选项设置 “-Xmx&quot;和”-Xms&quot;来进行设置 （设置的是年轻代+年老代的大小）： -Xms：表示堆区的启始内存大小，等价于 -XX:InitialHeapSize。 -Xmx：表示堆区的最大内存，等价于 -XX:MaxHeapSize。 说明：-X 是 JVM 的运行参数，ms 是 Memory Start 的缩写。 一旦堆区的内存大小超过-Xmx 所指定的最大内存时，将会抛出 OutOfMemoryError 异常。 默认情况下： 初始内存大小：物理电脑内存大小/64 最大内存大小：物理电脑内存大小/4 12345678910public class HeapSizeDemo &#123; public static void main(String[] args) &#123; long initialMemory = Runtime.getRuntime().totalMemory(); long maxMemory = Runtime.getRuntime().maxMemory(); System.out.println(&quot;-Xms:&quot; + (initialMemory / 1024 / 1024) + &quot; M&quot;); System.out.println(&quot;-Xms:&quot; + (maxMemory / 1024 / 1024) + &quot; M&quot;); System.out.println(&quot;系统内存大小：&quot; + (initialMemory * 64 / 1024 / 1024 / 1024) + &quot; G&quot;); System.out.println(&quot;系统内存大小：&quot; + (maxMemory * 4 / 1024 / 1024 / 1024) + &quot; G&quot;); &#125;&#125; 查看 JVM 的内存参数设置： 方式一（命令行指令）： jps：查看 jvm 线程 jstat -gc 线程代号 新生代空间：S0C+S1C+EC 老年代空间：EC 方式二：JVM 运行参数 -XX:+PrintGCDetails PSYoungGen 新生代、ParOldGen 老年代、Metaspace 元空间 一般将-Xms 和-Xmx 两个参数设置为相同的值，可以使垃圾回收机制在清理完堆区之后不需要重新分隔计算堆区的大小，从而提升性能。 3.5.4 OOM 参考文档：[基础] 第 8 篇：常见 OOM 异常 1234567891011121314/** * VM args: -Xms20m -Xmx20m * Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space */public class HeapOomCase &#123; static class OomObject &#123; &#125; public static void main(String[] args) &#123; ArrayList&lt;OomObject&gt; list = new ArrayList&lt;&gt;(); while (true) &#123; list.add(new OomObject()); &#125; &#125;&#125; 过程参见 3.5.6。 3.5.5 年轻代和老年代内存结构 存储在 JVM 中的 Java 对象可以被划分为两类： 一类是生命周期毕竟短的瞬时对象。这类对象的创建和消亡都非常迅速。 另外一类对象的生命周期却非常长，在某些极端的情况下还能与 JVM 的生命周期保持一致。 Java 堆区进一步细分的话，可以划分为年轻代（YoungGen）和老年代（OldGen）： 其中年轻代又可以分为 Eden 空间，Survivor0 空间和 Survivor1 空间（有时也叫 from 区和 to 区）。 空间大小比例（一般不做修改）： Young : Old = 1 : 2 Eden : s0 : s1 = 8 : 1 : 1（Hotspot 中的缺省比例） 但使用时比例为 6:1:1，这是因为虚拟机的自适应内存分配策略。 -XX:-UseAdaptiveSizePolicy表示关闭自适应的内存分配策略，但使用时发现并不能其作用。 要想看到 8:1:1 的内存分配，可使用-XX:SurvivorRatio=8设置。 配置新生代老年代在堆结构的占比： -XX:NewRatio=2（默认）：表示新生代占 1，老年代占 2，新生代占整个堆的 1/3。 -XX:NewRatio=4：表示新生代占 1，老年代占 4，新生代占整个堆的 1/5。 几乎所有的 Java 对象都是在 Eden 区被 new 出来的： 绝大部分的 Java 对象的销毁都在新生代进行了。 IBM 公司的专门研究表明：新生代 80% 的对象都是”朝生夕死“的。 可以使用-Xmn设置新生代的最大内存大小（一般不使用该指令设置，而使用-XX:NewRatio=2，如果二者同时设置了，-Xmn优先。 3.5.6 图解对象的内存分配过程 new 的对象先放在伊甸园区。 当伊甸园区填满的时候，程序又需要创建新的对象，JVM 的垃圾回收器将对伊甸园进行垃圾回收（YGC/Minor GC），将伊甸园区中的不不再被其他对象所引用的对象进行销毁，再加载新的对象放在伊甸园区。然后将伊甸园区的幸存对象移动到 S0 区。 红色代表不再使用的对象。 绿色中的数字代表年龄，移动一次则+1。 如果伊甸园区再次触发垃圾回收，此时上次幸存下来的放到幸存者 S0 区域的，如果没有回收，就放到 S1 区。伊甸园区的对象放到 S1 区。此时 S0 为空。 后续如果伊甸园区再次经历垃圾回收，此时会重新放回 S0 区，接着再去 S1 区，循环此过程。即伊甸园区的有用对象总是先放到空的 S 区。 注意：只有 Eden 区满了之后才会触发 YGC，而幸存者区满了不会触发 YGC，但是触发 YGC 会将 Eden 区和幸存者区一起回收。 当一个对象经历了 15 次 Minor GC 之后，就会放到养老区。 可以设置参数：-XX:MaxTenuringThreshold=n 进行设置 当养老区的内存不足的时候，再次触发 GC：Major GC 进行养老区的内存清理。 如果养老区执行了 Major GC 之后依旧无法进行对象的保存，就会产生 OOM 异常。 总结： 针对幸存者 s0，s1 区的总结：复制之后有交换，谁空谁是 to。 关于垃圾回收：频繁在新生区收集，很少在养老区收集，几乎不在永久区/元空间收集。 特殊情况： 新创建的大对象在伊甸园区放不下，会尝试在老年区存放，老年区存放不下会 OOM。 存放老年代时，jdk8 会不动 Eden 区，直接把新对象放到 old 区。jdk7 会先把 eden 区的放到幸存者区，然后 eden 区放新创建的对象。 S 区存放来自伊甸园区的对象时，如果对象太大，可以直接放到老年区。 3.5.7 堆空间参数设置 -XX:+PrintFlagsInitial：查看所有参数的默认初始值。 -XX:+PrintFlagsFinal：查看所有参数的当前设置值（如果修改了默认值，显示的时候会以:=的形式显示，没有修改的没有:。 -Xms：初始堆空间内存 （默认大小为 物理内存空间/64）。 -Xmx：最大堆空间内存（默认大小为 物理内存空间/4）。 -Xmn：设置新生代的大小（初始值及最大值）。 -XX:NewRatio：配置新生代与老年代在堆结构的占比。 -XX:SurvivorRatio：设置新生代中 Eden 和 S0、S1 空间的比例。 -XX:MaxTenuringThreshold：设置新生代垃圾最大年龄。 -XX:+PrintGCDetails：输出详细的 GC 处理日志。 -XX:PrintGC或-verbose:gc：打印 GC 的简要信息。 -XX:HandlePromotionFailure：是否设置空间分配担保。 -XX:HandlePromotionFailure参数说明：在发生 Minor GC 之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总空间。 如果大于，则此此 Minor GC 是安全的。 如果小于，则虚拟机会查看-XX:HandlePromotionFailure 设置值是否允许担保失败： 如果 HandlePromotionFailure=true，那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代的对象的平均大小。 如果大于，则尝试进行一次 Minor GC，但这次 Minor GC 仍然是有风险的 如果小于，则改为一次 Full GC（新生代、老年代进行一次回收）。 如果 HandlePromotionFailure=false 则改为进行一次 Full GC。 在 JDK6 Update24（jdk7）之后，HandlePromotionFailure 参数不会再影响到虚拟机的空间分配担保策略（即默认为 true，且不能调整），虽然源码中定义了 HandlePromotionFailure 参数，但是在代码中，没有使用它。 3.5.8 逃逸分析 将堆上的对象分配到栈上，需要使用逃逸分析手段。这是一种可以有效减少 Java 程序中同步负载和内存堆分配压力的跨函数全局数据流的分析算法。通过逃逸分析，Java Hotspot 编译器能够分析出一个新的对象的引用的适用范围从而觉得是否要将这个对象分配到堆上。 逃逸分析的基本行为就是分析对象的动态作用域： 当一个对象在方法中被定义之后，对象只在方法内部使用（内部消亡），则认为没有发生逃逸。没有发生逃逸的对象，就可以分配到栈上，随着方法执行结束，栈空间就被移除。 当一个对象在方法中被定义后，它被外部的方法所引用，则认为发生逃逸，例如调用参数传递到其他方法中。 123456789101112public static StringBuilder method3() &#123; StringBuilder sb = new StringBuilder();//栈内存中线程不安全（sb未内部消亡） sb.append(&quot;a&quot;); sb.append(&quot;b&quot;); return sb;&#125;public static String method4() &#123; StringBuilder sb = new StringBuilder();//栈内存中线程安全（sb内部消亡，因为toString()它有返回值，method4方法的最终结果是toString()的返回结果） sb.append(&quot;a&quot;); sb.append(&quot;b&quot;); return sb.toString();&#125; 上述代码中，method3 中 sb 对象发生了逃逸，method4 中没有发生逃逸。 快速判断是否发生逃逸分析：new 的对象实体是否可以在方法外部调用，简而言之，只是在方法内部使用此对象则没有发生逃逸。 JDK6u23（jdk7）版本之后，Hotspot 中默认开启了逃逸分析 如果使用较早的版本 -XX:+DoEscapeAnalysis：显式开启逃逸分析。 -XX:+PrintEscapeAnalysis：查看逃逸分析筛选结果。 结论：开发中能使用局部变量的，就不要在方法外进行定义。 使用逃逸分析，编译器可以对代码做如下优化： 栈上分配。将堆分配转化为栈分配， 同步策略。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 分离对象或者标量替换。有的对象可能不需要作为一个联系的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存（堆中），而是存储在 CPU 寄存器中（栈中）。 栈上分配： JIT 编译器在编译期间根据逃逸分析的结果，发现如果一个对象并没有逃逸出方法的话，就可能被优化成栈上分配。分配完成之后，继续在调用栈内执行，最后线程结束，栈空间被回收，局部变量对象也被回收，这样就无需进行垃圾回收了。 常见的栈上分配场景： 给成员变量赋值、方法返回值、实例引用传递（引用成员变量）发生了逃逸，除此之外可视为没有发生逃逸，可以使用栈上分配。 同步策略 - 锁消除： 线程之间同步的代价是非常高的，同步的后果是降低并发性和性能。 在动态编译的同步块的时候，JIT 编译器可以借助逃逸分析来判断同步块所使用的锁对象是否只能被一个线程访问而没有被发布到其他线程。如果没有（只是当前线程在使用），那么 JIT 编译器在这个同步块的时候就会取消这部分代码的同步，这样就可以大大提高并发性和性能。这个取消同步的过程就叫同步省略，也叫锁消除。 分离对象或者标量替换： 标量（Scalar）是指一个无法在分解成更小的数据的数据，Java 中的原始数据类型就是标量。 相对的还可以分解的数据就叫做聚合量（Aggregate），Java 中的对象就是聚合量，因为可以分解为其他的聚合量和标量。 在 JIT 阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过 JIT 优化，就会把这个对象拆解成若干个成员变量来替代。这个过程就是标量替换。 标量替换可以大大减少堆内存的占用，因为不需要创建对象，那么就不再需要分配堆内存。 标量替换为栈上分配提供了很好的基础。 开启标量替换：-XX:+EliminateAllocations（默认打开）。 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; alloc(); &#125; public static void alloc() &#123; Point point = new Point(1, 23); System.out.println(point.x); System.out.println(point.y); &#125;&#125;class Point &#123; public int x; public int y; public Point(int x, int y) &#123; this.x = x; this.y = y; &#125;&#125; 可以看到，Point 这个聚合变量经过逃逸分析之后，发现他并没有逃逸，就可以被替换为两个聚合量了： 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; alloc(); &#125; public static void alloc() &#123; int x = 1; int y = 23; System.out.println(x); System.out.println(y); &#125;&#125;class Point &#123; public int x; public int y; public Point(int x, int y) &#123; this.x = x; this.y = y; &#125;&#125; 总结： 关于逃逸分析的论文在 1999 就发表了，但是在 JDK6 才实现，技术不是很成熟。 根本原因就是 无法保证逃逸分析的性能消耗一定能高于他的消耗，虽然经过逃逸分析可以做标量替、栈上分配、和锁消除，但是逃逸分析自身也需要进行一系列复杂的分析，也是一个相对耗时的过程。 虽然技术不是很成熟，但是他是即时编译器优化技术中一个十分重要的手段。 3.5.9 垃圾回收策略 3.5.9.1 Minor GC、Major GC 和 Full GC JVM 在进行 GC 的时候，并非每次都对新生代、老年代、方法区三个内存区域一起回收，大部分的时候回收都是指新生代。 针对 Hotspot VM 的实现，它里面的 GC 按照回收区域又分为 2 种类型：一种是部分收集（Partial GC），一种是整堆收集（Full GC）： 部分收集：不是完整的整个 Java 堆收集。 新生代收集（Minor GC 完全等价于 YGC）：只是在新生代的垃圾收集。 老年代收集（Major GC/ Old GC）：只是对老年代的垃圾收集。 目前只有 CMS GC 会有单独收集老年代的行为。 注意很多时候 Major GC 和 Full GC 混淆使用，需要具体判别老年代回收还是整堆回收。 混合收集（Mixed GC）：收集整个新生代和部分老年代的垃圾收集。 目前 只有 G1 GC 会有这种行为。 整堆收集（Full GC）：收集整个 Java 堆和方法区的垃圾收集。 3.5.9.2 年轻代 GC（Minor GC/YGC）的触发机制 当年轻代空间不足的时候，就会触发 Minor GC，这里的年轻代满指的是 Eden 区满，而不是 S0 或者 S1 满。Servivor 满不会触发 Minor GC（但是每次 Minor GC 会清理年轻代的内存，包括 Eden/S0/S1） 因为 Java 对象大多都具备朝生夕死的特性，所以 Minor GC 非常频繁，一般回收速度也很快。 Minor GC 会引发 STW（Stop The World），暂停其他用户线程，等待垃圾回收结束，用户线程才恢复执行。 3.5.9.3 老年代 GC（Major GC/Full GC）的触发机制 指发生在老年代的 GC，对象从老年代消失的时候，Major GC 或 Full GC 就发生了。 出现了 Major GC ，经常会伴随至少一次的 Minor GC（但是非绝对的，在 Parallel Scavenge 收集器的收集策略里就有直接进行 Major GC 的策略选择过程） 也就是在老年代空间不足的时候，会先尝试触发 Minor GC。如果之后空间还不足，则触发 Major GC。 Major GC 的速度会比 Minor GC 慢十倍一样，STW 的时间更长。 如果 Major GC 之后，内存还不足，就报 OOM。 3.5.9.4 Full GC 触发机制 触发 Full GC 的情况有有下面五种 调用 System.gc()的时候，系统建议执行 Full GC，但是不是必然执行的。 老年代空间不足。 方法区空间不足。 通过 Minor GC 后进入老年代的平均大小大于老年代的可用内存。 由 Eden 区、Survivor space0（From Space）区向 Survivor space1（To Space）区复制的时候，对象大小大于 To Space 的可用内存，则把该对象转存到老年代。且老年代 的可用内存小于该对象大小。 说明：Full GC 是开发或者调优种尽量避免的，这样暂停的时间会短一些。 3.6 方法区（元空间） 3.6.1 栈、堆、方法区之间的交互关系 3.6.2 方法区的理解 《Java 虚拟机规范》中明确说明：尽管所有的方法区在逻辑上是属于堆的一部分，但是一些简单的实现可能不会选择区进行垃圾收集或者压缩。但是对于 HotSpot 虚拟机而言，方法区还有个名字叫 Non-Heap（非堆），目的就是要和堆分开。所以，方法区可以看作是一块独立于 Java 堆的内存空间。 方法区（Method Area）和堆一样，是各个线程共享的内存区域。 方法区在 JVM 启动的时候被创建，并且它的实际物理内存空间中和 Java 堆区都一样可以是不连续的。 方法区的大小和堆空间一样，可以选择固定大小或者可拓展。 方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类，导致方法区溢出，虚拟机就会抛出内存溢出错误：java.lang.OutOfMemory:PermGen space（jdk7 之前）或者java.lang.OutOfMemory:Metaspace 如加载了大量的第三方 jar 包。或者 Tomcat 部署的工程过多。大量动态生成反射类。 关闭 JVM 就会释放这个区域的内存。 3.6.3 Hostpot 中方法区的演进 Hostpot 虚拟机中，在 JDK7 以及之前，习惯上把方法区称为永久代。在 JDK8 以及以后，使用元空间取代了永久代。本质上方法区和永久代并不等价，仅仅只是对 Hotspot 而言的。《Java 虚拟机规范》对如何实现方法区不做统一要求。 现在看来，当年使用永久代，不是好的 idea，导致 Java 程序更容易 OOM（超过 -XX:MaxPermSize 上限）。 JDK8 完全废弃了永久代的概念，改用和 JRockit、J9 一样在本地内存中实现的元空间（Metaspace）： 元空间的本质和永久代类似，都是 JVM 规范中方法区的体现，不过元空间与永久代最大区别就是：元空间不在虚拟机设置的内容中，而是使用本地内存。 永久代、元空间不仅仅是名字改了，内部结构也进行了调整。 根据《Java 虚拟机规范》的规定，如果方法区无法满足新的内存分配需要的时候，将抛出 OOM 异常。 Hotspot 中方法区的变化： JDK1.6 之前用永久代（permanent generation） 静态变量存放在永久代上。 JDK1.7 有永久代，但是已经逐步“去永久代”，字符串常量池、静态变量移除，保存在堆中。 JDK1.8 之后 无永久代。类型信息、字段、方法、常量保存在本地内存的元空间，但是字符串常量池、静态变量仍在堆。 上述图中静态变量指的是变量引用，参考 3.6.6.5。 永久代为什么要被元空间替换： 直接原因是要整合 JRockit 虚拟机，JRockit 没有永久代。 永久代的设置空间大小时很难确定的。 如 web 工程，运行中需要动态加载很多类，如果加载动态类过多，很容易导致永久代内存满（java.lang.OutOfMemoryException:PerGen)。 元空间和永久代的区别就是：元空间不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅仅受本地内存限制。 对永久代进行调优时很困难的。 StringTable 为什么需要调整： JDK7 中将 StringTable 放到了堆空间中。因为永久代的回收效率很低，在 full GC 才会触发。而 full GC 在老年代不足、永久代不足才会触发。这就导致 StringTable 回收效率不高，而在开发中会有大量的字符串被创建，回收效率低，导致永久代内存不足。放到堆里，能及时回收内存。 静态引用的对象实体始终都存在堆空间的老年代。引用名 jdk1.6 在永久代，jdk1.7 在堆，jdk1.8 在元空间。 staticObj（的引用）随着 Test 类信息的加载存放在方法区。 instanceObj（的引用）随着 Test 对象的实例存放在 Java 堆区。 localObj（的引用）存放在 foo()方法栈帧的局部变量表中。 但三者的对象实例均存放在堆中。 3.6.3.1 直接内存（本地内存） 直接内存不是虚拟机运行时数据区的一部分，也不是《Java 虚拟机规范》中定义的内存区域。直接内存是在 Java 堆外的、直接向系统申请的内存空间。 来源于 NIO，通过 DirectByteBuffer 可以直接操作 Native 内存。 通常访问直接内存的速度高于 Java 堆，也就是读写性能更高。 因此读写频繁的场合可能会考虑使用直接内存。 Java 的 NIO 库允许 Java 程序使用直接内存，用户数据缓冲区。 注意点： 也可能导致 OutOfMemoryError 异常 由于直接内存存在在 Java 堆外，因此它的大小不会直接受限于-Xmx 指定的最大堆大小，但是系统内存是有限的，Java 堆和直接内存的总和依旧受限于操作系统能给出的操作内存 缺点： 分配回收成本高 不受 JVM 内存回收管理 直接内存的大可以通过 MaxDirectMemorySize 设置 如果不指定，默认与堆的最大值-Xmx 参数保持一致。 简单理解：Java Process Memory = Java Heap + native Memory 3.6.4 设置方法区的大小及 OOM 方法区的大小不必是固定的，JVM 可以根据应用的需要动态调整： JDK7 以及以前： -XX:PermSize：设置永久代的初始分配空间，默认值是 20.75M。 -XX:MaxPermSize：来设定永久代的最大可分配空间，32 位机器默认时 64M，64 位机器默认是 82M。 当 JVM 加载的类的信息容量超过了这个值，就会抛出异常 java.lang.OutOfMemoryError:PermGen space。 JDK8 以及以后： 元空间大小可以使用参数-XX:MetaspaceSize 和-XX:MaxMetaspaceSize 指定。 默认值依赖于平台。windows 下-XX:MetaspaceSize是 21M，-XX:MaxMetaspaceSize的值是-1，即没有限制。 与永久代不同，如果不指定大小，默认情况下，虚拟机会耗尽所有的可用系统内存，如果元数据区发生溢出，虚拟机一样会抛出异常 OutOfMemoryError:Metaspace。 -XX:MetaspaceSize 设置初始值的大小。对于 64 位服务端的 JVM 来说，其默认的 -XX:MetaspaceSize 值为 21 M，这就是最高水线。一旦触及这个水位线，Full GC 将会被触发并卸载没用的类。然后这个高水位线会被重置。新的水位线取决于 GC 之后释放了多少空间。如果释放过多，就适当降低该值，如果释放过低，就提升该值，前提是不超过 -XX:MaxMetaspaceSize 如果初始化的高为太低，上述的调整就会发生很多次，可以发现执行了多次 Full GC，建议将 -XX:MetaspaceSize 设置一个较高的值 3.6.5 元空间 OOM 演示： 1234567891011121314151617181920212223242526272829303132/** * JDK8 * VM:-XX:MetaspaceSize=10M -XX:MaxMetaspaceSize=10M */public class OomTest extends ClassLoader &#123; public static void main(String[] args) &#123; int j = 0; try &#123; OomTest oomTest = new OomTest(); for (int i = 0; i &lt; 10000; i++) &#123; // 用于生成类的二进制字节码 ClassWriter classWriter = new ClassWriter(0); // 指明版本号 // 指明访问权限 // 类的名字 // 包名 // 父类 // 接口 classWriter.visit(Opcodes.V1_8, Opcodes.ACC_PUBLIC, &quot;Class&quot; + i, null, &quot;java/lang/Object&quot;, null); // 返回 byte[] byte[] bytes = classWriter.toByteArray(); // 类的加载 oomTest.defineClass(&quot;Class&quot; + i, bytes, 0, bytes.length); j++; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(j); &#125; &#125;&#125; 解决 OOM: 要解决 OOM 异常或者 heap space 异常，一般的手段是首先通过内存映射分析工具如（Eclipse Memory Analyzer）对 dump 出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是先分清楚到底是出现了内存泄漏（Memory Leak）还是内存溢出（Memory Overflow）。 如果只是内存泄漏，可以进一步通过工具检查泄漏对象到 GC Roots 的引用链。于是就能找到泄漏对象是通过怎样的路径与 GC Roots 相关联并导致垃圾回收器无法自动回收，掌握了泄漏对象的类型信息，以及 GC Roots 的引用链信息，就可以比较准确的定位出泄漏代码的位置。 如果不存在内存泄漏，也就是说内存中的对象必须要存活，那就应该检查虚拟机的堆参数 （-Xms -Xmx）与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象的生命周期过长、持有状态时间过长、尝试减少运行期的内存消耗。 3.6.6 方法区的存储内容 《深入理解 Java 虚拟机》书种对方法区（Method Area）存储的内容描述如下：它用于存储已经被虚拟机加载的类型信息、常量、域信息、静态变量、即时编译器编译（JIT）之后的代码缓存等。 3.6.6.1 类型信息 类 Class、接口 interface 、枚举 enum、注解 annotation，JVM 必须在方法区存储以下类型信息。 这个类型的完整有效名称（全名=包名.类名）。 这个类型的直接父类的完整有效名（对于 interface 或是 java.lang.Object）都没有父类。 这个类型的修饰符（public abstract，final 的某个子集）。 这个类型直接接口的一个有序列表。 3.6.6.2 运行时常量池 要弄清楚方法区，需要理解清楚 ClassFile，因为加载类的信息都在方法区。而要弄清楚方法区的运行时常量池，需要理解清楚 ClassFile 中的常量池。 一个有效的字节码文件除了包含类的版本信息、字段、方法以及接口等描述信息外，还包含一项信息那就是常量池表（Constant Pool Table） ，包括了各种字面量和对类型、域、方法的符号引用等。 一个 Java 源文件中的类、接口、编译后会产生一个字节码文件，而 Java 中的字节码需要数据支持，通常这种数据类型会很大以至于不能直接存到字节码里面，换另一种方式，可以存到常量池，这个字节码包含了指向常量池的引用，在动态链接的时候会用到运行时常量池。 常量池中的内容： 数值 字符串值 类引用 字段引用 方法引用 小结：常量池可以看作是一张表。虚拟机指令根据这张常量表找到要执行的类名、方法名、参数类型、字面量等类型。 常量池表（Constant Pool Teble）是 Class 文件的一部分，用于存放编译器生成的各种字面量与符号引用，这部分内容将在类加载之后存放在方法区的运行时常量池中。 JVM 会为每个已经加载的类型（类和接口）都维护一个常量池，池中的数据项像数组项一样，是通过索引访问的。 运行时常量池中包含多种不同的常量，包括编译器就已经明确的数值字面量，也包括运行期解析后才能够获得的方法或者字段引用。此时不再是常量池中的符号地址了，这里换为真实地址。 运行时常量池，相较于 Class 文件常量池的另一重要特性：具备动态性。 运行时常量池类似于传统编程语言的符号表（Symbol table），但是他所包含的数据却比符号表更加丰富一些。 当创建类或者接口的时候的运行时常量池的时候，如果构造运行时常量池所需的内存空间超过了方法区所能提供的最大值，则 JVM 会抛出 OutOfMemeoryError 异常。 3.6.6.3 域（Field）信息 JVM 必须在方法区中保存类型的所有域相关的信息以及域的声明顺序。 域名称、域类型、域修饰符（public private protected static final volatile transient 的某个子集）。 3.6.6.4 方法（Method）的信息 JVM 必须保存所有方法的以下信息，同域信息一样包括声明顺序。 方法名称。 方法的返回类型（或者 void）。 方法参数的数量和类型（按顺序）。 方法的修饰符 （public private protected static final synchronized native abstract 的某个子集）。 方法的字节码（bytecodes）、操作数栈、局部变量表以及大小（abstract 和 native 方法除外）。 异常表（abstract 和 native 方法除外） 每个异常处理的开始位置、结束位置、代码处理在程序计数器中的偏移地址、被捕获的异常类的常量池索引。 3.6.6.5 non-final 的类变量（静态变量） 静态变量和类关联在一起，随着类的加载和而加载它们成为类数据在逻辑上的一部分。 类变量被类的所有实例共享，即使没有类实例的时候也可以访问它。 被声明为 final 的类变量的处理方式不同，其在编译器就会被分配了值。 3.6.7 方法区的垃圾回收 《Java 虚拟机规范》对于是否进行方法区的垃圾回收没有进行约束，如 Java11 中的 ZGC 收集器就不支持类卸载。 方法区的垃圾收集主要回收两部分内容：常量池中废弃的常量和不再使用的类型。 常量回收： 方法区常量池主要存放两大类常量：字面量和符号引用。字面量比较的是比较接近 Java 语言的常量概念，如文本字符串，被声明为 final 的常量值等。而符号引用则属于编译原理方面的概念，包括下面这三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 Hotspot 虚拟机对常量池的回收策略是很明确的，只要常量池中的常量没有被任何地方引用，就可以被回收。 回收废弃常量与回收 Java 堆中的对象非常相似。 不再使用的类型回收： 不再使用的类型需要同时满足三个条件： 该类所有的实例都已经被回收（堆内存中已被回收） 加载该类的类加载器已经被回收 该类对应的 Class 类对象没有任何在任何地方被引用（无法通过反射访问该类）。 上述三个条件的满足条件非常苛刻，同时在大量使用反射、动态代理、CGLIB 等字节码框架，动态生成的 jsp 以及 osgi 的这类频繁自定义类加载器的场景中，通常需要 Java 虚拟机具备类型卸载能力，以保证不会对方法区造成过大的内存压力。 3.7 本地方法栈（Native Method Stack） Java 虚拟机用于管理 Java 方法的调用，而本地方法栈用于管理本地方法的调用。 线程私有。 允许被实现固定或者是可拓展的内存大小（在内存溢出方面与虚拟机栈是相同的）： 如果线程请求分配的栈容量超过本地方法栈的最大容量，Java 虚拟机会抛出一个 StackOverflowError 异常。 如果可以动态拓展，那么在拓展到无法申请到足够的内存的时候或者在创建新的线程没有足够的内存的时候，就会抛出 OutOfMemaryError 异常。 Native Method Stack 中登记 native 方法，在 Execution Engine 执行的时候加载本地方法库。 当某个线程调用一个本地方法的时候，就进入了一个全新的并且不再受虚拟机限制的世界，他和虚拟机有相同的权限。 本地方法可以通过本地方法接口访问虚拟机内部的运行时数据区。 可以直接使用本地处理器中的寄存器。 可以直接从本地内存的堆中分配任意数量的内存。 并不是所有的 JVM 都支持本地方法，因为 Java 虚拟机规范并没有明确要求本地方法栈的使用语言、具体实现方式、数据结构等，如果 JVM 产品不打算支持 native 方法，也可以无需实现本地方法栈。 在 Hotspot JVM 中直接将本地方法栈和虚拟机栈合二为一。 3.8 常见面试题 3.8.1 虚拟机栈（栈） 虚拟机栈：栈中可能出现的异常。 Java 虚拟机规范允许 Java 栈的大小是动态的或者是固定不变的。 如果采用固定大小的 Java 虚拟机栈，那每一个线程的 Java 虚拟机栈容量可以在线程创建的时候独立选定，如果请求超过虚拟机栈的最大容量，Java 虚拟机将会抛出 StackOverflowError 异常。 如果 Java 虚拟机栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程的时候，没有足够的内存去创建虚拟机栈。那么 Java 虚拟机将会抛出 OutOfMemoryError 异常。 举例栈溢出的情况： 递归调用深度太深 （StackOverflowError） 通过 -Xss 设设置大小 整个内存不足了就会出现 OOM 问题 调整栈的大小，就能保证不出现溢出吗？ 不一定，如死循环只能延迟栈溢出的时间。 但有可能可以解决栈内存溢出问题。 分配的栈内存越大越好吗？ 并不是，内存空间是有限的，栈内存变大了，其他的空间就少了。 垃圾回收是否会涉及到虚拟机栈？ 不会。垃圾回收只涉及堆空间和方法区。 方法中定义的局部变量是否线程安全？ 具体问题具体分析。 123456789101112131415161718192021222324252627282930//内部产生，内部消亡的，就是安全的，否则就是不安全的public class StringBuilderTest &#123; public static void method1() &#123; StringBuilder sb = new StringBuilder();//线程安全（sb内部消亡） sb.append(&quot;a&quot;); sb.append(&quot;b&quot;); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(sb.toString()); &#125; public static void method2(StringBuilder sb) &#123;//线程不安全（sb外部产生） sb.append(&quot;a&quot;); sb.append(&quot;b&quot;); &#125; public static StringBuilder method3() &#123; StringBuilder sb = new StringBuilder();//线程不安全（sb未内部消亡） sb.append(&quot;a&quot;); sb.append(&quot;b&quot;); return sb; &#125; public static String method4() &#123; StringBuilder sb = new StringBuilder();//线程安全（sb内部消亡，因为toString()它有返回值，method4方法的最终结果是toString()的返回结果） sb.append(&quot;a&quot;); sb.append(&quot;b&quot;); return sb.toString(); &#125;&#125; 3.8.2 堆空间 堆是分配对象存储的唯一选择吗？ 不是。在《深入理解 Java 虚拟机》中关于 Java 堆内存中有这样的一段描述： 随着 JIT 编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上就渐渐变得不是那么绝对了。 在 Java 虚拟机中。对象是在 Java 堆中分配内存的，这是一个普遍的常识。但是有一种特殊情况，如果经过逃逸分析（Escape Analysis）后发现，一个对象并没有逃逸出方法的话，那么就有可能被优化成栈上分配，这样就无需再堆上分配内存，也无需进行垃圾回收了，这是最常见的对外存储技术。 此外，前面提到的基于 OpenJDK 深度定制的 TaoBaoVM，其中创新的 GCIH（GC Invisible heap）技术实现 off-heap，将生命周期比较长的 Java 对象从 heap 移到 heap 外，并且 GC 不能管理 GCIH 内部的对象，以此达到降低 GC 的回收频率和提升 GC 的回收效率的目的。 但是，由于逃逸分析并不成熟，且逃逸分析自身需要进行一系列复杂分析，此过程也相对耗时，所以可以认为对象实例是分配到堆上的。 而且 jdk7 之后，intern 字符串的缓存和静态变量也是分配到堆上的，也符合对象实例分配在堆上。 jdk7 之前，intern 字符串的缓存和静态变量分配在永久代上。 JVM 内存模型，有哪些区？分别是干嘛的？ Java8 的内存分代改进 JVM 内存分哪几个区域？每个区域的作用是什么 JVM 的内存分布/内存结构？栈和堆的区别？堆的结构？为什么需要两个 Survivor 区 Eden 和 Survivor 的比例分配 JVM 内存分区，为什么要有新生代和老年代？新生代和老年代为什么要分为 Eden 和 Survivor JVM 的运行时数据区 什么时候对象会进入老年代 jvm 永久代中会发生垃圾收集吗？Java 规范并不约束， 3.9 本章总结 stack 是虚拟机栈，stack frame 是栈帧。 第 4 章 本地方法接口 本地方法接口： 简单的来讲，一个 Native Method 就是一个 Java 调用非 Java 代码的接口。 Native Method 的实现由非 Java 语言实现，比如 C。这个特征并非 Java 所特有，如 C可以使用 extern “c” 告知 C编译器去调用以个 C 的函数。 native method 的定义像 Java Interface，Java 中并不提供实现体，因为其实现体是由其他语言在外面实现的。 本地接口的作用是融合不同的编程语言为 Java 所用，它的初衷是融合 C/C++程序。 native 关键字可以与除了 abstract 外的关键字公用（因为 abstract 修饰说明不能有方法体，但是 native 是有方法体的，不过是别的语言实现的）。 为什么需要使用 Native Method：有些层次的任务用 Java 实现起来不容易，或者对程序的效率很在意时： 与 Java 环境外交互（主要原因）： 本地方法提供了一种交流机制，为程序员提供了一个非常简洁的接口，而无需去了解 Java 应用之外的繁琐的细节。 与操作系统交互（执行效率）： JVM 不是一个完整的系统，它经常依赖于一些底层系统的支持，如操作系统。通过使用本地方法，就可以用 Java 实现 jre 与底层系统的交互，甚至 JVM 的一部分就是使用 C 写的。还有，如果要使用一些 Java 语言本身没有提供封装的操作系统的特性的时候，也需要使用本地方法。 Sun’s Java： Sun 的解释器使用 C 实现的，这使得它能像一些普通的 C 一样与外界交互。 jre 大部分是用 Java 实现的，它也通过一些本地方法与外界交互。 例如：类 java.lang.Thread 的 setPriority()方法使用 Java 实现的，但是调用的是 setPriority0(),这个是本地方法 ，C 实现的，被植入 JVM 内部。 现状： 目前该方法使用的越来越少了，除非是与硬件相关的应应用，比如通过 Java 程序驱动打印机或者 Java 系统管理生产设备，在企业级应用很少见。 第 5 章 执行引擎 5.1 执行引擎的作用和工作流程 虚拟机与物理机的区别是物理机的执行引擎是建立在处理器、缓存、指令集和操作系统层面上的，而虚拟机的执行引擎则是由软件自行实现的，因此可以不受物理条件的制约指令集和执行引擎的结构体系，能够执行哪些不被硬件直接支持的指令集格式。 JVM 的主要任务是负责 装载字节码到其内部，但是字节码并不能够直接运行在操作系统之上，因为字节码指令并非等价于本地机器指令，它内部包含的仅仅只是一些能够被 JVM 所识别的字节码指令、符号表、以及其他辅助信息。 执行引擎（Execution Engine）的任务就是 将字节码指令解释/编译为对应平台的本地机器指令才可以，简单来说 JVM 的执行引擎充当了高级语言翻译为机器语言的译者。 每当执行完一项指令之后，PC 寄存器就会更新下一条需要被执行的指令的地址，执行引擎在操作数栈中找到具体的指令，翻译成机器指令。 方法在执行的过程中，执行引擎可能通过存储在局部变量表中的对象引用准确定位到存储在 Java 堆区中的对象的实例信息，以及通过对象头中的元数据指针定位到目标对象的类型信息。 从外观上来看，所有的 Java 虚拟机的执行引擎的输入、输出都是一致的：输入是字节码二进制流，处理过程是字节码解析执行的等效流程，输出的是执行结果。 5.3 高级语言执行过程 指令： 由于机器码是由 0 和 1 组成的二进制序列，可读性实在太差，于是人们发明了指令 指令就是把机器码中特定的 0 和 1 序列，简化成相应的指令（一般使用英文简写，如 mov，inc 等），可读性稍好。 由于不同的硬件平台，执行同一个操作，对应的机器码可能不同，所以不同的硬件平台的同一种指令（如 mov）对应的机器码也不同。 指令集： 不同的硬件平台，各自支持的指令，是有差别的。因此每个平台所支持的指令，称之为对应平台的指令集： x86 指令集，对应的是 x86 架构的平台 ARM 指令集，对应的是 ARM 架构的平台 机器码： 各种用二进制编码的方式表示的指令，叫做机器指令码，用它编写的程序就是机器语言。 机器语言虽然能够被计算机理解和接受，但是和人们的语言差别太大，不容易被人们理解和记忆，并且用它编程容易出差错。 用它编写的程序一经输入到计算机，CPU 直接读取执行，因此和其他语言编写的程序相比，执行速度最快。 机器指令与 CPU 紧密相关，所以不同种类的 CPU 所对应的机器指令也就不同。 汇编语言： 由于机器指令的可读性还是太差，于是人们有发明了汇编语言。 在汇编语言中，用助记符（Mnemonics）代替机器指令的操作码，用地址符号（Symbol）或者标号（Label）代替指令操作数的地址。 在不同的硬件平台，汇编语言对应着不同的机器语言指令集，通过汇编过程转成机器指令。 由于计算机只认识指令码，所以用汇编语言编写的程序还必须翻译成机器指令码，计算机才能识别和执行。 高级语言： 为了使得计算机用户编程序更容易一些，后来就出现了各种高级计算机语言，高级语言比机器语言、汇编语言更接近人的语言。 当计算机执行高级语言编写程序的时候，仍然需要将程序解释和编译成机器的指令码。完成这个过程的程序就叫做解释程序或者编译程序。 字节码： 字节码是一种中间状态（中间码）的二进制代码（文件），它比机器码更抽象，需要直译器转译之后才能成为机器码。 字节码主要是为了实现特定软件运行和软件环境、与硬件环境无关。 字节码的实现方式是通过编译器和虚拟机器，编译器将源码编译成字节码，特定平台上的虚拟机器将字节码转译为可以执行的命令。 5.3 Java 程序的编译运行和解释运行 5.3.1 执行过程总览 上图中最下蓝色行为编译原理的过程。 大部分的程序代码转换成物理机的目标代码或者虚拟机可以执行的指令集之前，都需要经过上图的各个步骤。 解释器：当 Java 虚拟机启动的时候会根据预定义的规范 对字节码采用逐行解释的方式执行 ，将每条字节码文件中的内容&quot;翻译&quot;为对应平台的本地机器指令执行。 JIT(Just In Time Compiler)编译器：将源代码直接编译成和本地机器平台相关的机器语言。 在 JVM 执行 Java 源码的时候，通常解释执行和编译执行二者结合起来执行，所以说 Java 语言式半解释半编译的运行。 5.3.2 解释器 解释器真正意义上所承担的角色就是一个运行时“翻译者”，将字节码文件中的内容“翻译”为对应平台的本地机器指令执行。 当一条字节码指令被解释执行完成之后，接着再根据 PC 寄存器中记录的下一条需要被执行的字节码指令执行解释操作。 在 Java 发展历史里，一共有两套解释执行器，即古老的字节码解释器、现在普遍使用的模板解释器： 字节码解释器：在执行时通过纯软件代码模拟字节码的执行，效率非常地下。 模板解释器：将每一条字节码和一个模板函数相关联，模板函数中直接产生这条字节码执行的机器码，从而很大程度上提高了解释器的性能。 在 Hotspot VM 中，解释器主要由 Interpreter 模块和 Code 模块构成： Interpreter 模块：实现了解释器的核心功能。 Code 模块：用于管理 Hotspot VM 在运行时生成的本地机器指令。 由于解释器在设计上和实现上非常简单，因此除了 Java 之外，其他语言也基于解释器执行。但是今天基于解释器执行已经沦落为低效的代名词。 为了解决这个问题，JVM 平台支持一种即时编译的技术。即时编译的目的是避免函数被解释执行，而是将整个函数体编译成机器码，每次该函数执行的时候，只执行编译后的机器码即可，这种方式可以使得执行效率大幅度提升。 5.3.3 JIT 编译器 Hotspot 虚拟机是采用解释器和即时编译器并存的架构，在其运行的时候，会找到解释器和即时编译器相互合作的节点。各自取长补短。 既然 Hotspot VM 中已经内置了 JIT 编译器了，那么为啥还需要再使用解释器来“拖累”程序的执行性能呢？比如 JRrockit VM 就不包含解释器，字节码全部依靠即时编译后执行 - 所以它号称最快的虚拟机 当程序启动的时候，解释器可以马上发挥作用，省去编译的时间，响应速度快。编译器想要发挥作用，需要把代码编译为本地代码，会消耗一定的时间。但是编译为本地代码后，执行效率高。 所以 JRockit 虚拟机执行性能很高，但是在启动的时候会会花费更多的时间。 Hotspot 虚拟机解释器与 JIT 编译器二者皆用，当虚拟机启动的时候，解释器首先发生作用，不必等待即时编译器全部编译完再执行，这样可以省去不必要的编译时间。随着时间的推移，越来越多的热点代码被编译为机器指令代码，所以程序执行会越来越快。 同时，如果遇到编译器不能编译成机器指令的时候，还可以使用解释器作为后备方案。 Java 语言的编译期其实是一段不确定的操作过程，因为它可能是指一个前端编译器（也叫编译器的前端）把.java文件转换成.class文件的过程；也可能是指后端运行编译器（JIT 编译器 Just In Time Compiler） 把字节码转换为机器码的过程；还可能是使用静态提前编译器（AOT 编译器 Ahead Of Time Compiler）直接把.java 文件编译成本地机器码的过程。 5.3.4 热点代码及探测方式 是否需要启动 JIT 编译器将字节码直接编译为对应平台的本地机器指令，则需要根据代码被调用执行的频率 而定。这样的被编译为本地代码的字节码，称为热点代码。 JIT 编译器在运行时会针对那些被频繁调用的“热点代码”做出深度优化，将其直接编译为对应平台的本地机器指令，以提升 Java 程序的执行性能。 一个被多次调用的方法，或者是一个方法体内部循环次数比较多的循环体都可以称为是热点代码，因此都可以通过 JIT 编译器翻译为本地机器指令。由于这种编译方式发生在方法的执行过程中，因此也被称为栈上替换。或者简称为 OSR（On Stack Replacement）。 Hotspot VM 采用的热点探测方式是基于计数器的热点探测。Hotspot VM 为每一个方法都建立 2 个不同类型的计数器，分别为方法调用计数器，分别为方法调用计数器（Invocation Counter）和回边计数器（Back Edge Counter）。 方法调用计数器： 用于统计方法被调用的次数，默认阈值在 Client 模式下是 1500 次，在 Server 模式下是 10000 次。这个阈值可以通过虚拟机参数 -XX:ComplieThreshold 来设定。 当一个方法被调用的时候，会先检查该方法是否存在被 JIT 编译过的版本，如果存在，则优先使用编译后的本地代码来执行，如果不存在已经被编译的版本，则将此方法的调用计数器值加 1，然后判断方法调用计数器和回边调用计数器之和是否超过方法调用计数器的阈值，如果超过，就向编译器提交一个该方法的编译请求。 回边计数器 统计方法中循环体代码的执行次数，建立回边计数器统计就是为了触发 OSR 编译。 热度衰减： 随着程序运行时间加长，方法调用次数肯定会达到阈值，但实际上程序的运行环境和运行要求并没有发生变化。所以方法调用计数器是一个相对值，即一段时间之内方法被调用的次数。 当超过一定的时间限制，调用次数仍未达到阈值，则该方法的调用计数器就会减少一半，这个过程成为方法调用计数器热度的衰减（Counter Decay），而这段时间就称为此方法统计的半衰周期（Counter Half Life Time）。 进行热度衰减的动作是在虚拟机内部进行垃圾收集的时候顺便执行的，可以使用虚拟机参数-XX:-UseCouinterDecay 关闭，使用-XX:CounterHalfLifeTime 参数可以设置半衰周期的时间，单位是秒。 5.4 Hostpot VM 5.4.1 Hostpot VM 设置程序的执行方式 缺省情况下 Hotspot 虚拟机使用解释器与编译器并存的架构，也可以对其进行调整，设置为完全采用解释器或者完全采用即时编译器 -Xint:完全采用解释器模式执行程序 -Xcomp:完全采用即时编译器模式执行程序，如果编译出现问题，解释器会介入 -Xmixed:采用解释器+即时编译器混合模式执行程序 命令行修改： 虚拟机运行参数修改（针对当前的代码）： 5.4.2 Hostpot VM 中 JIT 分类 在 Hotspot VM 内嵌两个 JIT 编译器，分别为 Client/ Server Compiler，但大多数情况下称之为 C1 编译器和 C2 编译器，可以使用命令行指令或参数设置使用哪一种编译器（64 位的 JDK 不支持切换，只能使用 server 版本）。 -clien：C1 编译器会对字节码进行简单和可靠的优化，耗时短，以达到更快的编译速度。 -server C2 编译器进行耗时较长的优化，以及激进优化。但是优化的代码执行效率高。 C1 编译器优化策略： 方法内联：将引用的函数代码编译到引用点处，这样就可减少栈帧的生成，减少参数的传递和跳转过程。 去虚拟化：对唯一的实现类进行内联。 冗余消除：在运行期间会把一些不会执行的代码折叠掉。 C2 的优化主要是在全局层面，逃逸分析是优化的基础。基于逃逸分析在 C2 上有如下几种优化： 标量替换：用标量替换聚合变量。 栈上分配：对于为逃逸的对象分配对象的在栈而不是在堆。 同步消除：清除同步动作，通常指的是 synchronized。 分层编译（Tiered Compiler）策略：程序解释执行（不开启性能监控）可以触发 C1，将字节码编译成机器码，可以进行简单优化，也可加上性能优化，C2 编译会根据性能监控信息进行激进优化。分层策略也就是说 C1、C2 编译器并没有只是用一个。 不过在 Java7 版本之后，一旦显式使用&quot;-server&quot;命令时，默认使用 c1 和 c2 共同执行。 总结： JIT 编译器出来的机器码性能高于解释器解释执行的代码。 C2 编译器启动时长比 C1 编译器长，系统稳定执行后，C2 编译器执行速度远快于 C1 编译器。 5.4.3 Graal 编译器与 AOT 编译器 Graal 编译器（相比对 C2 编译器）：在 JDK10 之后，Hotspot 加入了一个全新的即时编译器：Graal 编译器，编译效果可以追平 C2 编译器。目前还在实验阶段，需要激活才能使用： -XX:+UnlockExperimentalVMOptions -XX:+UseJVMCICompiler AOT 编译器（相比于 JIT 编译器）：JDK9.0 之后加入了 AOT 编译器（静态提前编译器 Ahead Of Time Compiler），同时引入了实验性的 AOT 编译工具 jaotc ，它借助了 Graal 编译器，将所输入的 Java 类文件转换为机器码，并存放到生成的动态共享库中。 AOT 编译器是与即时编译器对立的一个概念，即时编译器在程序运行的时候，而 AOT 编译器是在程序运行之前。 最大好处：可以直接执行，不必等程序预热，减少第一次运行慢的体验。 缺点： 破坏了 Java”一次编译，到处运行“，因为直接编译之后的文件是固定的，必须为每种系统都编译。 降低了 Java 动态链接的动态性，加载的代码在编译器期就必须全部已知。 还需要继续优化中，最初只支持 Linux x64 java base。 5.5 垃圾回收器 参加本文第 8 章垃圾回收器 第 6 章 对象的实例化内存布局与访问定位 6.1 对象的实例化 对象的创建方式及创建过程： 判断对象的对应的类是否加载、链接、初始化： 虚拟机遇到一条 new 指令，首先要去检查这个指令的参数能否在 Metaspace 的常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已经被加载、解析、和初始化（即判断类元信息是否存在），如果没有，那么在双亲委派机制模式下，使用当前类加载器以及 ClassLoader+全限定名为 key 查找对应的.class文件，如果没有找到文件，就抛出 ClassNotFoundException 异常。如何找到，则进行类加载，并生成对应的 Class 类对象。 为对象分配内存： 首先区计算对象占用空间的大小，接着在内存中划分一块内存给新的对象，如果实例成员变量是引用变量，仅仅分配引用变量空间即可，也就是 4 个字节大小。 如果内存规整 指针碰撞 如果内存是规整的，那么虚拟机将采用指针碰撞法（Bump The Pointer）来为对象分配内存 意思是所有用过的内存在一边，空闲的内存在另一边，中间放着一个指针作为分界点的指示器，分配内存就仅仅是把指针向空闲的那边挪动一段与对象大小相等的举例罢了，如果垃圾回收器选择的是 Serial、ParNew 这种基于压缩算法的，虚拟机采用这种分配方式。一般使用带有 compact（整理）过程的收集器的时候，使用指针碰撞。 如果内存不规整 虚拟机需要维护一个列表。 空闲列表分配：如果对象不是规整的，已经使用的内存和未使用的内存相互交错，那么虚拟机将采用的是空闲列表法来为对象分配内存。意思是虚拟机维护了一个列表，记录上哪些内存是可用的吗，再分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的内容，这种分配方式成为“空闲列表（Free List）” 处理并发安全问题： 采用 CAS 失败重试、区域加锁保证更新的原子性。 每个线程预先分配一块 TLAB。- 通过-XX:+/-UseTLAB 参数来设定。 初始化分配到的空间（默认初始化）： 所有属性是设置默认时值，保证对象实例字段在不赋值的时候可以直接使用。 设置对象的对象头 将对象的所属类（即类的元数据信息）、对象的 HashCode 和对象的 GC 信息、锁信息、等数据存储再对象的对象头中。在这个过程具体设置方式取决于 JVM 的实现。 执行 init 方法进行初始化 在 Java 程序员的视角看来，初始化才正式开始。初始化成员变量、执行实例化代码块，调用类的构造方法，并把堆内对象首地址赋值给引用变量。 因此一般来说（由字节码中是否跟随着有 invokespecial 指令决定），new 指令之后会执行方法，把对象按照程序员的意愿进行初始化，这样一个真正的可用的对象才算完全的创建出来。 6.2 对象的内存布局 对象头： 运行时元数据（Mark World） 哈希值 GC 分代年龄 锁状态标志 线程持有的锁 偏向线程 ID 偏向时间戳 类型指针 指向类元素数据的 InstanceClass，确定该对象的所属类型 实例数据（Instance Data） 对象真正存储的有效信息，包括程序代码中定义的各种类型的字段（包括从父类继承下来的和本身拥有的字段） 规则 相同宽度的字段总是被分配到一起 父类中定义的变量会出现在子类之前 如果 CompactFields 参数为 true（默认为 true）：子类的窄变量可能插入到父类变量的空隙 对齐填充（Padding）：不是必须的，也没有特殊的含义，仅仅是占位符。 案例： 123456789101112131415161718public class ConsumerTest &#123; public static void main(String[] args) &#123; Customer customer = new Customer(); &#125;&#125;class Customer &#123; int id = 1001; String name; Account acct; &#123; name = &quot;匿名用户&quot;; &#125; public Customer() &#123; acct = new Account(); &#125;&#125;class Account &#123;&#125; 问题：字符串常量池在堆空间吗？3.6.6.2 作何理解？ 6.3 对象的访问定位 JVM 中如何通过栈帧的对象引用访问到其内部的对象实例？ Java 虚拟机规范并未明确规定使用哪种方式，Hotspot 使用了直接指针的方式。 句柄访问： 优点：reference 中存储稳定句柄地址，对象被移动（垃圾收集时候移动对象很普遍）时会改变句柄中的实例数据指针即可，reference 本身不需要被修改。 缺点：需要多占用一些空间。 直接指针： 6.4 面试题 对象在 JVM 中如何存储？ 对象的头信息包含哪些东西？ 第 7 章 String Table 7.1 String 的基本特性 String：字符串，使用一对&quot;&quot;引起来表示： String s1 = &quot;hello&quot;; String s2 = new String(&quot;hello&quot;); String 类声明为 final 的，不可被继承 String 实现了 Serializable 接口：表示字符串是支持序列化的；实现了 Comparable 接口：表示 String 可以比较大小、排序。 在 JDK8 以及之前，String 内部定义了final char[] value用来存储字符串数据，JDK9 之后使用 byte[]加编码标记，节约了一些空间。 同时 StringBuilder、StringBuffer 进行了修改。 String：代表不可变的字符序列。简称：不可变性 当对字符串重新赋值的时候，需要重写指定内存区域赋值，而不是修改原有的 value 进行赋值。 不能对现有的字符串进行连接操作，也需要重新指定内存区域，不是修改原有的 value 进行赋值。 当调用 String 的 replace()方法修改指定字符或者字符串的时候，也需要重新指定内存区域赋值，不能使用原有的 value 赋值 通过字面量的方式（区别于 new）给一个字符串赋值，此时的字符串声明在常量池中。 字符串常量池中是不会存储相同内容的字符串。 String 的 String Pool 是一个固定大小的 HashTable，默认长度为 1009，如果放进去 String Pool 的 String 非常多，就会造成 Hash 冲突严重，从而导致链表很长，而链表长了之后，会直接影响调用 String.intern()的性能。 使用 -XX:StringTableSize 可设置 StringTable 的长度 在 JDK6 中的 StringTable 是固定的，就是 1009 的长度，如果字符串常量池中的字符串过多就会导致效率下降很快，StirngTableSize 设置没有要求 在 JDK7 中，StringTable 的默认长度为 60013，StirngTableSize 没有要求 JDK8 开始 1009 是可设置的最小值 Stirng.intern()方法：如果字符串常量池中没有对应的字符串的话，就添加在常量池 7.2 String 的内存分配 在 Java 语言中有 8 种基本数据类型和 String，这些类型为了使它们在运行速度中更快，更节省内存，都提供了常量池。 常量池就相似于一个 Java 系统级别提供的缓存。8 中基本类型都是系统协调的，String 类型的常量池比较特殊，主要的使用方法有 2 种： 直接使用双引号声明 String 的对象会直接存储在常量池种 如果不是使用双引号声明的 String 对象，可以使用 String 提供的 intern() 方法 在 JDK6 以及以前，字符串常量池存放在永久代。JDK7 字符串常量池的位置调整到 Java 堆内。 所有的字符串都保存在堆中，和其他普通对象一样，这样可以在进行调优的时候仅仅需要调整堆大小就可以了 因而 jdk7 中，使用 String.intern();创建字符串更优。 Java8 元空间，字符串常量池在堆。 StringTable 为什么需要调整 之前的永久代比较小，放大量的字符串，会占用很大的空间 永久代垃圾回收的频率很低 Java 语言规范要求完全相同的字符字面常量，应该包含同样的 Unicode 字符序列，并且必须是指向同一个 String 类实例 123456789101112public class Memory &#123; public static void main(String[] args) &#123; int i = 1; Object obj = new Object(); Memory mem = new Memory(); mem.foo(obj); &#125; public void foo(Object param) &#123; String str = param.toString(); System.out.println(str); &#125;&#125; 7.3 字符串的拼接操作 常量与常量的拼接结果在常量池，原理是编译期优化。 常量池中不会存在相同内容的常量。 只要其中有一个是变量，结果就在堆中。变量拼接的原理是 StringBuilder，并调用 append()方法，在调用 toString()方法。 但如果拼接的变量都用 final 修饰（常量引用），则任然使用编译器优化放在常量池，不使用 Stringbuilder()。 针对 final 修饰类、方法、基本数据类型、引用数据类型的量的结果的时候，能使用 fianl 就使用 final。 如果拼接的结果调用 intern()方法，则主动将常量池中还没有的字符串对象放入池中，并返回此对象的地址。 使用 StringBuilder 的 append()方式执行效率远高于拼接，全程只会创建一个 StringBuilder。 拼接会创建较多的 StringBuilder 和 String 对象，内存占用更大，且进行 GC 时，会花费更多的时间。 在使用 StringBuilder 和 StringBuffer 的时候，如果已知具体的大小，可以使用有参构造函数创建一个指定初始大小的数组，从避免返回扩容转移数组的空间和资源消耗，进一步提高效率。 7.4 intern()方法 intern 方法会从字符串常量池中查询当前字符串是否存在，若不存在，就会将当前字符串放入常量池中。 只要字符串对象调用了 intern() 方法，那么返回值是指向字符串常量池中的数据。具体参考 7.5.2 2022.10.08 个人理解：intern() 方法并不改变原字符串变量的指向，只是维护字符串常量池，如果常量池中已经有，jdk6、7、8 均不做操作，如果常量池中没有，jdk6 复制一份新对象放到常量池，jdk7、8 复制一份对象地址放到常量池。 结论：对于程序中大量的重复的字符串，可以使用 intern() 方法节省空间。 7.5 面试题 7.5.1 字符串拼接 new String(“ab”) 会创建几个对象？ 两个，一个是 new 关键字在堆空间创建的，一个是在字符串常量池创建的 ab。字节码文件如下： 123456 0 new #2 &lt;java/lang/String&gt; 3 dup 4 ldc #3 &lt;ab&gt; 6 invokespecial #4 &lt;java/lang/String.&lt;init&gt;&gt; 9 astore_110 return new String(“a”) + new String(“b”) 创建了几个对象？ 五个。常量池中 a、b，堆中 new String 的 a、b，堆中 new StringBuilder。 如果再深究，由于 new Builder()会调用 toString()方法，也就是还会创建 new String(“ab”)，也就是第六个对象。但是，常量池里没有创建 ab 对象。 7.5.2 intern() 判断结果： 123456789101112public class StringInternDemo &#123; public static void main(String[] args) &#123; String s = new String(&quot;1&quot;); s.intern(); String s2 = new String(&quot;1&quot;); System.out.println(s == s2);//false、false String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;); s3.intern(); String s4 = &quot;11&quot;; System.out.println(s3 == s4);//false、true &#125;&#125; jdk6：false、false jdk7、jdk8：false、true jdk7、8 的原因： new String(&quot;1&quot;)创建了两个对象，所以s.intern()并不会再起作用。 new String(&quot;1&quot;) + new String(&quot;1&quot;)创建了 5 个对象，但常量池中没有”11“这个数，s3.intern()在常量池创建了”11“。 进阶：判断结果。 123456789101112public class StringInternDemo &#123; public static void main(String[] args) &#123; String s1 = new String(&quot;1&quot;) + new String(&quot;1&quot;); s1.intern(); String s2 = &quot;11&quot;; System.out.println(s1 == s2);//true String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;); String s4 = &quot;11&quot;; s3.intern(); System.out.println(s3 == s4);//false &#125;&#125; s1.intern()在常量池创建了 11，s3.intern()什么也没干。 123456789101112public class StringInternDemo &#123; public static void main(String[] args) &#123; String s1 = new String(&quot;1&quot;) + new String(&quot;1&quot;); s1.intern(); String s2 = &quot;11&quot;; System.out.println(s1 == s2);//true String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;); String s4 = &quot;11&quot;; String s5 = s3.intern(); System.out.println(s5 == s4);//true &#125;&#125; 总结： jdk6 中，将这个字符串对象尝试放入串池 如果串池中有，则不会放入，返回已有的串池中的对象的地址 如果没有，会把此对象复制一份，放入串池，并返回串池中的对象地址 jdk7 中，将这个字符串对象尝试放入串池 如果串池中有，则并不会放入。返回已有的串池中的对象的地址 如果没有，会把此对象的引用地址复制一份，放入串池，并返回串池中引用地址的地址 进阶 进阶 7.6 G1 的 String 去重操作 针对的是堆区，常量池本身就 1 份。 Java 应用的数据测试有如下结果： 堆存活数据集合里面的 String 对象占用了 25% 堆存活数据集合里面重复的对象有 13.5% String 的平均长度是 45 重复是指 equals 为真。 第 8 章 垃圾回收 8.1 概述 8.1.1 垃圾的概念 Java 和 C++的区别核心就是内存动态分配和垃圾收集技术。 1960 年，第一门开始使用内存动态分配和垃圾收集技术的 Lisp 语言诞生。 垃圾是指在运行程序中没有任何指针指向的对象，这个对象就是需要被回收的垃圾。 8.1.2 垃圾回收的意义 对于高级语言来说如果不进行垃圾回收，内存迟早都会被消耗完。如果不及时对内存中的垃圾进行清理，那么这些垃圾会一直占用空间，直到程序运行结束，被占用的空间就浪费了，可能会导致内存溢出。 除了释放没用的对象，垃圾回收也可以清除内存里的记忆碎片。碎片整理将所占用的堆内存移动到堆的一端，以便 JVM 将整理出来的内存分配给新的对象。 在早期的 C/C++时代，垃圾回收基本上是手动执行的，开发人员可以使用 new 关键字进行内存申请 ，并使用 delete 关键字进行内存释放。 这种方式可以灵活控制内存释放的时间，但是会给开发人员带来 频繁申请和释放内存的管理负担，倘若有一处内存区间由于程序员编码的问题忘记被回收，那么就会产生内存泄漏，垃圾对象永远多无法被清除，随着系统运行时间的不断增长，垃圾对象所耗内存可能持续上升，直到出现了内存溢出并造成应用程序崩溃。 内存泄漏：一个对象已经成为了垃圾，但是无法被回收。 8.1.3 Java 的自动内存管理 好处： 自动内存管理，无需开发人员手动参与内存的分配与回收，这样降低内存泄漏和内存溢出的风险。 自动内存管理机制，将程序员从繁重的内存管理中释放出来，可以更专心的专注于业务开发。 担忧： 对于 Java 开发人员来讲，自动内存管理就是一个黑匣子。会严重弱化 Java 开发人员在程序出现内存溢出是定位问题和解决问题的能力。 此时了解 JVM 的自动内存分配和内存回收原理就显得非常重要，只有在真正了解 JVM 是如何自动管理内存，才能在遇见 OutOfMemoryError 的时候，快速的根据错误异常日志定位问题和解决问题。 当需要排查各种内存溢出、内存泄漏的问题的时候，当垃圾收集称为系统达到更高并发量的瓶颈的时候，我们就必须针对这些“自动化”的技术实施必要的监控和调节。 垃圾回收器可以对年轻代回收，也可以对老年代回收，甚至是全堆和方法区的回收 其中，Java 堆是垃圾回收的工作重点。从次数上来讲： 频繁收集 Young 区 较少收集 Old 区 基本不动 Perm 区（元空间） 8.2 垃圾回收算法 8.2.1 垃圾标记阶段 垃圾标记阶段：在 GC 执行垃圾回收之前，首先需要区分出内存中哪些是存活对象，哪些是已经死亡的对象。只有被标记为死亡的对象，GC 才会执行垃圾回收，释放掉其所占用的内存空间。 当一个对象已经不再被任何存活对象引用的时候，就可以宣判为已经死亡。 判断对象存活一般有两种方式：引用计数法和可达性分析算法。 8.2.1.1 引用计数算法 对每个对象保存一个整型的引用计数器属性，用于记录对象被引用的情况。对于一个对象 A，只要有一个对象引用类 A，则 A 的引用计数器+1，当引用失效的时候，引用计数器就减去 1，只要对象 A 的引用计数器为 0；即表示 A 不可能再被使用，可进行回收。 优点：实现简单，垃圾对象便于识别；判定效率高，回收没有延迟性。 缺点： 它需要单独的字段存储计数器，增加了储存空间的开销。 每次赋值都需要更新计数器，伴随着加法和减法操作，增加了时间开销。 引用计数器有一个严重的问题，即无法处理循环引用的情况，这是一条致命的缺陷，存在内存泄漏的问题，导致在 Java 的垃圾回收器中没有使用到这类算法。 引用计数算法是很多语言的资源回收选择，如 Python 支持引用计数和垃圾收集机制。Python 解决循环引用问题的方式： 手动解除：在合适的时机，解除引用关系 使用弱引用 weakref：weakref 是 Python 的标准库，为了解决循环引用。 8.2.1.2 可达性分析（或者根搜索算法、追踪性垃圾收集） 可达性分析算法是以根对象集合（GC Roots）为起始点，按照从上至下的方式搜索被根对象集合所连接的目标对象是否可达，只有能够被根对象集合直接或者间接连接的对象才是存活对象。如果目标对象没有任何引用链相连，则是不可达的，就意味着该对象已经死亡，可以标记为垃圾对象。 &quot;GC Roots&quot;根集合就是一组必须活跃的引用。 使用可达性分析算法后，内存中的存活对象都会被根对象集合直接或者间接连接着，搜索所走过的路径叫做引用链（Reference Chain）。 在 Java 语言中，GC Roots 包括以下几类元素 虚拟机栈中引用的对象（局部变量表中） 比如：各个线程被调用的方法中使用到的参数、局部变量 本地方法栈内 JNI（本地方法）引用的对象 方法区中类的静态属性引用的对象 比如：Java 类的引用类型静态变量 方法区中常量引用对象 比如：字符串常量池（StringTable）里的引用 所有同步锁 synchronized 持有的对象 Java 虚拟机内部的引用 基本数据类型对应的 Class 对象，一些常驻得异常对象（NullPointerException、OutOfMemory）、系统类加载器。 反射 Java 虚拟机内部情况得 JMXBean、JVMTI 中注册得回调、本地代码缓存 除了这些固定的 GC Roots 集合以外，根据用户所选用的垃圾收集器以及当前回收内存区域不同，还可以有其他对象“临时性”的加入，共同构成完成 GC Roots 集合，比如：分代收集和局部回收（Partial GC） GC Roots 记忆小技巧：由于 Root 采用栈方式存储变量和指针，所以如果一个指针保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那它就是一个 Root。 注意： 如果要使用可达性分析算来判断内存是否可回收，那么分析工作必须在一个能保障一致性的快照中进行，这点不满足的话分析结果的准确性就无法保证。 这点也是导致 GC 进行时必须&quot;Stop The World&quot;的一个重要原因 即使是号称（几乎）不会发生停顿的 CMS 收集器中，枚举根节点的时候也是必须要停顿的。 8.2.2 清除阶段 8.2.2.1 标记 - 清除算法（Mark-Sweep） 最先被 J.McCarthy 等人在 1960 年提出并应用于 Lisp 语言。 执行过程：当堆中的有效空间（available memory）被耗尽的时候，就会停止整个程序（也称为 Stop The World），然后进行两项工作： 标记：Collector 从引用根节点开始遍历，标记所有被引用的对象，一般是在对象的 Header 中记录为可达对象 （标记的是非清除对象）。 清除：Collector 对堆内存从头到尾进行线性的遍历，如果发现某个对象在其 Header 中没有标记为可达对象，则将其回收。 缺点： 效率不算高。 在进行 GC 的时候，需要停止整个应用程序，导致用户体验差。 这种方式清理出来的空闲内存是不连续的，产生内存碎片，需要维护一个空闲列表（空闲列表参考 6.1 第 2 点）。 注意： 这里所说的清除不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里，下次有新的对象需要加载的时候，判断垃圾位置的空间是否足够，够就存放（覆盖）。 8.2.2.2 复制算法（copyting） 为了解决标记-清除算法在垃圾回收效率方面的缺陷，复制算法出现。其核心思想是将或者的内存空间分为两块，每次只使用其中一块，在垃圾回收的时候将正在使用的内存的活着的对象复制到未使用的内存块中，之后清除正在使用的内存块中的所有对象，交换两个内存的角色，最后完成垃圾回收。（幸存者 1 区和 2 区使用了这种算法） 优点： 没有标记和清除过程，实现简单，运行高效 复制过去之后保证空间的连续性，不会出现”碎片“问题 缺点： 需要 2 倍的空间 对于 G1 这种将内存拆分成大量 region 的 GC，复制意味着 GC 需要维护 region 之间对象的引用关系，不管是内存或者时间开销都不小。 如果系统中的垃圾对象很少，复制算法就不是很理想，它要复制的对象太多。 应用场景 在新生代，对常规的应用的垃圾回收，一次通常可以回收 70%~99%的内存空间，回收性价比很高，所以现在的商业虚拟机都是使用这种方式作为收集算法回收新生代。 8.2.2.3 标记-压缩算法（mark-compact 算法、标记整理算法） 复制算法的高效性是建立在存活对象少、垃圾对象多的前提下的，这种情况在新生代经常发生，但是在老年代，更常见的是大部分对象都是存活对象。如果依然使用复制算法，因为存活的对象很多，复制的成本也很高。 标记清除算法可以应用在老年代中，但是执行效率低效，而且在执行完内存回收之后还会产生内存碎片，所以 JVM 的设计者在此基础上设计了标记 - 压缩算法。 执行过程： 第一阶段和标记清除算法一样，从根节点开始标记所有被引用的对象。 第二阶段将所有的存活对象压缩到内存的一端，按照顺序排放 之后，清理边界外所有的空间 标记-压缩算法的最终效果等同于标记 - 清除算法执行完成之后，再进行一次内存碎片整理，因此，也可以称之为标记-清除-压缩（Mark-Sweep-Compact）算法。二者的本质是标记清除算法是一种非移动式的回收算法，标记-压缩是移动式的。 优点： 消除了标记-清除算法的内存区域分散的缺点。给新对象分配内存的时候，JVM 只需要持有一个内存的起始地址即可。 消除了复制算法当中，内存减半的高额代价。 缺点： 从效率上说，标记-整理算法要低于复制算法。 移动对象的同时，如果对象被引用，还需要调整引用的地址。 移动过程种，需要全程暂停用户应用程序，即 STW。 8.2.2.4 算法对比 8.2.2.5 分代收集算法 没有一种算法可以完全取代其他算法，它们都有自己的优点和缺点，所以不同生命周期的对象可以采取不同的收集方式，以便提供回收效率。 核心思想：具体问题具体分析。 几乎所有的 GC 都是采用分代收集（Generational Collecting）算法执行垃圾回收的。 Hotspot 虚拟机： 年轻代：区域相对老年代较小，对象生命周期短，存活率低，回收频繁，使用复制算法最快。 老年代：区域较大，对象生命周期长，存活率高、回收不及年轻代频繁。这个时候一般使用标记-清除或者标记清除与标记整理一起出现。 标记(Mark)阶段的开销于存活对象的数量成正比。 清除(Sweep)阶段的开销与所管理区域的大小成正比。 整理(Compact)阶段的开销与存活对象的数据成正比。 以 HotSpot 中的 CMS 回收器为例，CMS 是基于 Mark-Sweep 实现的，对于对象的回收效率很高。对于碎片问题，CMS 采用基于 Mark-Compact 算法的 Serial Old 回收器作为补偿措施：当内存回收不佳（碎片导致的 Concurrent Mode Failure 时），将采用 Serial Old 收集器执行 Full GC 对老年代进行内存的整理。 8.2.2.6 增量收集算法 在上述的算法，在垃圾回收的过程中，都会出现 STW 的状态，影响程序的运行，所以增量收集（Incremental Collecting ）算法的诞生。 基本思想： 让垃圾收集线程和应用程序线程交替执行。每次垃圾回收线程只收集一小部分区域空间，接着切换到应用线程，依此反复，直到完成。 总的来说，基础仍然是传统的标记-清除算法和复制算法。增量收集算法通过对线程间冲突的妥善管理，允许垃圾收集线程以分阶段的方式完成标记、清理、或复制工作。 缺点：由于在垃圾回收过程中，间断性的还执行了应用程序代码，虽然能减少系统的停顿时间，但是线程上下文切换，会使得垃圾回收的总体成本上升，造成系统的吞吐量下降。 8.2.2.7 分区算法 一般来说，在相同的条件下，堆空间越大，一次 GC 时所需要的时间就越长，GC 产生的停顿就越长。为了控制好停顿时间，将堆分为不同的区块，从而减少一次 GC 的影响。 分代算法按照对象的生命周期长短划分为 2 个部分，分区算法将整个堆划分为连续的不同小空间，每个小空间独立使用，独立回收，可以一次控制多个小区。 8.3 对象的 finalization 机制 Java 语言提供了对象终止（finalization）机制来允许开发人员提供对象被销毁之前的自定义处理逻辑。 当垃圾回收器发现没有引用指向一个对象，即：垃圾回收此对象之前，总会先调用这个对象的 finalize() 方法。 finalize()方法允许在子类中被重写，用于在对象被回收时进行资源释放，通常在这个方法中进行一些资源释放和清理的工作，比如关闭文件、套接字和数据库连接等。 永远不要主动调用某个对象的 finalize() 方法，应该交给垃圾回收机制调用，理由包括下面三点： 在 finalize()时可能导致对象复活。 finalize()方法的执行时间是没有保障的，它完全由 GC 线程决定，极端情况下，若不发生 GC，则 finalize()方法没有执行机会。 一个糟糕的 finalize()会严重影响 GC 性能 由于 finalize()方法的存在，虚拟机中的对象一般处于三种可能的状态：一般来说，如果从所有根节点都无法访问到某个对象，说明对象已经不再使用此对象了，需要被回收。但是事实上，也并非是 “非死不可”的，这时候它们暂时处于”缓刑“的状态，一个无法触及的对象由可能再某个条件下”复活“，此时对它的回收就是不合理的。 可触及的：从根节点开始，可以达到这个对象 可复活的：对象所有的引用都被释放，但是可能再 finalize()中复活，但是只有一次复活的机会。 不可触及的：对象的 finalize()被调用，并且没有复活，那么就会进入不可触及状态。不可触及的状态不可能被复活，因为 finalize()只调用一次。 具体过程：判断一个对象 objA 是否可被回收，至少要经历 2 次标记过程： 如果对象 objA 到 GC Roots 没有引用连接，则进行第一次标记。 进行筛选，判断此对象是否有必要执行 finalize()方法： 如果没有重写 finalize()方法，或者 finalize()方法已经被虚拟机调用过，则虚拟机视为”没有必要执行“，objA 被判断是不可触及的。 如果对象 objA 重写了 finalize()方法，且还未执行过，那么 objA 会被插入到 F-Queue 队列中，由虚拟机自动创建的、低优先级的 Finalizer 线程触发其 finalize()方法执行。 finalize()方法是对象逃亡的最后机会，稍后 GC 会对 F-Queue 队列中的对象进行第二次标记。此时如果 finalize()方法与引用链的任何一个对象建立了联系，那么 objA 会被移出即将回收集合。后续 objA 再出现不可触及状态时，直接进行回收，因为 finalize()方法只能被调用一次。 12345678910111213141516171819202122232425262728293031323334public class CanReliveObj &#123; public static CanReliveObj obj;// 类变量，属于 GC Roots @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println(&quot;调用当前类重写的finalize()方法&quot;); obj = this;// 当前对象与引用链上任意一个对象建立了联系 &#125; public static void main(String[] args) &#123; try &#123; obj = new CanReliveObj(); // 对象第一次调用自己 obj = null; System.gc(); System.out.println(&quot;第一次GC&quot;); Thread.sleep(2000);//Finalizer线程优先级低，暂停2秒进行等待 if (obj == null) &#123; System.out.println(&quot;obj is dead&quot;); &#125; else &#123; System.out.println(&quot;obj is alive&quot;); &#125; System.out.println(&quot;第二次GC&quot;); obj = null; System.gc(); if (obj == null) &#123; System.out.println(&quot;obj is dead&quot;); &#125; else &#123; System.out.println(&quot;obj is alive&quot;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 8.4 GC Roots 分析 8.4.1 MAT 工具 MAT 是 Memory Analyzer 的简称 它是一款功能强大的 Java 堆内存分析器，用于查找内存泄漏以及查看内存消耗情况。 MAT 是基于 Eclipse 开发的，是一款免费的性能分析工具。官网 ： http://eclipse.org/mat/ 获取 dump 文件： 方式一：命令行使用 jmap 方式二：使用 jvisualvm 右键快照即可保存。 生成 dump 文件后，使用 MAT 分析即可。 8.4.2 Jprofiler 进行 GC Roots 溯源 需要配合 IDEA 插件使用。 8.5 垃圾回收相关概念 8.5.1 System.gc() 默认情况下，通过 System.gc()或者 Runtime.getRuntime().gc()的调用，会显式触发 Full GC，同时对老年代和新生代进行回收，尝试释放被丢弃对象占用的内存。 然而 System.gc()调用附带也该免责声明，无法保证对垃圾收集器的调用。 JVM 实现者可以通过 System.gc() 调用来决定 JVM 的 GC 行为 ，而一般情况下，垃圾回收应该自自动进行的，无需手动触发，否则就太过于麻烦了。 8.5.2 内存溢出与内存泄漏 内存溢出： javadoc 中对 OutOfMemoryError 的解释是，没有空闲内存，并且垃圾收集器也无法提供更多内存： Java 虚拟机的堆内存不够 比如：可能存在内存泄漏问题；也有可能是堆的大小不合理，比如我们要处理比较可观的数据量，但是没有显示指定 JVM 堆大小或者指定数值偏小。 代码中创建了大量的大对象，并且长时间不能被垃圾收集器收集（存在引用）。 在抛出 OutOfMemoryError 之前，通常垃圾收集器会被触发尽其所能去收集空间。但也不是任何情况下垃圾回收器都会被触发，比如分配一个超级大的对象，JVM 判断出垃圾收集不能解决这个问题，就直接抛出 OutOfMemoryError。 内存泄漏：也称作”存储渗漏“ ，严格来说：只有对象不会被程序用到了，但是 GC 又不能回收他们的情况，才叫内存泄漏。但是实际情况很多时候一些不太好的实践（或疏忽）会导致对象的生命周期变得很长，甚至 OOM，也可以叫做宽泛意义上的内存泄漏。尽管内存泄漏不会立刻引起程序崩溃，但是一旦发生，程序中的内存就会被逐步蚕食，直到发生 OutOfMemoryError。 举例： 单例模式：单例的生命周期和应用程序时一样长的，所以单例程序中，如果持有对象外部引用的话，那么这个外部对象时不能被回收的，则会导致内存泄漏的发生。 一些提供 close 的资源未关闭而导致内存泄漏：数据库连接、网络连接、和 IO 连接必须手动 close，否则不能回收。 8.5.3 Stop The World Stop The World 简称 STW，指的是 GC 事件发生的过程中，会产生应用程序的停顿。停顿产生时整个应用程序线程都会被暂停，没有任何响应，有点卡死的感觉，所以要减少 STW 的发生。 可达性分析的枚举根节点（GC Roots）会导致所有的 Java 执行线程停顿： 分析工作必须在一个确保一致性的快照中进行 一致性指的是整个分析过程系统像是冻结在某个点上 如果出现分析过程中对象引用关系还在不断地变化，则分析地结果无法保证 STW 事件和采用哪款 GC 无关，所有地 GC 都有这个事件。STW 是 JVM 后台自动发起和完成的，在用户不可见的情况下，把用户正常的工作线程全部停掉。System.gc()会导致 Stop-the-world 的发生，所以要避免使用。 8.5.4 垃圾回收的并行与并发 并发（concurrent）： 操作系统中，一个时间段有几个程序都处于启动到运行完毕之间，且这几个程序都是在同一个处理器上运行。 并发不是真正的 ”同时进行“ 只是 CPU 的时间片轮转，让用户觉得很快，没有切换的感觉。 并行（parallel）： 当系统有一个以上的 CPU 的时候，一个 CPU 执行一个进程，另一个 CPU 可以执行另外一个进程。两个进程互不抢占 CPU 资源。可以同时进行，故称之为并行（Parallel） 其实决定并行的不是 CPU 的数量，而是 CPU 的核心数量，一个 CPU 多核心也可以并行。 二者对比 并发：指的是多个事情，在同一个时间段内同时发生了。 并行：指的是多个事情，在同一个时间点上同时发生了。 并发的多个任务是相互抢占资源的。 并行的读个任务是不相互抢占资源的。 只有多个 CPU 或者一个 CPU 多核心的情况中，才会发生并行。否则，看似同时发生的事情，其实都是并发执行的。 垃圾回收的并发与并行 并行（Parallel）：指多条垃圾回收线程并行工作。 串行（Serial）：单线程执行 如果内存不够，程序暂停，启动 JVM 垃圾回收器进行垃圾回收。回收完在启动程序的线程。 并发（Concurrent）：指用户线程与垃圾收集器线程同时执行（但一定不是并行，因为仍然存在 STW）。 用户程序继续执行，垃圾在另外一个核心上执行。 8.5.5 安全点和安全区域 程序执行时并非在所有地方都能停下来开始 GC，只有在特定位置才能停下来开始 GC。这些位置称为 “安全点（Safe Point）” Safe Point 的选择，通常会根据是否具有让程序长时间执行的特性为标准。比如：选择一些执行时间较长的指令作为 Safe Point ，如方法调用、循环跳转、异常跳转等。 如何在 GC 发生时，检查所有的线程都跑到最近的安全点停顿下来呢？ 抢断式中断（目前没有虚拟机采用） 首先中断所有线程，如有线程不在安全的，就恢复线程，让线程跑到安全点。 主动式中断 设置一个中断标志，各个线程运行到 Safe Point 的时候主动轮询这个标志，为 true，就将自己进行中断挂起。 安全区域（safe region）：指一段代码片段中对象的引用关系不会发生变化，这个区域的任何位置开始 GC 都是安全的。如 sleep 或 blocked 状态的线程。安全区域可以看作安全点的扩展。 执行过程： 当线程运行到 safe region 的代码时，首先标识已经进入了 safe region，如果这段时间内发生了 GC，JVM 会忽略标识为 safe region 状态的线程。 当线程即将离开 safe region 时，会检查 JVM 是否已经完成 GC，如果完成了，则继续运行，否则线程必须等待直到收到可以安全离开 safe region 的信号为止。 8.5.6 各类引用 jdk1.2 对 Java 引用的概念进行了扩充，分为强引用（strong reference）、软引用（soft reference）、弱引用（weak reference）和虚引用（phantom reference）4 种，这 4 种引用强度一次减弱。 除了强引用，其余 3 种可以在 java.lang.ref 包中找到定义，可以在开发中直接使用。 除了 FinalReference 是使用默认修饰（包内可见），其他都是 public 修饰的。 8.5.6.1 强引用 程序代码之中普遍存在的引用赋值（默认的引用类型），如 Object o = new Object()，只要强引用存在，GC 就永远不会回收被引用的对象，就算是内存不足的时候，JVM 直接抛出 OutOfMemoryError 也不会去回收。 如果想中断强引用与对象之间的联系，可以显式的将强引用赋值为 null，这个时候就等待 GC 回收对对象即可。 特点： 强引用可以直接访问目标对象 强引用所指向的对象在任何时候都不会被系统回收，即使 OOM。 强引用可能导致内存泄漏（Java 内存泄漏的主要原因之一）。 8.5.6.2 软引用 软引是用来描述一些非必需但是仍有用的对象。在内存足够的时候，软引用对象不会被回收，只有在内存不足的时候，系统会回收软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常，这种特性非常适合用来做缓存。【不足即回收】 8.5.6.3 弱引用 被弱引用关联的对象只能生存到下一次垃圾收集器之前，当垃圾收集器工作室，无论内存空间是否足够，都会回收掉被弱引用关联的对象。【发现及回收】 WeakHashMap 类及其实现 WeakHashMap 类在 java.util 包内，它实现了 Map 接口，是 HashMap 的一种实现，它使用弱引用作为内部数据的存储方案。 12345678910111213public void test() &#123; Map map; // 弱引用 HashMap map = new WeakHashMap(); for (int i = 0; i &lt; 10000; i++) &#123; map.put(&quot;key&quot; + i, new byte[i]); &#125; // 强引用HashMap map = new HashMap(); for (int i = 0; i &lt; 10000; i++) &#123; map.put(&quot;key&quot; + i, new byte[i]); &#125;&#125; WeakHashMap 会在系统内存紧张时使用弱引用，自动释放掉持有弱引用的内存数据。但如果 WeakHashMap 的 key 都在系统内持有强引用，那么 WeakHashMap 就退化为普通的 HashMap，因为所有的表项都无法被自动清理。 8.5.6.4 虚引用 又称为幻影引用或幽灵引用。 一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来获得一个对象的实例。为一个对象设置虚引用关联的唯一目的时对象被收集器回收的时候收到一个系统通知。【对象跟踪】 创建虚引用需要在构造器内传入一个引用队列。 引用队列可以与软引用、弱引用以及虚引用一起配合使用，当垃圾回收器准备回收一个对象时，如果发现它还有引用，那么就会在回收对象之前，把这个引用加入到与之关联的引用队列中去。 程序可以通过判断引用队列中是否已经加入了引用，来判断被引用的对象是否将要被垃圾回收，这样就可以在对象被回收之前采取一些必要的措施。 8.5.6.5 终结器引用 终结器引用用于实现对象 finalize()方法，无需手动编码，其内部配合引用队列使用。 在 GC 时，终结器引用入队，由 finalizer 线程通过终结器引用找到被引用的对象并调用它的 finalize()方法，第二次 GC 时才能回收被引用的对象。 8.6 垃圾回收器 8.6.1 GC 分类与性能指标 垃圾回收器没有在规范中进行过多的规定，可以由不同厂商、不同版本的 JVM 来实现。 按线程数：可分为串行垃圾回收器和并行垃圾回收器。 串行回收：在同一时间段内只允许有一个 CPU 用于执行垃圾回收操作，此时工作线程被暂停（STW），直至垃圾收集工作结束。 在单 CPU 处理器或者较小的应用内存等硬件平台不是特别优越的场合，串行回收器的性能表现可以超过并行回收器和并发回收器。所以串行回收默认被应用在客户端的 Client 模式下的 JVM 中 并行回收：可以运用多个 CPU 共同执行垃圾回收，因此提升了应用的吞吐量，不过并行回收仍然与串行回收一样，采用独占式，使用 “Stop the World”机制。 在并发能力较强的 CPU 上，并行回收器产生的停顿时间要短于串行回收器。 按工作模式： 并发式垃圾回收器与应用程序线程交替工作，以尽可能较少程序的停顿时间。 独占式垃圾回收器（Stop The World）一旦运行，就停止程序中所有的用户线程，直到垃圾回收过程完全结束。 按照碎片处理方式： 压缩式垃圾回收器会在回收完成之后，对存活的对象进行压缩整理，消除回收后的碎片。 非压缩式的垃圾回收器不进行这个操作。 按照工作的内部区间划分，又可以分为年轻代垃圾回收器和老年代垃圾回收器。 性能指标： 吞吐量：运行用户代码的时间占总运行时间的比例。 （总运行时间：程序的运行时间+内存回收的时间）。 垃圾收集器开销：垃圾收集所有时间与总运行时间的比例。 暂停时间：执行垃圾回收的时候，程序的工作线程被暂停的时间。 收集频率：相对于应用程序的执行，收集操作发生的频率。 内存占用：Java 堆区所占用的内存大小。 快速：一个对象从诞生到回收所经历的时间。 吞吐量、暂停时间、内存占用 共同构成了一个“不可能三角”，由于三者不可同时满足，所以一般只追求吞吐量和暂停时间。 吞吐量： 吞吐量就是 CPU 用于运行用户代码的时间与 CPU 总消耗时间的比值，即吞吐量 = 运行用户代码时间 / （用户代码时间 + 垃圾收集时间） 虚拟机运行 100 分钟，垃圾收集 1 分钟，那么吞吐量就是 99% 这种情况下，应用程序能容忍比较高的暂停时间，因此，高吞吐量的应用程序有更长的时间基准，快速响应式不必考虑的 吞吐量优先，意味着在单位时间内，STW 的时间最短。 暂停时间： 暂停时间是指一个时间段内应用程序线程暂停，让 GC 执行的状态。 GC 期间 100 毫秒的暂停时间，说明这 100 毫秒期间没有任何应用程序是活动的。 暂停时间优先，意味着让单次 STW 的时间最短。 吞吐量和暂停时间的对比： 高吞吐量较好是因为让程序的最终用户感觉只有用户程序在做“生产性”工作，直觉上，吞吐量越高程序运行越快。 对于一个交互式应用程序具有低的较大的暂停时间非常重要。 然而高吞吐量和低暂停时间是一对相互竞争（矛盾）的目标。吞吐量优先需要降低内存回收的执行频率，导致需要更长的暂停时间。低暂停时间意味着 GC 只能频繁执行回收内存，导致吞吐量下降。 在设计或者使用 GC 算法的时候，必须明确目标：一个 GC 算法只能针对两个目标之一，或者尝试找到一个二者的折衷。 现在标准：在最大吞吐量优先的情况下，降低停顿时间。 12345678910111213141516171819//终端的命令及显示内容的含义jstat -gc 2764 250 20 //2764表示进程id ，250表示250毫秒打印一次 ，20表示一共打印20次S0C：第一个幸存区的大小S1C：第二个幸存区的大小S0U：第一个幸存区的使用大小S1U：第二个幸存区的使用大小EC：伊甸园区的大小EU：伊甸园区的使用大小OC：老年代大小OU：老年代使用大小MC：方法区大小MU：方法区使用大小CCSC:压缩类空间大小CCSU:压缩类空间使用大小YGC：年轻代垃圾回收次数YGCT：年轻代垃圾回收消耗时间FGC：老年代垃圾回收次数FGCT：老年代垃圾回收消耗时间GCT：垃圾回收消耗总时间 8.6.2 不同垃圾回收器发展 历史： 1999 年随 JDK1.3.1 一起来的是串行方式的 Serial Gc ，它是第一款 GC。ParNew 垃圾收集器是 serial 收集器的多线程版本（并行） 2002 年 2 月 26 日，Parallel Gc 和 concurrent Mark SweepGC（CMS）跟随 JDK1.4.2—起发布 Parallel Gc 在 JDK6 之后成为 HotSpot 默认 GC。 2012 年，在 JDK1.7u4 版本中，G1 可用。 2017 年，JDK9 中 G1 变成默认的垃圾收集器，以替代 CMS。 2018 年 3 月，JDK10 中 G1 垃圾回收器的并行完整垃圾回收，实现并行性来改善最坏情况下的延迟。 2018 年 9 月，JDK11 发布。引入 Epsilon 垃圾回收器，又被称为&quot;No-Op(无操作)&quot; S 回收器。同时，引入 ZGC:可伸缩的低延迟垃圾回收器(Experimental)。 2019 年 3 月，JDK12 发布，增强 G1，自动返回未用堆内存给操作系统，同时引入 Shenandoah GC：低停顿时间的 GC。 2019 年 9 月，JDK13 发布。增强 ZGC，自动返回未用堆内存给操作系统。 2020 年 3 月，JDK14 发布。删除 CMS 垃圾回收器。扩展 ZGC 在 macoS 和 windows 上的应用。 7 款经典垃圾收集器（在 JDK11 的 ZGC 之前） 串行回收器：Serial、Serial Old 并行收集器：ParNew、Parallel Scavenge、Parallel Old 并发回收器：CMS、G1 7 款经典垃圾回收器与垃圾分代之间的关系： 新生代收集器：Serial、ParNew、Parallel Scavenge 老年代收集器：Serial Old、Parallel Old、CMS 整堆收集器：G1 组合关系（两个收集器间有连线，表明它们可以搭配使用）： Serial/Serial old、Seria1/CMS、ParNew/Serial old、ParNew/CMS、Parallel Scavenge/Serial old、Parallel Scavenge/Parallel old、G1、ZGC 其中 serial old 作为 CMS 出现&quot;concurrent Mode Failure&quot;失败的后备预案。 (红色虚线)由于维护和兼容性测试的成本，在 JDK 8 时将 Serial+CMS、ParNew+Serial old 这两个组合声明为废弃(JEP 173)，并在 JDK 9 中完全取消。 (绿色虚线)JDK14 弃用 Parallel Scavenge 和 Serial0ld Gc 组合(JEP336)。 (虚线边框)JDK14 删除 CMS 垃圾回收器(JEP 363)。 8.6.3 查看默认垃圾回收器 -XX:+PrintCommandLineFlags 查看命令行参数，包括使用的垃圾回收器，JDK8 没有使用 G1，也没有使用 CMS，JDK9 使用了 G1。 执行命令行指令：jinfo -flag 相关垃圾回收器参数 进程id。 8.6.4 串行回收（Serial 与 Serial Old） Serial 收集器是最基本、历史最悠久的垃圾收集器。JDK1.3 之前回收新生代唯一的选择。 Serial 收集器作为 Hotspot 中 Client 模式下的默认新生代垃圾收集器。 Serial 收集器采用复制算法、串行回收和”Stop-the-world“ 机制的方式执行内存回收。 Serial Old 收集器是运行在 Client 模式下默认的老年代垃圾收集器。 Serial Old 收集器同样采用了串行回收和”Stop the World“机制，只不过内存回收算法使用的是标记-压缩算法。 Serial Old 在 Server 模式下主要有两个用途： 与新生代的 Parallel Scavenge 配合使用。 作为老年代 CMS 收集器的后备垃圾收集方案。 串行回收进行垃圾收集的时候，必须暂停其他所有的工作线程，直到它收集结束(Stop-The-World)。 优势：简单而高效(和其他收集器的单线程比)。对于限定的单个 CPU 的环境来说，Serial 收集器由于没有线程相互的开销，专心做垃圾收集，自然可以获得最高的单线程的收集效率。 运行在 Client 模式下的虚拟机是个不错的选择。 在用户的桌面应用场景中，可用内存一般不大(几十 MB 至一两百 MB)，可以在较短时间内完成垃圾收集(几十 ms 至一百多 ms)，只要不频繁发生,。使用串行回收器是可以接受的。 在 HotSpot 虚拟机中，使用-XX:+UseSerialGC等价于新生代用 Serial GC，且老年代用 Serial Old GC。 总结：现在已经不用串行的了。而且在限定单核 CPU 才可以使用，现在都不是单核的了。对于交互性较强的应用而言，串行回收的 STW 是不可接受的。 8.6.5 并行回收（ParNew） Par 是 Parallel 的缩写，New：只能处理的是新生代。 ParNew 收集器除了采用并行回收的方式执行内存回收外，与 Serial 几乎没有任何区别。ParNew 收集器在年轻代中同样也是采用复制算法、&quot;stop-the-World&quot;机制。 ParNew 是很多 JVM 运行在 server 模式下新生代的默认垃圾收集器。 对于新生代，回收次数频繁，使用并行方式高效；对于老年代，回收次数少，使用串行方式节省资源（CPU 并行需要切换线程，串行可以省去切换线程的资源）。 ParNew 收集器在多 CPU 环境下，可以利用多核心的资源，可以更快完成垃圾收集。但是在单个 CPU 的环境下，ParNew 收集器不比 Serial 收集器跟高效。 除了 Serial 外，目前只有 ParNew GC 能与 CMS 收集器配合工作。 参数配置： -XX:UseParNewGC：手动设置是否开启。 -XX:ParallelGCThreads：限制线程数量，默认开启个 CPU 数据相同的线程数。 8.6.6 吞吐量优先回收（Parallel Scavenge 与 Parallel Old） HotSpot 的新生代中除了 ParNew 收集器是基于并行回收的以外，Parallel scavenge 收集器同样也采用了复制算法、并行回收和&quot;stop the World&quot;机制。 和 ParNew 收集器不同，Parallel Scavenge 收集器的目标则是达到一个可控制的吞吐量（Throughput），它也被称为吞吐量优先的垃圾收集器。自适应调节策略也是 Parallel scavenge 与 ParNew 一个重要区别。 高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。因此，常在服务器环境中使用，例如：批处理、订单处理、工资支付、科学计算的应用程序。 Parallel old 收集器，jdk1.6 时用来代替老年代的 serial old 收集器。Parallel old 收集器采用了标记-压缩算法，但同样也是基于并行回收和”Stop - the World“机制。 在程序吞吐量优先的应用场景中，Parallel 收集器和 Parallel Old 的结合在 Server 模式下的内存回收性能很不错，Java8 中默认是此收集器。 -XX:+UseParallelGC手动设置年轻代使用 Parallel 并行收集器执行回收任务 -XX:+UseParallelOldGC：手动指定老年代使用并行回收收集器 上面的两个参数，开启一个，另外一个也会激活。且在 JDK8 默认开启。 -XX:ParallelGCThreads：设置年轻代并行收集器的线程数。 默认情况下，当 CPU 的数量小于 8 个，ParallelGCThreads 的值等于 CPU 数量 当 CPU 数量大于 8 个，ParallelGCThreads 的值等于3+[[5*CPU_Count]/8]。 -XX:MaxGCPauseMillis：设置垃圾收集器最大停顿时间（也就是 STW 的时间）单位是毫秒 为了尽可能把停顿的时间控制在 MaxGCPauseMillis 内，收集器工作的时候会设置堆的大小或者其他的参数。 使用此参数需要谨慎。 -XX:GCTimeRatio：垃圾回收时间占总时间的比例（= 1/（N+1）） 用于衡量吞吐量的大小 取值范围（1~100 ） 默认 99。 -XX:+UseAdaptiveSizePolicy：设置具有自适应调节（默认开启）。 该模式会自动调整年轻代的大小，包括 Eden 和 Survivor 的比例，晋升老年代对象的年龄。 8.6.7 低延迟回收（CMS） 在 JDK1.5 时期，Hotspot 推出了一款在强交互应用中几乎可认为时代意义的垃圾收集器：CMS (Concurrent-Mark-Sweep)收集器，这款收集器是 HotSpot 虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。 CMS 收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短（低延迟）就越适合与用户交互的程序，良好的响应速度能提升用户体验。 目前很大一部分的 Java 应用集中在互联网站或者 B/s 系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。 CMS 的垃圾收集算法采用标记-清除算法，并且也会&quot;Stop-the-world&quot;。 CMS 作为老年代的收集器，却无法与 JDK1.4 中已经存在新生代收集器 Parallel Scavenge 配合工作，所以采用 CMS 来收集老年代时，新生代只能在 ParNew 或者 Serial 收集器中选择一个。 在 G1 出现之前，CMS 使用还是非常广泛的，一直到今天仍然有很多系统使用 CMS GC。 CMS 的执行过程比之前的收集器都要复杂，整个过程分为 4 个主要阶段，即初始标记阶段、并发标记阶段、重新标记阶段和并发清除阶段。 初始标记（Initial-Mark）阶段：在这个阶段，程序中所有的工作线程都会因为 STW 机制而出现短暂的暂停，这个阶段的任务仅仅是为了标记出 GC Roots 能直接关联的对象 一旦标记完成之后就会恢复之前被暂停的所有应用线程，由于直接关联对象比较小，所以这里速度非常快。 并发标记（Concurrent-Mark）阶段：从 GC Roots 的直接关联对象开始遍历整个对象图的过程，这个过程耗时长但是不需要停顿用户线程，可以与工作线程一起并发执行。 重新标记（Remark）阶段：由于在并发标记阶段中，程序的工作线程会和垃圾收集线程同时运行或者交叉运行，因此为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象标记记录，这个阶段的停顿稍长，但也远比并发标记阶段短。存在 STW 的情况。 并发清除（Concurrent-Sweep）阶段：清理删除掉标记的已经判断为死亡的对象，释放内存空间 因为不需要移动存活对象，所以也是并发执行。 尽管 CMS 收集器采用的并发回收（非独占式），在初始标记与重新标记这两个阶段需要执行 Stop the world 机制来暂停工作线程，但时间很短，而且最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低延迟的。 由于在垃圾收集阶段用户线程没有中断，所以在 CMS 回收过程中，还应该确保应用程序用户线程有足够的内存可用。因此，CMS 收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，而是当堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在 CMS 工作过程中依然有足够的空间支持应用程序运行。要是 CMS 运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用 Serial old 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。 CMS 使用标记清除算法，不可避免会有内存碎片。CMS 只能选择空闲列表进行内存分配。无法使用指针碰撞，只能使用空闲列表执行内存分配。 有人觉得 Mark-Sweep 会造成内存碎片，那么为什么不把算法换成 Mark-Compact？因为在并发清除的时候，用户线程还在执行，那要是把地址修改，明显不合适。所以标记压缩更适合 STW 的场景使用。 CMS 优点： 并发收集 低延迟 CMS 的弊端 会产生内存碎片。大对象无法分配时，导致提前触发 FullGC。 CMS 收集器对 CPU 资源非常敏感。与工作线程并行，占用 CPU 资源，总吞吐量会降低。 CMS 收集器无法处理浮动垃圾。并发标记阶段如果产生新的垃圾对象，CMS 将无法对这些垃圾进行标记，导致回收不及时，只能在下一次执行 GC 时释放。 CMS 设置的参数 -XX:+UseConcMarkSweepGC：手动指定使用 CMS 收集器执行内存任务（作用于老年代） 开启该参数之后，会自动将-XX:+UseParNewGC打开。即：ParNew(Young 区)+CMS(Old 区)+Serial Old 的组合 Serial Old 预备收集，防止 CMS 与用户线程同时进行时内存不足。 -XX:CMSInitiatingOccupanyFraction：设置内存使用率的阈值，达到阈值就开始回收。 JDK5 之前默认为 68%，6 之后为 92%。 如果内存增长缓慢，可以设置一个稍大的值，可以有效降低 CMS 的触发频率。反之，如果应用程序内存增长很快，应该降低这个阈值，避免频繁触发 CMS，甚至触发 Serial Old。 -XX:+UseCMSCompactAtFullCollection：用于指定在执行完 Full GC 后对内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长了。 -XX:CMSFullGCsBeforeCompaction：设置在执行多少次 Full GC 后对内存空间进行压缩整理。 -XX:ParallelCMSThreads：设置 CMS 的线程数量。CMS 默认启动的线程数是（ParallelGcThreads+3)/4，ParallelGcThreads 是年轻代并行收集器的线程数。 8.6.8 区域化分代式（Gabage First） 8.6.8.1 概述 出现背景： 为了适应不断扩大的内存和不断增加的处理器数量，进一步降低暂停时间（pause time），同时兼顾良好的吞吐量。官方给 G1 设定的目标是在延迟可控的情况下获得尽可能高的吞吐量，担当全能收集器的重任与期望。 命名由来： G1 是一个并行回收器，它把内存分割为很多不相关的区域（Region）（物理上不连续的）， 使用不同的 Region 来表示 Eden、幸存者 0 区、幸存者 1 区、老年代。G1 GC 有计划的避免对 Java 堆进行全区域的垃圾收集，而是跟踪各个 Region 里面垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。由于这种方式的侧重点在于回收垃圾最大量的区间（Region)，所以命名为：垃圾优先（Garbage First） 。 Gl (Garbage-First）是一款面向服务端应用的垃圾收集器，主要针对配备多核 CPU 及大容量内存的机器，以极高概率满足 GC 停顿时间的同时，还兼具高吞吐量的性能特征。 发展历史： 在 JDK1.7 版本正式启用，移除了 Experimental（实验）的标识，是 JDK 9 以后的默认垃圾回收器，取代了 CMS 回收器以及 Parallel + Parallel old 组合。被 oracle 官方称为全功能的垃圾收集器。 在 jdk8 中还不是默认的垃圾回收器，需要使用-XX:+UseG1Gc来启用。 CMS 已经在 JDK 9 中被标记为废弃（deprecated）。 8.6.8.2 G1 的优势（特点） 并行与并发 并行性：G1 在回收期间，可以有多个 GC 线程同时工作，有效利用多核计算能力。此时用户线程 STW。 并发性：G1 拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此，一般来说，不会在整个回收阶段发生完全阻塞应用程序的情况。 分代收集： 从分代上看，G1 依然属于分代型垃圾回收器，它会区分年轻代和老年代，年轻代依然有 Eden 区和 survivor 区。但从堆的结构上看，它不要求整个 Eden 区、年轻代或者老年代都是连续的，也不再坚持固定大小和固定数量。 将堆空间分为若干个区域（Region)，这些区域中包含了逻辑上的年轻代和老年代。 和之前的各类回收器不同，它同时兼顾年轻代和老年代。 空间整合： CMS：”标记-清除”算法、内存碎片、若干次 GC 后进行一次碎片整理。 G1 将内存划分为一个个的 region，内存的回收是以 region 作为基本单位的。Region 之间是复制算法，但整体上实际可看作是标记-压缩（Mark-Compact)算法，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 GC。尤其是当 Java 堆非常大的时候，G1 的优势更加明显。 可预测的停顿时间模型 （软实时：Soft Real-Time）： 这是 G1 相对于 CMS 的另一大优势，G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 由于分区的原因，G1 可以只选取部分区域进行内存回收，这样缩小了回收的范围，因此对于全局停顿情况的发生也能得到较好的控制。 G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。保证了 G1 收集器在有限的时间内可以获取尽可能高的收集效率。 相比于 CMS GC，G1 未必能做到 CMS 在最好情况下的延时停顿，但是最差情况要好很多。 G1 回收器的缺点： 相较于 CMS，G1 还不具备全方位、压倒性优势。比如在用户程序运行过程中，G1 无论是为了垃圾收集产生的内存占用（Footprint）还是程序运行时的额外执行负载（overload）都要比 CMS 要高。 从经验上来说，在小内存应用上 CMS 的表现大概率会优于 G1，而 G1 在大内存应用上则发挥其优势。平衡点在 6-8GB 之间。 8.6.8.3 G1 回收器参数设置 -XX:+UseG1GC：手动指定使用 G1 收集器执行内存回收任务。JDK9 不用设置为默认。 -XX:G1HeapRegionsize：设置每个 Region 的大小。值是 2 的幂，范围是 1MB 到 32MB 之间，目标是根据最小的 Java 堆大小划分出约 2048 个区域。默认是堆内存的 1/2000。 -XX:MaxGCPauseMillis：设置期望达到的最大 GC 停顿时间指标(JVM 会尽力实现，但不保证达到)。默认值是 200ms。 -XX:ParallelGCThread：设置 STW 时 GC 的工作线程数的值。最多设置为 8 -XX:ConcGCThreads：设置并发标记的线程数。将 n 设置为并行垃圾回收线程数(ParallelGcThreads)的 1/4 左右。 -XX:InitiatingHeapoccupancyPercent：设置触发并发 GC 周期的 Java 堆占用率阈值。超过此值，就触发 Gc。默认值是 45。 8.6.8.4 G1 的适用场景 面向服务端应用，针对具有大内存、多处理器的机器。(在普通大小的堆里表现并不惊喜) 最主要的应用是需要低 Gc 延迟，并具有大堆的应用程序提供解决方案。 如在堆大小约 6GB 或更大时，可预测的暂停时间可以低于 0.5 秒；(G1 通过每次只清理一部分而不是全部的 Region 的增量式清理来保证每次 GC 停顿时间不会过长）。 用来替换掉 JDK1.5 中的 CMS 收集器;在下面的情况时，使用 G1 可能比 CMS 好: 超过 50%的 Java 堆被活动数据占用。 对象分配频率或年代提升频率变化很大。 GC 停顿时间过长（长于 0.5 至 1 秒）。 HotSpot 垃圾收集器里，除了 G1 以外，其他的垃圾收集器使用内置的 JVM 线程执行 GC 的多线程操作，而 G1 GC 可以采用应用线程承担后台运行的 GC 工作，即当 JVM 的 Gc 线程处理速度慢时，系统会调用应用程序线程帮助加速垃圾回收过程。 8.6.8.5 分区 Region：化整为零 使用 G1 收集器时，它将整个 Java 堆划分成约 2048 个大小相同的独立 Region 块，可以通过-Xx:GlHeapRegionsize 设定。所有的 Region 大小相同，且在 JVM 生命周期内不会被改变。 虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的它们都是一部分 Region（不需要连续）的集合。通过 Region 的动态分配方式实现逻辑上的连续。 一个 region 有可能属于 Eden，Survivor 或者 old/Tenured 内存区域。但是一个 region 只可能属于一个角色，可以变换（如 old 变为 eden）。图中的 E 表示该 region 属于 Eden 内存区域，s 表示属于 survivor 内存区域，o 表示属于 old 内存区域。图中空白的表示未使用的内存空间 Gl 垃圾收集器还增加了一种新的内存区域，叫做 Humongous（大对象）内存区域，如图中的 H 块。主要用于存储大对象，如果超过 1.5 个 region，就放到 H。 设置 H 的原因 对于堆中的大对象，默认直接会被分配到老年代，但是如果它是一个短期存在的大对象就会对垃圾收集器造成负面影响。为了解决这个问题，G1 划分了一个 Humongous 区，它用来专门存放大对象。如果一个 H 区装不下一个大对象，那么 G1 会寻找连续的 H 区来存储。为了能找到连续的 H 区，有时候不得不启动 Full GC。G1 的大多数行为都把 H 区作为老年代的一部分来看待。 对象分配过程：指针碰撞及线程本地分配缓存（TLAB） 8.6.8.6 G1 回收垃圾的过程 主要包括以下三个环节： 年轻代 GC （Young GC） 老年代并发标记过程（Concurrent Marking） 混合回收（Mixed GC） （如果需要。单线程、独占式、高强度的 Full GC 还是继续存在的，它针对 GC 的评估失败提供了一种失败保护机制，即强力回收） 顺时针：Young GC -&gt; Young GC + Concurrent mark -&gt; Mixed GC 顺序进行垃圾回收。 应用程序分配内存，当年轻代的 Eden 区用尽时开始年轻代回收过程；G1 的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1 GC 暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到 survivor 区间或者老年区间，也有可能是两个区间都会涉及。 当堆内存使用达到一定值（默认 45%）时，开始老年代并发标记过程。 标记完成马上开始混合回收过程。对于一个混合回收期，G1 GC 从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的 G1 回收器和其他 GC 不同，G1 的老年代回收器不需要整个老年代被回收，一次只需要扫描/回收小部分老年代的 Region 就可以了。同时，这个老年代 Region 是和年轻代一起被回收的。 举个例子：一个 web 服务器，Java 进程最大堆内存为 4G，每分钟响应 1500 个请求，每 45 秒钟会新分配大约 2G 的内存。G1 会每 45 秒钟进行一次年轻代回收，每 31 个小时整个堆的使用率会达到 45%，会开始老年代并发标记过程，标记完成后开始四到五次的混合回收。 G1 回收过程一：年轻代 GC 当 JVM 启动的时候，G1 先准备好 Eden 区，程序在执行的过程中不断创建对象到 Eden 区，当 Eden 区空间耗尽的时候，G1 会启动一次年轻代垃圾回收过程（只有 Eden 区能触发 GC，幸存者区不会触发，但是 Eden 出发了也会同时回收幸存者区） YGC 时，首先 G1 停止应用程序的执行（stop-The-world），G1 创建回收集（Collection set），回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代 Eden 区和 survivor 区所有的内存分段。 第一阶段，扫描根（GC Roots） 根是指 static 变量指向的对象，正在执行的方法调用链条上的局部变量等。根引用连同 RSet 记录的外部引用作为扫描存活对象的入口。 第二阶段，更新 RSet。 处理 dirty card queue(脏卡表)中的 card，更新 RSet。此阶段完成后，RSet 可以准确的反映老年代对所在的内存分段中对象的引用。 应用程序中引用赋值语句 object.field=object，JVM 会在之前和之后执行特殊的操作以在 dirty card queue(脏卡表)中入队一个保存了对象引用信息的 card。在年轻代回收的时候，G1 会对 dirty card queue(脏卡表)中所有的 card 进行处理，以更新 RSet，保证 RSet 实时准确反映引用关系。 在赋值语句处直接更新 RSet 时，由于 RSet 需要线程同步，开销大，使用队列会提高性能。 第三阶段，处理 RSet。 识别被老年代对象指向的 Eden 中的对象，这些被指向的 Eden 中的对象被认为是存活的对象。 第四阶段，复制对象。 此阶段，对象树被遍历，Eden 区内存段中存活的对象会被复制到 survivor 区中空的内存分段，Survivor 区内存段中存活的对象如果年龄未达阈值，年龄会加 1，达到阀值会被会被复制到 old 区中空的内存分段。如果 survivor 空间不够，Eden 空间的部分数据会直接晋升到老年代空间。 第五阶段，处理引用。 处理 Soft，Weak，Phantom，Final，JNI Weak 等引用。最终 Eden 空间的数据为空，GC 停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。 G1 回收过程二：并发标记过程： 初始标记阶段 标记从根节点直接可达的对象。这个阶段是 STW 的，并且会触发一次年轻代 GC。 根区域扫描(Root Region Scanning) G1 GC 扫描 survivor 区直接可达的老年代区域对象，并标记被引用的对象。这一过程必须在 young gc 之前完成。 并发标记(Concurrent Marking) 在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被 young GC 中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那这个区域会被立即回收。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 再次标记(Remark) 由于应用程序持续进行，需要修正上一次的标记结果。是的。G1 中采用了比 CMS 更快的初始快照算法:snapshot-at-the-beginning (SATB) 独占清理(cleanup,STW) 计算各个区域的存活对象和 GC 回收比例，并进行排序识别可以混合回收的区域。为下阶段做铺垫。是 STW 的。 这个阶段并不会实际上去做垃圾的收集。 并发清理阶段 识别并清理完全空闲的区域。 G1 回收过程三：混合回收 当越来越多的对象晋升到老年代 old region 时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即 Mixed Gc，该算法并不是一个 old Gc，除了回收整个 Young Region，还会回收一部分的 old Region。这里需要注意:是一部分老年代，而不是全部老年代。可以选择哪些 old Region 进行收集，从而可以对垃圾回收的耗时时间进行控制。也要注意的是 Mixed Gc 并不是 Full GC。 并发标记结束以后，老年代中百分百为垃圾的内存分段被回收了，部分为垃圾的内存分段被计算了出来。默认情况下，这些老年代的内存分段会分 8 次（可以通过-xx:G1MixedGccountTarget 设置）被回收。 混合回收的回收集(Collection Set）包括八分之一的老年代内存分段，Eden 区内存分段，Survivor 区内存分段。混合回收的算法和年轻代回收的算法完全一样，只是回收集多了老年代的内存分段。具体过租请参考上面的年轻代回收过程。 由于老年代中的内存分段默认分 8 次回收，G1 会优先回收垃圾多的内存分段。垃圾占内存分段比例越高的，越会被先回收。并且有一个阈值会决定内存分段是否被回收，-XX:G1MixedGCLiveThresholdPercent，默认为 65%，意思是垃圾占内存分段比例要达到 65%才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。 混合回收并不一定要进行 8 次。有一个阈值-xX:G1HeapwastePercent，默认值为 10%，混合回收并不一定要进行 8 次。有一个阈值-XX:G1HeapwastePercent，默认值为 10%，存的比例低于 10%，则不再进行混合回收。因为 GC 会花费很多的时间但是回收到的内存却很少。 G1 回收过程四：Full GC G1 的初衷就是要避免 Full GC 的出现。但是如果上述方式不能正常工作，G1 会停止应用程序的执行（Stop-The-World），使用单线程的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长。 要避免 Full GC 的发生，一旦发生需要进行调整。什么时候会发生 Full Gc 呢?：比如堆内存太小，当 G1 在复制存活对象的时候没有空的内存分段可用，则会回退到 full gc，这种情况可以通过增大内存解决。 导致 G1Full GC 的原因可能有两个: Evacuation 的时候没有足够的 to-space 来存放晋升的对象; 并发处理过程完成之前空间耗尽。 G1 回收过程：补充 从 Oracle 官方透露出来的信息可获知，回收阶段(Evacuation)其实本也有想过设计成与用户程序一起并发执行，但这件事情做起来比较复杂，考虑到 G1 只是回收一部分 Region, 停顿时间是用户可控制的，所以并不迫切去实现，而选择把这个特性放到了 G1 之后出现的低延迟垃圾收集器(即 ZGC) 中。另外，还考虑到 G1 不是仅仅面向低延迟，停顿用户线程能够最大幅度提高垃圾收集效率，为了保证吞吐量所以才选择了完全暂停用户线程的实现方案。 8.6.8.7 G1 回收器优化建议 年轻代大小 避免使用-Xmn 或-XX:NewRatio 等相关选项显示设置年轻代大小，因为固定年轻代的大小会覆盖暂停时间目标 暂停时间目标不要太过严苛 G1 GC 的吞吐量目标是 90%的应用程序时间和 10%的垃圾回收时间 评估 G1 GC 的吞吐量时，暂停时间目标不要太严苛。目标太过严苛表示你愿意承受更多的垃圾回收开销，而这些会直接影响到吞吐量。 8.6.8.8 Remembered Set（记忆集 RSet） G1 比 CMS 多占用 10%~20%的空间，其中一部分就用于存放记忆集。 垃圾回收时可能存在的问题：一个 Region 不可能是孤立的，一个 Region 中的对象可能被其他任意 Region 中对象引用。判断对象存活时，需要扫描整个 Java 堆才能保证准确，如回收新生代也不得不同时扫描老年代，这样的话会降低 Minor Gc 的效率。所有收集器都有这个问题，只是 G1 更严重。 解决方案： 无论 G1 还是其他分代收集器，JVM 都是使用 Remembered Set 来避免全局扫描。 每个 Region 都有一个对应的 Remembered Set，每次 Reference 类型数据写操作时，都会产生一个 write Barrier（写屏障）暂时中断操作，然后检查将要写入的引用指向的对象是否和该 Reference 类型数据在不同的 Region（其他收集器：检查老年代对象是否引用了新生代对象）。 如果不同，通过 cardTable（卡表）把相关引用信息记录到引用指向对象的所在 Region 对应的 Remembered set 中。 当进行垃圾收集时，在 GC 根节点的枚举范围加入 Remembered Set，就可以保证不进行全局扫描，也不会有遗漏。 8.6.9 垃圾回收器对比 Hotspot 有这么多的垃圾回收器，Serial GC、Parallel GC、Concurrent Mark Sweep GC 这三个有什么不同呢？ 如果你要最小化使用内存和并行开销，请选 Serial GC。 如果你想要最大化应用程序的吞吐量，请选 Parallel GC。 如果你想要最小化 GC 的中断或停顿时间，请选 CMS GC。 其他：JDK9 中 CMS 标记过时。JDK14 全面删除 CMS。 Java 垃圾收集器的配置对于 JVM 优化来说是一个很重要的选择，选择合适的垃圾收集器可以让 JVM 的性能有一个很大的提升。 优先调整堆的大小让 JVM 自适应完成。 如果内存小于 100M，使用串行收集器 如果是单核、单机程序，并且没有停顿时间的要求，串行收集器。 如果是多 CPU、需要高吞吐量、允许停顿时间超过 1 秒，选择并行或者 VM 自己选择。 如果是多 CPU、追求低停顿时间，需快速响应（比如延迟不能超过 1 秒，如互联网应用），使用并发收集器。 官方推荐 G1，性能高。现在互联网的项目，基本都是使用 G1。 8.6.10 垃圾回收器的未来发展 G1 GC 在 JDK 10 以后，Full Gc 已经是并行运行，在很多场景下，其表现还略优于 Parallel Gc 的并行 Full GC 实现。 在 Serverless 等新的应用场景下，Serial GC 找到了新的舞台。 CMS GC 因为其算法的理论缺陷等原因，在 JDK9 中已经被标记为废弃，并在 JDK14 版本中移除。 ZGC 的目标是在尽可能对吞吐量影响不大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。《深入理解 Java 虚拟机》一书中这样定义 ZGC: zGc 收集器是一款基于 Region 内存布局的，（暂时）不设分代的，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的标记-压缩算法的，以低延迟为首要目标的一款垃圾收集器。 ZGC 的工作过程可以分为 4 个阶段:并发标记-并发预备重分配-并发重分配-并发重映射等。ZGC 几乎在所有地方并发执行的，除了初始标记的是 STW 的。所以停顿时间几乎就耗费在初始标记上，这部分的实际时间是非常少的。 8.7 常用的显示 GC 日志的参数 内存分配与垃圾回收的参数列表 -XX:+PrintGC：输出 Gc 日志。类似-verbose:gc -XX:+PrintGCDetails：输出 Gc 的详细日志 -XX:+PrintGCTimestamps：输出 Gc 的时间戳（以基准时间的形式) -XX:+PrintGCDatestamps：输出 Gc 的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800) -XX :+PrintHeapAtGC：在进行 GC 前后打印出堆栈信息 -Xloggc:../logs/gc.log：日志文件的输出路径 logs 位于工程目录的根目录下 案例一： 日志分析工具：GCViewer、GCEasy、GcHisto、GCLogViewer、Hpjmeter、garbagecat 等。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://sk370.github.io/tags/JVM/"}]},{"title":"Java语言基础（中）","date":"2022-07-13T07:57:50.000Z","path":"2022/07/13/javase/Java语言基础（中）/","text":"本文介绍了Java语言的高级特性，如多线程、集合、泛型、IO流等，开发中最常使用；网络编程介绍了Java语言如何进行网络通信，是Javaweb开发的基础，企业开发中可能封装成了各种工具使用；反射介绍了Java的反射原理和作用，是理解各种Java框架的基础。 第 8 章 多线程基础 8.1 基本概念 8.1.1 程序、进程、线程 程序（program）：完成特定任务、用某种语言编写的指令集合。是一段静态的代码。 进程（process）：程序的一次执行过程，或者正在运行的一个程序。 进程是系统分配资源的基本单位，根据进程执行的生命周期，系统会为不同时期的进程分配不同的内存空间。 线程（thread）：程序内部的一条执行路径，一个进程可以含有多个进程。 如果一个进程可以并行 执行多个线程，则进程支持多线程。 每个线程拥有独立的运行栈和程序计数器。 同一进程的多个线程共享相同的堆空间（对象、属性共享）、方法区，优点是线程间通信更便捷、高效，但多个线程同时操作公共资源会有安全隐患。 Java 中线程的分类（区别在于 JVM 何时离开）： 守护线程：服务用户线程，通过在 start()方法前调用thread.setDaemon(true)可以把一个用户线程变成守护线程。 垃圾回收是一个典型的守护线程 JVM 中都是守护线程时，JVM 就会退出。 用户线程： 8.1.2 单核、多核 单核：CPU 仅有一个核心数，同一时间内，只能执行一个线程任务。执行多个线程时，采取的是不断切换线程的方式。 由于 CPU 频率高、线程切换时间短，让人感觉”同时“执行了多个线程 多核：CPU 有多个核心，每个核心可以执行一个线程。 java.exe：一个 Java 运行程序至少有 3 个线程： main()：主线程 gc()：垃圾回收线程 异常处理线程：发生异常时，会影响主线程。 8.1.3 并行、并发 并行：多个 CPU 执行多个任务。 并发：一个 CPU 同时执行多个任务。 8.2 创建多线程 8.1 方式一：继承 Thread 类 创建步骤： 资源类继承 Thread 类 资源类中重写 Thread 类中 run()方法 创建 Thread 子类对象 创建一个对象即代表开启一个线程，要开启多个该线程，需要创建多个该对象。 调用子类对象的 start()方法。 注意点： 使用 Tread 子类对象直接调用run()方法不会开启分支线程，它表示在 main 线程内，调用了 Thread 子类对象的方法。 使用 Thread 子类对象调用start()方法会开启一个线程，开启线程后run()方法何时执行全由 CPU 调度决定，即 main 线程和分支线程中的语句执行具有随机性。 一个实例化的 Thread 子类对象只能调用一次star()方法，重复调用时，会抛出异常：IllegalThreadStateException。 一般不采用此方法，因为 Java 的单继承性，导致代码扩展受限。 常用方法： start()：启动当前线程；调用当前线程的 run()方法 run()：通常需要重写 Thread 类中的此方法，将创建的线程要执行的操作声明在此方法中 currentThread()：静态方法，返回执行当前代码的线程 getName()：获取当前线程的名字 setName(&quot;str&quot;)：设置当前线程的名字 yield()：释放调用线程在 cpu 中的执行权，后续执行哪个线程由 CPU 确定，有可能还是这个线程。当前进程进入就绪状态。 join()：在线程 a 中调用线程 b 的 join()，此时线程 a 就进入阻塞状态，直到线程 b 完全执行完以后，线程 a 才结束阻塞状态。 有异常问题，可以根据使用位置进行 throws 或 try-catch 处理。 ~~stop()~~：已过时。当执行此方法时，强制结束当前线程。 sleep(long millitime)：让当前线程“睡眠”指定的 millitime 毫秒。在指定的 millitime 毫秒时间内，当前线程是阻塞状态。 有异常问题，由于该方法使用在run()方法中，而run()方法是对父类 Thread 中run()方法的重写，且 Thread 中run()方法没有抛出异常，根据继承的特性（子类的异常不大于父类），所以子类的run()方法不能抛出异常，只能由 try-catch 处置。 isAlive()：判断当前线程是否存活。 线程优先级： 等级： MAX_PRIORITY：10 MIN _PRIORITY：1 NORM_PRIORITY：5 （默认等级） 方法： getPriority()：返回线程优先等级 setPriority(int num)：设置有限等级 注意点：设置了高等级的优先级，并不代表一定执行完该线程后执行其他线程，而是提高了 CPU 执行该线程的概率而已。 Thread 类构造器 Thread()： Thread(String threadname)：创建指定名称的线程 搭配super(threadname)才能在 getName()时获得名字 Thread(Runnable target)： Thread(Runnable target, String name)： （通常使用此方法创建线程对象） 8.2.2 方式二：实现 Runnable 接口 创建步骤： 定义类，实现 Runnable 接口。 实现类中实现 Runnable 接口中的 run 方法 创建实现类对象 创建 Thread 类对象，将实现类对象作为参数传入。 使用 Thread 类的对象调用start()方法。 两种方式比较： 相同点： 实现类（继承类）都需要重写run() 都具有线程安全问题 不同点： 方式二没有单继承的局限性 方式二更适合多个线程共享数据（数据只有一份）的情况 开发中优先选择方式二 8.2.3 方式三：实现 Callable 接口 创建步骤： 创建一个实现 Callable 的实现类 实现 call 方法，将此线程需要执行的操作声明在 call()中 创建 Callable 接口实现类的对象 创建 FutureTask 的对象，将此 Callable 接口实现类的对象作为传递到 FutureTask 构造器中 创建 Thread 对象，将 FutureTask 的对象作为参数传递到 Thread 类的构造器中，Thread 的对象调用 start() FutureTask 的实例对象调用 get()方法，获取重写 call 方法的返回值。 Callable 的优点： call()可以返回值的。 call()可以抛出异常，被外面的操作捕获，获取异常的信息。 Callable 是支持泛型 8.2.4 方式四：使用线程池（ThreadPool） 创建步骤： 提供指定线程数量的线程池：ExecutorService service = Executors.newFixedThreadPool(10); service1.setCorePoolSize(15) service1.setKeepAliveTime() 执行指定的线程的操作。需要提供实现 Runnable 接口或 Callable 接口实现类的对象 service.execute(Runnable runable) service.submit(Callable callable) 关闭连接池：service.shutdown() 优点： 提高响应速度（减少了创建新线程的时间） 降低资源消耗（重复利用线程池中线程，不需要每次都创建） 便于线程管理： corePoolSize：核心池的大小 maximumPoolSize：最大线程数 keepAliveTime：线程没任务时最多保持多长时间后会终止 8.3 线程的生命周期 新建： 继承方式（方式一）：Thread 类子类的对象被创建。 实现方式（方式二）：Thread 类声明并创建。 就绪：处于新建状态的线程调用 start()后，将进入线程队列等待 CPU 时间片，此时它已具备了运行的条件，只是没分配到 CPU 资源 运行：当就绪的线程被调度并获得 CPU 资源时,便进入运行状态。 阻塞：线程被人为挂起或执行输入输出操作时，会让出 CPU 资源，并临时中止自己的执行，即进入阻塞状态。 阻塞时临时状态，不可以作为最终状态。 死亡：线程完成了它的全部工作、线程被提前强制性地中止、出现异常导致结束。 死亡是线程的最终状态。 8.4 线程安全 5.4.1 线程安全问题——同步机制 线程安全问题：未处理的多线程任务在处理共享数据时，会造成数据破坏（重复数据、缺失数据、数据超范围等）。 原因：处理共享数据的情况时，一个线程多条语句只执行了一部分，未处理完时，另一个线程参与进来，也要处理共享数据，造成共享数据错误。 解决办法：单线程处理数据，执行完后再让其他线程参与——同步机制。 解决原理：给共享资源加锁，第一个访问资源的线程进行资源锁定，在解锁之前其他线程无法访问，解锁之后，其他线程可以锁定并使用。 8.4.2 Synchronized 处理线程安全问题 Synchronized（同步）语法： 同步代码块：synchronized(同步监视器)&#123;&#125; 同步方法：public synchronized void show()&#123;&#125; Synchronized 细节： 同步监视器必须唯一。 同步代码块：同步监视器可设置为类名.class、this、任一对象（静态或非静态），取决于是否唯一。 同步方法：静态方法同步监视器默认为类名.class，非静态方法同步监视器默认为this 同步监视器一般情况： 在实现 Runnable 接口创建多线程的方式中，可以考虑使用 this 充当同步监视器。 在继承 Thread 类创建多线程的方式中，慎用 this 充当同步监视器，考虑使用当前类充当同步监视器。 8.4.3 死锁及 lock 处理线程安全问题 死锁：不同的线程分别占用对方需要的同步资源不放弃，都在等待对方放弃自己需要的同步资源。 出现死锁后，不会出现异常、不会出现提示、程序也不会运行，处于阻塞状态，无法继续。 Lock（JDK5.0 新增）： 引入java.util.concurrent.locks.ReentrantLock;包 java.util.concurrent.locks.Lock接口是控制多个线程对共享资源进行访问的工具。 ReentrantLock类实现了 Lock。 创建ReentrantLock对象：private ReentrantLock lock = new ReenTrantLock(); 根据对象是否唯一（lock 是否唯一），可以在声明时使用 static、或 static final 修饰。 在出现共享资源操作的代码前调用lock()方法 在结束共享资源操作的代码后调用unlock()方法 如果操作资源共享的代码需要使用 try 包裹，则必须把unlock()写入 finally 语句块，lock()则不是必须要写入 try 中 synchronized 与 Lock 的异同： 相同：二者都可以解决线程安全问题 不同： synchronized 机制在执行完相应的同步代码以后，自动的释放同步监视器。 Lock 需要手动的启动同步，同时结束同步也需要手动的实现。 使用的优先顺序：Lock —&gt; 同步代码块（已经进入了方法体，分配了相应资源 ) —&gt;同步方法（在方法体之外) 同步代码块包裹的共享资源操作代码可以更小。 8.4.4 同步的深入理解 同步的范围： 确定同步代码范围时，要将所有操作共享数据的语句包裹在内。 范围太大：操作数据的语句变为单线程的，没有发挥多线程的功能。 范围太小：操作共享数据的语句由遗漏，同步不起作用。 同步的问题： 优点：解决了线程安全的问题。 缺点：操作同步代码时，只有一个线程运行，其他线程等待，相当于单线程过程，效率低。 释放锁的操作： 同步方法、同步代码块执行结束 同步方法、同步代码块中遇到 break、return 同步方法、同步代码块中出现未处理的 Error 或 Exception 同步方法、同步代码块中执行了线程对象的wait() 不会释放锁的操作: 同步方法、同步代码块中调用Thread.sleep()、Thread.yield()方法暂停当前线程的执行 其他线程调用了当前执行线程的suspend()方法将该线程挂起。 应尽量避免使用suspend()和resume()控制线程。 线程安全的懒汉式单例模式123456789101112131415161718192021222324class Singleton &#123; private static Singleton instance = null; private Singleton() &#123; &#125; // 1. 方式一 public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; // 2. 方式二 public static Singleton getInstance() &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 方式一效率优于方式二，假如有 n 个线程需要创建当前对象，多核 CPU 让 k 个线程运行到getInstance()： 方式一中共有 k 个线程判断对象是否等于 null，1 个线程执行同步代码块并创建对象，k-1 个线程执行同步代码块结束判断，后续 n-k 个线程不会再进入同步代码块。 方式二中共有 1 个线程执行同步代码块并创建对象，k-1 个线程执行同步代码块结束判断，后续 n-k 个线程还会执行同步代码块进行判断。 8.5 线程通信 wait()：一旦执行此方法，当前线程就进入阻塞状态，并释放同步监视器。 notify()：一旦执行此方法，就会唤醒被 wait 的一个线程。如果有多个线程被 wait，由 JVM 决定执行哪个。 notifyAll()：一旦执行此方法，就会唤醒所有被 wait 的线程。 说明： 三个方法必须使用在同步代码块或同步方法中。 三个方法的调用者必须是同步代码块或同步方法中的同步监视器。否则，会出现 IllegalMonitorStateException 异常 三个方法是定义在 java.lang.Object 类中。 sleep() 和 wait()的异同 相同点： 都可以使当前进程进入阻塞状态 不同点： 声明位置不同：slee()声明在 Thread 类中，wait()声明在 Object 类中。 调用要求不同：slee()可以在任何需要的场景下调用，wait()必须在同步方法、同步代码块中调用。 sleep()不会释放同步监视器、wait()会释放同步监视器。 第 9 章 集合 9.1 概念 用于存储数量不等的多个对象、具有映射关系的关联数组。 9.2 集合框架体系 单列集合（Collection）：存放单个对象，有两个重要的子接口，其实现子类都是单列集合。 List： ArrayList Vector LinkedList Set： HashSet LinkedHashSet TreeSet 双列集合（Map）：存放 k-v 形式的数据 Hashtable Properties HashMap LinkedHashMap TreeMap 9.3 Collection 9.3.1 Collection 接口的特点 Collection 接口没有直接的实现子类，是通过它的子接口 Set 和 List 实现的。 Collection 实现子类可以存放多个元素，每个元素可以是 Object Collection 的实现类，List 是有序的，Set 不是有序的 Collection 的实现类，有的可以存放重复的元素，有的不可以 9.3.2 Collection 接口实现类遍历元素方式 Tterator（迭代器）： 所有实现了 Collection 接口的集合类都有一个iterator()方法，用以返回一个迭代器 Iterator 仅用于集合遍历，本身并不存放对象。 使用流程： 创建迭代器：Iterator it = 集合对象.iterator() 遍历对象：while(it.hasNext())&#123;it.next()&#125; 使用it.next()之前必须使用hasNext()方法进行判断，否则在调用完最后一条时会抛出NoSuchElementException异常。 while 循环结束后，迭代器指向集合中最后一个元素，如果再调用next()方法，会抛出异常，如果希望再次遍历，则需要重置迭代器：it = 集合对象.iterator() IDEA 快捷模板：itit，Ctrl+j显示所有的快捷模板 增强 for 循环： 语法：for( 元素类型 元素名 : 集合名或数组名)&#123; 访问元素 &#125; 只能用于遍历集合和数组。 本质就是简化版的迭代器（底层代码）。 不可用普通 for 循环，因为子接口 Set 不支持索引 9.3.3 集合的选择 9.4 List 9.4.1 特点 List 集合类的元素有序（添加和取出顺序一致），且可重复。 List 集合类每个元素都有器对应的顺序索引，可通过索引取出元素——使用get() List 接口实现类常用的有ArrayList、LinkedList、Vector 9.4.2 常用方法 变量和类型 方法 描述 void add(int index, E element) 将指定元素插入此列表中的指定位置（可选操作）。 boolean add(E e) 将指定的元素追加到此列表的末尾（可选操作）。 boolean addAll(int index, Collection&lt;? extends E&gt; c) 将指定集合中的所有元素插入到指定位置的此列表中（可选操作）。 boolean addAll(Collection&lt;? extends E&gt; c) 将指定集合中的所有元素按指定集合的迭代器（可选操作）返回的顺序追加到此列表的末尾。 void clear() 从此列表中删除所有元素（可选操作）。 boolean contains(Object o) 如果此列表包含指定的元素，则返回 true 。 boolean containsAll(Collection&lt;?&gt; c) 如果此列表包含指定集合的所有元素，则返回 true 。 static List copyOf(Collection&lt;? extends E&gt; coll) 以迭代顺序返回包含给定 Collection 的元素的 unmodifiable List 。 boolean equals(Object o) 将指定对象与此列表进行比较以获得相等性。 E get(int index) 返回此列表中指定位置的元素。 int indexOf(Object o) 返回此列表中第一次出现的指定元素的索引，如果此列表不包含该元素，则返回-1。 boolean isEmpty() 如果此列表不包含任何元素，则返回 true 。 Iterator&lt;E&gt; iterator() 以适当的顺序返回此列表中元素的迭代器。 int lastIndexOf(Object o) 返回此列表中指定元素最后一次出现的索引，如果此列表不包含该元素，则返回-1。 E remove(int index) 删除此列表中指定位置的元素（可选操作）。 boolean remove(Object o) 从该列表中删除指定元素的第一个匹配项（如果存在）（可选操作）。 boolean removeAll(Collection&lt;?&gt; c) 从此列表中删除指定集合中包含的所有元素（可选操作）。 E set(int index, E element) 用指定的元素替换此列表中指定位置的元素（可选操作）。 int size() 返回此列表中的元素数。 default void sort(Comparator&lt;? super E&gt; c) 根据指定的Comparator引发的顺序对此列表进行排序。 List&lt;E&gt; subList(int fromIndex, int toIndex) 返回指定的 fromIndex （包含）和 toIndex （不包括）之间的此列表部分的视图。 Object[] toArray() 以适当的顺序（从第一个元素到最后一个元素）返回包含此列表中所有元素的数组。 T[] toArray(T[] a) 以适当的顺序返回包含此列表中所有元素的数组（从第一个元素到最后一个元素）; 返回数组的运行时类型是指定数组的运行时类型。 9.4.3 遍历方式 使用 Iterator 迭代器：迭代器的next()取出的就是每个对象 使用增强 for 循环 使用普通 for 循环 9.4.4 ArrayList 特点： 可以存放 null，且可存放多个 基本等同于 Vector，但 ArrayList 线程不安全。 底层： ArrayList 的数据由一个transient修饰的Object[] 存储。可变数组 transient表示不可被序列化 ArrayList()：初始容量为 0，第一次添加元素容量变为 10，后续扩容按照当前容量的 1.5 倍增大 ArrayList(int initialCapacity)：初始容量为 initialCapacity，后续扩容按照当前容量的 1.5 倍增大 IDEA 补充知识： IDEA 在 debug 时，会阉割数据 解决方式：（去掉下列两个红框的勾选）。 9.4.5 Vector 特点： 可以存放 null，且可存放多个 Vector 类的操作方法带有synchronized，所以是线程安全的。 底层： 底层使用Object[]存放数据 Vector()：初始容量 0，第一次添加后 10，不足时按当前容量的 2 倍扩容。 Vector(int initialCapacity)：初始容量为 initialCapacity，不足时按当前容量的 2 倍扩容。 9.4.6 LinkedList 特点： 可以存放 null，且可存放多个 线程不安全，添加删除效率比数组高，查找效率较低 底层： 底层维护了一个双向链表 LinkedeList 中有两个属性 first、last，分别指向链表中第一个和最后一个节点 每个节点都含有 3 个属性：prev、next、item 只有无参构造器LinkedList()，默认容量 0，添加一个数组，容量+1。 LinkedList 和 ArrayList 的选择 改查较多，选择 ArrayList 增删较多，选择 LinkedList 业务中，一般查询较多、增删较少，建议使用 ArrayList 9.5 Set 9.5.1 特点 无序（添加和取出顺序不一定一致），元素不可重复，最多只可包含一个 null。 元素不可重复按照 equals 是否相等理解。（基本数据类型会自动装箱，但包装类重写了 equals 方法，比较的还是具体值；字面量字符串在常量池；构造器创建的字符串比较的也是具体内容（equals）） 但一旦数据确定，每次的取出顺序一致 hashcode 是根据对象的地址值确定的 没有索引 常用实现类有：HashSet、LinkedHashSet、TreeSet 9.5.2 常用方法 变量和类型 方法 描述 boolean add(E e) 如果指定的元素尚不存在，则将其添加到此集合（可选操作）。 boolean addAll(Collection&lt;? extends E&gt; c) 如果指定集合中的所有元素尚未存在（可选操作），则将其添加到此集合中。 void clear() 从该集合中删除所有元素（可选操作）。 boolean contains(Object o) 如果此 set 包含指定的元素，则返回 true 。 boolean containsAll(Collection&lt;?&gt; c) 如果此集合包含指定集合的所有元素，则返回 true 。 static Set copyOf(Collection&lt;? extends E&gt; coll) 返回包含给定 Collection 的元素的 unmodifiable Set 。 boolean equals(Object o) 将指定对象与此 set 进行相等性比较。 boolean isEmpty() 如果此集合不包含任何元素，则返回 true 。 Iterator&lt;E&gt; iterator() 返回此 set 中元素的迭代器。 boolean remove(Object o) 如果存在，则从该集合中移除指定的元素（可选操作）。 boolean removeAll(Collection&lt;?&gt; c) 从此集合中删除指定集合中包含的所有元素（可选操作）。 boolean retainAll(Collection&lt;?&gt; c) 仅保留此集合中包含在指定集合中的元素（可选操作）。 int size() 返回此集合中的元素数（基数）。 default Spliterator&lt;E&gt; spliterator() 在此集合中的元素上创建 Spliterator 。 Object[] toArray() 返回包含此 set 中所有元素的数组。 T[] toArray(T[] a) 返回一个包含此 set 中所有元素的数组; 返回数组的运行时类型是指定数组的运行时类型。 9.5.3 遍历方式 迭代器 增强 for 循环 不能使用普通 for 循环 9.5.4 HashSet 特点： 可以存放 null，但只能存放一个。 相同数据只能存放一个 注意字符串的特殊性（面试题）： 其他对象可以添加成功： 数据存入与取出顺序不一定一致 底层（数据结构）： 底层实际上是 HashMap，而 HashMap 的底层是数组+链表+红黑树 调用 HashSet 构造器后，会先执行 HashMap()构造器，并创建 table 表（数组），初始容量为 0，再取得加载因子 0.75 添加元素时，先计算得到该元素的 hash 值，并转换得到索引值 第一次添加时，将 table 扩容到 16，并把第一次添加的元素放到表中的指定位置 后续添加时，比较索引值，如果表中该索引已有值，则在该值后的数据链中比较，添加到最后 注意：这里比较元素内容是否相同，取决于加入元素的equals()方法，通过重写equals()和hasCode()可以控制元素能否加入 JAVA8 中，如果数据链的数据已有 8 个，先对表按 2 倍进行扩容，如果表已经扩容过，并且表到了 64，则将该表转换为红黑树 平时链表数据没到 8 个，但表到达 12 个（16*0.75），就会对表按照 2 倍进行扩容，并以此类推 数组类型为HashMap$Node，数据类型为HashMap$Node。 9.5.5 LinkedHashSet 特点： HashSet 的子类、并实现了 Set 接口 可以存放 null、但只能存放一个 相同数据只能添加一个 元素取出顺序固定，且与加入顺序一致。 底层： 底层使用了一个 hashtable（数组） 和双向链表存储数据， 数组是HashMap$Node类型 数组有 head 和 tail 数据类型为LinkedHashMap$Entry类型，LinkedHashMap 是 HashMap 的子类。 每个元素有 before 和 after 初始容量为 0，第一次添加时，直接将数组 table 扩容到 16，再添加一个元素时，先求 hash 值，再求索引，确定在表中的位置，然后再将元素添加到链表中【机制同 HashSet】 9.5.6 TreeSet 特点： 使用无参构造器时，仍然是无序的（输出顺序与输入顺序不一致）。 使用带比较器的构造器TreeSet(Comparator&lt;? super E&gt; comparator)：可以设定指定的添加规则及顺序。 添加规则：取决于比较器中比较的属性，比较器返回 0 时则不能加入 顺序：根据比较器指定的顺序或者逆序确定。 匿名内部类的比较器规则会被创建时底层的 compare()调用 底层： 底层就是 TreeMap，TreeMap 底层就是 Entry 存放的数据类型是 TreeMap$Entry 初始化大小是 0，添加一个容量+1 9.6 Map 9.6.1 Map 接口的特点 Map 与 Collection 并列存在，用于保存具有映射关系的数据。key-value（双列元素）。 Map 中的 key 和 value 可以时任意数类型，会封装到HashMap$Node对象中，数据类型为HashMap$Node。 Map 中的 key 不允许重复，原因同 HashSet（key 相同则 hashcode 和索引值相同，在 table 表（数组）的位置相同），相同的 key 等价于替换元素。 Map 中的 value 可以重复 Map 中的 key 可以为 null，value 也可以为 null，key 为 null 时只能添加一个（等价于替换），value 可以多个为 null Map 中常用 String 作为 key，实际上 key 只要是 Object 就都可以（包含基本数据类型） Map 中的 key 和 value 存在一一对应关系，使用get(key)，就可以得到唯一的一个 valuewarning Map 中的数据存储：一对 k-v 是存放在HashMap$Node中的，由于 Node 实现了 Entry 接口，所以在一定理解上，可以说一对 k-v 就是一个 Entry 源码韩顺平分析： k-v 最后是 HashMap$Node node = newNode(hash, key, value, null) k-v 为了方便程序员的遍历，还会创建 EntrySet 集合 ，该集合存放的元素的类型 Entry, 而一个 Entry 对象就有 k,v EntrySet&lt;Entry&lt;K,V&gt;&gt; 即： transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; entrySet 中， 定义的类型是 Map.Entry ，但是实际上存放的还是 HashMap$Node 这是因为 static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; 当把 HashMap$Node 对象 存放到 entrySet 就方便我们的遍历, 因为 Map.Entry 提供了重要方法 K getKey(); V getValue(); 源码个人理解： k-v 在创建时执行的是HashMap$Node node = newNode(hash, key, value, null)，所以 k-v 的数据类型就是HashMap$Node，数组类型也是HashMap$Node。 Map.Entry 有重要的getKey()、getValue()可以获取元素的 k-v 值，有了这两个方法，就大大方便了使用（遍历）。 而在 Map 中，通过keySet()获取Set类型的所有 key，values()获取Collection类型的所有 vaule，将这二者存放在了一个 Set 表中。 这个 Set 表就是 EntrySet 集合，集合中每一个元素是 Entry 类型，拥有 key 和 value：transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; EntrySet 和 Node 都是 HashMap 的内部类 为了使HashMap$Node类型的数据能够用上面两个方法，HashMap$Node 类实现了 Map.Entry：static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; 常用实现类：HashMap（使用率最高）、Hashtable、Properties 9.6.2 Map 接口常用方法 变量和类型 方法 描述 void clear() 从此映射中删除所有映射（可选操作）。 boolean containsKey(Object key) 如果此映射包含指定键的映射，则返回 true 。 Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() 返回此映射中包含的映射的Set视图。 V get(Object key) 返回指定键映射到的值，如果此映射不包含键的映射，则返回 null 。 boolean isEmpty() 如果此映射不包含键 - 值映射，则返回 true 。 Set&lt;K&gt; keySet() 返回此映射中包含的键的Set视图。 V put(K key, V value) 将指定的值与此映射中的指定键相关联（可选操作）。 V remove(Object key) 如果存在，则从该映射中移除键的映射（可选操作）。 int size() 返回此映射中键 - 值映射的数量。 Collection&lt;V&gt; values() 返回此映射中包含的值的Collection视图。 9.6.3 Map 接口遍历方法 方式一：使用 Map 的keySet()方法取出所有的 key，而取出的 key 类型为 Set，因此可以使用两种方式： 增强 for 循环： 使用迭代器： 迭代器的next()取出的是 key 方式二：使用 Map 的values()方法取出所有的 value，而取出的 value 类型为 Collection，因此可以使用两种方式： 增强 for 循环： 使用迭代器： 迭代器的next()取出的是 value 方式三：使用 Map 的entrySet()方法取出所有的 entry，而取出的 entry 类型为 HashMapNode，HashMapNode，HashMapNode，HashMapNode 实现了 Map.Entry，而 Map.Entry 有getKey()和getValue() 使用 EntrySet 的增强 for 循环： 前提条件：使用向上转型，转为 Map.Entry。 使用迭代器： 迭代器的next()取出的是 entry，entry 类型为 HashMapNode，HashMapNode，HashMapNode，HashMapNode 实现了 Map.Entry。 9.6.4 HashMap 特点： 使用率最高 key 不能重复，但 value 可以重复，允许使用 null 的 key 和 null 的 value 如果添加相同的 key，则会覆盖原来的 key-val，等同于修改（替换） 线程不安全 底层： 底层以 key-val 的方式来存储数据（数组及数据均为 HashMap$Node 类型） 与 HashSet 一样，不保证映射顺序（存入与取出不一定一致），因为底层是以 hash 表的方式来存储（数组+链表+红黑树） 扩容机制：同 HashSet， 唯一区别在于 HashSet 计算哈希值是元素，HashMap 计算时是 key 初始化数据容量为 0 9.6.5 LinkedHashMap 特点： 继承了 HashMap，实现了 Map。 线程不安全 如果有相同 key，后者替换前者；value 可以重复，允许使用 null 的 key 和 null 的 value 有序（插入与取出顺序一致，但不会排序） 底层 底层使用数组+双向链表 转 红黑树 初始容量为 0 9.6.6 Hashtable 特点： 键和值都不能为 null，会抛出 NullPointerException 线程安全的 底层： 底层是一个数组，类型为Hashtable$Entry，实现了 Map.Entry。元素类型也为Hashtable$Entry 初始化大小为 11，临界值为 8（11*0.75） 当添加数据数量到达 8 时，对数组进行扩容，扩容为当前容量 _ 2 + 1，新的临界值变为当前容量 _ 0.75 9.6.7 Properties 特点： 继承了 Hashtable：不能添加 null 的 key 或 value，会发生 NullPointerException 线程安全 如果有相同 key，后者替换前者 可以用与xxx.properties文件中，加载数据到Properties类中进行读取和修改 底层 初始容量为 0，底层使用数组，元素类型、扩容机制同 Hashtab 9.6.8 TreeMap 特点： 使用无参构造器时，仍然是无序的（输出顺序与输入顺序不一致）、但输出排序。【2022.09.24，使用时发现是跟 key 是 int 类型是，输出时是排好序的】 使用带比较器的构造器TreeMap(Comparator&lt;? super E&gt; comparator)：可以设定指定的添加规则及顺序。 添加规则：取决于比较器中比较的属性，比较器返回 0 时则不能加入 顺序：根据比较器指定的顺序或者逆序确定。 匿名内部类的比较器规则会被创建时内部的 compare()调用 底层： 底层就是 Entry 存放的数据类型是TreeMap$Entry 初始化大小是 0，添加一个容量+1warning 比较（个人总结）： 注意：容量和 size()不同。 size()指的是有多少实际的数据。 9.7 Collections 工具类 9.7.1 介绍 操作 Set、List、Map 等集合的工具类。 能够对集合进行排序、查找、修改等工作。 9.7.2 排序操作： reverse(List)：反转 List 中的元素顺序 shuffle(List)：对 List 集合进行随机排序，每次调用都进行一次随机 sort(List)：对 List 结合的元素进行自然排序（从小到大） sort(List, Comparator)：根据比较器的规则对 List 集合的元素进行排序 swap(List, int i, int j)：交换 List 和 i 和 j 位置的元素 i 或 j 超范围时抛出索引越界异常 变量和类型 方法 描述 static void reverse(List&lt;?&gt; list) 反转指定列表中元素的顺序。 static void shuffle(List&lt;?&gt; list) 使用默认的随机源随机置换指定的列表。 static &lt;T extends Comparable&lt;? super T&gt;&gt; void sort(List list) 根据其元素的natural ordering ，将指定列表按升序排序。 static void sort(List list, Comparator&lt;? super T&gt; c) 根据指定比较器引发的顺序对指定列表进行排序。 static void swap(List&lt;?&gt; list, int i, int j) 交换指定列表中指定位置的元素。 9.7.3 查找、替换 变量和类型 方法 描述 static &lt;T extends Object &amp; Comparable&lt;? super T&gt;&gt; T max(Collection&lt;? extends T&gt; coll) 根据元素的 自然顺序返回给定集合的最大元素。 static T max(Collection&lt;? extends T&gt; coll, Comparator&lt;? super T&gt; comp) 根据指定比较器引发的顺序返回给定集合的最大元素。 static &lt;T extends Object &amp; Comparable&lt;? super T&gt;&gt; T min(Collection&lt;? extends T&gt; coll) 根据元素的 自然顺序返回给定集合的最小元素。 static T min(Collection&lt;? extends T&gt; coll, Comparator&lt;? super T&gt; comp) 根据指定比较器引发的顺序返回给定集合的最小元素。 static int frequency(Collection&lt;?&gt; c, Object o) 返回指定集合中等于指定对象的元素数。 static boolean replaceAll(List list, T oldVal, T newVal) 用列表替换列表中所有出现的指定值。 static void copy(List&lt;? super T&gt; dest, List&lt;? extends T&gt; src) 将一个列表中的所有元素复制到另一个列表中。 使用copy()方法前必须确认目标集合与源集合有同样的 size（注意 size 不是容量），否则会发生数组越界异常。 第 10 章 泛型 10.1 泛型的引出 10.1.1 个人理解 集合中的添加的元素可以是任意类型，但某些特定需求下，需要将添加的类型限定在某一种类型，以保证开发效率、代码规范等目的。 这种情况下靠人为去控制添加的元素类型不具有显示意义，编译器也不会在编译程序的时候指出这种失误。 为了保证添加的元素类型一致、编译器能够在编译阶段（程序书写阶段）就发现错误、后续的代码中简化代码，java 在 JDK5.0 时引入了泛型，它在集合、接口等有确定传入类型需求的类、接口声明时添加了占位元素，使得在使用时，用指定的元素类型替换占位元素，以达到上述目的。 10.1.2 官方理解 泛型又称参数化类型，用于解决数据类型的安全性问题 需要在类声明或实例化时指定具体类型。 java 泛型可以保证程序在编译时不出现警告，运行时也就不会产生 ClassCastException，同时能够使 diamagnetic 更简洁、健壮。 泛型的作用是：可以在类声明时，通过一个标识，表示类中某个属性、某个方法的返回值、某个方法的参数是指定类型。 10.2 使用泛型 10.2.1 泛型的实例化 在创建对象时，&lt;&gt;中写上具体的参数类型。 例子：List&lt;String&gt; list = new ArrayList&lt;String&gt;(); 简写：List&lt;String&gt; list = new ArrayList&lt;&gt;(); 好处：由编译器去自动推断数据类型，进一步保证数据安全。 细节： 写入的数据类型只能是引用类型，传入基本数据类型会报错 可以写入声明时指定的类型，或指定类型的子类型 &lt;&gt;内不传入时，默认的泛型是 Object 10.2.2 获取泛型的方法 getGenericSuperclass()：与getSuperclass()方法类似，获取带泛型的父类类型。返回类型为Type。 10.3 自定义泛型 10.3.1 自定义泛型类 泛型的声明 class 类名&lt;K, V&gt;&#123;&#125; K、V、T 都不表示具体值或具体的类型，而是一个占位标记符，可以是任意标识符。 一般 E 使用在集合中 T 使用在类中 K 表示键 V 表示值 N 表示数值类型 ?表示不确定的 java 类型 &lt;&gt;内可以写入多个，表示多个泛型 声明细节： 泛型可以是属性、方法参数、方法返回值、构造器参数 泛型是数组时，不能在声明时初始化。 数组在new时，不确定泛型类型就不能在内存中开辟空间 静态属性、静态方法（参数、返回值）不能是泛型 静态属性和静态方法随类的加载而加载，此时对象还没有创建，JVM 无法初始化泛型 泛型的类型在创建对象时确定 如果没有指定类型，则默认为 Object 10.3.2 自定义泛型接口 泛型的声明 interface 接口名&lt;T&gt;&#123;&#125; K、V、T 都不表示具体值或具体的类型，而是一个占位标记符，可以是任意标识符。 一般 E 使用在集合中 T 使用在类中 K 表示键 V 表示值 N 表示数值类型 ?表示不确定的 java 类型 &lt;&gt;内可以写入多个，表示多个泛型 声明细节： 泛型可以是方法参数、方法返回值 不可以是属性，接口的属性是public static final 静态方法（参数及返回值）不能是泛型 泛型的类型在继承接口或实现接口时确定 如果没有指定类型，则默认为 Object，且实现类默认使用 Object 继承接口时确定：子接口的实现类会自动使用子接口确定的类型 实现接口时确定：实现类会自动使用实现类确定的类型 10.3.3 自定义泛型方法 泛型的声明：修饰符 &lt;参数类型列表&gt; 返回类型 方法名(参数列表)&#123;&#125; 声明细节： 可以定义在普通类、普通接口中，也可以定义在泛型类、泛型接口中。 泛型方法被调用时类型会确定 public void eat(E e)&#123;&#125;修饰符后没有泛型标识，表示这个方法不是泛型方法，而是方法使用了（类声明的）泛型 泛型方法可以使用类声明的泛型，也可以使用自己声明的泛型 10.4 泛型的继承和通配符 通配符注意点： 第 11 章 IO 流 11.1 文件 11.1.1 文件流 流：数据在数据源（文件）和程序（内存）之间经历的路径 输入流：数据从数据源（文件）到程序（内存）的路径 输出流：数据从程序（内存）到数据源（文件）的路径 11.1.2 常用操作 创建文件对象 new File(String str)：通过指定路径创建 File 实例 File(File parent, String child)：通过父目录文件和子路径搭配创建 File 实例 File(String parent, String child)：通过父目录和子路径搭配创建 File 实例 获取文件相关信息 createNewFile()：在指定路径生成目标文件，并保存在磁盘里 该方法有异常问题 getName()：获取文件名 getAbsolutePath()：获取文件的绝对路径（带文件名） getParent()：获取父级目录 length()获取文件内容有多少字节，根据文件的编码获取 UTF-8 编码中，汉字 3 个字节 exists()：文件是否存在 isFile()：是不是一个文件 isDirectory()：是不是一个目录 目录操作和文件删除： delete()：删除文件或空文件夹 mkdir()：创建一级目录 mkdirs()：创建多级目录 11.2 IO 流原理及分类 11.2.1 I/O 流原理 用于数据传输，如读写文件，网络通讯。 java.io 包下提供了各种流（stream）类和接口，用以获取不同类的数据，并通过方法输入或输出数据。 输入 input：读取外部数据到内存（程序）中 输出 output：将程序（内存）中的数据输出到存储设备中。 11.2.2 流的分类：写、out：自身 → 外部，读、input：外部 → 自身 按照操作数据单位分：字节流（8 字节）二进制文件、字符流（按字符）文本文件 按数据流向：输入流、输出流 按流的角色：节点流、处理流/包装流 节点流：从一个特定的数据源读写数据 数据源：存放数据的地方，可以是文件、字符串、数组、管道 处理流/包装流：建立在已有的流之上，为程序提供更强大、更灵活的读写功能 - 如 BufferedReader、BufferedWriter - 处理流类的底层封装了节点流的 Wrtie 或 Reader 对象，根据多态，可以使用各种 Writer 和 Reader 的子类对象 节点流和处理流的区别和联系： 节点流是底层流，直接更数据源相接 处理流包装节点流，可以消除不同节点流的实现差异，也可以提供更方便的方法完成输入输出 处理流对节点流的包装，使用了修饰器涉及模式，不会直接与数据源相连 处理流提高了性能：主要以增加缓冲的方式提高输入输出的效率 处理流提供了操作的便捷：处理流可能提供了一系列编辑的方法来一次输入输出大批量的数据，使用更加灵活方便。 | 抽象基类 | 字节流 | 字符流 | | -------- | ------------ | ------ | | 输入流 | InputStream | Reader | | 输出流 | OutputStream | Writer | java 的 io 流涉及 40 多个类，都是从上述 4 个抽象基类派生而来 这 4 个基类派生出来的子类名称都是以基类名作为子类名的后缀 IO 流体系图 11.3 节点流/文件流 11.3.1 FileInputStream 创建文件输入流对象：new FileInputStream(filePath)： 有异常问题 read()：从输入流中读取一个字节的数据， 到达文件末尾时，返回-1（读取完毕） 有异常问题 读入的字节数据是 ASCII 编码，使用字符强转可得到原文 read(byte[] b)：一次读取 b.length()个字节存入到 b 中，返回实际读取的字节数量 到达文件末尾时，返回-1（读取完毕） 有异常问题 读入的字节数据是 ASCII 编码组成的数组，转换为字符串时可使用String(byte[] bytes, int offset, int length)构造器 offset：byte[]数组起点下标 length：读入 byte[]字节长度 read(byte[] b, int off, int len)：开发中使用这个 到达文件末尾时，返回-1（读取完毕） 有异常问题 close()：关闭文件输入流，并释放与该流相关的所有系统资源 由于创建输入流对象有异常问题，该方法须在 finally 块中调用，以保证确实执行。 11.3.2 FileOutputStream 创建文件输出流对象： new FileOutputStream(filePath)：覆盖模式 new FileOutputStream(filePath, true)：追加模式：第二个参数设置为 true 时，输出的字节流是追加的方式 有异常问题 write(int b)：将单个字节数据输出到输出流，传入的是一个 ASCII 整数，或者一个字符（会自动类型转换为整型） 有异常问题 write(byte[] b)：将 b.length()个字节输出到输出流 有异常问题 可搭配 String 的getBytes(str)使用，将字符串转换为字符数组 write(byte[] b, int off, int len)：将字节数组 b，从下标 off 开始的 len 个字节数据输出到输出流。开发中使用这个 有异常问题 close()：关闭文件输出流，并释放与该流相关的所有系统资源 由于创建输出流对象有异常问题，该方法须在 finally 块中调用，以保证确实执行。 11.3.3 文件拷贝 搭配输入输出流使用 11.3.4 FileReader 创建文件读取对象：new FileReader(filePath)： 有异常问题 reader()：读取单个字符，读入的是个整数 到达文件末尾时，返回-1（读取完毕） 有异常问题 读入的字符数据是 ASCII 编码，使用字符强转可得到原文 read(byte[] b)：一次读取 b.length()个字符存入到 b 中，返回实际读取的字符数量 到达文件末尾时，返回-1（读取完毕） 有异常问题 读入的字符数据是 ASCII 编码组成的数组，转换为字符串时可使用String(char[] chars, int offset, int length)构造器 offset：char[]数组起点下标 length：读入 char[]字节长度 read(char[] b, int off, int len)：开发中使用这个 到达文件末尾时，返回-1（读取完毕） 有异常问题 close()：关闭文件读取对象，并释放与该流相关的所有系统资源 由于创建文件读取对象有异常问题，该方法须在 finally 块中调用，以保证确实执行。 11.3.5 FileWrite 创建文件写入流对象： new FileWriter(filePath)：覆盖模式 new FileWriter(filePath, true)：追加模式：第二个参数设置为 true 有异常问题 write(int b)：将单个字符数据输出到输出流，传入的是一个 ASCII 整数，或者一个字符（会自动类型转换为整型） 有异常问题 write(char[] b)：将 b.length()个字符输出到输出流 有异常问题 write(char[] b, int off, int len)：将字符数组 b，从下标 off 开始的 len 个字符数据输出到输出流。开发中使用这个 有异常问题 write(String str)：写入整个字符串 write(String str, int off, int len)：写入字符串指定部分 close()：关闭文件输出流，并释放与该流相关的所有系统资源 由于创建输出流对象有异常问题，该方法须在 finally 块中调用，以保证确实执行。 FileWtrie 必须关闭流(close())或(flush()，否则不能真正保存到文件 等价于 flush()和关闭 11.4 处理流 BufferedReader 和 BufferedWriter 属于字符流，一般用于处理文本文件，处理声音、视频、doc、pdf 等二进制文件时有数据丢失的风险。 关闭流时，只需要关闭外层的 BufferedReader 或 BufferedWriter 即可，而不是传入的 Reader、Writer 对象，底层会自动关闭对应的传入 Reader、Writer 对象 BufferedInputStream 和 BufferedOutputStream 属于字节流，既可处理文本文件，也可处理二进制文件 11.4.1 BufferedReader 创建对象：new BufferedReader(Reader in)： 有异常问题。 异常存在于传入的 Reader in 对象 readLine()：按行读取，读取完毕返回 null 不读入换行 11.5.2 BufferedWriter 创建对象：new BufferedWriter(Writer out) 有异常问题。 异常存在于传入的 Writer out 对象 write(String s, int off, int len)：写入字符串的一部分 好像不要 off 和 len 的时候是传入整个字符串？ newLine()：插入一个和系统相关的换行符 11.4.3 BufferedInputStream new BufferedInputStream(InputStream in) 类似 BufferedReader 使用的方法是：read(byte[] b, int off, int len) 11.4.4 BufferedOutputStream new BufferedOutputStream(OutputStream out) 类似于 BufferedWriter 使用的方法是：write(byte[] b, int off, int len) 11.4.5 序列化和反序列化 序列化：保存值和数据类型到文件 反序列化：将保存到文件的数据值和数据类型进行恢复 对象要可序列化，则要求该对象实现Serializable或Externalizable接口。 一般使用Serializable，该接口为标记接口——没有任何属性和方法 Externalizable，继承自Serializable，需要重写两个方法 注意事项 读写顺序要一致 序列化或反序列化的对象必须实现Serializable或Externalizable接口。 序列化的对象建议添加private static final long serialVersionUID，提高兼容性 序列化的对象，会默认初始化除 static 和 transient 修饰的成员 序列化的对象，其属性类型也需要实现序列化接口，有异常问题 序列化具有继承性，其子类自动可序列化 11.4.6 ObjectOutputStream 特征：字节流、处理流、对象流、提供序列化功能 创建对象：new ObjectOutputStream(OutputStream out) writeInt(int)：自动装箱，写一个 32 位的整形数 writeBoolean(boolean)：自动装箱 writeChar(int)：自动装箱，写一个 16 位字符，可传入字符，会自动转换为 int writeDouble(double)：自动装箱，写一个 64 位双精度浮点数 writeUTF(String)：以 UTF-8 模式写入字符串 writeObject(Object)：将指定对象写入 close() 上述均有异常问题。 11.4.7 ObjectInputStream 特征：字节流、处理流、对象流、提供反序列化功能，反序列化顺序必须与序列化顺序一致。 创建对象：new ObjectInputStream(InputStream input) readInt() readBoolean() readChar() readDouble() readUTF() readObject()：底层表现多塔的特征，编译类型是 Object，运行类型是实际的类型，底层会进行造型和强转。但如果使用运行类型的方法，需要进行显式强转。 close() 上述均有异常问题 11.5 标准输入流和输出流（java.lang.Object） System.in类：编译类型为InputStream、运行类型为BufferedInputStream 标准输入 输入源键盘 返回InputStream System.out类：编译类型为PrintStream、运行类型为PrintStream 标准输出 输出源显示器 返回OutputStream 11.6 转换流 InputStreamReader（字符流）： 默认情况下，读取文件时按照 utf-8 的形式， Reader的子类 可以将InputStream（字节流）包装成/转换成Reader（字符流） InputStreamReader(InputStream in, Charset cs)：按照读取文件的 cs 编码格式进行读入 OutputStreamWriter（字符流）： 指定处理编码 Writer的子类 可以将OutputStream（字节流）转换成Writer（字符流） OutputStreamReader(OutputStream in, Charset cs) 主要用于解决中文文本数据 中文文本数据有乱码问题 中文文本按照字符流处理（读取和写入）效率更高 11.7 打印流 只有输出流没有输入流 11.7.1 PrintStream 构造器（常用）： PrintStream(OutputStream out)： PrintStream(System.out)：默认屏幕输出 PrintStream(File file)：可指定编码 PrintStream(String fileName)：可指定编码 PrintStream(Writer out)：可指定编码 使用：PrintStream out = System.out out.print()：默认情况下，输出位置是标准输出——显示器 print()底层使用的是write()，故可以使用字节流的write()方法进行时输出 修改输出位置：System.setOut(PrintStream ps)，然后调用out.print() 根据创建的ps对象，可以输出到指定文件 需要关闭。 11.7.2 PrintWriter 构造器： PrintWriter(OutputStream out)： PrintWriter(System.out)：默认屏幕输出 PrintWriter(File file)：可指定编码 PrintWriter(String fileName)：可指定编码 PrintWriter(Writer out)：可指定编码 使用： 调用print()方法 需要关闭。 11.8 Properties 作用：专门用于读写配置文件的集合类 配置文件格式：键=值 键值对不需要有空格 值不需要使用引号 默认类型为 String 创建对象：new Properties() 常见方法： load(Reader reader) loaad(InputStream instream)：加载配置文件 list(PrintWriter out)：将此属性列表打印到指定的输出流（键值对的形式） list(System.out)：屏幕输出 list(PrintStream out) getProperty(String key)：根据 key 获取 value setProperty(String key, String value)：修改、添加键值对到 Properties 类的对象（此时还没添加到文件里） store(OutputStream out, String comments)：将 k-v 存储到文件中 comments 会添加文件的开头，以#注释，一般可置为 null store(Writer writer, String comments) IDEA 中使用路径： 绝对路径 工程路径：src/io.properties 模块路径：模块名/src/io.properties 11.9 NIO 11.9.1 概述 可以理解为 No Blocking IO，是从 Java1.4 版本开始引入的一个新的 IO API，可以替代标准的 Java IO API。 NIO 与 IO 有同样的作用和目的，但使用方式完全不同，NIO 支持面向缓冲区、基于通道的 IO 操作，可以更加高效地进行文件读写操作。用于解决高并发、I/O 高性能问题。 NIO 模型图说明： 每个 Channel 对应一个 Buffer。 Selector 对应一个线程，一个线程对应多个 Channel。 该图反应了有三个 Channel 注册到该 Selector。 程序切换到那个 Channel 是由事件决定的（Event）。 Selector 会根据不同的事件，在各个通道上切换。 Buffer 就是一个内存块，底层是一个数组。 数据的读取和写入是通过 Buffer，但是需要 flip()切换读写模式，而 BIO 是单向的，要么输入流要么输出流。 11.9.2 NIO 对比 BIO BIO 即传统 IO，由于会发生阻塞（资源不足，线程等待）。 在高并发服务器的实现中，BIO 模型中客户端每发起一个连接服务器的请求就会启动一个线程，NIO 模型中，服务器处理多个请求，客户端发送的连接请求都会注册到多路复用器上，多路复用器轮循到连接有 I/O 请求就进行处理。 上述模型中，BIO 的问题在于请求 i 过多会过度消耗内存资源。NIO 模型中的 selector 是一个多路复用接口，阻塞的同时监听多个客户端的 IO 请求，一旦收到 IO 请求就调用对应的函数处理。 IO NIO 面向流（Stream Orientend） 面向缓冲区（Buffer Oriented） 阻塞 IO（Blocking IO） 非阻塞 IO（Non Blocking IO） 单向（输入流、输出流） 双向（缓冲区） 无 选择器（Selectors） BIO 即同步阻塞 IO，也就是干完一件事，再去干别的事。这种 IO 简单，但是效率低下。 JDK1.4 之后出来了 NIO，即同步非阻塞，也就是这个线程依然要等待返回结果，但是可以去干点别的事，不用一直在这等着了。 JDK1.7 之后又出了 NIO2.0 也就是 AIO，这就是异步非阻塞，即这个线程连结果都不等待了，直接返回干别的事，返回结果操作系统会通知相应的线程来进行后续的操作。 11.9.3 NIO 核心之一（Buffer 缓冲区） 11.9.3.1 创建缓冲区 缓冲区负责数据的存取，底层为数组，基本数据类型除 boolean 均有对应的缓冲类。Heap 开头的子类，数据是存放在 JVM 堆中的。 非直接缓冲区：在 JVM 堆中创建缓冲区：allocate(int capacity) 直接缓冲区：在系统内存创建缓冲区：allocatDirect(int capacity) 没有 os 和 jvm 间的 copy 过程，可以提高效率。 弊端： 在物理内存中开辟、销毁空间，资源消耗大。 不易控制，垃圾回收时机不易控制。 通过数组创建缓冲区：wrap(byte[] arr) 11.9.3.2 java.nio.Buffer 的核心属性 capacity：容量，表示缓冲区中最大存储数据的容量，一旦声明不可改变（因为底层是数组）。 limit：界限，表示缓冲区中可操作数据区域的大小，limit 之后的数据不能读写。 position：位置，表示缓冲区中正在操作的数据的位置。 mark：标记，表示对某位置进行标记，调用reset()重置时，position 会回到 mark 标记的地方。 0&lt;=mark&lt;=position&lt;=limit&lt;=capacity 11.9.3.3 java.nio.Buffer 的常用方法 获取缓冲区：allocate(int capacity)，静态方法，获取指定大小的缓冲区。 如ByteBuffer.allocate(1024)代表获取一个 1024 字节大小的缓冲区。 存入数据到缓冲区：put() 获取缓冲区中的数据：get() 切换读写数据模式：flip() 将 limit 设置为当前 position 位置。 将当前 position 位置设置为 0。 丢弃 mark 标记。 可重复读数据：rewind() 将当前 position 位置设置为 0。 还原数组状态：clear()，注意数据并未清空，处于被遗忘状态，可以读到，但数据的位置、界限不可见。 将 position 设置为：0 将限制 limit 设置为容量 capacity 丢弃标记 mark 返回可操作数的数量：hasRemaining() 11.9.3.4 MappedByteBuffer MappedByteBuffer 存放在堆外的直接内存中，可以映射到文件。 MappedByteBuffer 允许 Java 程序直接从内存中读取文件内容，通过将整个或部分文件映射到内存，由操作系统来处理加载请求和写入文件，应用只需要和内存打交道，这使得 IO 操作非常快。 Mmap 内存映射和普通标准 IO 操作的本质区别在于它并不需要将文件中的数据先拷贝至 OS 的内核 IO 缓冲区，而是可以直接将用户进程私有地址空间中的一块区域与文件对象建立映射关系，这样程序就好像可以直接从内存中完成对文件读/写操作一样。 只有当缺页中断发生时，直接将文件从磁盘拷贝至用户态的进程空间内，只进行了一次数据拷贝，对于容量较大的文件来说（文件大小一般需要限制在 1.5~2G 以下），采用 Mmap 的方式其读/写的效率和性能都非常高，RocketMQ 就使用了该技术。 11.9.4 NIO 核心之二（Channel） 11.9.4.1 通道的理解 引用程序使用 IO 接口时，最终会调用到本地主机的 IO 接口，而早期计算机 IO 接口的调用均是由 CPU 完成，导致 CPU 占用过高。后来发展中出现了 DMA(Direct Memory Access，直接存储器访问) ，在进行 DMA 传输时，DMA 控制器掌管总线（CPU 释放总线控制权）。 后续发展中，又衍生出通道替代 DMA，完全替代 IO 操作，进一步提高性能。 Channel 是双向的，一个对象既可以调用读取的方法也可以调用写出的方法。Channel 在读取和写出的时候，要使用 ByteBuffer 作为缓冲数组。 11.9.4.2 获取通道 java.nio.channels.Channel 主要实现类： FileChannel：从文件读取数据 DatagramChannel：读写 UDP 网络协议数据 SocketChannel：读写 TCP 网络协议数据 ServerSocketChannel：可以监听 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel。 获取方式一：通过支持通道的类的 getChannel()方法。 FileChannel：FileInputStream、FileOutputStream、RandomAccessFile 的 getChannel()方法。 SocketChannel：java.net.Socket 的 getChannel()方法。 DatagramChannel：java.net.DatagramSocket 的 getChannel()方法。 ServerSocketChannel：java.net.ServerSocket 的 getChannel()方法。 获取方式二：对应通道类的静态 open()方法。 获取方式三：java.nio.file.Files 工具类的 newByteChannel()。 11.9.4.3 使用通道 使用步骤为：获取通道、创建字节缓冲区、读写操作、关闭流。 核心方法： write(buf)：将 buf 中的数据写入到 channel 中。 read(buf)：从 channel 中读取数据到 buf 中。 文件复制案例： 123456789101112131415161718public class DemoFileChannel &#123; public static void main(String[] args) throws Exception&#123; FileInputStream fis = new FileInputStream(&quot;day19\\\\aaa\\\\123.txt&quot;); FileOutputStream fos = new FileOutputStream(&quot;C:\\\\Users\\\\jin\\\\Desktop\\\\123.txt&quot;); FileChannel c1 = fis.getChannel(); FileChannel c2 = fos.getChannel(); ByteBuffer buffer = ByteBuffer.allocate(1024); while (c1.read(buffer) != -1)&#123; buffer.flip(); c2.write(buffer); buffer.clear(); &#125; c1.close(); c2.close(); fis.close(); fos.close(); &#125;&#125; 结合 MappedByteBuﬀer（直接缓冲区）实现高效读写 ： 12345678910111213141516171819public class Demo01_2G &#123; public static void main(String[] args) throws IOException &#123; RandomAccessFile f1 = new RandomAccessFile(&quot;C:\\\\资料\\\\小资料\\\\文件设置加密.avi&quot;,&quot;r&quot;); RandomAccessFile f2 = new RandomAccessFile(&quot;C:\\\\Users\\\\jin\\\\Desktop\\\\复制.avi&quot;,&quot;rw&quot;); FileChannel c1 = f1.getChannel(); FileChannel c2 = f2.getChannel(); long size = c1.size(); MappedByteBuffer buffer1 = c1.map(FileChannel.MapMode.READ_ONLY, 0, size); MappedByteBuffer buffer2 = c2.map(FileChannel.MapMode.READ_WRITE, 0, size); for(int i=0; i&lt;size; i++)&#123; byte b = buffer1.get(); buffer2.put(b); &#125; c1.close(); c2.close(); f2.close(); f1.close(); &#125;&#125; 通道之间的数据传输：transferFrom()与 transferTo()（直接缓冲区方式） 123456789101112public class Demo01_2G &#123; public static void main(String[] args) throws IOException &#123; RandomAccessFile f1 = new RandomAccessFile(&quot;C:\\\\资料\\\\小资料\\\\文件设置加密.avi&quot;,&quot;r&quot;); RandomAccessFile f2 = new RandomAccessFile(&quot;C:\\\\Users\\\\jin\\\\Desktop\\\\复制.avi&quot;,&quot;rw&quot;); FileChannel c1 = f1.getChannel(); FileChannel c2 = f2.getChannel(); c1.transferTo(0, c1.size(), c2); // c2.transferFrom(c1, 0, c1.size()); c1.close(); c2.close(); &#125;&#125; 11.9.4.4 分散读取与聚集写入 分散读取（Scattering Reads）是指从 Channel 中读取的数据“分散”到多个 Buffer 中。（注意缓冲区的顺序） 聚集写入（Gathering Writes）是指将多个 Buffer 中的数据“聚集”到 Channel。按照缓冲区的顺序，写入 position 和 limit 之间的数据到 Channel 。 1234567891011121314151617public class Demo02 &#123; public static void main(String[] args) throws IOException &#123; RandomAccessFile raf1 = new RandomAccessFile(&quot;1.txt&quot;, &quot;rw&quot;); FileChannel channel1 = raf1.getChannel();//获取通道 ByteBuffer buf1 = ByteBuffer.allocate(100); ByteBuffer buf2 = ByteBuffer.allocate(1024); ByteBuffer[] bufs = &#123;buf1, buf2&#125;; channel1.read(bufs);//分散读取 for( ByteBuffer byteBuffer : bufs)&#123; byteBuffer.flip(); &#125; RandomAccessFile raf2 = new RandomAccessFile(&quot;2.txt&quot;, &quot;rw&quot;)); FileChannel channel2 = raf2.getChannel();//获取通道 channel2.write(bufs);//聚集写入 &#125;&#125; 11.9.4.5 阻塞式网络通信 SocketChannel 类：TCP 客户端使用 SocketChannel 与服务端进行交互的流程为： 打开通道，连接到服务端： SocketChannel channel = SocketChannel.open(); 打开通道，此时还没有打开 TCP 连接 channel.connect(new InetSocketAddress(&quot;localhost&quot;, 9090)); 连接到服务端 分配缓冲区： ByteBuffer buf = ByteBuffer.allocate(10); 分配一个 10 字节的缓冲区，不实用，容量太小 配置是否为阻塞方式（默认为阻塞方式）： channel.configureBlocking(false);配置通道为非阻塞模式 与服务端进行数据交互。 关闭连接：channel.close(); ServerSocketChannel 类：网络通信 IO 操作，TCP 协议，针对面向流的监听套接字的可选择通道（一般用于服务端），流程如下： 打开一个 ServerSocketChannel 通道： ServerSocketChannel server = ServerSocketChannel.open(); 绑定端口 server.bind(new InetSocketAddress(9090)); 阻塞等待连接到来，有新连接时会创建一个 SocketChannel 通道，服务端可以通过这个通道与连接过来的客户端进行通信。等待连接到来的代码一般放在一个循环结构中。 SocketChannel client = server.accept(); 通过 SocketChannel 与客户端进行数据交互 关闭 SocketChannel：client.close(); 11.9.5 NIO 核心之三（Selector 选择器） 11.9.5.1 使用选择器 获取选择器：Selector selector = Selector.open(); 将通道注册到选择器上：socketChannel.register(selector, ops);，ops 取值如下： 读 : SelectionKey.OP_READ （ 1） 写 : SelectionKey.OP_WRITE （ 4） 连接 : SelectionKey.OP_CONNECT （ 8） 接收: SelectionKey.OP_ACCEPT （ 16） 若注册时不止监听一个事件， 则可以使用“ 位或” 操作符连接。 常用方法： 方 法 描 述 Set keys() 所有的 SelectionKey 集合。 代表注册在该 Selector 上的 Channel selectedKeys() 被选择的 SelectionKey 集合。 返回此 Selector 的已选择键集 int select() 监控所有注册的 Channel， 当它们中间有需要处理的 IO 操作时，该方法返回， 并将对应得的 SelectionKey 加入被选择的 SelectionKey 集合中， 该方法返回这些 Channel 的数量。 int select(long timeout) 可以设置超时时长的 select() 操作 int selectNow() 执行一个立即返回的 select() 操作， 该方法不会阻塞线程 Selector wakeup() 使一个还未返回的 select() 方法立即返回 void close() 关闭该选择器 11.9.5.2 SelectionKey SelectionKey 表示 SelectableChannel 和 Selector 之间的注册关系。 每次向选择器注册通道时就会选择一个事件(选择键)。 选择键包含两个表示为整数值的操作集。 操作集的每一位都表示该键的通道所支持的一类可选择操作。 方 法 描 述 int interestOps() 获取感兴趣事件集合 int readyOps() 获取通道已经准备就绪的操作的集合 SelectableChannel channel() 获取注册通道 Selector selector() 返回选择器 boolean isReadable() 检测 Channal 中读事件是否就绪 boolean isWritable() 检测 Channal 中写事件是否就绪 boolean isConnectable() 检测 Channel 中连接是否就绪 boolean isAcceptable() 检测 Channel 中接收是否就绪 11.9.5.3 非阻塞网络通信 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class SocketNIO &#123; @Test public void client() throws Exception&#123; SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;127.0.0.1&quot;, 9898));//1. 获取通道 socketChannel.configureBlocking(false);//2. 设置非阻塞模式 ByteBuffer allocate = ByteBuffer.allocate(1024);//3. 指定缓冲区大小 Scanner scanner = new Scanner(System.in); while (scanner.hasNext())&#123; String str = scanner.next(); allocate.put((new Date().toString() + &quot;\\n&quot; + str).getBytes());//4. 给缓冲区存放数据 allocate.flip();//5. 切换读写模式 socketChannel.write(allocate);//6. 通道内写入数据 allocate.clear();//7. 清空缓冲区 &#125; socketChannel.close();//8. 关闭通道 &#125; @Test public void server() throws Exception&#123; ServerSocketChannel socketChannel = ServerSocketChannel.open();//1. 获取通道 socketChannel.configureBlocking(false);//2. 设置非阻塞模式 socketChannel.bind(new InetSocketAddress(9898));//3. 绑定连接 Selector selector = Selector.open();//4. 获取选择器 socketChannel.register(selector, SelectionKey.OP_ACCEPT);//5. 将通道注册到选择器，并监听”接收“事件 while(selector.select() &gt; 0)&#123;//6. 轮询式获取选择器上已经准备就绪的事件 Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator();//7. 获取当前选择器中所有注册的选择键（已就绪监听事件） while (iterator.hasNext())&#123; SelectionKey next = iterator.next();//8. 获取就绪的事件 if(next.isAcceptable())&#123;//9. 判断已就绪的事件类型，做相应处理 SocketChannel accept = socketChannel.accept();// 10. 获取已接收就绪的事件的通道 accept.configureBlocking(false);// 11. 切换为非阻塞模式 accept.register(selector, SelectionKey.OP_READ);//12. 将该通道注册到选择器上 &#125;else if (next.isReadable())&#123; //9. 判断已就绪的事件类型，做相应处理 SocketChannel readChannel = (SocketChannel) next.channel();// 10. 获取已读就绪的事件的通道 ByteBuffer allocate = ByteBuffer.allocate(1024);//11. 指定缓冲区大小 int len = 0; while ((len = readChannel.read(allocate)) &gt; 0)&#123;// 12. 循环读取数据 allocate.flip();//13. 切换读写模式 System.out.println(new String(allocate.array(), 0, len)); allocate.clear();//14. 清空缓冲区 &#125; &#125; iterator.remove();//15. 取消选择键（取消next已就绪事件） &#125; &#125; socketChannel.close();//16. 关闭通道 &#125;&#125; 11.9.6 Charset 与编解码 获取支付的字符编码：Map&lt;String, Charset&gt; map = Charset.avaliableCharsets() 编码：字符串转为字节数组。 解码：字节数组转换为字符串。 123456@Testpublic void test()&#123; Charset cs1 = Charset.forName(&quot;GBK&quot;); CharsetEncoder ce = cs1.newEncoder();//获取编码器 CharsetDecoder cd = cs1.newDecoder();//获取解码器&#125; 11.9.7 阻塞与非阻塞 同步： 同步是一种可靠的有序运行机制，当进行同步操作时，后续任务等待当前调用返回后，才会进行下一步。 异步： 异步正好相反，其他任务不需要等待当前调用返回，通常依靠事件、回调等机制来实现任务间次序关系。 阻塞： 在进行阻塞操作时，当前线程会处于阻塞状态，无法从事其他任务，只有当条件就绪才能继续，比如 serversocket 新连接建立完成，或者数据读取、写入操作完成； 非阻塞： 不管 IO 操作是否结束，直接返回，相应操作在后台继续处理。 结论： 阻塞/非阻塞， 同步/异步的概念要注意讨论的上下文： 在进程通信层面， 阻塞/非阻塞， 同步/异步基本是同义词， 但是需要注意区分讨论的对象是发送方还是接收方。 发送方阻塞/非阻塞（同步/异步）和接收方的阻塞/非阻塞（同步/异步） 是互不影响的。 在 IO 系统调用层面（ IO system call ）层面， 非阻塞 IO 系统调用和异步 IO 系统调用存在着一定的差别， 它们都不会阻塞进程， 但是返回结果的方式和内容有所差别， 但是都属于非阻塞系统调用（ non-blocing system call ） 传统的 IO 流都是阻塞式的。 也就是说， 当一个线程调用 read() 或 write()时， 该线程被阻塞， 直到有一些数据被读取或写入， 该线程在此期间不能执行其他任务。 因此， 在完成网络通信进行 IO 操作时， 由于线程会阻塞， 所以服务器端必须为每个客户端都提供一个独立的线程进行处理，当服务器端需要处理大量客户端时， 性能急剧下降。 Java NIO 是非阻塞模式的。 当线程从某通道进行读写数据时， 若没有数据可用时， 该线程可以进行其他任务。 线程通常将非阻塞 IO 的空闲时间用于在其他通道上执行 IO 操作， 所以单独的线程可以管理多个输入和输出通道。 因此， NIO 可以让服务器端使用一个或有限几个线程来同时处理连接到服务器端的所有客户端。 非阻塞系统调用（non-blocking I/O system call 与 asynchronous I/O system call） 的存在可以用来实现线程级别的 I/O 并发， 与通过多进程实现的 I/O 并发相比可以减少内存消耗以及进程切换的开销。 11.9.8 管道 Java NIO 管道是 2 个线程之间的单向数据连接。Pipe 有一个 source 通道和一个 sink 通道。 数据会被写到 sink 通道， 从 source 通道读取。 12345678910111213141516171819public class TestPipe &#123; @Test public void test01() throws IOException &#123; Pipe pipe = Pipe.open(); // 向管道中写入数据 ByteBuffer allocate = ByteBuffer.allocate(1024); Pipe.SinkChannel sinkChannel = pipe.sink(); allocate.put(&quot;通过管道单向发送数据&quot;.getBytes()); allocate.flip(); sinkChannel.write(allocate); // 读取缓冲区中的数据 Pipe.SourceChannel sourceChannel = pipe.source(); allocate.flip(); int len = sourceChannel.read(allocate); System.out.println(new String(allocate.array(), 0, len)); sourceChannel.close(); sinkChannel.close(); &#125;&#125; 11.9.9 NIO.2 JDK 7 的发布， Java 对 NIO 进行了极大的扩展， 增强了对文件处理和文件系统特性的支持。 Java 7 增加了一个新特性， 该特性提供了另外一种管理资源的方式， 这种方式能自动关闭文件。 这个特性有时被称为自动资源管理(Automatic Resource Management, ARM)， 该特性以 try 语句的扩展版为基础。 自动资源管理 主要用于， 当不再需要文件（ 或其他资源） 时，可以防止无意中忘记释放它们。 自动资源管理基于 try 语句的扩展形式： 123456789101112131415try(需要关闭的资源声明)&#123;//可能发生异常的语句&#125;catch(异常类型 变量名)&#123;//异常的处理语句&#125;……finally&#123;//一定执行的语句&#125;当try代码块结束时，自动释放资源。因此不需要显示的调用close()方法。该形式也称为“ 带资源的try语句” 。注意：①try语句中声明的资源被隐式声明为final，资源的作用局限于带资源的try语句②可以在一条try语句中管理多个资源， 每个资源以“;” 隔开即可。③需要关闭的资源，必须实现了AutoCloseable接口或其子接口Closeable 11.9.9.1 Path 与 Paths java.nio.file.Path 接口代表一个平台无关的平台路径， 描述了目录结构中文件的位置。 Paths 提供的 get() 方法用来获取 Path 对象： Path get(String first, String … more) : 用于将多个字符串串连成路径。 Path 常用方法： boolean endsWith(String path) : 判断是否以 path 路径结束 boolean startsWith(String path): 判断是否以 path 路径开始 boolean isAbsolute() : 判断是否是绝对路径 Path getFileName(): 返回与调用 Path 对象关联的文件名 Path getName(int idx): 返回的指定索引位置 idx 的路径名称 int getNameCount(): 返回 Path 根目录后面元素的数量 Path getParent()： 返回 Path 对象包含整个路径， 不包含 Path 对象指定的文件路径 Path getRoot()： 返回调用 Path 对象的根路径 Path resolve(Path p):将相对路径解析为绝对路径 Path toAbsolutePath(): 作为绝对路径返回调用 Path 对象 String toString()： 返回调用 Path 对象的字符串表示形式 11.9.9.2 Files 类 java.nio.file.Files 用于操作文件或目录的工具类。 Files 常用方法： Path copy(Path src, Path dest, CopyOption … how)：文件的复制 Path createDirectory(Path path, FileAttribute&lt;?&gt; … attr)： 创建一个目录 Path createFile(Path path, FileAttribute&lt;?&gt; … arr)： 创建一个文件 void delete(Path path) : 删除一个文件 Path move(Path src, Path dest, CopyOption…how)：将 src 移动到 dest 位置 long size(Path path)：返回 path 指定文件的大小 Files 常用方法： 用于判断 boolean exists(Path path, LinkOption … opts)：判断文件是否存在 boolean isDirectory(Path path, LinkOption … opts)：判断是否是目录 boolean isExecutable(Path path)：判断是否是可执行文件 boolean isHidden(Path path)：判断是否是隐藏文件 boolean isReadable(Path path) ：判断文件是否可读 boolean isWritable(Path path) ：判断文件是否可写 boolean notExists(Path path, LinkOption … opts) ： 判断文件是否不存在 public static &lt;A extends BasicFileAttributes&gt; A readAttributes(Path path,Class&lt;A&gt; type,LinkOption... options)： 获取与 path 指定的文件相关联的属性。 Files 常用方法： 用于操作内容 SeekableByteChannel newByteChannel(Path path, OpenOption…how)：获取与指定文件的连接，how 指定打开方式。 DirectoryStream newDirectoryStream(Path path)：打开 path 指定的目录 InputStream newInputStream(Path path, OpenOption…how)：获取 InputStream 对象。 OutputStream newOutputStream(Path path, OpenOption…how)：获取 OutputStream 对象。 11.9.9.3 FileLock 文件锁 文件锁在 OS 中很常见，如果多个程序同时访问、修改同一个文件，很容易因为文件数据不同步而出现问题。给文件加一个锁，同一时间，只能有一个程序修改此文件，或者程序都只能读此文件，这就解决了同步问题。 文件锁是进程级别的，不是线程级别的。文件锁可以解决多个进程并发访问、修改同一个文件的问题，但不能解决多线程并发访问、修改同一文件的问题。使用文件锁时，同一进程内的多个线程，可以同时访问、修改此文件。 文件锁是当前程序所属的 JVM 实例持有的，一旦获取到文件锁（对文件加锁），要调用 release()，或者关闭对应的 FileChannel 对象，或者当前 JVM 退出，才会释放这个锁。 一旦某个进程（比如说 JVM 实例）对某个文件加锁，则在释放这个锁之前，此进程不能再对此文件加锁，就是说 JVM 实例在同一文件上的文件锁是不重叠的（进程级别不能重复在同一文件上获取锁）。 文件锁分类： 排它锁：又叫独占锁。对文件加排它锁后，该进程可以对此文件进行读写，该进程独占此文件，其他进程不能读写此文件，直到该进程释放文件锁。 共享锁：某个进程对文件加共享锁，其他进程也可以访问此文件，但这些进程都只能读此文件，不能写。线程是安全的。只要还有一个进程持有共享锁，此文件就只能读，不能写。 12345678//创建 FileChannel 对象，文件锁只能通过 FileChannel 对象来使用FileChannel fileChannel=new FileOutputStream(&quot;./1.txt&quot;).getChannel();//对文件加锁FileLock lock=fileChannel.lock();//对此文件进行一些读写操作。//.......//释放锁lock.release() 有 4 种获取文件锁的方法： lock()对整个文件加锁，默认为排它锁。 lock(long position, long size, booean shared) 自定义加锁方式。前 2 个参数指定要加锁的部分（可以只对此文件的部分内容加锁），第三个参数值指定是否是共享锁。 tryLock()对整个文件加锁，默认为排它锁。 tryLock(long position, long size, booean shared) 自定义加锁方式。如果指定为共享锁，则其它进程可读此文件，所有进程均不能写此文件，如果某进程试图对此文件进行写操作，会抛出异常。 lock 是阻塞式的，如果未获取到文件锁，会一直阻塞当前线程，直到获取文件锁。 tryLock 和 lock 的作用相同，只不过 tryLock 是非阻塞式的，tryLock 是尝试获取文件锁，获取成功就返回锁对象，否则返回 null，不会阻塞当前线程 。 boolean isShared() 此文件锁是否是共享锁 boolean isValid() 此文件锁是否还有效 在某些 OS 上，对某个文件加锁后，不能对此文件使用通道映射。 11.9.10 AIO 在进行 I/O 编程中，通常用到两种模式：Reactor 和 Proactor 。Java 的 NIO 就是 Reactor，当有事件触发时，服务器端得到通知，进行相应的处理。JDK 7 引入了 Asynchronous I/O，即 AIO。AIO 叫做异步非阻塞的 I/O，引入了异步通道的概念，采用了 Proactor 模式，简化了程序编写，有效的请求才会启动线程，特点就是先由操作系统完成后才通知服务端程序启动线程去处理，一般用于连接数较多且连接时长较长的应用。 两种 IO 多路复用方案：Reactor and Proactor。Reactor 模式是基于同步 I/O 的，而 Proactor 模式是和异步 I/O 相关的。 11.9.10.1 创建 AsynchronousFileChannel 1234567Path path = Paths.get(&quot;d:\\\\atguigu\\\\01.txt&quot;);try &#123; AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ);&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 11.9.10.2 通过 Future 读取数据 可以通过两种方式从 AsynchronousFileChannel 读取数据。第一种方式是调用返回 Future 的 read()方法。 12345678910111213141516Path path = Paths.get(&quot;d:\\\\atguigu\\\\001.txt&quot;);AsynchronousFileChannel fileChannel = null;try &#123; fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ);&#125; catch (IOException e) &#123; e.printStackTrace();&#125;ByteBuffer buffer = ByteBuffer.allocate(1024);long position = 0;Future&lt;Integer&gt; operation = fileChannel.read(buffer, position);while(!operation.isDone());buffer.flip();byte[] data = new byte[buffer.limit()];buffer.get(data);System.out.println(new String(data));buffer.clear(); 11.9.10.3 通过 CompletionHandler 读取数据 第二种方法是调用 read()方法，该方法将一个 CompletionHandler 作为参数。 1234567891011121314151617181920212223Path path = Paths.get(&quot;d:\\\\atguigu\\\\001.txt&quot;);AsynchronousFileChannel fileChannel = null;try &#123; fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ);&#125; catch (IOException e) &#123; e.printStackTrace();&#125;ByteBuffer buffer = ByteBuffer.allocate(1024);long position = 0;fileChannel.read(buffer, position, buffer, new CompletionHandler&lt;Integer,ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; System.out.println(&quot;result = &quot; + result); attachment.flip(); byte[] data = new byte[attachment.limit()]; attachment.get(data); System.out.println(new String(data)); attachment.clear(); &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; &#125;&#125;); 11.9.10.4 通过 Future 写数据 和读取一样，可以通过两种方式将数据写入一个 AsynchronousFileChannel 123456789101112131415Path path = Paths.get(&quot;d:\\\\atguigu\\\\001.txt&quot;);AsynchronousFileChannel fileChannel = null;try &#123; fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.WRITE);&#125; catch (IOException e) &#123; e.printStackTrace();&#125;ByteBuffer buffer = ByteBuffer.allocate(1024);long position = 0;buffer.put(&quot;atguigu data&quot;.getBytes());buffer.flip();Future&lt;Integer&gt; operation = fileChannel.write(buffer, position);buffer.clear();while(!operation.isDone());System.out.println(&quot;Write over&quot;); 注意：文件必须已经存在。如果该文件不存在，那么 write()方法将抛出一个 java.nio.file.NoSuchFileException。 11.9.10.5 通过 CompletionHandler 写数据 123456789101112131415161718192021222324252627282930Path path = Paths.get(&quot;d:\\\\atguigu\\\\001.txt&quot;);if(!Files.exists(path))&#123;try &#123; Files.createFile(path);&#125; catch (IOException e) &#123; e.printStackTrace();&#125;&#125;AsynchronousFileChannel fileChannel = null;try &#123; fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.WRITE);&#125; catch (IOException e) &#123; e.printStackTrace();&#125;ByteBuffer buffer = ByteBuffer.allocate(1024);long position = 0;buffer.put(&quot;atguigu data&quot;.getBytes());buffer.flip();fileChannel.write(buffer, position, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; System.out.println(&quot;bytes written: &quot; + result); &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; System.out.println(&quot;Write failed&quot;); exc.printStackTrace(); &#125;&#125;); 第 12 章 网络编程 12.1 网络相关概念 12.1.1 网络通信 概念：两台设备之间通过网络实现数据传输 网络通信：将数据通过网络从一台设备传输到另一台设备。 java.net 包提供了一系列接口，供编程使用，往后台网络通信。 12.1.2 网络 概念：两台或多态设备通过一定物理设备连接起来就构成了网络。 分类：根据覆盖范围分 局域网： 城域网 广域网：万维网时广域网的代表 12.1.3 IP 地址 概念：用于唯一标识网络中的每台计算机/主机 ipconfig：查看本机 ip 地址 IPV4 地址表示形式：点分十进制，xxx.xxx.xxx.xxx 组成：网络地址 + 主机地址 IPV4 分类： 本地主机地址：127.0.0.1 12.1.4 域名 概念：将 ip 地址根据 http 协议，映射成域名，便于记忆 端口号： 概念：用于标识计算机上某个特定的网络程序（服务） 表示形式：以整数表示，端口范围 0~65535（216-1） 0~1024 一般系统预留（已被占用） 22：ssh 21：ftp 25：smtp 80：http 常见网络程序端口号： 8080：tomcat 3306：mysql 1521：oracle 1433：sqlserver netstat 指令： netstat -an：查看当前主机网络情况，包括端口监听和网络连接状态 Listenning 表示端口正在监听 netstat -an | more：分页显示网络情况，按空格显示下一页 12.1.5 网络通信协议（TCP/IP） TCP：Transmission Control Protocol，传输控制协议 使用 TCP 协议前，须建立 TCP 连接，形成数据传输通道 传输前，采用“三次握手”确保连接可靠 “三次握手”可靠后，可进行大数据量传输 传输完毕后，需要释放已经建立的连接，效率低。 TCP 协议通信时用到的两个应用进程：客户端、服务端 IP：Internet Protocol，网络通讯协议 UDP 协议（用户数据协议） 将数据、源、目的封装成数据包，不需要建立连接 不可靠 每个数据包大小现在在 64k 内，不适合大数据量传输 传输完毕不需要释放资源，速度快 12.2 InetAddress 类 getLocalHost()：静态方法，获取本机 InetAddress 对象 返回：域名/ip 地址 getByName(String host)：根据指定主机名/域名，获取其 ip 地址对象 getHostAddress：通过 InetAddress 对象，获取其 ip 地址（主机名） getHostName()：通过 InetAddress 对象，获取其主机名或域名 12.3 Socket（套接字） 组成：端口号+ip 地址 作用：Socket 允许程序把网络当成一个流，数据在两个 Socket 间通过 IO 传输 分类：一般将主动发起通信的应用程序成为客户端，等待通信请求的为服务端 12.3.1 TCP 网络通信编程 必须关闭 Socket，否则会造成服务器占用，导致无法连接 ServerSocket 每调用一次 accept()，就会开启一个 Socket，所以也必须关闭 ServerSocket 单向数据传输（客户端单向数据流，服务器单向数据流），可不设置结束标志，可以正常通信。 双向数据传输，必须设置结束标志，且关闭位置必须紧邻输出流。否则服务器会阻塞，处于等待状态，无正常输出。 结束标志：socket.shutdownOutput() 字符流输出中，可利用newLine()和readLine()作为结束标志，而不使用socket.shutdownOutput() 当客户端连接到服务端后，客户端也会通过一个端口与服务端通讯，该端口由 TCP/IP 随机分配 12.3.2 UDP 网络编程 DatagramSocket类和DatagramPacket类【数据包/数据报】实现了基于 UDP 协议的网络程序 UDP 协议发送的数据不一定能够安全到达目的地，也不确定何时到达。 DatagramSocket类和DatagramPacket类的对象封装了 UDP 数据报，包含了发送端的 IP 地址和端口号，以及接收端 IP 地址和端口号 UDP 协议的每个数据报都包含了完整的地址信息，因此无需建立发送方和接收放的连接 基本流程： 通过DatagramSocket建立发送端和接收端 没有服务端、客户端的概念 将数据封装到DatagramPocket对象 调用DatagramSocket的方法接收、发送数据 调用DatagramPocket的getData()方法解析数据 关闭DatagramSocket 注意：必须先启动先接收到数据的端，不然服务会一直阻塞。 第 13 章 反射(Reflection) 13.1 反射概述 13.1.1 反射机制 基本认识：反射机制允许程序在执行期间，借助 Reflection API 取得任何类的内部信息，如成员变量，构造器，成员方法等，并能操作对象的属性和方法，而不会对操作的类代码产生修改。 大概解释：类加载完后，JVM 堆内存中就产生一个 Class 类型的对象（一个类只有一个 Class 对象），这个对象包含了类的完整结构信息，通过该对象可以得到类的结构。对象相当于镜子。 类加载器使用了同步代码块，保证了即使时多线程情况下，一个类的 Class 类对象只有一个 13.1.2 反射机制原理示意图（后期再理解，暂时不解） 13.1.3 作用 在运行时判断任意一个对象所属的类 在运行时构造任意一个类的对象 在运行时得到任意一个类所具有的成员变量和方法 在运行时调用任意一个对象的成员变量和方法 生成动态代理 13.1.4 反射相关的主要类 java.lang.Class：代表一个类，Class 类对象表示某个类加载后在堆中的对象 java.lang.reflect.Method：代表类的方法，Method 对象表示某个类的方法 java.lang.reflect.Field：代表类的成员变量，Field 对象表示某个类的成员变量 java.lang.reflect.Constructor：代表类的构造方法，Constructor 对象表示构造器 13.1.5 反射的优缺点 优点：可以动态的创建和使用对象（框架底层核心），使用灵活，没有反射机制，框架技术就失去底层支撑。 缺点：反射是解释执行，对执行速度有影响。 13.1.6 反射调用优化——关闭访问检查 Method、Field 和 Constructor 的对象都有setAccessible()方法 setAccessible()作用是启动和禁用访问安全检查的开关 传入 true 表示反射的对象在使用时取消访问检查，提高反射效率。 传入 false 表示反射的对象在使用是执行访问检查，默认为 false。 13.2 Class 类 13.2.1 基本介绍 Class 是类的一种，继承自 Object 类 Class 类对不是 new 出来的，是系统通过类加载器（ClassLoader）的 loadClass()方法创建的。 对于某个类的 Class 类对象在内存中只有一份，因为类只加载一次。 同一个类的 Class 类对象，hashcode 相同 每个类的实例都会记得自己是由哪个 Class 实例所生成。 利用 Class 类的一系列 API 可以完整得到该 Class 类对象所对应的那个类的完整结构。 Class 类的对象存放在堆空间中。 类的字节码二进制数据存放在方法区，称为元数据（包括方法代码、变量名、方法名、访问权限等） 13.2.2 Class 类的常用方法 Class&lt;?&gt; cls = Class.forName(classPath)： classPath 是从 src 路径开始的全类名 &lt;?&gt;表示不确定的 java 类型，也可以省略。 输出cls时，显示的是class classPaht forName 有异常问题 cls.getClass()：输出 cls 的运行类型——class java.lang.Class cls.getName()：得到全类名——classPath cls.getPackage().getName()：从 src 路径开始的全包名 cls.newInstance：创建实例对象，与 new 出来的实例对象是两个不同对象 与 classPath 的类一致（强转后一致） 有异常问题 cls.getField(&quot;属性名&quot;)：通过反射获取类的属性对象 通过此方式访问设置为 private 的属性会报错 cls.getField(&quot;属性名&quot;).getName()：获取属性名 cls.getField(&quot;属性名&quot;).get(对象名)：获取该对象中该属性的值 这里的对象可以是 new 出来的，也可以是 newInstace 出来的 cls.getField(&quot;属性名&quot;).set(对象名, 新值）：通过反射给属性赋值 这里的对象可以是 new 出来的，也可以是 newInstace 出来的 cls.getFields()：获取所有属性的对象数组 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package reflect_;import java.lang.reflect.Field;public class Test &#123; public static void main(String[] args) throws ClassNotFoundException, InstantiationException, IllegalAccessException, NoSuchFieldException &#123; Car car1 = new Car(); car1.brand = &quot;奔驰&quot;; System.out.println(car1);//Car&#123;brand=&#x27;奔驰&#x27;, price=&#x27;50000&#x27;, color=&#x27;red&#x27;&#125; System.out.println(car1.getClass());//class reflect_.Car System.out.println(car1.getClass().getClassLoader());//sun.misc.Launcher$AppClassLoader@18b4aac2 System.out.println(Test.class.getClassLoader());//sun.misc.Launcher$AppClassLoader@18b4aac2 System.out.println(car1.getClass().getClassLoader()==Test.class.getClassLoader());//true Person person = new Person(); System.out.println(Person.class.getClassLoader()==person.getClass().getClassLoader());//true System.out.println(Person.class.getClassLoader()==car1.getClass().getClassLoader());//true System.out.println(Test.class);//class reflect_.Test System.out.println(Test.class.getClass());//class java.lang.Class String classPath = &quot;reflect_.Car&quot;; Class&lt;?&gt; cls = Class.forName(classPath); System.out.println(cls);//class reflect_.Car System.out.println(cls.getClass());//class java.lang.Class System.out.println(cls.getName());//reflect_.Car System.out.println(cls.getPackage());//package reflect_ System.out.println(cls.getPackage().getName());//reflect_ Field brand = cls.getField(&quot;brand&quot;); System.out.println(brand);//public java.lang.String reflect_.Car.brand System.out.println(brand.getName());//brand Car car = (Car)cls.newInstance(); System.out.println(car);//Car&#123;brand=&#x27;宝马&#x27;, price=&#x27;50000&#x27;, color=&#x27;red&#x27;&#125; System.out.println(brand.get(car));//宝马 System.out.println(brand.get(car1));//奔驰 brand.set(car, &quot;华晨宝马&quot;); brand.set(car1, &quot;梅赛德斯奔驰&quot;); System.out.println(brand.get(car));//华晨宝马 System.out.println(brand.get(car1));//梅赛德斯奔驰 Field[] fields = cls.getFields(); System.out.println(fields);//[Ljava.lang.reflect.Field;@74a14482 for(Field field : fields)&#123; System.out.print(field.getName() + &quot;\\t&quot;);//brand price color &#125; &#125;&#125; 13.2.3 获取 Class 类对象（实例） 根据类加载的不同阶段，可以在不同阶段使用不同方式获取 Class 类。 不同方式获取到的 Class 类对象是同一个，原因为堆内存中一个类只有一个 Class 对象（反射机制） Class.forName(classPath)：调用Class的静态方法 前提：已知一个类的全类名，且该类在类路径下？ 应用场景：加载配置文件，读取类全路径，加载类 注意点：有 ClassNotFoundEception 阶段：程序编写阶段 类名.class：调用运行时类的属性 前提：已知一个具体的类 应用场景：多用于参数传递（当作参数传进去），比如通过反射得到对应构造器的对象 注意点：最安全可靠，性能最高 阶段：类的加载阶段 对象.getClass()：调用运行时类的getClass()方法 前提：已知某个类的对象实例 应用场景：有对象实例 注意点：获取到的是运行类型 阶段：程序运行阶段 类加载器【4 种】 对象.getClass().getClassLoader()得到ClassLoader（类加载器）。 或者使用类.class.getClassLoader()得到，二者返回的 classLoader()是同一个。一个类一个类加载器吗？还是许多类一个加载器？测试好像是所有类一个类加载器。 通过类加载器得到 Class 类对象：类加载器的对象.loadClass(classPath) Class&lt;包装类&gt; cls = 基本数据类型.class 基本数据类型按照上述方式获得 输出时会自动拆箱，得到基本数据类型 Class&lt;包装类&gt; cls = 包装类.TYPE 基本数据类型的包装类可以按照上述方式获得 输出时会自动拆箱，得到基本数据类型 底层基本数据类型和其包装类是同一个 Class 类对象 13.2.4 具有 Class 对象的类 外部类、成员内部类、静态内部类、局部内部类、匿名内部类 Class 也有，因为 Class 类是外部类的一种 interface enum 数组 annotation void 基本数据类型 13.3 类加载 13.3.1 基本说明 静态加载：编译时加载相关的类，如果类不存在则报错，具有高依赖性 动态加载：运行时加载需要得类，如果运行时不用该类，即使不存在该类，也不报错，降低了依赖性 java 通过反射机制实现了动态语言的动态加载，让原本在编译时加载的类到运行时才加载 13.3.2 类的加载时机 new 创建对象时——静态加载 子类被加载时，父类也被加载——静态加载 调用类中的静态成员时——静态加载 通过反射——动态加载 13.3.3 类的加载过程 加载阶段：JVM 将来自不同数据源（class 文件、jar 包、网络等）的字节码文件，转化为二进制字节流加载到内存中，并生成一个代表该类的 java.lang.Class 对象 连接阶段： 验证：jvm 检查字节流信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全 包括文件格式验证（是否以魔数 oxcafebabe 开头）、元数据验证、字节码验证和符号引用验证 可以使用-Xverify:none参数关闭大部分类的验证措施，缩短虚拟机类加载的时间 准备：JVM 对静态变量进行默认初始化并分配内存，这些变量所使用的内存在方法区进行分配 无修饰的变量不会分配内存，那会默认初始化吗？ static 修饰变量会默认初始化，会分配内存 static 和 final 修饰的变量会默认初始化，也会分配内存 解析：虚拟机将常来给你吃内的符号引用替换为直接引用 初始化阶段：初始化阶段时执行&lt;clint&gt;()方法的过程 &lt;clint&gt;()方法时由编译器按语句在源文件中出现的顺序，依次自动收集类中所有的静态变量赋值动作和静态代码块中的语句，并进行合并 虚拟机会保证一个类的&lt;clint&gt;()方法在多线程环境中被正确的加锁、同步，如果多个线程同时初始化一个类，哪个只有一个线程去执行类的&lt;clint&gt;()方法，其他线程都会阻塞等待，直到活动线程执行&lt;clint&gt;()方法完毕。 13.4 通过反射获取类的结构信息 13.4.1 java.lang.Class 类及其方法 13.4.2 java.lang.reflect.Field 类 getType()返回的是属性的类型 13.4.3 java.lang.reflect.Method 类 getReturnType()返回的是返回类型的类型 13.4.4 java.lang.reflect.Constructor 类 13.5 通过反射创建对象 在 13.2.3 的基础上： 方式一： Class.forName(classPath)：得到 Class 类对象 Class类对象.newInstance()：得到实例对象 方式二： Class.forName(classPath)：得到 Class 类对象 Class类对象.getConstructor(已知参数类名.class)：得到带参构造器对象 得到的只是 public 修饰的构造器 带参构造器对象.newInstance(实参)：得到对象实例 方式三（私有构造器流程）： Class.forName(classPath)：得到 Class 类对象 Class类对象.getDeclaredConstructor(已知参数类名.class)：得到带参构造器对象 得到的是各种权限修饰符修饰的构造器（上面只是获得了私有构造器，还不能使用，使用会报错） 构造器对象.setAccessible(true)：爆破，使得私有的构造器可以使用 构造器对象.newInstance(实参)：得到对象实例 以上方式创建的实例对象都是 Object 类型，但是可以向下转型为指定类型（体现多态） 13.6 通过反射访问类中的成员 13.6.1 访问属性 得到类对应的 Class 类对象：Class.forName(classPath) 通过 13.5 的方式创建 Class 类对象的实例对象 方式一（访问公开属性）： 通过 Class 类对象的getField(已知属性名)得到本类及父类中 public 修饰的属性对象 属性对象.get(实例对象名)可以获得实例对象中的属性值 属性对象.set(实例对象名)可以修改实例对象中的属性值 方式二（访问非公开属性）： 通过 Class 类对象的getDeclaredField(已知属性名)得到本类中各类修饰符修饰的属性对象 属性对象.setAccessible(true)：爆破，使得私有的构造器可以使用 属性对象.get(实例对象名)可以获得实例对象中的属性值 属性对象.set(实例对象名,value)可以修改实例对象中的属性值 如果是静态属性，则 set 和 get 中的实例对象名，也可以写成 null 13.6.2 访问方法 得到类对应的 Class 类对象：Class.forName(classPath) 通过 13.5 的方式创建 Class 类对象的实例对象 方式一（访问公开方法）： 通过 Class 类对象的getMethod(已知方法名, 形参对象类.class)得到本类及父类中 public 修饰的方法对象 能不能写成getMethod(已知方法名, 形参对象实例.getClass())？不能，会报找不到这样方法的异常【2022.07.06】【可以，2022.07.12 研究，那是因为那个例子中，对类有改造。但还是不明白具体情况下该用哪个】 方法对象.invoke(实例对象名, 实参)可以调用实例对象中的方法 方式二（访问非公开方法）： 通过 Class 类对象的getDeclaredMethod(已知方法名, 形参对象类.class)得到本类中各类修饰符修饰的方法对象 属性对象.setAccessible(true)：爆破，使得私有的构造器可以使用 方法对象.invoke(实例对象名, 实参)可以调用实例对象中的方法 如果是静态属性，则 invoke 中的实例对象名，也可以写成 null 反射中，方法如果有返回值，统一返回 Object（编译类型），运行类型按照方法实际的类型执行。","tags":[{"name":"Java","slug":"Java","permalink":"https://sk370.github.io/tags/Java/"}]},{"title":"Spring5","date":"2022-07-12T00:09:46.000Z","path":"2022/07/12/spring5/Spring5/","text":"Spring 是轻量级的开源 JavaEE 框架，具有高效、简洁开发的特点，用于解决企业应用开发的复杂性。 1. Spring 概述 1.1 Spring 框架介绍 1.1.1 基本介绍 是什么：Spring 是轻量级的开源 JavaEE 框架 框架：高效、简洁开发 作用：解决企业应用开发的复杂性 核心：IoC 和 Aop IoC：控制反转，把创建对象过程交给 Spring 进行管理 Aop：面向切面，不修改源代码进行功能增强 特点： 方便解耦，简化开发 Aop 编程支持 方便程序测试 方便和其他框架进行整合 方便进行事务操作 降低 API 开发难度 1.1.2 框架结构图 4.x 版本 spring5 1.2 Spring5.2 下载 1.2.1 手动下载，引入 IDEA 下载地址： 确认版本： 前往下载页面： 第一个框中的为最新版本，第二个框中的为历史版本 下载源码： 下载后找个位置存放即可。 IDEA 中创建普通 Java 工程【不同版本 IDEA 的步骤不同】 文件结构说明：通过本地文件存储结构，可以看到项目名称【1】spring 只是一个虚拟的文件管理路径，模块【3】spring5 是真实的文件路径。同时由于项目名称和模块名称不一致，所以 IDEA 自动创建了模块的本地存放路径【4】，【2】的直接子路径下也没有 src 目录。即如果采取【1】【3】同名的方式，则不会创建【4】路径，而是【4】=【2】，同时【2】的直接子路径下也会生成 src 目录。——原因为 IDEA 工具的项目、模块关系设计决定。 导入 jar 包作为 lib 库 spring5 下新建 libs 文件夹，作为本模块的依赖库 将 spring5 的核心 jar 包拷贝至 libs 目录 这里还引入了commons-logging.jar，这是一个 apache 的一个日志文件，spring5 需要这个文件，下载引入即可。下载地址： 将 jar 包作为【模块】依赖库——也可以作为项目依赖库，看具体需求 注意：这样添加的 5 个包会用一个名字。单独一个一个添加可以创建 5 个名字。 1.2.2 基于 maven 构建 引入依赖 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;iceriver.spring&lt;/groupId&gt; &lt;artifactId&gt;maven-structre&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt;&lt;!-- 必须引入--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.3.22&lt;/version&gt; &lt;/dependency&gt;&lt;!-- 按需引入juint--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 基于配置文件时，配置文件写在 resources 路径下，名称任意。 1.3 体验 spring 创建对象 1.3.1 使用配置文件方式 编写类及方法【任意】。 创建配置文件（位置选择 src 目录，名字任意）： 这个 id 不是要创建的对象名字，而是通过这个 id 可以找到对应的类，相当于属性名。 创建 javabean： 1234567891011121314151617181920212223242526272829303132333435363738package iceriver.spring;/** * @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/7/12 8:52 */public class User &#123; private String name; private String password; public User(String name, String password) &#123; this.name = name; this.password = password; &#125; public User() &#123; &#125; public void say()&#123; System.out.println(&quot;lalalla&quot;); &#125; public void setName(String name) &#123; this.name = name; &#125; public void setPassword(String password) &#123; this.password = password; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, password=&#x27;&quot; + password + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 创建 User 对象 2. IoC 及 IoC 操作 2.1 IoC 简述 2.1.1 IoC 概念 含义：控制反转 作用：把对象创建和对象之间的调用过程，交给 Spring 进行管理 目的：降低耦合度 底层：xml 解析、工厂模式、反射 2.2.2 底层原理 在 xml 文件中配置类的路径 classPath 使用类工厂创建对象 解析 xml，读取 classPath 使用 classPath，利用反射机制Class.forName(classPath)获取类的对象 使用newInstance()实例化对象 2.2.3 IoC 接口 IoC 思想基于 IoC 容器，该容器的底层就是对象工厂 容器：为某种特定组件的运行提供必要支持的软件环境 IoC 容器：管理所有轻量级的 JavaBean 组件，提供生命周期管理、配置和组装服务、AOP 支持、建立在 AOP 基础上的声明式事务等。 IoC 容器的实现方式（接口） BeanFactory接口：IOC 的基本实现，Spring 内部使用，不建议开发人员使用 加载配置文件时不会创建，获取对象时才去创建 AplicationContext接口：BeanFactory的子接口，功能更强大，给开发人员使用 加载配置文件时即创建对象 ApplicationContext的两个重要实现类：（取决于使用哪种方式加载 xml 配置文件） IDEA 中 ctrl+h 打开层次结构图 ClassPathXmApplicationContext FileSystemXmlApplicationContext 基于注解的开发使用AnnotationConfigApplicationContext BeanFactory： 2.2.4 Bean 管理 Bean 管理指：Spring 创建对象、Spring 注入属性 操作方式：基于 xml 配置文件实现、基于注解方式实现 2.2 基于 xml 操作——Bean 管理 2.2.1 创建对象 xml 文件中：使用&lt;bean&gt;标签，id指定该类的对象名字，class指向类的全路径 &lt;bean id=&quot;user&quot; class=&quot;iceriver.spring.User&quot;&gt;&lt;/bean&gt; 要使用该对象的类中：加载类对象： BeanFactory context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); 【建议】ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); 要使用该对象的类中：创建对象（默认使用类的无参构造器）： User user = context.getBean(&quot;user&quot;, User.class); 没有无参构造器/无参构造器不可用时会报错。——xml 文件找不到类 2.2.2 注入属性 DI：依赖注入，即注入属性 方式一：使用属性的 setter 【前提】：JavaBean 的类中声明了对应属性的 setter 方法、同时无参构造器可用。 xml 文件中配置&lt;bean&gt;标签的子标签&lt;property&gt; name表示给哪个属性赋值 value表示该属性的注入的值 name是&lt;poperty&gt;的必需属性，value属性可以作为&lt;property&gt;的子标签设置 方式二：使用带参构造器 【前提】：JavaBean 中有对应属性的构造器方法，同时也显式地声明无参构造器，用于 xml 文件找到对应地类文件。 xml 文件中配置&lt;bean&gt;标签的子标签&lt;constructor-arg&gt; name表示给哪个属性赋值；也可以使用索引形式，从 0 开始 value表示该属性的注入的值 方式三：p 名称空间注入：对基于 xml 配置文件注入属性的优化 配置&lt;beans&gt;标签的 p 名称空间： &lt;beans xmlns:p=&quot;http://www.springframework.org/schema/p&quot;&gt; 给&lt;bean&gt;标签添加属性：&lt;bean id=&quot;&quot; class=&quot;&quot; p:name1=&quot;&quot; p:name2=&quot;&quot;&gt; xml 注入其他类型的属性： null值： 特殊字符值&gt;、&lt;： 使用转义字符：&lt;代表&lt;，&gt;代表&gt; 将带&gt;、&lt;的文本写入至CDATA 如果有回车符也会原样输出。 注入属性——外部 Bean 原始方法：在 service 类中创建 dao 对象，使用 dao 对象调用其方法 注入方式：在 service 类中，把 dao 当作私有化属性，并创建其 setter，在 xml 文件中给 service 类进行注入属性配置。 注入属性——内部 Bean 和级联赋值 内部 Bean：一个&lt;bean&gt;中嵌套了另一个&lt;bean&gt; dept2 是 Dept 的私有属性，dept3 是 Emp 的私有属性。 级联赋值：配置嵌套配置【上面的配置也是一种级联赋值】 方式一： 方式二： 方式二中，要使用dept3.dept2，还得给Emp类中的Dept属性类设置 getter，否则会找不到。这样设置后，注释内容（安保部）会失效。 注入数组、集合类属性： 注入数组： 注入 List 集合： 注入 Map 类型集合： 注入 Set 类型集合： 注入指定类型【List】的集合： 注入指定类型【Map】的集合： 提取集合类型的属性注入： 配置 util 空间： 注意事项：注入数组、List 集合、set 集合的配置中，&lt;array&gt;、&lt;list&gt;、&lt;set&gt;三个标签可互换，其中，&lt;set&gt;标签包裹的&lt;value&gt;值如果有重复时，重复项只取一项。 2.2.3 FactoryBean Spring 有两种Bean，一种是普通Bean，另一种是FactoryBean 普通Bean：xml 文件中 class 配置的是什么类，创建实例的时候就返回该类型 工厂Bean（FactoryBean）：xml 文件中 class 配置的类型，和创建实例时返回的类型可以不一致。 FactoryBean的使用： 创建普通 Bean 时，让其实现FactoryBean接口。 FactoryBean使用泛型，指定需要创建实例时，返回的类型。 在实现FactoryBean的getObject()方法中，将返回值类型指定为传入的泛型类 getObject()：通过一个对象交给 ioc 容器管理 getObjectType()：设置所提供对象的类型 isSingleton()：所提供的对象是否单例 此时得到的类型不再是实现FactoryBean接口的类的类型，而是传给FactoryBean接口范型的类型（即FactoryBean方法返回的类型。获取实现FactoryBean接口的类对象会出错。 2.2.4 Bean 作用域 Spring 中，默认情况下，创建的 Bean 实例默认是单实例——内存中只有一个实例对象 设置多实例： &lt;bean&gt;标签设置scope属性 singleton：单实例，默认值，加载 xml 配置文件时，即创建对象 prototype：多实例，调用getBean()时创建多实例对象 request：每次 HTTP 请求时创建实例 session：每个会话中创建实例 2.2.5 Bean 生命周期（共 7 步） 通过构造器创建 bean 实例（无参数构造） 读取配置文件，创建 Class 类对象 为 bean 的属性设置值和对其他 bean 引用（调用 set 方法） 读取配置文件，创建 Class 类对象 把 bean 实例传递 bean 前置处理器的方法 postProcess**Before**Initialization() 要求类实现BeanPostProcessor接口 实现BeanPostProcessor接口的类的配置，会导致本配置文件中的所有类都添加后置处理器 调用 bean 的初始化的方法（需要进行配置初始化的方法） &lt;bean&gt;须配置属性init-method = &quot;initMethod()&quot;——方法名任意 把 bean 实例传递 bean 后置处理器的方法 postProcess**After**Initialization() bean 可以使用了（对象获取到了） 通过 Class 对象创建实例对象完毕。 当容器关闭时候，调用 bean 的销毁的方法（需要进行配置销毁的方法） &lt;bean&gt;须配置属性destory-method = &quot;destoryMethod()&quot;——方法名任意 类中显式调用context.close()——context是根据 xml 文件获得的 Class 类对象 2.2.6 xml 自动装配 根据指定的装配规则（属性名称或属性类型），Spring 自动将匹配的属性值进行注入 &lt;bean&gt;标签配置autowrite属性 byName：根据属性名注入，要求外部 bean 的 id 值，和要注入的类中的属性名称一样 byType：根据属性类型注入。 2.2.7 配置文件装配数据源 xml 文件直接配置 Druid 连接池： 添加 druid 连接池 jar 包 xml 读取配置文件进行配置 Druid 连接池 新建配置文件jdbc.properties 引入名称空间： 引入配置文件 使用表达式： &#123;&#125;中的值要与.properties配置文件中的名称一致。 2.3 基于注解操作——Bean 管理 2.3.1 Spring 中 Bean 管理的注解 @Component @Service @Controller @Repostory 注意：上述四个注解作用一致，只是针对不同 JavaBean 进行语义化使用 2.3.2 基于注解创建对象 引入依赖spring-aop-5.2.6.RELEASE 引入名称空间： 开启组件扫描： 方式一：逗号分隔多个包 方式二：扫描上级包 配置注解： 组件扫描配置 设置扫描指定注解为Controller的类 设置不扫描指定注解为Controller的类 2.3.3 属性注入 @Autowried：根据属性类型自动装配 在要注入的属性声明上面，添加@Autowired 该属性的 setter 不是必须 @Qualifier：根据属性名进行注入，需要配合@Autowired使用 适用于@Autowried查找到多个实现类的情况 给被注入的类指定value值 在要注入的属性声明上面，添加@Autowired和@Qualifier，给@Qualifier配置value值 @Resource：可以根据类型注入，也可以根据名称注入 在要注入的属性声明上面，添加@Resource——根据类型注入 在要注入的属性声明上面，添加@Resource，并配置name属性值——根据类型注入 @Resource来源于import javax.annotation.Resource;，所以官方不推荐使用这个。 @Value：给普通属性注入值 在要注入的属性声明上面，添加@Value 使用value属性设置值 2.3.4 完全注解开发 创建配置类，替代 xml 配置文件 类的名称任意 使用@Configuration声明这是一个配置类 使用@ComponentScan(basePackages = &#123;&quot;iceriver.spring&quot;&#125;)指定扫描的基本路径 使用AnnotationConfigApplicationContext加载配置类 3. Aop 3.1 Aop 介绍 3.1.1 Aop 概念 Aspect Oriented Programming的缩写，意为面向切面编程。 是一种通过预编译和运行期间动态代理实现程序功能统一维护的一种技术。 Aop 是 OOP 的延续，也是函数式编程的一种衍生泛型。 Aop 可以对业务逻辑的各个部分进行隔离，使业务逻辑各部分之间的耦合度降低，提高程序的可重用性，提高开发的效率。 不修改源代码的情况下，在主干功能里面添加新功能。 3.1.2 Aop 底层原理 底层使用了动态代理进行实现，有接口情况下，使用 JDK 实现动态代理；没有接口情况下，通过 CGLIB 动态代理。 JDK 实现动态代理：创建接口实现类的代理对象，增强类的方法。 通过java.lang.reflect.Proxy中的newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) loader - 定义代理类的类加载器 interfaces - 代理类要实现的接口列表 h - 指派方法调用的调用处理程序：可以使用匿名内部类，也可以创建要被代理的类的实现类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package iceriver.spring.jdk;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.util.Arrays;/** * @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/7/16 7:58 */public class JDKProxy &#123; public static void main(String[] args) &#123; Class[] interfaces = &#123;UserDao.class&#125;;// 1. 方式一：第三个参数使用匿名内部类// Proxy.newProxyInstance(JDKProxy.class.getClassLoader(), interfaces, new InvocationHandler() &#123;// @Override// public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;// return null;// &#125;// &#125;);// 2. 方式二：第三个参数使用代理类 UserDao userDao = new UserDaoImpl(); UserDao dao = (UserDao) Proxy.newProxyInstance(JDKProxy.class.getClassLoader(), interfaces, new UserDaoProxy(userDao)); int result = dao.add(2, 3); System.out.println(result); &#125;&#125;/** * 创建代理对象 */class UserDaoProxy implements InvocationHandler&#123;// 增强原始类功能的代码 private Object obj; public UserDaoProxy(Object obj) &#123; this.obj = obj; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;方法之前执行……&quot; + method.getName() + &quot;，传递的参数：&quot; + Arrays.toString(args)); Object res = method.invoke(obj, args); System.out.println(&quot;方法之后执行&quot; + obj); return res; &#125;&#125; CGLIB 实现动态代理：创建当前类的子类代理对象，增强类的方法。 3.1.3 Aop 术语 Joinpoint(连接点)：类中可以被增强的方法。 Pointcut(切入点)：实际真正被增强的方法。 Advice(通知/增强)：实际增强的逻辑部分。 前置通知：@Before注解，被代理类的方法【切入点】执行之前执行 后置通知：@AfterReturning注解，返回结果后执行，后于最终通知 异常通知：@AfterThrowing注解，有异常才执行 最终通知：@After注解，即使有异常也执行 环绕通知：@Arround注解，被代理类的方法【切入点】执行之前、之后都执行 顺序： 【无异常】 【有异常】 Aspect(切面)：把通知应用到切入点的过程。 Target(目标对象)：代理的目标对象(要增强的类) Weaving(织入)：把切面应用到目标对象来创建新的代理对象的过程。Spring 采用动态代理织入，而 AspectJ 采用编译期织入和类装载期织入。 Proxy(代理)：一个类被 AOP 织入增强后，就产生一个结果代理类。 Introduction(引介)：引介是一种特殊的通知在不修改类代码的前提下，Introduction 可以在运行期为类动态地添加一些方法或 Field。 3.2 Aop 操作 3.2.1 操作方式 Spring 框架一般都是基于 AspectJ 实现 Aop 操作 AspectJ 是独立的 Aop 框架，与 Spring 框架配合使用。 前置准备： 切入点表达式； 作用：直到对哪个类的方法进行增强 语法结构：excution([权限修饰符][返回类型][类的全路径][方法名称][参数列表]) 权限修饰符可以用*代替，表示任意类型修饰符 返回类型可以省略 对指定方法进行增强：excution(* iceriver.spring.aop.User.add(..)) 对类中所有方法进行增强：excution(* iceriver.spring.aop.User.*(..)) 对包下所有类、所有方法增强：excution(* iceriver.spring.aop.*.*(..)) 3.2.2 基于注解进行 Aop 操作——（AspectJ 注解） 创建被增强【被代理】类，在类中定义方法 创建增强【代理】类，编写增强逻辑 进行通知配置： xml 文件中，开启注解扫描： 使用注解创建代理类和被代理类的对象【2.3.2】 代理类添加@Aspect注解，生成代理对象 xml 文件中，开启 Aspect 生成代理对象 配置不同类型的通知： 在代理类中，根据不同的通知类型使用对应的注解，并用切入点表达式进行配置 123456789101112131415161718192021222324252627282930313233343536package iceriver.spring.annotation;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;/** * @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/7/16 14:15 */@Component@Aspectpublic class UserProxy &#123; @Before(value = &quot;execution(* iceriver.spring.annotation.User.add(..))&quot;) public void before()&#123;//前置通知 System.out.println(&quot;before&quot;); &#125; @AfterReturning(value = &quot;execution(* iceriver.spring.annotation.User.add(..))&quot;) public void afterReturning()&#123;//后置通知 System.out.println(&quot;afterReturning&quot;); &#125; @After(value = &quot;execution(* iceriver.spring.annotation.User.add(..))&quot;) public void after()&#123;//最终通知 System.out.println(&quot;after&quot;); &#125; @AfterThrowing(value = &quot;execution(* iceriver.spring.annotation.User.add(..))&quot;) public void afterThrowing()&#123;//异常通知 System.out.println(&quot;afterThrowing&quot;); &#125; @Around(value = &quot;execution(* iceriver.spring.annotation.User.add(..))&quot;) public void around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable &#123;//环绕通知 System.out.println(&quot;around之前&quot;); proceedingJoinPoint.proceed();//被增强的方法执行，如果不写这句，则被代理的类中的方法、以及前置通知不会被执行 System.out.println(&quot;around之后&quot;); &#125;&#125; 重用切入点的定义：将不同通知类型的配置中，相同的切入点表达式进行抽取 定义方法【方法名任意】，抽取相同切入点 调用方法，作为切入点使用 被代理类有多个代理类，可以设置优先级 被代理类使用@Order()注解，传入整数，设置优先级，值越小优先级越高。 设置了的比不设置的高 3.2.3 完全注解开发 创建配置类，替代 xml 配置文件 类的名称任意 使用@Configuration声明这是一个配置类 使用@ComponentScan(basePackages = &#123;&quot;iceriver.spring&quot;&#125;)指定扫描的基本路径 使用@EnableAspectJAutoProxy(proxyTargetClass = true)开启 Aspect 生成代理对象 3.2.4 基于 xml 配置文件进行 Aop 操作 创建被增强【被代理】类，在类中定义方法 创建增强【代理】类，编写增强逻辑 在 xml 文件中创建两个类对象 在 xml 文件中配置切入点。 4. Jdbc Template 4.1 Jdbc Template 介绍 Spring 框架对 Jdbc 进行了封装，简化数据库操作 使用前提是添加了相关的依赖： xml 中配置 Druid： 创建 Jdbc Temlate 对象并注入数据源信息，完成数据库连接 创建 Service 类、Dao 类，在 Dao 类中注入 Jdbc Template 对象【Bean 的创建对象也依赖注入】，完成持久层持有 Jdbc 4.2 Jdbc Template 对象的使用——操作数据库 123456CREATE DATABASE `user_db`;use user_db;CREATE table `t_book`( `user_id` int, `username` VARCHAR(100), `ustatus` VARCHAR(50)); 4.2.1 添加功能 update()： 12345public void add(Book book) &#123; String sql = &quot;insert into t_book values(?,?,?)&quot;; int update = jdbcTemplate.update(sql, book.getUserId(), book.getUserName(), book.getUstatus()); System.out.println(update);&#125; 12345678910@Testvoid add() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); Book book = new Book(); book.setUserId(1); book.setUserName(&quot;java&quot;); book.setUstatus(&quot;a&quot;); bookService.addBook(book);&#125; 4.2.2 修改功能 update() 12345public void update(Book book) &#123; String sql = &quot;update t_book set username=?,ustatus=? where user_id = ?&quot;; int update = jdbcTemplate.update(sql, book.getUserName(), book.getUstatus(),book.getUserId()); System.out.println(update);&#125; 12345678910@Testvoid update() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); Book book = new Book(); book.setUserId(1); book.setUserName(&quot;java&quot;); book.setUstatus(&quot;b&quot;); bookService.updateBook(book);&#125; 4.2.3 删除功能 update() 12345public void delete(Integer id) &#123; String sql = &quot;delete from t_book where user_id=?&quot;; int update = jdbcTemplate.update(sql, id); System.out.println(update);&#125; 123456@Testvoid delete() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); bookService.deleteBook(1);&#125; 4.2.4 查询功能 查询返回某个值： queryForObject(sql, 返回值类型.class) 123456public int findCount() &#123; String sql = &quot;select count(*) from t_book &quot;; int count = jdbcTemplate.queryForObject(sql, Integer.class); System.out.println(count); return count;&#125; 123456@Testvoid findCount() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); bookService.findCount();&#125; 查询返回对象： queryForObject(sql, new BeanPropertyRowMapper&lt;&gt;(Book.class), id); 根据 id 查询一条数据 123456public Book findOne(Integer id) &#123; String sql = &quot;select * from t_book where user_id = ?&quot;; Book book = jdbcTemplate.queryForObject(sql, new BeanPropertyRowMapper&lt;&gt;(Book.class), id); System.out.println(book); return book;&#125; 123456@Testvoid findOne() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); bookService.findOne(500);&#125; 查询返回对象集合： query(sql, new BeanPropertyRowMapper&lt;&gt;(Book.class)); 可以传入第三个参数，也可以没有。 123456public List&lt;Book&gt; findAll() &#123; String sql = &quot;select * from t_book&quot;; List&lt;Book&gt; bookList = jdbcTemplate.query(sql, new BeanPropertyRowMapper&lt;&gt;(Book.class)); System.out.println(bookList); return bookList;&#125; 123456@Testvoid findAll() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); bookService.findAll();&#125; 4.3 Jdbc Template 批量操作数据库 4.3.1 批量添加 12345public void batchAdd(List&lt;Object[]&gt; batchArgs) &#123; String sql = &quot;insert into t_book values(?,?,?)&quot;; int[] add = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(add));&#125; 12345678910111213@Testvoid batchAdd() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;(); Object[] book1 = &#123;3, &quot;java&quot;, &quot;a&quot;&#125;; Object[] book2 = &#123;4, &quot;java&quot;, &quot;a&quot;&#125;; Object[] book3 = &#123;5, &quot;java&quot;, &quot;a&quot;&#125;; batchArgs.add(book1); batchArgs.add(book2); batchArgs.add(book3); bookService.batchAdd(batchArgs);&#125; 4.3.2 批量修改 12345public void batchUpdate(List&lt;Object[]&gt; batchArgs) &#123; String sql = &quot;update t_book set username=?,ustatus=? where user_id = ?&quot;; int[] update = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(update));&#125; 12345678910111213@Testvoid batchUpdate() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;(); Object[] book1 = &#123;&quot;前端&quot;, &quot;a&quot;,3&#125;; Object[] book2 = &#123;&quot;mysql&quot;, &quot;a&quot;,4&#125;; Object[] book3 = &#123;&quot;web&quot;, &quot;a&quot;,5&#125;; batchArgs.add(book1); batchArgs.add(book2); batchArgs.add(book3); bookService.batchUpdate(batchArgs);&#125; 4.3.3 批量删除 12345public void batchDelete(List&lt;Object[]&gt; batchArgs) &#123; String sql = &quot;delete from t_book where user_id=?&quot;; int[] add = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(add));&#125; 12345678910111213@Testvoid batchDelete() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;iceriver.xml&quot;); BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;(); Object[] book1 = &#123;3&#125;; Object[] book2 = &#123;4&#125;; Object[] book3 = &#123;5&#125;; batchArgs.add(book1); batchArgs.add(book2); batchArgs.add(book3); bookService.batchDelete(batchArgs);&#125; 4.4 事务操作 1234567use user_db;CREATE table `t_accout`( `id` VARCHAR(20), username VARCHAR(100), money double); 4.4.1 声明式事务管理 Spring 中事务管理分为两种方式：编程式、声明式。 编程式，即 jdbc 操作过程。 声明式事务管理有两种操作方式：基于注解、xml 文件配置。 声明式事务管理底层 Aop 原理。 开启事务：connection.setAutoCommit(false)，对应 AOP 前置通知 提交事务：connection.commit()，对应 AOP 返回通知 回滚事务：connection.rollBack()，对应 AOP 异常通知 释放连接：connection.close()，对应 AOP 后置通知 Sping 中进行事务管理的类——事务管理器： 4.4.2 注解声明式事务管理 在 xml 文件中，引入名称空间 tx 在创建事务管理器： 在 xml 文件中开启事务注解驱动 在业务逻辑类【Service 类】添加@Transactional @Transactional可以添加在类上面：表示该类中所有方法都添加事务。 @Transactional可以添加到方法上面：表示给该方法添加事务。 4.4.3 注解声明式事务管理配置参数 propagation：事务传播行为， 事务传播行为是为了解决业务层方法之间互相调用的事务问题。 当事务方法被另一个事务方法调用、或调用其他事务方法时，必须指定事务应该如何传播。——是否开启新事务，新事务如何运行。 isolation：事务隔离级别【默认可重复读】 脏读：一个未提交的事务读取到另一个未提交事务的数据 不可重复读：一个未提交事务读取到另一个提交事务修改的数据。 幻读：一个未提交事务读取到拎一个提交事务添加的数据。 timeout：超时时间，规定事务在一定的时间内进行提交，如果未提交就进行回滚，默认为-1，表示不超时。 readOnly：是否只读，默认值 false，表示可以进行 CRUD，设置为 true 时，只能进行查询。 rollbackFor：回滚，设置出现哪些异常进行回滚 noRollbackFor：不回滚，设置出现哪些异常不进行回滚 4.4.4 xml 声明式事务管理 配置事务管理器 配置通知 配置切入点和切面 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop https://www.springframework.org/schema/aop/spring-aop.xsd&quot; &gt; &lt;context:component-scan base-package=&quot;iceriver.spring&quot;/&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:13306/user_db&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;dimitre123&quot; /&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;/bean&gt; &lt;!-- 创建jdbc template对象--&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!-- 注入dataSource —— 使用setter注入--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 1. 创建事务管理器--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 注入数据源--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 2. 配置通知--&gt; &lt;tx:advice id=&quot;txadvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;!-- 配置事务参数--&gt; &lt;tx:attributes&gt; &lt;!--指定哪种规则上面添加事务--&gt; &lt;tx:method name=&quot;accoutMoney&quot;/&gt; &lt;!-- &lt;tx:method name=&quot;accout*&quot;/&gt;&lt;!–给前缀是accout的所有方法添加–&gt;--&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 3. 配置切入点和切面--&gt; &lt;aop:config&gt; &lt;!-- 配置切入点--&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* iceriver.spring.service.UserService.*(..))&quot;/&gt; &lt;!-- 配置切面--&gt; &lt;aop:advisor advice-ref=&quot;txadvice&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;/aop:config&gt;&lt;/beans&gt; 4.5 完全注解开发 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package iceriver.spring.config;import com.alibaba.druid.pool.DruidDataSource;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.datasource.DataSourceTransactionManager;import org.springframework.transaction.annotation.EnableTransactionManagement;import javax.sql.DataSource;/*** @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w* @date: 2022/7/17 12:02*/@Configuration//1. 声明这是一个配置类@ComponentScan(basePackages = &quot;iceriver.spring&quot;)//2. 开启组件扫描@EnableTransactionManagement//3. 声明开启事务public class TxConfig &#123; // 4. 创建数据库连接池 @Bean public DruidDataSource getDruidDatasource()&#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); dataSource.setUrl(&quot;jdbc:mysql://localhost:13306/user_db&quot;); dataSource.setUsername(&quot;root&quot;); dataSource.setPassword(&quot;dimitre123&quot;); return dataSource; &#125; // 4. 创建jadbc Template对象 @Bean public JdbcTemplate getJdbcTemplate(DataSource dataSource)&#123; JdbcTemplate jdbcTemplate = new JdbcTemplate(); // jdbcTemplate.setDataSource(getDruidDatasource());//注入dataSource，但这种方式会执行两次getDruidDatasource jdbcTemplate.setDataSource(dataSource); return jdbcTemplate; &#125; // 6. 创建事务管理器对象 @Bean public DataSourceTransactionManager getDataSourceTransactionManager(DataSource dataSource)&#123; DataSourceTransactionManager transactionManager = new DataSourceTransactionManager(); transactionManager.setDataSource(dataSource); return transactionManager; &#125;&#125; @Bean 注解默认是单例的 5. Spring5 新功能 5.0 新功能介绍 整个 Spring5 框架的代码基于 Java8，运行时兼容 JDK9， 许多不建议使用的类和方法在代码库中删除。 Spring5 框架自带了通用的日志封装，但也可以整合其他日志框架。 Spring 框架不支持 Log4j，因为移除了 Log4jConfigListener，官方推荐使用 Log4j2。 5.1 整合 Log4j2 日志框架 引入 jar 包 创建log4j2.xml配置文件——该文件名固定 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--日志级别以及优先级排序: OFF &lt; FATAL &lt; ERROR &lt; WARN &lt; INFO &lt; DEBUG &lt; TRACE &lt; ALL --&gt;&lt;!--Configuration后面的status用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，可以看到log4j2内部各种详细输出--&gt;&lt;configuration status=&quot;INFO&quot;&gt; &lt;!--先定义所有的appender--&gt; &lt;appenders&gt; &lt;!--输出日志信息到控制台--&gt; &lt;console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;!--控制日志输出的格式--&gt; &lt;PatternLayout pattern=&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n&quot;/&gt; &lt;/console&gt; &lt;/appenders&gt; &lt;!--然后定义logger，只有定义了logger并引入的appender，appender才会生效--&gt; &lt;!--root：用于指定项目的根日志，如果没有单独指定Logger，则会使用root作为默认的日志输出--&gt; &lt;loggers&gt; &lt;root level=&quot;info&quot;&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;/root&gt; &lt;/loggers&gt;&lt;/configuration&gt; 整合日志框架前： 整合日志框架后： 手动输入日志内容至控制台： 创建日志类： 导入import org.slf4j.Logger;和import org.slf4j.LoggerFactory; 创建 log 对象：Logger _log _= LoggerFactory._getLogger_(UserLog.class); 调用相关的方法：info(str)、warn(str)等 5.2 核心容器支持@Nullable注解 @Nullable 注解可以使用在方法上面，属性上面，参数上面，表示方法返回可以为空，属性值可以为空，参数值可以为空。 5.3 核心容器支持函数式风格（GenericApplicationContext) 程序员手动创建的对象交给 Spring 管理； 使用方式： 5.4 整合 JUnit4 引入spring-test-5.2.6.RELEASE.jar依赖【当然还有 junit4，以及 hamcrest-core-1.3 对 junit4 进行增强】 使用注解创建测试类： 使用注解后，便可通过注入属性的方式创建测试类的对象 5.5 整合 JUnit5 引入spring-test-5.2.6.RELEASE.jar依赖【当然还有 junit5】 使用注解创建测试类： 复合注解方式： 5.6 SpringWebflux 需要 SpringMVC、SpringBoot、Maven、Java8 新特性知识基础。 5.6.1 介绍 5.6.2 响应式编程 5.6.3 基于注解编程模型 5.6.4 基于函数式编程模型","tags":[{"name":"Spring5","slug":"Spring5","permalink":"https://sk370.github.io/tags/Spring5/"}]},{"title":"Java Web","date":"2022-06-29T01:44:11.000Z","path":"2022/06/29/javaweb/javaweb/","text":"JavaWeb 是指，所有通过 Java 语言编写可以通过浏览器访问的程序的总称，叫 JavaWeb。JavaWeb 是基于请求和响应来开发的。 本项目所使用到到的外部jar包 1. 前端基础 参见 Wbe 前端知识库：Web 前端 表单提交后，数据未提交（发给服务器）的三种情况： 表单标签没有 name 属性 单选、复选、下拉列表（&lt;option&gt;）都添加上 value 属性 表单标签没有放在&lt;form&gt;标签内 action 属性不能带参数时，可以设计隐藏域&lt;input type=&quot;hidden&quot;&gt;使用 GET 请求的特点是： 浏览器地址栏中的地址是：action 属性[+?+请求参数] 请求参数的格式是：name=value&amp;name=value 提交内容明文显示，不安全 它有数据长度的限制 POST 请求的特点是： 浏览器地址栏中只有 action 属性值 相对于 GET 请求要安全 理论上没有数据长度的限 &amp;&amp;运算符：全为真时，返回最后一个表达式的值： alert(&quot;abc&quot;&amp;&amp;true)，返回 true alert(true&amp;&amp;&quot;abc&quot;)，返回 abc &amp;&amp;运算符：全为假时，返回第一个表达式为假的值： alert(&quot;abc&quot;&amp;&amp;false)，返回 false || 运算符： 当表达式全为假时，返回最后一个表达式的值 只要有一个表达式为真，就会把回第一个为真的表达式的值 js 中变量未初始化，默认为 undefined。 隐形参数 arguments： js 中定义函数时声明形参个数与调用时参数无关，即声明时不传参数，但调用可传入参数，或声明传入参数、调用不传入参数。 在函数内部使用 arguments 可以获取实际传入的参数个数，arguments 是一个数组，可以遍历。 相当于定义函数时：function(arguments)，arguments 是一个可变长参数 表单验证不合法时阻止提交：本质——onsubmit=&quot;return false&quot; 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script type=&quot;text/javascript&quot;&gt; // 静态注册表单提交事务 function onsubmitFun() &#123; // 要验证所有表单项是否合法，如果，有一个不合法就阻止表单提交 alert(&quot;静态注册表单提交事件----发现不合法&quot;); return flase; &#125; window.onload = function () &#123; //1 获取标签对象 var formObj = document.getElementById(&quot;form01&quot;); //2 通过标签对象.事件名 = function()&#123;&#125; formObj.onsubmit = function () &#123; // 要验证所有表单项是否合法，如果，有一个不合法就阻止表单提交 alert(&quot;动态注册表单提交事件----发现不合法&quot;); return false; &#125;; &#125;; &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;!--return false 可以阻止 表单提交 --&gt; &lt;form action=&quot;http://localhost:8080&quot; method=&quot;get&quot; onsubmit=&quot;return onsubmitFun();&quot; &gt; &lt;input type=&quot;submit&quot; value=&quot;静态注册&quot; /&gt; &lt;/form&gt; &lt;form action=&quot;http://localhost:8080&quot; id=&quot;form01&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;动态注册&quot; /&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 验证提示效果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;&lt;/title&gt; &lt;link type=&quot;text/css&quot; rel=&quot;stylesheet&quot; href=&quot;../../static/css/style.css&quot; /&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;../../static/script/jquery-1.7.2.js&quot; &gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; // 页面加载完成之后 $(function () &#123; // 给注册绑定单击事件 $(&quot;#sub_btn&quot;).click(function () &#123; // 验证用户名：必须由字母，数字下划线组成，并且长度为5到12位 //1 获取用户名输入框里的内容 var usernameText = $(&quot;#username&quot;).val(); //2 创建正则表达式对象 var usernamePatt = /^\\w&#123;5,12&#125;$/; //3 使用test方法验证 if (!usernamePatt.test(usernameText)) &#123; //4 提示用户结果 $(&quot;span.errorMsg&quot;).text(&quot;用户名不合法！&quot;); return false; &#125; // 验证密码：必须由字母，数字下划线组成，并且长度为5到12位 //1 获取用户名输入框里的内容 var passwordText = $(&quot;#password&quot;).val(); //2 创建正则表达式对象 var passwordPatt = /^\\w&#123;5,12&#125;$/; //3 使用test方法验证 if (!passwordPatt.test(passwordText)) &#123; //4 提示用户结果 $(&quot;span.errorMsg&quot;).text(&quot;密码不合法！&quot;); return false; &#125; // 验证确认密码：和密码相同 //1 获取确认密码内容 var repwdText = $(&quot;#repwd&quot;).val(); //2 和密码相比较 if (repwdText != passwordText) &#123; //3 提示用户 $(&quot;span.errorMsg&quot;).text(&quot;确认密码和密码不一致！&quot;); return false; &#125; // 邮箱验证：xxxxx@xxx.com //1 获取邮箱里的内容 var emailText = $(&quot;#email&quot;).val(); //2 创建正则表达式对象 var emailPatt = /^[a-z\\d]+(\\.[a-z\\d]+)*@([\\da-z](-[\\da-z])?)+(\\.&#123;1,2&#125;[a-z]+)+$/; //3 使用test方法验证是否合法 if (!emailPatt.test(emailText)) &#123; //4 提示用户 $(&quot;span.errorMsg&quot;).text(&quot;邮箱格式不合法！&quot;); return false; &#125; // 验证码：现在只需要验证用户已输入。因为还没讲到服务器。验证码生成。 var codeText = $(&quot;#code&quot;).val(); //去掉验证码前后空格 // alert(&quot;去空格前：[&quot;+codeText+&quot;]&quot;) codeText = $.trim(codeText); // alert(&quot;去空格后：[&quot;+codeText+&quot;]&quot;) if (codeText == null || codeText == &quot;&quot;) &#123; //4 提示用户 $(&quot;span.errorMsg&quot;).text(&quot;验证码不能为空！&quot;); return false; &#125; // 去掉错误信息 $(&quot;span.errorMsg&quot;).text(&quot;&quot;); &#125;); &#125;); &lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; .login_form &#123; height: 420px; margin-top: 25px; &#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;login_header&quot;&gt; &lt;img class=&quot;logo_img&quot; alt=&quot;&quot; src=&quot;../../static/img/logo.gif&quot; /&gt; &lt;/div&gt; &lt;div class=&quot;login_banner&quot;&gt; &lt;div id=&quot;l_content&quot;&gt; &lt;span class=&quot;login_word&quot;&gt;欢迎注册&lt;/span&gt; &lt;/div&gt; &lt;div id=&quot;content&quot;&gt; &lt;div class=&quot;login_form&quot;&gt; &lt;div class=&quot;login_box&quot;&gt; &lt;div class=&quot;tit&quot;&gt; &lt;h1&gt;注册尚硅谷会员&lt;/h1&gt; &lt;span class=&quot;errorMsg&quot;&gt;&lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;form&quot;&gt; &lt;form action=&quot;http://localhost:8080&quot;&gt; &lt;label&gt;用户名称：&lt;/label&gt; &lt;input class=&quot;itxt&quot; type=&quot;text&quot; placeholder=&quot;请输入用户名&quot; autocomplete=&quot;off&quot; tabindex=&quot;1&quot; name=&quot;username&quot; id=&quot;username&quot; /&gt; &lt;label&gt;用户密码：&lt;/label&gt; &lt;input class=&quot;itxt&quot; type=&quot;password&quot; placeholder=&quot;请输入密码&quot; autocomplete=&quot;off&quot; tabindex=&quot;1&quot; name=&quot;password&quot; id=&quot;password&quot; /&gt; &lt;label&gt;确认密码：&lt;/label&gt; &lt;input class=&quot;itxt&quot; type=&quot;password&quot; placeholder=&quot;确认密码&quot; autocomplete=&quot;off&quot; tabindex=&quot;1&quot; name=&quot;repwd&quot; id=&quot;repwd&quot; /&gt; &lt;label&gt;电子邮件：&lt;/label&gt; &lt;input class=&quot;itxt&quot; type=&quot;text&quot; placeholder=&quot;请输入邮箱地址&quot; autocomplete=&quot;off&quot; tabindex=&quot;1&quot; name=&quot;email&quot; id=&quot;email&quot; /&gt; &lt;label&gt;验证码：&lt;/label&gt; &lt;input class=&quot;itxt&quot; type=&quot;text&quot; style=&quot;width: 150px;&quot; id=&quot;code&quot; /&gt; &lt;img alt=&quot;&quot; src=&quot;../../static/img/code.bmp&quot; style=&quot;float: right; margin-right: 40px&quot; /&gt; &lt;input type=&quot;submit&quot; value=&quot;注册&quot; id=&quot;sub_btn&quot; /&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;bottom&quot;&gt; &lt;span&gt; 尚硅谷书城.Copyright ©2015 &lt;/span&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 正则表达式：常用正则表达式.txt patt.test(str)：验证 str 是否满足 patt 的规则。 &lt;base&gt;标签： 设置当前页面中所有相对路径的参照路径 web 中/的不同含义：代表绝对路径 浏览器解析：【http://ip:port/】 服务器解析：【http://ip:port/工程路径】 服务器重定向时（response.sendRediect(&quot;/&quot;)）：表示把/发送给浏览器解析，得到的是【http://ip:port/】 2. BS 和 CS 2.1 BS 浏览器服务器架构模式 优点：客户端不需要安装，升级维护成本较低 缺点：所有计算和存储都放在服务器端，服务器计算完成后将结果传输给客户端，服务器符合较重，且客户端与服务器端会进行非常频繁的数据通信，导致网络负荷较重。 2.2 CS 客户端服务器架构模式 优点：充分来利用客户端机器资源，减轻服务器的负荷。 一部分安全要求不高的计算任务、存储任务放在客户端执行，减轻服务器的压力，也减轻网络负荷。 缺点：需要安装客户端程序，且升级维护成本较高。 3. Tomcat 的使用 3.1 Tomcat 简介 Tomcat 服务器是一个免费的开放源代码的 Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试 JSP 程序的首选。 java 官方提供。 版本 import 包时，tomcat8 和 tomcat10 的包名不一样 3.2 Tomcat 使用 下载： 使用 JDK8 开发，本文下载了 Tomcat8.5 版本。 安装：解压至非中文、无空格、特殊字符路径下。 目录介绍： bin：程序运行文件夹 lib：程序运行的依赖库 conf：配置文件 logs：运行日志 temp：缓存文件目录 webapps：项目布署目录 work：工作目录 配置环境变量： 前提：需要配置JAVA_HOME环境变量。虽然安装 java 时，环境变量Path下已经有默认配置，能够在任何位置的 cmd 终端执行java命令，但配置 Tomcat 仍然需要重新配置JAVA_HOME，否则会出现执行 Tomcat 的开启命令会出现一闪而过的情况，Tomcat 也不会运行起来。 配置了 java 环境变量后，执行bin/startup.bat 乱码问题：修改conf/logging.properties文件，添加java.util.logging.ConsoleHandler.encoding = GBK指令。 访问：http://localhost:8080/ 新建 Web 项目，并布署： 在webapps路径下，新建网址名/WEB-INF，如webdesign，将写好的网页放到该目录下 WEB-INF不能修改。 4. IDEA 中新建 Web 项目 4.1 创建项目 4.1.1 方式一 不同版本的 IDEA 创建 web 项目的操作方式，本文以 IDEA2022.1.3 为例。 新建项目（模块），指明存放路径，jdk 版本 右键项目（模块），添加 Web 框架支持 4.1.2 方式二 新建模块： 设置项目结构 添加 Web 程序： 生成部署包（工件、atrifacts）： 拓展：war 包，压缩后的 web 应用程序包，打包放到 tomcat 的 webapps 路径下，会自动解压 4.1.2 导入其他人项目 同 4.1.1 操作 4.2 运行配置（Tomcat 配置） 添加 Tomcat 本地服务器 配置访问地址： url 表示默认打开浏览器时的地址。 部署服务器端的文件（使服务器能打开本地文件） 应用程序上下文：设置为/时，表示访问服务器根目录即能访问到该 web 程序。 如果找不到工件，则按照下面的方式 配置总结： 这个页面的地址最后带不带/都没有关系 这个页面的这个名称要写到第一个图的地址后面 这个界面的路径要到web目录下 4.3 热更新配置 设置热更新： 4.4 配置 servlet api 要能写服务端程序，则要求改程序必须实现自 servlet 接口或继承自 GenericServlet 类和 HttpServlet 类（参见 5.1） 方式一：添加servlet-api.jar至 lib 文件夹，同 Druid 方式二： 添加后会显示为外部库文件 4.5 部分错误类型及解决方式 4.5.1 未添加 Servlet 运行程序（容易 404） 5. Servlet 入门 06尚硅谷_Servlet王振国 - 课堂笔记.pdf 5.1 Servlet 简介 5.1.1 Servlet 介绍 作用：Servlet 是 Server Applet 的缩写，称为服务器端小程序，是一种使用 Java 语言来开发动态网站的技术。 Serlet 几乎可以使用 Java 的所有 API。 出现背景：使用 Java 原生代码开发动态网站需要手动解析 HTTP 请求报头、需要分析用户的请求参数、需要加载数据库组件……过程极其麻烦，基于上述原因，Java 官方推出了 Servlet 技术，对开发动态网站需要使用的 JavaAPI 进行了封装，形成了一套新的 API，称为 Servlet API。 本质：Servlet 是一套 Java Web 的开发规范（开发技术标准），只有实现（编写代码实现 Servlet 规范中的各类功能——类、方法、属性）了该规范才能进行 web 开发。 体现：一个 Servlet 程序实际上是按照 Servlet 规范编写的 Java 类，Servlet 程序需要编译成字节码文件（.class）才能部署到服务器运行。 Servlet 功能实现方式：实现 Servlet 接口能获得 Servlet 的所有功能。但需要实现很多方法，比较麻烦，所以 Servlet 规范提供了两个实现了 Servlet 接口常用功能（方法）的类：GenericServlet 类和 HttpServlet 类，其中 HttpServlet 类的使用更为方便。 Servlet 就是 JavaWeb 三大组件之一。三大组件分别是：Servlet 程序、Filter 过滤器、Listener 监听器 。 5.1.2 Servlet 继承关系 类图： Tomcat8 中，包为javax，Tomcat10 中，包为jakarta 相关方法： javax.servlet.Servlet接口: void init(config) - 初始化方法 void service(request,response)- 服务方法 void destory()- 销毁方法 javax.servlet.GenericServlet抽象类： void service(request,response)- 仍然是抽象的 javax.servlet.http.HttpServlet 抽象子类： void service(request,response)- 不是抽象的 当有请求过来时，service 方法会自动响应（其实是 tomcat 容器调用的） 5.2 Servlet 容器（Web 容器） 5.2.1 Web 服务器 web 服务器功能比较单一，一般只能提供 http(s)服务，使用户可以访问静态资源（HTML 文档、图片、CSS 文件、JavaScript 文件等），它们不能执行任何编程语言，也不能访问数据库，更不能让用户注册和登录。——静态网站。 要部署动态网站，还要有编程语言运行环境和数据库管理系统的支持。 运行环境：支持脚本语言运行的部件（解释器、垃圾回收器、标准库等）的统称。又称运行时（Runtime) 数据库：mysql 等 常见 web 服务器： 运行 PHP 网站一般选择 Apache 或者 Nginx； 运行 ASP/ASP.NET 网站一般选择 IIS； 运行 Python 网站一般选择内置的 WSGI 服务器模块——wsgiref。 动态网站三大组件：Web 服务器+脚本语言运行环境+数据库 5.2.2 web 容器 servlet 容器：servlet 代码的运行环境，包括： jre（JVM、Java 核心类库、一些属性文件），并不支持 Servlet 规范？ 实现 Servlet 规范定义的各类接口和类： 管理用户编写的 Servlet 类，以及实例化以后的对象——我们编写的 Servlet 类没有 main()，不能独立运行，知道能当作一个模块加载到 Servlet 容器，由容器进行实例化并调用方法。 提供 HTTP 服务，相当于简化 web 服务器。 常见 web 容器：Tomcat、Jboss、Jetty、WebLogic 等，实现了 Servlet 规范。 工作过程：一个动态页面对应一个 Servlet 类，开发一个动态页面就是编写一个 Servlet 类，当用户请求到达时，Servlet 容器会根据配置文件（web.xml）来决定调用哪个类。 5.3 url 地址到 Servlet 程序的访问 5.3.1 Servlet 部署 Servlet 规范规定，JavaWeb 应用必须采用固定的目录结构，即每种组件在 JavaWeb 应用中都有固定的存放目录。 目录 描述 是否必需 \\servletDemo Web 应用的根目录，属于该 Web 应用的所有资源都存放在这个目录下。 是 \\servletDemo\\WEB-INF 存放 web.xml、lib 目录以及 classes 目录等。 是 \\servletDemo\\WEB-INF\\classes 存放各种 .class 文件或者包含 .class 文件的目录，Servlet 类的 .class 文件也存放在此。 否 \\servletDemo\\WEB-INF\\lib 存放应用所需的各种 jar 包，例如 JDBC 驱动程序的 jar 包。 否 \\servletDemo\\WEB-INF\\web.xml web.xml 中包含应用程序的配置和部署信息。 是 部署（手动不使用 IDEA 工具的方式）：将 servletDemo 文件夹复制到/webapps目录下 实质是在/webapps目录下编写程序。 5.3.2 Servlet 访问配置 5.3.3 Servlet 虚拟路径映射 Servlet 单一映射： 方式一：在 web.xml 文件中配置 方式二：使用@WebServlet：@WebServlet(&quot;/MyServlet&quot;)，省略了 urlPatterns = Servlet 多重映射： 方式一：在 web.xml 文件中配置多个&lt;servlet-mapping&gt;元素 方式一：在 web.xml 文件中配置多个&lt;url-pattern&gt;元素 方式二：使用@WebServlet：@WebServlet(urlPatterns = &#123;&quot;/MyServlet&quot;, &quot;/MyServlet2&quot;&#125;) 5.3.4 项目中地址的使用 开发中一般使用绝对路径+相对路径的形式，或者绝对路径的形式 &lt;base&gt;标签设置在 html 文件中，只对当前的 html 页面生效。写路径时/配合&lt;base&gt;标签一起看。 一般不写/ 但也可以写为./ html 页面的所有地址【HTML 的和 servlet 的】，都要配合&lt;base&gt;标签一起看。 建议规范：html 页都不带/ 在 servlet 程序中，默认为工程路径（Tomcat 配置的 http 全路径）【忽略了 src 目录、各个包目录，直接到类】，写路径时带不带/都不影响【有影响，但规律还没把握】 建议规范：servlet 页面都带/ &lt;%@include file=&quot;/pages/common/manager_meun.jsp&quot;%&gt;必须以/开头 /被 web 解析，表示http://ip:port/，被服务器解析，表示http://ip:port/工程路径 5.4 Servlet 的生命周期 Servlet 构造器方法（创建 Servlet 实例——由于是 tomcat 去创建，所以构造器不能用 private 修饰） 第一次访问时调用。 init 初始化方法 第一次访问时调用。 作用：建立数据库连接，获取配置信息等 service 方法 每次访问都会调用。 destroy 销毁方法 web 工程停止的时候调用（工程停止不是切换浏览器的访问地址） 默认情况下，第一此请求时 tomcat 才会创建 Servlet 实例化对象、调用init()，便于提高启动速度，但会造成首次请求等待时间较长。 xml 文件中，配置&lt;load-on-startup&gt;控制创建实例对象、初始化方法的时机【见 5.7.4】。这样的好处是提高了首次请求的响应时间，但带来问题是启动变慢。 Servlet 在容器中是单例、线程不安全的。 单例：所有的请求都是同一个实例去响应 线程不安全：一个线程需要根据这个实例中的某个成员变量值去做逻辑判断。但是在中间某个时机，另一个线程改变了这个成员变量的值，从而导致第一个线程的执行路径发生了变化 尽量的不要在 servlet 中定义成员变量。如果不得不定义成员变量，那么：① 不要去修改成员变量的值 ② 不要去根据成员变量的值做一些逻辑判断 5.5 GET 和 POST 请求分发处理 实现 Servlet 接口的 HttpServlet 类中的 service 方法，会根据 http 数据传输方式不同做不同的处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String method = req.getMethod(); long lastModified; if (method.equals(&quot;GET&quot;)) &#123; lastModified = this.getLastModified(req); if (lastModified == -1L) &#123; this.doGet(req, resp); &#125; else &#123; long ifModifiedSince; try &#123; ifModifiedSince = req.getDateHeader(&quot;If-Modified-Since&quot;); &#125; catch (IllegalArgumentException var9) &#123; ifModifiedSince = -1L; &#125; if (ifModifiedSince &lt; lastModified / 1000L * 1000L) &#123; this.maybeSetLastModified(resp, lastModified); this.doGet(req, resp); &#125; else &#123; resp.setStatus(304); &#125; &#125; &#125; else if (method.equals(&quot;HEAD&quot;)) &#123; lastModified = this.getLastModified(req); this.maybeSetLastModified(resp, lastModified); this.doHead(req, resp); &#125; else if (method.equals(&quot;POST&quot;)) &#123; this.doPost(req, resp); &#125; else if (method.equals(&quot;PUT&quot;)) &#123; this.doPut(req, resp); &#125; else if (method.equals(&quot;DELETE&quot;)) &#123; this.doDelete(req, resp); &#125; else if (method.equals(&quot;OPTIONS&quot;)) &#123; this.doOptions(req, resp); &#125; else if (method.equals(&quot;TRACE&quot;)) &#123; this.doTrace(req, resp); &#125; else &#123; String errMsg = lStrings.getString(&quot;http.method_not_implemented&quot;); Object[] errArgs = new Object[]&#123;method&#125;; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(501, errMsg); &#125;&#125; 如果后端的处理方法（do 方法）与请求方法不一致，会报 405 错误 5.6 IDEA 创建 Servlet 程序 确认模块添加了 Tomcat 依赖（步骤 4.4，即添加了 servlet api） 右键 src 目录，新建 servlet 程序 5.7 Servlet 底层原理 5.7.1 Servlet 类的继承关系 5.7.2 ServletConfig 类 servlet 程序的配置信息类 Servlet 程序和 ServletConfig 对象都是由 Tomcat 创建的，程序员负责使用 Servlet 程序默认是第一次访问时创建 ServletConfig 是每个 Servlet 程序创建时创建 获取（假设 MyServlet 继承自 HttpServlet）： 方式一：定义private ServletConfig servletConfig，重写init()方法 如果不声明servletConfig，则必须在重写init()方法时，显式的调用super.init(config)，否则会空指针异常 方式二：调用 GenericServlet 提供的 getServletConfig()方法：ServletConfig servletConfig = this.getServletConfig() 配置 Servlet 初始化参数： 方式一：在 web.xml 文件中配置 方式二：使用@WebServlet 配置 作用： 获取 Servlet 程序的别名&lt;servlet-name&gt;的值。（web.xml 文件中） 使用servletConfig.getServletName() 获取初始化参数&lt;init-param&gt; 使用servletConfig.getInitParameter(&quot;参数名&quot;)——参数名是 web.xml 文件中配置的&lt;param-name&gt;，获取到的值是&lt;param-value&gt; 获取 ServletContext 对象:getServletContext() 方法 5.7.3 ServletContext 类 表示 Servlet 上下文对象的一个接口，一个 web 工程，只有一个 ServletContext 实例。 Servlet 上下文：Servlet 容器启动时，为每个 Web 应用（webapps 文件夹下的每个目录）创建一个唯一的 ServletContext 对象，该对象称为 Servlet 上下文 ServletContext 对象是一个域对象 域对象：可以像 map 一样存储数据的对象，域指的是存储数据的操作范围，域对象是值服务器在内存上创建的存储空间，该空间用于不同资源之间共享数据。 ServletContext 的域是整个工程。所以不同 Servlet 之间可以通过 ServletContext 对象实现数据通信。 域对象特性： 另外两个域对象时 reques 和 session ServletContext 在 web 工程启动时创建，停止时销毁。 获取 ServletContext 对象： 通过 GenericServlet 提供的 getServletContext() 方法 通过 ServletConfig 提供的 getServletContext() 方法 通过 HttpSession 提供的 getServletContext() 方法 通过 HttpServletRequest 提供的 getServletContext() 方法 作用： 获取 web.xml 中配置的上下文参数 context-param getInitParameter(str) 获取当前的工程路径，格式: /工程路径 getContextPath() 获取工程部署后在服务器硬盘上的绝对路径，D:\\实践练习\\07.JavaWeb\\out\\artifacts\\servlet1_8_war_exploded\\ getRealPath(&quot;/&quot;) 像 Map 一样存取数据 获取上下文初始化参数： 设置上下文初始化参数： 方式一：在 web.xml 文件中，使用&lt;context-param&gt;及其子标签 方式二：在 Servlet 程序中，使用 ServletContext 对象的setAtrribute()方法 获取上下文初始化参数： 方式一： 方式二： 两种方式对比： 读取 web 应用下的资源文件 参数 path 代表资源文件的虚拟路径，它以正斜线/开始，/表示当前 Web 应用的根目录。 5.7.4 load-on-startup 元素 作用：控制 Servlet 启动优先级 使用在 web.xml 文件中，是&lt;servlet&gt;的子元素节点。 取值规则 它的取值必须是一个整数； 当值小于 0 或者没有指定时，则表示容器在该 Servlet 被首次请求时才会被加载； 当值大于 0 或等于 0 时，表示容器在启动时就加载并初始化该 Servlet，取值越小，优先级越高； 当取值相同时，容器就会自行选择顺序进行加载。 5.7.5 HttpServletRequest 类 作用：每次 Servlet 容器接收到 HTTP 请求，就会创建一个 HttpServletRequest 对象，通过该对象，可以获取对象传递给服务器的信息。 生命周期：Servlet 容器接到 HTTP 请求时创建，Servlet 容器将响应信息返回给客户端后销毁。 获取请求行的方法： getRemoteHost()：在浏览器中使用 localhost、127.0.0.1 访问时，返回 127.0.0.1，使用真实 ip 访问时，返回真实的客户端 ip 获取请求头的方法 获取表单数据的方法： 中文乱码问题： GET 请求：Get 请求将请求数据附加到 URL 后面作为参数，浏览器发送文字时采用的编码格式与页面编码保持一致（utf-8）。如果 Tomcat 没有设置字符集，接收 URL 时默认使用 ISO-8859-1 进行解码，ISO-8859-1 不兼容中文 POST 请求：POST 提交的数据在请求体中，其所使用的编码格式时页面一致（即 utf-8）。request 对象接收到数据之后，会将数据放到 request 缓冲区，缓冲区的默认字符集是 ISO-8859-1（该字符集不支持中文），两者使用的字符集不一致导致乱码。 GET 请求：【jdk8 之前，jdk8 及之后版本不会乱码】 方式一：修改 tomcat/conf/server.xml 中的配置：&lt;Connector port=&quot;80&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; URIEncoding=&quot;UTF-8&quot;/&gt;【70 行左右】 方式二：使用 URLEncoder 和 URLDecoder 进行编码和解码的操作（逆向编解码）： String username = request.getParameter(&quot;username&quot;); username = URLEncoder.encode(username, &quot;ISO8859-1&quot;); username = URLDecoder.decode(username, &quot;UTF-8&quot;); 方式三：使用 String 的构造方法：String(byte[] bytes, String charset) ，对字节数组（bytes）按照指定的字符集（charset）进行解码，返回解码后的字符串 String username = request.getParameter(&quot;username&quot;); username = new String(username.getBytes(&quot;ISO-8859-1&quot;),&quot;UTF-8&quot;); POST 请求： 在获取请求参数之前设置 request 缓冲区字符集为 utf-8 request.setCharacterEncoding(&quot;utf-8&quot;); String username = request.getParameter(&quot;username&quot;); 5.7.6 请求转发 含义：容器接收请求后，Servlet 会先对请求做一些预处理，然后将请求传递给其他 Web 资源，来完成包括生成响应在内的后续工作。 对象：RequestDispatcher 对象由 Servlet 容器创建，用于封装由路径所标识的 Web 资源。利用 RequestDispatcher 对象可以把请求转发给其他的 Web 资源。 获取 RequestDispatcher 对象： 调用 ServletContext 的 getRequestDispatcher(String path) 方法，参数 path 指定目标资源的路径，必须为绝对路径——以/打头； 调用 ServletRequest 的 getRequestDispatcher(String path) 方法，参数 path 指定目标资源的路径，可以为绝对路径，也可以为相对路径。 绝对路径是指以符号“/”开头的路径，“/”表示当前 Web 应用的根目录。相对路径是指相对当前 Web 资源的路径，不以符号“/”开头。 RequestDispatcher 对象的方法： 请求转发特点： 请求转发不支持跨域访问，只能跳转到当前应用中的资源。 请求转发之后，浏览器地址栏中的 URL 不会发生变化，因此浏览器不知道在服务器内部发生了转发行为，更无法得知转发的次数。 参与请求转发的 Web 资源之间共享同一 request 对象和 response 对象。 由于 forward() 方法会先清空 response 缓冲区，因此只有转发到最后一个 Web 资源时，生成的响应才会被发送到客户端。 request 域对象： 作用：不同 Servlet 程序间传递数据 方法： 与 Context 域对象的不同点： 5.7.7 HttpServletResponse 类 作用：每次 Servlet 容器接收到 HTTP 请求，就会创建一个 HttpServletResponse 对象，通过该对象，可以向客户端传递信息。 生命周期：Servlet 容器接到 HTTP 请求时创建，Servlet 容器将响应信息返回给客户端后销毁。 设置响应行的方法： 设置响应头的方法 向客户端传递信息（响应体设置）的方法： getOutputStream() 和 getWriter() 方法互相排斥，不可同时使用，否则会发生 IllegalStateException 异常。 中文乱码问题： 字节流输出：不一定乱码，取决于服务器 Servlet 程序的字符集是否与浏览器字符集一致。 字符流输出：一定乱码。response 缓冲区的默认字符集是 ISO-8859-1，该字符集不支持中文。 字节流解决方案：将中文转成字节数组时和浏览器默认采用的字符集保持一致 response.setHeader(&quot;Content-Type&quot;, &quot;text/html;charset=UTF-8&quot;); byte[] str = &quot;编程帮 www.biancheng.net&quot;.getBytes(&quot;UTF-8&quot;); 字符流解决方案：将 resopnse 缓冲区编码与浏览器设置一致 方式一： response.setCharacterEncoding(&quot;UTF-8&quot;); response.setHeader(&quot;Content-Type&quot;, &quot;text/html;charset=UTF-8&quot;); 方式二： response.setContentType(&quot;text/html;charset=UTF-8&quot;); 5.7.8 请求重定向 含义：服务器端接收到请求后，通知客户端去请求另外的地址。 属于客户端行为。 本质上是 2 次 Http 请求。 工作流程： 重定向和转发的区别： 方法： 方式一： 方式二：（推荐） 注意路径问题，该路径由浏览器解析 经典案例：生成验证码 5.8 @WebServlet 注解（Servlet 注解） 5.8.1 Servlet 注解介绍 出现原因：解决 Servlet 程序过多，web.xml 文件过长的问题。 使用方式：注解写在 Servlet 程序（类）中，只对当前类有效。 出现版本：Servlet3.0，使用注解使 web.xml 不再是必选项。 @WebServlet @WebInitParm @WebFilter @WebLitener 等 5.8.2 @WebServlet 注解的属性 5.8.3 @WebServlet 注解的使用 在 web.xml 文件中，将属性metadata-complete设置为 fasle 或不配置时，表示启用注解支持，配置为 true 时，表示容器部署时至依赖 web.xml 示例： 12345678910@WebServlet( asyncSupported =**true**, name =&quot;myServlet&quot;, description =&quot;name 描述&quot;, loadOnStartup =1, urlPatterns =&#123;&quot;/MyServlet&quot;,&quot;/\\*&quot;&#125;, initParams =&#123; @WebInitParam(name =&quot;编程帮&quot;, value =&quot;www.biancheng.net&quot;, description =&quot;init 参数 1&quot;), @WebInitParam(name =&quot;京东&quot;, value =&quot;www.jd.com&quot;, description =&quot;init 参数 2&quot;)&#125;) 注意点： 通过实现 Serlvet 接口或继承 GenericServlet 创建的 Servlet 类无法使用 @WebServlet 注解。 通过 web.xml 文件和注解同时配置属性时，若取值相同则忽略注解中的配置。 5.9 Listener 监听器 5.9.1 介绍 本质：JavaEE 的规范（接口） 作用：监听另一个 Java 对象的方法调用或属性改变，当监听到上述事件，监听器的方法立即执行。 5.9.2 分类 Servlet 规范中定义了 8 个监听器接口，可以用于监听 ServletContext、HttpSession 和 ServletRequest 对象的生命周期和属性变化事件。 开发 Servlet 监听器需要实现相应的监听器接口并重写接口中的方法。 按照监听的事件划分，分为三类： 监听对象创建和销毁的监听器 监听对象中属性变更的监听器 监听 HttpSession 中的对象状态改变的监听器 5.9.3 监听对象创建和销毁的监听器 目前仅有ServletContextListener有使用机会。 作用：监听SrvletContext对象的创建和销毁 生命周期：web 工程启动时创建，web 工程停止时销毁 5.9.4 监听属性变更的监听器 5.9.5 监听 Session 中对象状态改变的监听器 5.9.6 注册监听器 在 web.xml 中注册监听器； 使用 @WebListener 注册监听器。 使用 HttpSessionBindingListener 和 HttpSessionActivationListener 时，不必进行注册，直接创建 Java 类实现这两个接口即可。 5.9.7 设置监听器参数 在 xml 文件中设置初始化参数： 12345678910111213141516171819@WebListenerpublic class ContextLoaderListener implements ServletContextListener &#123; @Override public void contextInitialized(ServletContextEvent servletContextEvent) &#123; //1.获取ServletContext对象 ServletContext application = servletContextEvent.getServletContext(); //2.获取上下文的初始化参数 String path = application.getInitParameter(&quot;contextConfigLocation&quot;); //3.创建IOC容器 BeanFactory beanFactory = new ClassPathXmlApplicationContext(path); //4.将IOC容器保存到application作用域 application.setAttribute(&quot;beanFactory&quot;,beanFactory); &#125; @Override public void contextDestroyed(ServletContextEvent servletContextEvent) &#123; &#125;&#125; 5.9.7 经典案例——统计在线人数 5.x 其他技术 5.x.1 JSP Servlet 是第一代 Java Web 开发技术，编写 HTML 代码相当于拼接字符串，最后以字符串的形式向外输出。 JSP 是第二代 Java Web 开发技术。允许 HTML 代码和 JSP 代码分离，让程序员能够在 HTML 文档中直接嵌入 JSP 代码。 分离了还是嵌入了？？？？ 5.x.2 Applet Applet 是客户端小程序，一般嵌入 HTML 页面，运行在支持 Java 的浏览器（安装了 Java 虚拟机）中。 目前已经被 JavaScript 全面代替。 6. XML 05尚硅谷_xml王振国 - 课堂笔记.pdf 6.1 作用 用来保存数据，而且这些数据具有自我描述性 它还可以做为项目或者模块的配置文件 还可以做为网络传输数据的格式（现在 JSON 为主）。 6.2 语法 6.2.1 文档声明 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml要连写在一起，否则会报错 version版本号 encoding编码 standalone=&quot;yes/no&quot;表示这个 xml 文件是否是独立的 xml 文件 xml 是强语言，必须符合语法规则。使用浏览器打开会进行语法校验。 xml 文档必须要有一对根标签（顶级标签——没有父级标签的标签），且必须唯一。 6.2.2 xml 注释 同 HTML 6.2.3 xml 元素 元素是指从开始标签到结束标签的内容。 特殊字符语法同 HTML 文本区域（CDATA）：告诉 xml 解析器，改区域内的内容为纯文本，不需要解析 &lt;![CDATA[ 内容xxxx ]]&gt; 6.2.4 xml 标签 标签名可以含字母、数字以及其他的字符 标签名不能以数字或标点符号开始 标签名不能以xml、XML、Xml开始——验证可行。 标签名不能包含空格。 标签分为单标签和双标签，但必须闭合。 标签的属性值必须使用&quot;&quot;包裹 标签对大小写敏感 标签必须正确嵌套 6.3 xml 解析技术 6.3.1 dom 解析技术 w3c 制定 6.3.2 Sax 解析技术 sun 公司在 jdk5 版本对 dom 技术进行了升级：Sax（Simple api for xml） 使用了类似事件机制，边读取边解析。 6.3.4 dom4j 第三方解析 是对 jdom 的封装，jdom 是对 dom 的封装 使用： 引入 jar 包 创建 xml 对应的类（JavaBean） 创建一个 SAXReader 对象：SAXReader reader = new SAXReader() 通过 SAXReader 对象读取 xml 文件，并获取 document 对象：Document document = reader.read(url) 通过 document 对象，获取根元素对象（Element 类型）：Element root = document.getRootElement() 根元素对象的asXML()方法可以将 Element 对象转换为 String 通过根元素对象获取所有子元素集合：List&lt;Element&gt; books = root.elements() 单个子元素使用element()方法 可传入参数，传入参数时，参数名与子元素标签名一致。 循环遍历得到每个 book 对象的各类属性对象，仍然使用elements()或element()方法 使用getText方法得到元素的内容。 element()和getText()可以整合成一个elementText() 获取属性值：attributeValue() 1234567891011121314151617public class Dom4jParseXML &#123; public static void main(String[] args) throws DocumentException &#123; SAXReader saxReader = new SAXReader(); Document document = saxReader.read(&quot;file:\\\\D:\\\\实践练习\\\\07.JavaWeb\\\\parsexml\\\\src\\\\books.xml&quot;); Element rootElement = document.getRootElement(); List&lt;Element&gt; books = rootElement.elements(); for(Element book : books)&#123; String sn = book.attributeValue(&quot;sn&quot;); Element nameElement = book.element(&quot;name&quot;); String nameElementText = nameElement.getText(); Element priceElement = book.element(&quot;price&quot;); String priceElementText = priceElement.getText(); String authorElement = book.elementText(&quot;author&quot;); System.out.println(new Book(sn,nameElementText,Double.parseDouble(priceElementText),authorElement)); &#125; &#125;&#125; 6.3.5 pull 类似于 sax，利用事件机制，用于 Android 开发 7. HTTP 协议 7.1 Http 协议简述 HyperText Transfer Protocol：超文本传输协议 基于 TCP/IP 通信协议来传递数据 http 是无状态的，服务器无法判断两次请求时同一客户端发送，还是不同客户端发送。 通过会话跟踪技术解决无状态问题【session】 7.2 Http 请求 7.2.1 请求结构 由四个部分组成，分别为请求行、请求头、空行和请求体 上面的URL应该是URI？ 7.2.2 请求行 请求行是由请求方法、请求 URI（URI 全称为 Universal Resource Identifier，中文含义为“统一资源标志符”，是用来标识抽象或物理资源的字符串）和 HTTP 协议版本三个部分组成，每个部分使用空格分隔，在请求行的最后以回车符与换行符结尾。 8 种请求方法： 方法 描述 GET 请求指定 URI 所指向的资源，并返回 HEAD 与 GET 请求类似，但只获取由 URI 所指向资源的响应消息报头 POST 将数据提交到服务器（例如提交表单或者上传文件），数据被包含在请求体中 PUT 使用从客户端向服务器传送的数据替换指定文档的内容 DELETE 请求服务器删除 URI 所指向的目标资源 CONNECT HTTP/1.1 协议中预留的能够将连接改为管道方式的代理服务器 OPTIONS 允许客户端查看服务器的性能 TRACE 回显服务器收到的请求，主要用于测试或诊断 PATCH 是对 PUT 方法的补充，用来对已知资源进行局部更新 7.2.3 请求头 请求头是客户端传递给服务器的一系列有关本次请求和客户端本身的相关信息。请求头一般由头部字段名、冒号（:）、空格、值组成 7.2.4 空行 由回车符和换行符组成，无特殊含义，分隔请求行和请求体 7.2.5 请求体 一般出现在 POST 请中，表示提交给服务器的数据，以key=value&amp;key=value的形式传输 get 请求：没有请求体，但有一个 queryString 7.2.6 GET 请求 由请求行和请求头两部分组成 常见组成内容含义： GET 请求有哪些： form 标签 method=get a 标签 link 标签引入 css Script 标签引入 js 文件 img 标签引入图片 iframe 引入 html 页面 在浏览器地址栏中输入地址后敲回车 7.2.7 POST 请求 由请求行、请求头、请求体三部分组成 常见组成内容含义： POST 请求有哪些 form 标签 method=post 7.3 HTTP 响应 7.3.1 响应结构 由响应行（状态行）、响应头、空行和响应体组成 7.3.2 响应行 由 HTTP 协议版本、表示响应状态的状态码和形容这个状态词三部分组成 协议版本号，当前一般为HTTP1.1 响应状态： 状态词：OK、Not Modified、Found 等。 7.3.3 响应头 由头部字段名、冒号、空格和值组成。 7.3.4 空行 同请求空行 7.3.5 响应体 服务器根据客户端的请求返回给客户端的具体数据。 7.3.6 响应示例 7.3.7 常见响应状态码 200：请求成功。 302：请求重定向 404：服务器收到了请求，但客户端需要的资源不存在（请求地址有误） 500：服务器收到了请求，但服务器内部错误（代码错误） 7.4 MIME 类型 Multipurpose Internet Mail Extensions，多用途互联网邮件扩展类型”，它是一种标准，用来表示文档、文件或字节流的性质和格式。 设定在请求头的 Accept 中，格式如下： Accept: type/subtype [q=qvalue] [q=qvalue]是可选项，表示 MIME 优先顺序（权重），取值范围为 0~1 多个 MIME 使用;隔开 常见类型： 8. JSP 8.1 JSP 入门 8.1.1 JSP 介绍 JSP 是 Java Servlet Pages 的所系，是一种动态网页开发技术。 动态网页显式内容可以随时间、环境或数据库操作的结果而发生改变。 本质：JSP 本质是一个 Java 类（Servlet），可以在 JSP 页面嵌入 Java 代码。 运行过程：客户端请求 JSP 时，服务器内部会经历一次动态资源（JSP）到静态资源（HTML）的转化。服务器会自动把 JSP 中的 HTML 片段和数据拼接成静态资源响应给浏览器。也就是说，JSP 运行在服务器端，但最终发给客户端的是已经转换好的 HTML 静态页面。 组成：包括 Java Bean、自定义标签（Custom Tags）、EL 表达式（Expression Language）、JSTL 标准标签类库（Java Standard Tag Library）等。 JSP 与 Servlet 的异同： 相同点：都用于生成动态网页 不同点： 8.1.2 JSP 生命周期 编译：当浏览器请求 JSP 时，JSP 容器会首先检查是否需要编译页面。如果该页面从未被编译过，或者自上次编译以来对其进行了修改，则编译该页面。（JSP 容器就是 Servlet 容器——Tomcat） 解析 JSP：JSP 容器解析 JSP 文件，查看是否有语法错误 翻译 JSP：JSP 容器把 JSP 文件翻译为 Servlet 类 编译 Servlet 初始化：当容器加载 JSP 时，它将在处理任何请求之前调用 jspInit() 方法。一般情况下，程序只初始化一次。与 Servlet init 方法一样，我们通常在 jspInit() 方法中初始化数据库连接、打开文件。 执行：SP 页面完成初始化后，JSP 将会调用 _jspService() 方法。_jspService() 以 HttpServletRequest 对象和 HttpServletResponse 对象作为方法参数。 销毁：从容器中删除 JSP。jspDestroy() 方法等效于 Servlet 中的 destroy() 方法。通常在 jspDestroy() 方法中释放数据库连接、关闭打开的文件。 8.2 JSP 语句 8.2.1 JSP 脚本 含义：在 JSP 页面中，写入 Java 代码。 作用：在 jsp 页面中编写 java 功能 特点： 脚本翻译之后都在_jspService 方法中 在_jspService()方法中的现有对象都可以直接使用。 多个 jsp 脚本块组合完成一个完整的 java 语句。 和 jsp 表达式组合使用，在 jsp 中数据数据。 JSP 脚本可以包含任意数量的 Java 语句，变量、方法和表达式。JSP 脚本会把包含的内容插入到 Servlet 的 service() 方法中。 语法：&lt;% Java语句 %&gt; 等价于&lt;jsp:script&gt; Java语句 &lt;/jsp:script&gt; 任何文本、HTML 标签和 JSP 元素（声明，表达式等）都必须在脚本程序之外。 8.2.2 JSP 声明 作用：用于声明一个或多个变量、方法，以供后面的 Java 代码使用。 声明属性 声明 static 代码块 声明方法 声明内部类 语法：&lt;%! Java语句 %&gt; 等价于&lt;jsp:declaration&gt; Java语句 &lt;/jsp:declaration&gt; 与 JSP 脚本的区别： JSP 脚本只能声明变量，不能声明方法。JSP 声明语句可以声明变量和方法。 JSP 脚本声明的内容会插入到 Servlet 的 service()方法中，而 Java 中方法不允许嵌套方法 JSP 声明声明的内容会添加到 Servlet 类中，是类的成员。 8.2.3 JSP 表达式 作用：把变量或者表达式输出到网页上，不需要 out.print() 就能输出数据。通常用于打印变量和方法的值。 语法：&lt;%= Java语句 %&gt; 等价于&lt;jsp:expression&gt; Java语句 &lt;/jsp:expression&gt; 特点： 都会被翻译到_jspService() 方法中 都会被翻译成为 out.print()输出到页面上 _jspService()方法中的对象都可以直接使用。 表达式不能以分号结束。 8.2.4 JSP 注释 JSP 页面中，可以有四种注释。 HTML 注释：&lt;-- 注释内容 --&gt;。 会被翻译到 java 源代码中。在_jspService 方法里，以 out.writer 输出到客户端。 客户端访问看不到，客户端检查源码可以看到 HTM 注释中嵌套 JSP 表达式：&lt;-- 注释内容 &lt;%= java语句 %&gt; --&gt; 客户端访问看不到，客户端检查源码可以看到翻译后的 JSP 表达式 隐藏注释：&lt;%-- 注释内容 --%&gt; 客户端访问看不到，客户端检查源码也看不到 脚本程序（Scriptlet）中的注释：客户端访问看不到，客户端检查源码也看不到 单行注释：// 多行注释：/* */ 文档注释：/** */ 8.3 JSP 指令 作用：告诉 Web 服务器如何处理 JSP 页面的请求和响应。服务器会根据 JSP 指令来编译 JSP，生成 Java 文件。JSP 指令不产生任何可见输出，在生成的 Java 文件中，不存在 JSP 指令。 语法：&lt;%@ directive attribute = &quot;value&quot; [attribute2 = &quot;value2&quot; ...]%&gt; 分类： 8.3.1 JSP page 指令 作用：定义当前页面的相关属性。可以在 JSP 页面的任意位置编写，通常放在页面的顶部 语法：&lt;%@ page attribute = &quot;value&quot; %&gt; 常用属性： import 可以声明多个，其他只能出现一个 8.3.2 JSP include 指令 作用：在 JSP 页面引入其它内容，可以是 JSP 文件、html 文件和文本文件等，相当于把文件的内容复制到 JSP 页面。引入的文件和 JSP 页面同时编译运行。 语法：&lt;%@ include file=&quot;URL&quot; %&gt; ，url 代表相对路径，必须以/开头 优点： 增加代码的可重用性 使 JSP 页面的代码结构清晰易懂 维护简单 特点：静态包含的页面会出现在包含它的 jsp 页面翻译后的 servlet 程序中 out.write() 8.3.3 JSP taglib 指令 作用：声明并引入标签库。Java API 允许在页面中自定义标签，标签库就是自定义标签的集合。 语法：&lt;%@ taglib uri=&quot;tagliburl&quot; prefix=&quot;tagPre&quot; %&gt; uri 指定自定义标签库的存放位置；prefix 指定标签库的前缀。 8.4 JSP 动作 未学 8.5 JSP 九大内置对象 8.5.1 九大内置对象介绍 作用：简化页面开发过程。 特点：JSP 内置对象又称为隐式对象，它们由容器实现和管理。在 JSP 页面中，这些内置对象不需要预先声明，也不需要进行实例化，可以直接在脚本和表达式中使用。 Tomcat 将 jsp 页面翻译成 Servlet 源码后，内部提供 不能在 JSP 声明中使用 分类：含 4 大域对象 四大域对象：pageContext、request（对应 servlet 的 request 域对象）、session（对应 servlet 的 session 域对象）、以及 application（对应 servlet 的 application 域对象） 操作域对象的 3 个方法： 4 个域对象的不同点： 使用原则：从小到大。page 到 application。对于内存的优化 8.5.2 pageContext 对象 javax.servlet.jsp.PageContext 的实例对象。 本质是 Servlet 的 ServletContext 常用方法： 8.5.3 request 对象 javax.servlet.http.HttpServletRequest 的实例对象 常用方法： getScheme()：获取请求协议 getServerName()：获取服务器 IP 地址 getServerPort()：获取服务器端口号 getContextPath()：获取工程路径（名） 8.5.4 session 对象 javax.servlet.http.HttpSession 的实例对象，主要用来访问用户数据，记录客户的连接信息。 HTTP 协议是一种无状态的协议（即不保存连接状态的协议）。每次用户向服务器发出请求，且服务器接收请求并返回响应后，该连接就被关闭了，服务器端与客户端的连接被断开。此时，服务器端不保留连接的有关信息，要想记住客户的连接信息，就用到了 session 对象。 常用方法： 8.5.5 application 对象 javax.servlet.ServletContext 的实例对象。一般用于保存应用程序的公用数据。非常占用服务器资源，实际开发中一般不用。 application 是 pageContext 的一种，获取 pageContext 的参数可以获得 application\\session\\request\\page 范围的所有参数；而获取 application 参数则不能获取其他三个范围的参数。 常用方法： 8.5.6 config 对象 javax.servlet.ServletConfig 的实例对象，一般用于获取页面和 Servlet 的初始化参数。 常用方法： 8.5.7 response 对象 javax.servlet.http.HttpServletResponse 的实例对象 常用方法： 8.5.8 out 对象 javax.servlet.jsp.JspWriter 的实例，常用于输出内容到 HTML 中 常用方法： out.write()和 resopnse.getWrite().write()对象的区别 两个方法都有自己的缓冲区 jsp 页面中输出内容时即使不用 write()，jsp 页面翻译成 servlet 程序时，都是用的是 out.write() 输出是 Tomcat 服务器默认输出 response 缓冲区的内容，而 out 缓冲区的内容会追写到 response 的缓冲区中。【表现：总是 response 输出的内容先输出】 jsp 页面的内容执行完后，会默认执行out.flush()，将 out 缓冲区内容追写到 response 缓缓从区中。 out.write()和out.print() print()方法输出数据时，都会转为 String 类型 write()方法输出数据时，如果为非 String 类型，则有乱码问题（如 int 类型，输出的是整数型对应的 ascii 码） 缓冲区使用字符数组存放数据 8.5.9 page 对象 实质是 java.lang.Object 对象相当于 Java 中的 this 关键字。page 对象是指当前的 JSP 页面本身，在实际开发中并不常用。 常用方法： 8.5.10 exception 对象 8.6 JSP 标签 8.6.1 动态包含 语法：&lt;jsp:include page=&quot;url&quot;&gt;&lt;/js:include&gt; 特点：动态包含的页面会出现在包含它的 jsp 页面翻译后的 servlet 程序中 出现形式：JspRuntimeLibrary.include(request, response, &quot;url&quot;, out, false) 底层原理： 被包含的页面 out 对象是父页面传过去的，是同一个对象，所以共享缓冲区。 传递参数： 父页面中，使用 request 的 getParameter(“str”)可以获取属性值。 两个页面 request 是同一个 8.6.2 转发 语法：&lt;jsp:forward page=&quot;url&quot;&gt;&lt;/jsp:fowrard&gt; 作用：请求转发 8.6.3 自定义标签 没学会 8.7 EL 表达式 8.7.1 简介 Excepssion Language，简化 jsp 页面，主要用于代替 jsp 输出页面的数据【主要是域对象的数据】 语法：$&#123;&#125; 等同于&lt;%= %&gt; 不同：EL 表达式在输出 null 值时，输出为空串，jsp 表达式输出时，输出 null【用户体验不好】 8.7.2 运算 关系运算 逻辑运算 算数运算 条件表达式：$&#123; 条件表达式? true : false &#125; 其他运算符 empty：判断对象或变量是否为 null .和[]：访问 JavaBean 中的属性和隐式对象的数据。 .访问 JavaBean 属性【实际上调用的是属性的 geteer 或 isValue()，如果未设置会报错】或 Map 类型的值 []用来访问数组或者列表的元素。 8.7.3 内置对象 8.7.4 pageContext 域对象的使用 EL 域对象调用属性，其实是调用的该属性的 getter，如果是布尔值，则调用的是 isValue() 8.7.5 禁用 EL 表达式 禁用单个表达式：\\$&#123;2+3&#125;页面会输出$&#123;2+3 当前页面禁用：配置 page 指令中的isELIgnored属性 &lt;%@ page isELIgnored = &quot;true&quot; %&gt; 整个 web 禁用： 8.8 文件传输 8.8.0 Commons-FileUpload 组件 API ServletFileUpload 类：解析文件上传的数据 方 法 说 明 public void setSizeMax(long sizeMax) 设置上传文件总量的最大值 (包含文件和表单数据) public List parseRequest(HttpServletRequest req) 解析 form 表单提交的数据，返回一个 FileItem 实例的集合 public static final boolean isMultipartContent(HttpServletRequest req) 判断请求信息中的内容是否是”multipart/form-data“类型，是则返回 true，否则返回 false。 public void setHeaderEncoding(String encoding) 设置转换时所使用的字符集编码 FileItemFactory 接口与实现类：创建 ServletFileUpload 实例，DiskFileItemFactory 是 FileItemFactory 接口的实现类 方 法 说 明 public void setSizeThreshold(int sizeThreshold) 设置内存缓冲区的大小 public void setRepository(String path) 设置临时文件存放的目录 FileItem 接口：一个表单字段对应一个 FileItem 实例，其实现类之一是 DiskFileItem 方 法 说 明 public boolean isFormField() 用于判断 FileItem 类对象封装的数据是一个普通文本表单字段，还是一个文件表单字段，如果是普通表单字段则返回 true，否则返回 false。因此，可以使用该方法判断是否为普通表单域，还是文件上传表单域。 public String getName() 获取文件上传的文件名 public String getFieldName() 返回表单字段元素的 name 属性值 public long getSize() 获取上传文件的大小 public String getString() 将 FileItem 对象中保存的主体内容以一个字符串返回。其重载方法 public String getString(String encoding) 中的参数用指定的字符集编码方式 public void write() 将 FileItem 对象中保存的主体内容保存到指定的文件中。 8.8.1 文件上传表单设计 &lt;form&gt;标签属性为method=&quot;post&quot;、enctype=&quot;multipart/form-data&quot; get 方法也能传，但是长度有限，太短了不够用。 设置了 enctype=&quot;multipart/form-data&quot;后，提交给服务器的数据变为二进制流的形式，使用request.getParameter不能获取属性值【为 null】 &lt;input&gt;标签属性为type=&quot;file&quot; 8.8.2 文件上传请求体介绍 8.8.3 使用第三方包完成上传 导入文件上传包和 io 包： 创建表单 JSP 文件和文件上传 Servlet 类 123456789101112131415161718192021222324252627282930protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException,IOException &#123; //1 先判断上传的数据是否多段数据（只有是多段的数据，才是文件上传的） if (ServletFileUpload.isMultipartContent(req)) &#123; // 创建 FileItemFactory 工厂实现类 FileItemFactory fileItemFactory = new DiskFileItemFactory(); // 创建用于解析上传数据的工具类 ServletFileUpload 类 ServletFileUpload servletFileUpload = new ServletFileUpload(fileItemFactory); try &#123; // 解析上传的数据，得到每一个表单项 FileItem List&lt;FileItem&gt; list = servletFileUpload.parseRequest(req); // 循环判断，每一个表单项，是普通类型，还是上传的文件 for (FileItem fileItem : list) &#123; if (fileItem.isFormField()) &#123; // 普通表单项 System.out.println(&quot;表单项的 name 属性值：&quot; + fileItem.getFieldName()); // 参数 UTF-8.解决乱码问题 System.out.println(&quot;表单项的 value 属性值：&quot; + fileItem.getString(&quot;UTF-8&quot;)); &#125; else &#123; // 上传的文件 System.out.println(&quot;表单项的 name 属性值：&quot; + fileItem.getFieldName()); System.out.println(&quot;上传的文件名：&quot; + fileItem.getName()); fileItem.write(new File(&quot;e:\\\\&quot; + fileItem.getName())); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 8.8.4 使用第三方包完成下载 response.getOutputStream(); servletContext.getResourceAsStream(); servletContext.getMimeType()：获取要下载的文件类型 response.setContentType()：设置返回客户端的数据类型 1234567891011121314151617181920212223242526272829303132protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; // 1、获取要下载的文件名 String downloadFileName = &quot;2.jpg&quot;; // 2、读取要下载的文件内容 (通过ServletContext对象可以读取) ServletContext servletContext = getServletContext(); // 获取要下载的文件类型 String mimeType = servletContext.getMimeType(&quot;/file/&quot; + downloadFileName); System.out.println(&quot;下载的文件类型：&quot; + mimeType); // 4、在回传前，通过响应头告诉客户端返回的数据类型 resp.setContentType(mimeType); // 5、还要告诉客户端收到的数据是用于下载使用（还是使用响应头） // Content-Disposition响应头，表示收到的数据怎么处理 // attachment表示附件，表示下载使用 // filename= 表示指定下载的文件名 // url编码是把汉字转换成为%xx%xx的格式 if (req.getHeader(&quot;User-Agent&quot;).contains(&quot;Firefox&quot;)) &#123; // 如果是火狐浏览器使用Base64编码 resp.setHeader(&quot;Content-Disposition&quot;, &quot;attachment; filename==?UTF-8?B?&quot; + new BASE64Encoder().encode(&quot;中国.jpg&quot;.getBytes(&quot;UTF-8&quot;)) + &quot;?=&quot;); &#125; else &#123; // 如果不是火狐，是IE或谷歌，使用URL编码操作 resp.setHeader(&quot;Content-Disposition&quot;, &quot;attachment; filename=&quot; + URLEncoder.encode(&quot;中国.jpg&quot;, &quot;UTF-8&quot;)); &#125; /** * /斜杠被服务器解析表示地址为http://ip:prot/工程名/ 映射 到代码的Web目录 */ InputStream resourceAsStream = servletContext.getResourceAsStream(&quot;/file/&quot; + downloadFileName); // 获取响应的输出流 OutputStream outputStream = resp.getOutputStream(); // 3、把下载的文件内容回传给客户端 // 读取输入流中全部的数据，复制给输出流，输出给客户端 IOUtils.copy(resourceAsStream, outputStream);&#125; 9. JSTL 9.1 JSTL 简介 9.1.1 JSTL 介绍 JSP Standard Tag Library，核心标签库，替换代码脚本，优化 JSP 页面编写 使用：使用 JSTL 需要引入 JSTL 的 JAR 包和标签库描述符文件（扩展名为 .tld），标签库描述符文件内包含标签库中所有标签的定义、标签名、功能类及各种属性。 taglibs-standard-impl-1.2.1.jar和taglibs-standard-spec-1.2.1.jar JSTL 1.2 及之后版本只需要引入 jstl 包？ 地址： 组成： 9.1.2 使用 导入 taglibs-standard-impl-1.2.1.jar 和 taglibs-standard-spec-1.2.1.jar JSTL 的两个库文件只支持 Servlet4.0，Tomcat10 创建的 Servlet 程序为 5.0 版本，所以需要单独导入 servlet4.0 的 jar 包 使用 JSP taglib 指令引入使用的标签库地址 CORE 标签库：&lt;%@ taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/core&quot; %&gt; FMT 标签库：&lt;%@ taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/fmt&quot; %&gt; FUNCTIONS 标签库：&lt;%@ taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/functions&quot; % &gt; SQL 标签库：&lt;%@ taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/sql&quot; %&gt; XML 标签库：&lt;%@ taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/xml&quot; %&gt; 9.2 各库介绍 注：下文中表达式指 EL 表达式。 9.2.1 CORE 核心库 &lt;c:set var=&quot;varname&quot; value=&quot;表达式&quot; [scope=&quot;request|page|session|application&quot;]&gt;表达式&lt;/c:set&gt;： 等价于：&lt;c:set var=&quot;varname&quot; value=&quot;表达式&quot; [scope=&quot;request|page|session|application&quot;]/&gt; var：定义变量或属性名称 value：变量或属性值 scope：可选项，表示属性的作用域，默认为 page &lt;c:if test=&quot;判断条件&quot; [var=&quot;varname&quot;] [scope=&quot;request|page|session|application&quot;]&gt; 代码块 &lt;/c:if&gt; test：指定判断条件，返回值为 boolean var：可选项，判断条件的执行结果 scope：可选项，执行结果的作用域 &lt;c:choose&gt;&lt;c:when&gt;和&lt;c:otherwise&gt;标签 嵌套判断时，&lt;c:when&gt;的直接父标签只能是&lt;c:choose&gt;，且必须使用。 不存在穿透问题。 多个判断语句同时为 true 时，只会执行第一个判断为 true 的语句（同 java 只有一个入口） &lt;c:forEach&gt; items：要被循环的信息，可以是数组、Java 集合等； var：可选项，指定迭代之的别名； varStatus：可选项，当前迭代的状态信息； varStatus 可调用的方法，根据 EL 表达式的.运算符，可以直接写属性 begin：可选项，迭代开始的元素，起始下标为 0； end：可选项，迭代结束的元素； step：可选项，迭代的步长； 9.2.2 FMT 格式化库 9.2.3 FUNCTIONS 函数库 9.2.4 SQL 数据库 10. BeanUtils 依赖 https://commons.apache.org/proper/commons-beanutils/（引入） https://commons.apache.org/proper/commons-collections/（引入） https://commons.apache.org/proper/commons-logging/（不知道高版本还需要不需要引入） 注入原理：调用 JavaBean 的 seteer 作用：将参数注入到 JavaBean 的对象中，方便创建对象 11. MVC 11.1 MVC 的概念 MVC 全称：Model 模型、 View 视图、 Controller 控制器。 MVC 最早出现在 JavaEE 三层中的 Web 层，它可以有效的指导 Web 层的代码如何有效分离，单独工作。 Model 模型：将与业务逻辑相关的数据封装为具体的 JavaBean 类，其中不掺杂任何与数据处理相关的代码—— JavaBean/domain/entity/pojo。 View 视图：只负责数据和界面的显示，不接受任何与显示数据无关的代码，便于程序员和美工的分工合作—— JSP/HTML。 Controller 控制器：只负责接收请求，调用业务层的代码处理请求，然后派发页面，是一个“调度者”的角色——Servlet。 转到某个页面。或者是重定向到某个页面。 12. Cookie 12.1 Cookie 概述 12.1.1 Cookie 概念 服务器通知客户端保存键值对的一种技术。web 应用程序利用 Cookie 在客户端缓存服务器端的文件。 Cookie 时由服务器发送给客户端的，客户端保存时注明了 Cookie 的来源。 客户端再次访问服务器时，客户端的 Cookie 会保存在请求协议中，服务器端可以获取上次存储的缓存文件内容。 12.1.2 用途 管理浏览网站的人数（其中包含有多少人访问过，多少人是新用户等） 电子商城中购物车功能（每买一样商品，保存一个 Cookie） 用户自动登录功能（第一次登录时，将用户名和密码存储在 Cookie） 12.1.3 缺点 多人共用一台计算机（例如导致用户名和密码不安全等问题）。 Cookie 被删除时，利用 Cookie 统计用户数量出现偏差。 一人使用多台计算机（网站会将看成多个用户等问题） Cookie 会被附加在每次 Http 请求协议中，增加流量。 Cookie 使用明文（未加密）传递的，安全性低。 Cookie 的大小限制在 4KB 左右，无法存储复杂需求。 12.1.4 Cookie 规范 Cookie 存储的大小上限为 4KB。 一个服务器最多在客户端浏览器中可以保存 20 个 Cookie。 一个浏览器最多可以保存 300 个 Cookie。 不同浏览器之间不能共享 Cookiewarning 注意点：Cookie 规范时 Http 协议提供的，浏览器厂商对该 Cookie 规范进行了一些“扩展”。 12.2 使用 Cookie 12.2.1 服务器端创建 Cookie 创建 Cookie 对象：Cookie cookie= new Cookie(&quot;name&quot;, &quot;value&quot;); 发送给客户端保存：response.addCookie(cookie); 12.2.2 服务器端获取 Cookie 浏览器是通过请求头将 Cookie 发送给服务器的，所以可以使用获取请求头的方法： request.getCokies()：返回 Cookie 的对象数组 遍历 Cookie 数组，使用 equals 方法比较名字是否一致 获取 Cookie 的工具类： 12345678910111213141516171819public class CookieUtils &#123; /** * 查找指定名称的 Cookie 对象 * @param name * @param cookies * @return */ public static Cookie findCookie(String name , Cookie[] cookies)&#123; if (name == null || cookies == null || cookies.length == 0) &#123; return null; &#125; for (Cookie cookie : cookies) &#123; if (name.equals(cookie.getName())) &#123;//上面判断的为空的情况，所以不会出现空指针异常 return cookie; &#125; &#125; return null; &#125;&#125; 12.2.3 修改 Cookie 的值 方式一：创建同名的新 cookie 进行覆盖 创建 Cookie 对象：Cookie cookie= new Cookie(&quot;name&quot;, &quot;value&quot;); 发送给客户端保存：response.addCookie(cookie); 方式二：查找到指定的 Cookie，使用setValue()方法 查找指定 Cookie（使用工具类）：Cookie cookie = CookieUtils.findCookie(&quot;key&quot;,request.getCookies()); 发送给客户端保存：response.addCookie(cookie); cookie 不能保存空格、方括号、圆括号、等号、逗号、双引号、斜杠、问号、at 符号、冒号和分号以及汉字，同时空值在不同浏览器上表现页不一定一样。 要保存上述类型的值，可以使用 BSE64 编码对上述文本进行处置。 12.2.4 Cookie 的生命周期控制 使用 cookie 的setMaxAge(num)可以设置 cookie 如何过期： 正数，表示在指定的秒数后过期 负数，表示浏览器一关，Cookie 就会被删除（默认值是-1） 零，表示马上删除 Cookie 12.2.5 Cookie 有效 Path 的设置 setPath(&quot;路径&quot;) 13. Session 13.1 Session 概述 13.1.1 Session 会话介绍 Session 就一个接口（HttpSession）。 Session 就是会话。它是用来维护一个客户端和服务器之间关联的一种技术。 每个客户端都有自己的一个 Session 会话。 Session 会话中，我们经常用来保存用户登录之后的信息。 13.2 Session 的使用 13.2.1 Session 的创建、获取、判断、id 创建和获取 Session，使用的方法（API）是一样的：request.getSession() 第一次调用该方法：创建 Session 会话 后续掉调用该方法：获取已经存在的 Session 判断是否是新创建的 Session：session.isNew() true：标识新创建的 false：标识之前已经存在的 获取 sessionID：session.getId() 每个 session 都有一个 id，且唯一存在 13.2.2 Session 域数据存取 存储：调用 session 对象的setAttribute(&quot;name&quot;, &quot;value&quot;) 获取：调用 session 对象的getAttribute(&quot;name&quot;, &quot;value&quot;) 13.3.3 Session 生命周期控制 获取存活时间（超时后销毁）：session.getMaxInactiveInterval()，返回整数 s。 Tomcat 服务器中进行了默认配置：1800s（30min） 超时指的是客户端两次请求的最大间隔时长，否则请求一次会创建一次，时间重新开始算。 设置存活时间：session.setMaxInactiveInterval(int interval)，超过指定秒数，就销毁该 session 设置马上失效：session.invalidate() 13.3 应用 13.3.1 登录验证码 原理： 第一次访问表单时，给表单生成一个随机验证码，并保存到 session 域中 提交表单后，将 session 提交给服务器，服务器获得该验证码，同时服务器清空 sessionu 域（移除该 session 的值） 后续点击表单时，再随机生成验证码保存到 session 域中，提交表单后与服务器上的进行比较，是否相等（字符串内容） 谷歌验证码的使用（kaptcha）： 导入谷歌验证码的 jar 包 kaptcha-2.3.2.ja 在 web.xml 中去配置用于生成验证码的 Servlet 程 在表单中使用 img 标签去显示验证码图片并使用它 在服务器获取谷歌生成的验证码和客户端发送过来的验证码比较使用。 14. Filter 过滤器 14.1 Filter 概述 14.1.1 Filter 介绍 JavaWeb 的三大组件之一。三大组件分别是：Servlet 程序、Listener 监听器、Filter 过滤器 是 JavaEE 的规范。也就是接口，**javax.servlet.Filter** Filter 过滤器它的作用是：拦截请求，过滤响应 。对 Servlet 容器传给 web 资源的 request 对象和 response 对象进行检查和修改 常见拦截请求场景： 权限检查 日记操作 事务管理 特点： Filter 不能直接访问，本身也不能生成 request 和 response 对象。 在 Web 资源被访问前，检查 request 对象，修改请求头和请求正文，或对请求进行预处理操作。 将请求传递到下一个过滤器或目标资源。 在 Web 资源被访问后，检查 response 对象，修改响应头和响应正文。 14.1.2 Filter 的使用 编写类，实现 Filter 接口 实现过滤方法 doFilter() 在web.xml中注册 Filter 拦截路径 14.1.3 xml 配置 语法结构： 123456789101112131415161718192021222324&lt;filter&gt; &lt;filter-name&gt;myFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.baidu.www.MyFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;name&lt;/param-name&gt; &lt;param-value&gt;123&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;URL&lt;/param-name&gt; &lt;param-value&gt;www.baidu.com&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;myFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/login&lt;/url-pattern&gt; &lt;dispatcher&gt;REQUEST&lt;/dispatcher&gt; &lt;dispatcher&gt;FORWARD&lt;/dispatcher&gt;&lt;/filter-mapping&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;myFilter&lt;/filter-name&gt; &lt;servlet-name&gt;ServletDemo&lt;/servlet-name&gt;&lt;/filter-mapping&gt; 注意点： &lt;init-param&gt;标签内配置 filter 过滤器的初始化参数，可以配置多个 dispatcher指定给 filter 拦截的资源被 Servlet 容器调用的方式【Filter 什么时候工作】，默认值为REQUEST，可以设置多个。 REQUERST：用户直接访问地址时，容器调用过滤器 INCLUDE：通过请求转发RequestDispatcher的include()方法时调用 FORWARD通过请求转发RequestDispatcher的forward()方法时调用 ERROR：通过声明式异常处理机制访问，该过滤器被调用。 匹配方式：&lt;url-pattern&gt;的值： 精确匹配：如&lt;url-pattern&gt;/login.&lt;/url-pattern&gt; 目录匹配：如&lt;url-pattern&gt;/admin/*&lt;/url-pattern&gt; 后缀名匹配：如&lt;url-pattern&gt;*.html&lt;/url-pattern&gt; 14.1.4 注解配置 属性名 类型 描述 filterName String 指定过滤器的 name 属性，等价于 。 urlPatterns String[] 指定过滤器的 URL 匹配模式。等价于 标签。 value String[] 该属性等价于 urlPatterns 属性，但是两者不能同时使用。 servletNames String[] 指定过滤器将应用于哪些 Servlet。取值是 @WebServlet 中 filterName 属性的取值，或者 web.xml 中 的取值。 dispatcherTypes DispatcherType 指定过滤器拦截的资源被 Servlet 容器调用的方式。具体取值包括： ASYNC、ERROR、FORWARD、INCLUDE、REQUEST。 initParams WebInitParam[] 指定一组过滤器初始化参数，等价于 标签。 asyncSupported boolean 声明过滤器是否支持异步操作模式，等价于 标签。 description String 指定过滤器的描述信息，等价于 标签。 displayName String 指定过滤器的显示名，等价于 标签。 value、urlPatterns、servletNames 三者必需至少包含一个，且 value 和 urlPatterns 不能共存，如果同时指定，通常忽略 value 的取值。 其余均为可选项 通过@WebFilter 注解注册的 Filter，其加载顺序与执行顺序无关 通过@WebFilter 注解注册的 Filter，其加载顺序与注解的 filterName 值相关（底层通过 HashMap 存储，key 值即 filterName 值） 通过@WebFilter 注解注册的 Filter，其执行顺序与类名有关，按照类名的字典顺序执行 14.2 Filter 的生命周期 14.2.1 Filter 工作流程 客户端请求访问容器内的 Web 资源。 Servlet 容器接收请求，并针对本次请求分别创建一个 request 对象和 response 对象。 请求到达 Web 资源之前，先调用 Filter 的 doFilter() 方法，检查 request 对象，修改请求头和请求正文，或对请求进行预处理操作。 在 Filter 的 doFilter() 方法内，调用 FilterChain.doFilter() 方法，将请求传递给下一个过滤器或目标资源。 目标资源生成响应信息返回客户端之前，处理控制权会再次回到 Filter 的 doFilter() 方法，执行 FilterChain.doFilter() 后的语句，检查 response 对象，修改响应头和响应正文。 响应信息返回客户端。 14.2.2 Filter 生命周期阶段 初始化阶段： Servlet 容器负责加载和实例化 Filter。容器启动时，读取 web.xml 或 @WebFilter 的配置信息对所有的过滤器进行加载和实例化。 加载和实例化完成后，Servlet 容器调用 init() 方法初始化 Filter 实例。 在 Filter 的生命周期内，初始化阶段只执行一次。 执行 Filter 构造器方法和 init 方法 拦截和过滤阶段： 客户端请求的 URL 与过滤器映射匹配时，容器将该请求的 request 对象、response 对象以及 FilterChain 对象以参数的形式传递给 Filter 的 doFilter() 方法，并调用该方法对请求/响应进行拦截和过滤。 拦截一次执行一次。 执行doFilter()方法 销毁阶段： Filter 对象创建后会驻留在内存中，直到容器关闭或应用被移除时销毁。 销毁 Filter 对象之前，容器会先调用 destory() 方法，释放过滤器占用的资源。 在 Filter 的生命周期内，destory() 只执行一次。 14.3 FilterConfig 类 Tomcat 创建 Filter 类时，同时会创建一个 FilterConig 类，包含有 Filter 配置文件的配置信息 作用：获取 filter 过滤器的配置内容 获取 Filter 的名称——&lt;filter-name&gt;的内容 获取 Filter 中配置的&lt;init-param&gt;参数 获取 ServletContext 对象 返回值类型 方法 描述 String getInitParameter(String name) 根据初始化参数名 name，返回对应的初始化参数值。 Enumeration getInitParameterNames() 返回过滤器的所有初始化参数名的枚举集合。 ServletContext getServletContext() 返回 Servlet 上下文对象的引用。 String getFilterName() 返回过滤器的名称。 14.4 FilterChain——过滤器链 14.4.1 过滤器链概念 多个 Filter 都拦截同一目标资源，则它们就组成了一个 Filter 链（也称过滤器链）。 过滤器链中的每个过滤器负责特定的操作和任务，客户端的请求在这些过滤器之间传递，直到传递给目标资源。 **javax.servlet** 14.4.2 Filter 链拦截过程 14.4.3 Filter 链中 Filter 中的执行顺序 通过 web.xml 配置的 Filter 过滤器，执行顺序由 标签的配置顺序决定。 靠前，则 Filter 先执行，靠后则后执行。 通过 @WebFilter 注解配置的 Filter 过滤器，无法进行排序。 默认情况下，一个 Filter 链中的 Filter 只有一个线程——在同一个线程中 默认情况下，一个 Filter 链中的 Filter 只有一个 request 对象和一个 response 对象——即共享 request 域对象 15. ThreadLocal 作用：解决多线程的数据安全问题——事务操作的线程安全问题。称为本地线程。 特点： 可以给当前线程关联一个数据（可以是普通变量、对象、数组、集合等） 每个 ThreadLocal 对象，只能为当前线程关联一个数据，需要关联多个线程，则需要多个 ThreadLocal 对象实例。 每个 ThreadLocal 对象实例定义后，一般为 static 类型 ThreadLocal 对象实例销毁后，保存的数据由 JVM 虚拟机自动释放 set(obj)：在当前线程存储数据 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); //获取当前的线程 ThreadLocalMap map = getMap(t); //每一个线程都维护各自的一个容器（ThreadLocalMap） if (map != null) map.set(this, value); //这里的key对应的是ThreadLocal，因为我们的组件中需要传输（共享）的对象可能会有多个（不止Connection） else createMap(t, value); //默认情况下map是没有初始化的，那么第一次往其中添加数据时，会去初始化&#125; get()：在当前线程上获取数据 12345678910111213public T get() &#123; Thread t = Thread.currentThread(); //获取当前的线程 ThreadLocalMap map = getMap(t); //获取和这个线程（企业）相关的ThreadLocalMap（也就是工作纽带的集合） if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); //this指的是ThreadLocal对象，通过它才能知道是哪一个工作纽带 if (e != null) &#123; @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; //entry.value就可以获取到工具箱了 return result; &#125; &#125; return setInitialValue();&#125; 16. JSON 16.1 JSON 概述 JSON (JavaScript Object Notation) 是一种轻量级的数据交换格式。易于人阅读和编写。同时也易于机器解析和生成。JSON 采用完全独立于语言的文本格式，而且很多语言都提供了对 json 的支持（包括 C, C++, C#, Java, JavaScript, Perl, Python 等）。 这样就使得 JSON 成为理想的数据交换格式。 数据交换：客户端和服务器之间的业务数据传递 数据格式：json 是由键值对组成，并且由花括号（大括号）包围。每个键由引号引起来，键和值之间使用冒号进行分隔， 多组键值对之间进行逗号进行分隔。 保存形式： 对象形式：json 对象——常用于 json 数据操作 字符串形式：json 字符串——常用于数据交换 JSON 常用方法： JSON.stringify(xxx)：对象转为字符串 JSON.parse(xxx)字符串转为对象 16.2 操作 JSON 16.2.1 客户端操作 JSON 浏览器操作 JSON 由 JavaScript 完成。 操作方式：json对象.key或json对象[key] 16.2.1 服务器端操作 JSON jar 包：gson-2.2.4.jar JavaBean 和 json 相互转换： 创建 Gson 对象：Gson gson = new Gson(); JavaBean 转换为 json 字符串：调用gson.toJson(JavaBean对象); json 字符串转换为 JavaBean：调用gson.formJson(json字符串, JavaBean.class); List 和 json 相互转换 创建 Gson 对象：Gson gson = new Gson(); List 转换为 json 字符串：调用gson.toJson(List对象); json 字符串转换为 List：调用gson.formJson(json字符串, List.class);——得到的是一个 注意：此时得到的数据类型为LinkedTreeMap，而不是 List 中保存的对象类型 gson.formJson(json字符串, new PersonListType().getType());此时得到才是 List 类型，使用get(n)方法可以获取到具体的对象 PersonListType 类的定义：class PersonListType extends TypeToken&lt;List&lt;Person&gt;&gt;&#123;&#125;——反射机制 匿名内部类省去创建 PersonListType 类：gson.formJson(json字符串, new TypeToken&lt;ArrayList&lt;Integer, Person&gt;&gt;()&#123;&#125;.getType()) List 和 json 相互转换 创建 Gson 对象：Gson gson = new Gson(); Map 转换为 json 字符串：调用gson.toJson(Map对象); json 字符串转换为 Map：调用gson.formJson(json字符串, Map.class);——得到的是一个 注意：此时得到的数据类型为LinkedTreeMap，而不是 List 中保存的对象类型 gson.formJson(json字符串, new PersonListType().getType());此时得到才是 List 类型，使用get(n)方法可以获取到具体的对象 PersonListType 类的定义：class PersonListType extends TypeToken&lt;List&lt;Person&gt;&gt;&#123;&#125;——反射机制 匿名内部类省去创建 PersonListType 类：gson.formJson(json字符串, new TypeToken&lt;HashMap&lt;Integer, Person&gt;&gt;()&#123;&#125;.getType()) 举例： 12345678910111213141516171819public void test3()&#123; Map&lt;Integer,Person&gt; personMap = new HashMap&lt;&gt;(); personMap.put(1, new Person(1, &quot;国哥好帅&quot;)); personMap.put(2, new Person(2, &quot;康师傅也好帅&quot;)); Gson gson = new Gson(); // 把map集合转换成为json 字符串 String personMapJsonString = gson.toJson(personMap); System.out.println(personMapJsonString); //Map&lt;Integer,Person&gt; personMap2 = gson.fromJson(personMapJsonString, new PersonMapType().getType()); Map&lt;Integer,Person&gt; personMap2 = gson.fromJson(personMapJsonString, new TypeToken&lt;HashMap&lt;Integer,Person&gt;&gt;()&#123;&#125;.getType()); System.out.println(personMap2); Person p = personMap2.get(1); System.out.println(p);&#125; 17. Ajax 普通请求：后端处理完成后返回页面，浏览器使用使用页面替换整个窗口中的内容 Ajax 请求：后端处理完成后通常返回 JSON 数据， jQuery 代码使用 JSON 数据对页面局部更新 17.1 Ajax 概述 AJAX 即“Asynchronous Javascript And XML”（异步 JavaScript 和 XML），是指一种创建交互式网页应用的网页开发技术。 ajax 是一种浏览器通过 js 异步发起请求，局部更新页面的技术。 Ajax 请求的局部更新，浏览器地址栏不会发生变化。 局部更新不会舍弃原来页面的内容。 同步与异步的区别： 同步：用户发送请求后，页面不可使用，等待服务器响应完成 异步：页面还可以继续使用。 这也就是与重定向回原页面的区别，而且在响应完成后，页面的数据还是旧的。 17.2 Ajax 使用 17.2.1 原生 Ajax 请求演示 创建 ajax 对象：var ajax= new XMLHttpRequest(); 调用 open 方法设置请求参数：ajax.open(&quot;GET&quot;, url&amp;param); 绑定 onreadystatechange 事件：ajax.onreadystatechange = function()&#123;if (xmlhttprequest.readyState == 4 &amp;&amp; xmlhttprequest.status == 200) &#123; ajax.send(); &#125;&#125;| 调用 send 方法发送请求：ajax.send() 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;meta http-equiv=&quot;pragma&quot; content=&quot;no-cache&quot; /&gt; &lt;meta http-equiv=&quot;cache-control&quot; content=&quot;no-cache&quot; /&gt; &lt;meta http-equiv=&quot;Expires&quot; content=&quot;0&quot; /&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt; &lt;title&gt;Insert title here&lt;/title&gt; &lt;script type=&quot;text/javascript&quot;&gt; // 在这里使用javaScript语言发起Ajax请求，访问服务器AjaxServlet中javaScriptAjax function ajaxRequest() &#123; // 1、我们首先要创建XMLHttpRequest var xmlhttprequest = new XMLHttpRequest(); // 2、调用open方法设置请求参数 xmlhttprequest.open( &quot;GET&quot;, &quot;http://localhost:8080/16_json_ajax_i18n/ajaxServlet?action=javaScriptAjax&quot;, true ); // 4、在send方法前绑定onreadystatechange事件，处理请求完成后的操作。 xmlhttprequest.onreadystatechange = function () &#123; if (xmlhttprequest.readyState == 4 &amp;&amp; xmlhttprequest.status == 200) &#123; alert(&quot;收到服务器返回的数据：&quot; + xmlhttprequest.responseText); var jsonObj = JSON.parse(xmlhttprequest.responseText); // 把响应的数据显示在页面上 document.getElementById(&quot;div01&quot;).innerHTML = &quot;编号：&quot; + jsonObj.id + &quot; , 姓名：&quot; + jsonObj.name; &#125; &#125;; // 3、调用send方法发送请求 xmlhttprequest.send(); alert(&quot;我是最后一行的代码&quot;); &#125; &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;button onclick=&quot;ajaxRequest()&quot;&gt;ajax request&lt;/button&gt; &lt;div id=&quot;div01&quot;&gt;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 17.2.2 jQuery 中 ajax 演示 $.ajax(&#123;参数&#125;) url： type：请求类型 data：发送给服务区的数据 格式一：key1=value1&amp;key2=value2 格式二：&#123;key1:value1, key2;value2&#125; success：请求成功时，响应的回调函数，服务器返回的data，以参数的形式被 success 使用 dataType：响应数据类型 text xml json $.get(&#123;参数&#125;)和$.post(&#123;参数&#125;)：对$.ajax(&#123;参数&#125;)的进一步封装 url： data：发送给服务区的数据 格式一：key1=value1&amp;key2=value2 格式二：&#123;key1:value1, key2;value2&#125; callback：请求成功时，响应的回调函数，服务器返回的data，以参数的形式被 callback 使用 type：响应数据类型 text xml json $.getJSON(&#123;参数&#125;)：对$.get(&#123;参数&#125;)的进一步封装 url： data：发送给服务区的数据 格式一：key1=value1&amp;key2=value2 格式二：&#123;key1:value1, key2;value2&#125; callback：请求成功时，响应的回调函数，服务器返回的data，以参数的形式被 callback 使用 serialize()：表单序列化——将表单的内容以key1=value1&amp;key2=value2的形式拼接【表单的方法】 18. 国际化（i18） 12345678910username=usernamepassword=passwordsex=sexage=ageregist=registboy=boyemail=emailgirl=girlreset=resetsubmit=submit 12345678910username=用户名password=密码sex=性别age=年龄regist=注册boy=男girl=女email=邮箱reset=重置submit=提交 配置文件命名规则：basename_en_US.pproperties 获取语言国家信息（en_US 和 zh_CN)： java.utli.Locale类实例化Locale locale = Local.getDefault(); Locale的静态常量获取： Locale.CHINA Locale.US 通过指定的 basename 和 Locale 对象，读取配置文件及文件内容 ResourceBundle bundle = ResourceBundle.getBundle(&quot;i18n&quot;, locale); bundle.getString(&quot;key&quot;) 方式一：使用请求头实现国际化： 方式二：使用手动选择实现国际化： 方式三：使用 JSTL 标签库实现国际化：需要使用 fmt——格式化库，即导入相应包 16尚硅谷_JSON、Aajx、i18n王振国 - 课堂笔记.pdf 20. IoC 耦合/依赖 依赖指的是某某某离不开某某某。 在软件系统中，层与层之间是存在依赖的。我们也称之为耦合。 系统架构或者是设计的一个原则是： 高内聚低耦合。 层内部的组成应该是高度聚合的，而层与层之间的关系应该是低耦合的，最理想的情况 0 耦合（就是没有耦合） IOC - 控制反转 / DI - 依赖注入 控制反转： 之前在 Servlet 中，我们创建 service 对象 ， FruitService fruitService = new FruitServiceImpl(); 这句话如果出现在 servlet 中的某个方法内部，那么这个 fruitService 的作用域（生命周期）应该就是这个方法级别； 如果这句话出现在 servlet 的类中，也就是说 fruitService 是一个成员变量，那么这个 fruitService 的作用域（生命周期）应该就是这个 servlet 实例级别 之后我们在 applicationContext.xml 中定义了这个 fruitService。然后通过解析 XML，产生 fruitService 实例，存放在 beanMap 中，这个 beanMap 在一个 BeanFactory 中 因此，我们转移（改变）了之前的 service 实例、dao 实例等等他们的生命周期。控制权从程序员转移到 BeanFactory。这个现象我们称之为控制反转 依赖注入： 之前我们在控制层出现代码：FruitService fruitService = new FruitServiceImpl()； 那么，控制层和 service 层存在耦合。 之后，我们将代码修改成 FruitService fruitService = null ; 然后，在配置文件中配置: 123&lt;bean id=&quot;fruit&quot; class=&quot;FruitController&quot;&gt; &lt;property name=&quot;fruitService&quot; ref=&quot;fruitService&quot;/&gt;&lt;/bean&gt; 开发经验 1. 表单重复提交 1.1 情况一 问题描述： web提交完表单。服务器使用请求转来进行页面跳转。这个时候，用户按下功能键 F5，就会发起最后一次的请求。 造成表单重复提交问题 发生原因： 浏览器的地址带了请求参数，没有发生变化，刷新页面相当于再次提交请求。 解决方案： 使用重定向来进行跳转，改变浏览器ip 1.2 情况二 问题描述： 用户正常提交服务器，但是由于网络延迟等原因，迟迟未收到服务器的响应，这个时候，用户以为提交失败， 就会着急，然后多点了几次提交操作，也会造成表单重复提交 发生原因： 解决方案： 使用验证码 1.3 情况三 问题描述： 用户正常提交服务器。服务器也没有延迟，但是提交完成后，用户回退浏览器。重新提交。也会造成表单重复提交 发生原因： 解决方案： 使用验证码 2. 排错 如果发现访问不到资源，工程启动后不经过 servlet，问题多半是工件是 web exploded，将它新建为 war exploded 即可。 一个问题：idea 的项目部署的 web exploded 和 war exploded 和 war 的区别是啥？啥时候用哪个？","tags":[{"name":"JSP","slug":"JSP","permalink":"https://sk370.github.io/tags/JSP/"},{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://sk370.github.io/tags/JavaWeb/"}]},{"title":"Maven","date":"2022-06-27T16:33:22.000Z","path":"2022/06/28/maven/Maven/","text":"Maven 是一个 Java 项目管理和构建工具，它可以定义项目结构、项目依赖，并使用统一的方式进行自动化构建。 1. Maven 概述 1.1 Maven 的作用及必要性 1.1.0 概念 Apache 软件基金会组织维护的一款专门为 Java 项目提供构建和依赖管理支持的工具。 构建：Java 开发过程中，使用原材料生产产品的过程。 清理：删除上一次构建的结果，为下一次构建做好准备 编译：Java 源程序编译成 *.class 字节码文件 测试：运行提前准备好的测试程序 报告：针对刚才测试的结果生成一个全面的信息 打包 Java 工程：jar 包 Web 工程：war 包 安装：把一个 Maven 工程经过打包操作生成的 jar 包或 war 包存入 Maven 仓库 部署 部署 jar 包：把一个 jar 包部署到 Nexus 私服服务器上 部署 war 包：借助相关 Maven 插件（例如 cargo），将 war 包部署到 Tomcat 服务器上 依赖：如果 A 工程里面用到了 B 工程的类、接口、配置文件等等这样的资源，那么我们就可以说 A 依赖 B 依赖管理中要解决的具体问题： jar 包的下载：使用 Maven 之后，jar 包会从规范的远程仓库下载到本地 jar 包之间的依赖：通过依赖的传递性自动完成 jar 包之间的冲突：通过对依赖的配置进行调整，让某些 jar 包不会被导入 1.1.1 作用 Maven 简化并标准化了项目构建过程。它将项目的编译，生成文档，创建报告，发布，部署等任务无缝衔接，构建成一套完整的生命周期。 构建项目 生成文档 创建报告 维护依赖 软件配置管理 发布 部署 1.1.2 必要性 管理规模庞大的 jar 包，需要专门的工具 IDEA 完成了本地的构建操作，但项目部署、发布没有 IDEA 的环境，需要单独配置。 1.1.3 特点 设置简单。 所有项目的用法一致。 可以管理和自动进行更新依赖。 庞大且不断增长的资源库。 可扩展，使用 Java 或脚本语言可以轻松的编写插件。 几乎无需额外配置，即可立即访问新功能。 基于模型的构建：Maven 能够将任意数量的项目构建为预定义的输出类型，例如 JAR，WAR。 项目信息采取集中式的元数据管理：使用与构建过程相同的元数据，Maven 能够生成一个网站（site）和一个包含完整文档的 PDF。 发布管理和发行发布：Maven 可以与源代码控制系统（例如 Git、SVN）集成并管理项目的发布。 向后兼容性：您可以轻松地将项目从旧版本的 Maven 移植到更高版本的 Maven 中。 并行构建：它能够分析项目依赖关系，并行构建工作，使用此功能，可以将性能提高 20%-50％。 更好的错误和完整性报告：Maven 使用了较为完善的错误报告机制，它提供了指向 Maven Wiki 页面的链接，您将在其中获得有关错误的完整描述。 1.1.4 Maven 工作机制 没懂 2. Maven 配置 2.1 Maven 下载及安装 2.1.1 Maven 官网地址 下载链接： 2.1.2 解压 Maven 核心程序 核心程序压缩包：apache-maven-3.8.4-bin.zip，解压到非中文、没有空格的目录。例如： Maven 的核心配置文件：conf/settings.xml 2.2 配置 Maven 环境 2.2.1 指定本地仓库 本地仓库默认值：用户家目录/.m2/repository。 如 windows：C:\\Users\\iceri\\.m2\\repository 刚安装完没有启动过 maven 的话看不到这个目录 修改conf/settings.xml文件中的配置（53 行），可更换仓库目录。 非中文、没有空格的目录。 本地没有该目录时，执行 maven 命令会自动创建 不同版本的 maven 可以共用该仓库 2.2.2 修改远程镜像仓库 改成阿里云提供的镜像仓库，让 Maven 下载 jar 包的时候速度更快。 修改conf/settings.xml文件中的配置（160~165 行） 改之前： 改之后： 注意检查最新地址是否变化：，只需要看地址。 注意：阿里云镜像不支持索引。可能会有下面的错误，但暂时不知道有什么影响，暂未解决。 2.2.3 配置 Maven 工程的基础 JDK 版本 maven 使用的默认 JDK 版本是 1.5。 修改conf/settings.xml文件中的配置，更换为 JDK1.8。 在最后以一个&lt;/profiles&gt;标签（约 254 行）前，插入下述文本， maven 修改配置.md 2.2.4 检查 Java 环境变量 注意：安装 Java 后即使不配置**JAVA_HOME**路径，终端也能运行 Java 命令（如下），但这个环境变量配置在了 Path 里，而一些依赖 Java 运行环境的软件、工具，由于自身就是 Java 开发，或者某些配置需要 Java 环境变量，它依然会找**JAVA_HOME**，导致运行失败。 检查环境变量配置： 终端中输入echo %JAVA_HOME%、echo %PATH%能够正确显示本地 JDK 路径即可。 2.2.5 配置 MAVEN_HOME 和 Path MAVEN_HOME配置： Path配置： XXX_HOME 通常指向的是 bin 目录的上一级，PATH 指向的是 bin 目录 2.2.6 检查 Maven 配置是否成功 终端中输入mvn -v，是否正常显示相关信息 注意：在配置环境变量前打开的终端窗口，在配置完环境变量后要重新打开，以保证终端加载了配置的环境变量。 2.3 Maven 的核心概念 2.3.1 POM Project Object Model，项目对象模型。是模型化思想的具体体现。 POM 表示将工程抽象为一个模型，再用程序中的对象来描述这个模型 模型化思想：将现实生活中的事物抽象为模型，然后封装模型相关的数据作为一个对象，这使得可以在程序中计算与现实事物相关的数据。 POM 理念集中体现在 Maven 工程根目录下 pom.xml 这个配置文件中。所以这个 pom.xml 配置文件就是 Maven 工程的核心配置文件。其实学习 Maven 就是学这个文件怎么配置，各个配置有什么用。 2.3.2 约定的目录结构 目录结构： 另外还有一个 target 目录专门存放构建操作输出的结果 约定目录结构的意义：让构建过程能够尽可能自动化完成 约定大于配置： 3. 使用 Maven 命令行环境 3.1 maven 中的坐标，即本地文件存放位置 3.1.1 Maven 中的坐标 maven 中使用三个**『向量』在『Maven 的仓库』中唯一的定位到一个『jar』**包。 groupId：公司或组织的 id artifactId：一个项目或者是项目中的一个模块的 id version：版本号 3.1.2 三个向量的取值方式 groupId：公司或组织域名的倒序，通常也会加上项目名称 artifactId：模块的名称，将来作为 Maven 工程的工程名 version：模块的版本号，根据自己的需要设定 例如：SNAPSHOT 表示快照版本，正在迭代过程中，不稳定的版本 例如：RELEASE 表示正式版本 举例： groupId：com.atguigu.maven artifactId：pro01-atguigu-maven version：1.0-SNAPSHOT 3.1.3 坐标和仓库中 jar 包的存储路径之间的对应关系 坐标： 对应本地存储位置： 3.2 使用mvn archetype:generate命令生成 Maven 工程 3.2.1 创建工程 在需要存放 maven 工程的路径下打开 cmd 终端执行 输入 7，使用默认值 输入 groupId，如iceriver.mavenetst——组织名.项目名 输入 artifactId，如pro01-maven——工程名作为模块名 使用默认 包名可使用默认值，与 groupId 一致。 确认设置。 3.2.2 更改模块设置 模块结构： 修改模块（artifactId）目录下pro01-maven/pom.xml中 junit 的配置 可以删除App.java和AppTest.java两个文件 3.2.3 pom.xml文件解读 1234567891011121314151617181920212223242526272829303132&lt;!-- 根标签，表示对当前工程（模块）进行配置、管理 --&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;!-- 从Maven2开始就是4.0.0，代表pom.xml文件所采用的标签结构 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 当前Maven工程的坐标 --&gt; &lt;groupId&gt;iceriver.maventest&lt;/groupId&gt; &lt;artifactId&gt;pro01-maven&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!-- 打包方式：jar表示Java工程，war表示web工程，pom表示管理其他工程的工程 --&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;pro01-maven&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;!-- 定义maven使用的属性值 --&gt; &lt;properties&gt; &lt;!-- 工程读取源码时使用的字符集 --&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;!-- 配置具体依赖信息 --&gt; &lt;dependencies&gt; &lt;!-- 配置一个具体的依赖 --&gt; &lt;dependency&gt; &lt;!-- 依赖所在的坐标信息 --&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;!-- 配置依赖的作用范围 --&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 1234567891011121314151617181920212223242526272829&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/plugin&gt; &lt;!-- maven 打包时跳过测试 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 设置打包编码为utf-8 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;8&lt;/source&gt; &lt;target&gt;8&lt;/target&gt; &lt;encoding&gt;utf-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 3.3 在 maven 工程中编写代码 3.3.1 主体程序 package路径。 1234567package iceriver.maventest;public class Calculator &#123; public int sum(int i, int j) &#123; return i + j; &#125;&#125; 3.3.2 测试程序 123456789101112131415161718192021222324// 静态导入的效果是将Assert类中的静态资源导入当前类// 这样一来，在当前类中就可以直接使用Assert类中的静态资源，不需要写类名import static org.junit.Assert.*;public class CalculatorTest &#123; @Test public void testSum() &#123; // 1.创建Calculator对象 Calculator calculator = new Calculator(); // 2.调用Calculator对象的方法，获取到程序运行实际的结果 int actualResult = calculator.sum(5, 3); // 3.声明一个变量，表示程序运行期待的结果 int expectedResult = 8; // 4.使用断言来判断实际结果和期待结果是否一致 // 如果一致：测试通过，不会抛出异常 // 如果不一致：抛出异常，测试失败 assertEquals(expectedResult, actualResult); &#125;&#125; 3.4 执行 Maven 的构建命令 3.3.1 构建命令 mvn xxx 注意：mvn -v与构建无关，只要正确配置了 maven-path，在任何目录下都可执行。 运行与构建相关的命令，必须进入到pom.xml文件所在的目录 3.3.2 清理操作 mvn clean 删除 trget 目录 3.3.3 编译操作 主程序编译：mvn compile 主体程序编译结果存放的目录：target/classes 测试程序编译：mvn test-compile 测试程序编译结果存放的目录：target/test-classes 3.3.4 测试操作 mvn test 测试的报告存放的目录：target/surefire-reports 3.3.5 打包操作 mvn package 打包的结果——jar 包，存放的目录：target 3.3.6 安装操作 将本地构建生成的 jar 包存入到本地 Maven 仓库中。 mvn install 在 maven 在本地仓库的位置是pom.xml中的配置确定的 - mvn clean package 依次执行了 clean、resources、compile、testResources、testCompile、test、jar(打包)等７个阶段。 mvn clean install 依次执行了 clean、resources、compile、testResources、testCompile、test、jar(打包)、install 等 8 个阶段。 mvn clean deploy 依次执行了 clean、resources、compile、testResources、testCompile、test、jar(打包)、install、deploy 等９个阶段。 主要区别如下， package 命令完成了项目编译、单元测试、打包功能，但没有把打好的可执行 jar 包（war 包或其它形式的包）布署到本地 maven 仓库和远程 maven 私服仓库 install 命令完成了项目编译、单元测试、打包功能，同时把打好的可执行 jar 包（war 包或其它形式的包）布署到本地 maven 仓库，但没有布署到远程 maven 私服仓库 deploy 命令完成了项目编译、单元测试、打包功能，同时把打好的可执行 jar 包（war 包或其它形式的包）布署到本地 maven 仓库和远程 maven 私服仓库 3.3.7 查看当前工程的依赖 mvn dependency:list 第一个依赖为 junit 的依赖 3.5 创建 Maven 版的 web 工程 3.5.1 创建工程 mvn archetype:generate -DarchetypeGroupId=org.apache.maven.archetypes -DarchetypeArtifactId=maven-archetype-webapp -DarchetypeVersion=1.4 3.5.2 pom.xml文件配置 无特殊配置，确认下打包形式——war和junit版本即可 3.5.3 创建 servlet 在 main 目录下创建 java 目录，即为 servlet 程序的根目录 编写测试程序，如 HelloServlet.java 123456public class HelloServlet extends HttpServlet&#123; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; response.getWriter().write(&quot;hello maven web&quot;); &#125;&#125; 配置web.xml index.jsp页面中插入超链接，访问 HelloServlet 程序 此时还不能启动 web 程序，是因为缺少必须的依赖: 配置servlet-api.jar 将依赖的配置信息放到pom.xml文件中 编译该 web 程序，在模块目录下（与pom.xml同级）执行mvn compile 打包 web 程序——执行mvn package 部署：将上面生成的 war 包拷贝至tomcat安装目录/webapps路径下 启动 tomcat： 浏览器访问该 war 包的地址 3.6 让 web 工程依赖 Java 工程 3.6.1 概念 将我们自己的 java 工程打包成 jar 包，放在 web 工程的WEB-INF/lib目录下 3.6.2 配置依赖 如将 3.2 构建的 Java 工程作为 3.5 创建的 web 工程的依赖： 配置pom.xml文件： 即将 3.2Java 工程的坐标，作为 3.5web 工程的依赖 &lt;scope&gt;标签可写可不写，默认值为compile 3.6.3 创建 test 程序的坐标 根据依赖的groupId，创建相关的文件目录 将 3.3 创建的CalculatorTest.java文件复制到 maven 目录下 执行mvn test 3.6.4 打包 执行mvn package成功后即可在 3.5war 包下的 lib 目录看到 3.2 的 jar 包 3.7 依赖生效范围 3.7.1 依赖范围的配置 在pom.xml文件中dependencies/dependency/**scope**标签中配置 可选值：**compile**/**test**/**provided**/system/runtime/**import** compile：构建后直接放入 classpath test：仅在测试时使用，正常运行时并不需要。如 junit runtime：编译时不需要，但运行时需要，如 JDBC 驱动 provided：编译时需要，运行时不需要，运行时由环境中的其他支持提供，如 servletapi 3.7.2 compile、test 和 provided 对比 main 目录（空间） test 目录（空间） 开发过程（时间） 部署到服务器（时间） compile 有效 有效 有效 有效 test 无效 有效 有效 无效 provided 有效 有效 有效 无效 compile：通常使用的第三方框架的 jar 包这样在项目实际运行时真正要用到的 jar 包都是以 compile 范围进行依赖的。比如 SSM 框架所需 jar 包。 test：测试过程中使用的 jar 包，以 test 范围依赖进来。比如 junit。 provided：在开发过程中需要用到的“服务器上的 jar 包”通常以 provided 范围依赖进来。比如 servlet-api、jsp-api。而这个范围的 jar 包之所以不参与部署、不放进 war 包，就是避免和服务器上已有的同类 jar 包产生冲突，同时减轻服务器的负担。 3.7.3 依赖的传递性 在 A 依赖 B，B 依赖 C 的前提下，C 是否能够传递到 A，取决于 B 依赖 C 时使用的依赖范围。 B 依赖 C 时使用 compile 范围：可以传递 B 依赖 C 时使用 test 或 provided 范围：不能传递，所以需要这样的 jar 包时，就必须在需要的地方明确配置依赖才可以。 3.7.4 依赖的排除 当 A 依赖 B，B 依赖 C 而且 C 可以传递到 A 的时候，A 不想要 C，需要在 A 里面把 C 排除掉。而往往这种情况都是为了避免 jar 包之间的冲突。 &lt;dependency&gt;标签中使用&lt;exclusion&gt;子标签，将依赖 jar 包的坐标（除&lt;scope&gt;）写在&lt;exclusion&gt;标签内。 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;iceriver.maventest&lt;/groupId&gt; &lt;artifactId&gt;pro01-maven&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;!-- 使用excludes标签配置依赖的排除 --&gt; &lt;exclusions&gt; &lt;!-- 在exclude标签中配置一个具体的排除 --&gt; &lt;exclusion&gt; &lt;!-- 指定要排除的依赖的坐标（不需要写version） --&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 3.7.5 依赖的继承 Maven 工程之间，A 工程继承 B 工程，本质上是 A 工程的 pom.xml 中的配置继承了 B 工程中 pom.xml 的配置。 作用：通过父工程统一管理各个模块的依赖及版本，保证项目中依赖的统一，减少冲突等异常情况。 只有打包方式为 pom 的 Maven 工程能够管理其他 Maven 工程。打包方式为 pom 的 Maven 工程中不写业务代码，它是专门管理其他 Maven 工程的工程。 子工程在父工程的本地存储路径下，与父工程的src目录同级 父工程的pom.xml文件配置： 子工程需要使用父工程管理的依赖时，还需要按需引入。 子工程pom.xml文件配置： 子工程的groupId与父工程一致时，子工程坐标的groupId可以省略，version一致时，也可以省略。 子工程配置依赖时，version可以省略，如果子工程配置了version，最终会以子工程的版本为准。 子工程可以引入父工程没有管理的依赖吗？可以。 父工程的属性： 设置在&lt;properties&gt;标签中 在&lt;properties&gt;标签中声明的自定义标签即为自定义属性，使用时$&#123;标签名&#125;，属性值为标签中间的文本 3.7.6 工程的聚合 使用一个“总工程”将各个“模块工程”汇集起来，作为一个整体对应完整的项目。 好处： 一键执行 Maven 命令：很多构建命令都可以在“总工程”中一键执行。 配置聚合之后，各个模块工程会在总工程中展示一个列表，让项目中的各个模块一目了然。 配置： 在父工程中使用&lt;moudles&gt; 如果 A 工程依赖 B 工程，B 工程依赖 C 工程，C 工程又反过来依赖 A 工程，那么在执行构建时，会报：[ERROR] [ERROR] The projects in the reactor contain a cyclic reference: 4. IDEA 中使用 Maven 4.0 知识补充 4.0.1 DependencyManagement 和 Dependencies dependencyManagement 提供了一种管理依赖版本号的方式。通常会在一个组织或者项目的最顶层的父 POM 中看到 dependencyManagement 元素。使用父 pom 中的 dependencyManagement 元素能让所有在子项目中引用一个依赖而不用显式的列出版本号。 Maven 加载子 pom 的依赖时，会找到父 pom 中的 dependencyManagement 元素，然后使用这个父 pom 管理的依赖版本号。 这样做的好处就是：如果有多个子项目都引用同一样依赖，则可以避免在每个使用的子项目里都声明一个版本号，这样当想升级或切换到另一个版本时，只需要在顶层父容器里更新，而不需要一个一个子项目的修改 ；另外如果某个子项目需要另外的一个版本，只需要声明 version 就可。 dependencyManagement 里只是声明依赖，并不实现引入，因此子项目需要显示的声明需要用的依赖。 如果不在子项目中声明依赖，是不会从父项目中继承下来的；只有在子项目中写了该依赖项，并且没有指定具体版本，才会从父项目中继承该项，并且 version 和 scope 都读取自父 pom；如果子项目中指定了版本号，那么会使用子项目中指定的 jar 版本。 tiips：父 pom 管理的依赖如果本地没有，会报红，可以先去掉 dependencyManagement 标签，让本地从远程仓库拉取，完成后再把 dependencyManagement 加上。 4.0.2 跳过单元测试 好处：加快项目构建过程 4.0.3 好的习惯 父工程的 pom 设置完后，使用 maven clean 和 maven install 可以将父依赖打包加到本地仓库中，加快子模块的依赖引入过程。 4.1 创建父工程 演示版本： 创建工程： 更换本地 maven 配置和仓库【重要】： 不进行此操作可能会出现 IDEA 创建的模块使用 idea 的配置文件，造成版本错误、jar 包冲突等问题。 由于不同版本IDEA创建的过程不一样，需要根据具体的IDEA工具去执行操作，所以下面没必要看 4.2 创建 Maven-java【子】工程 4.2.1 创建过程演示 4.2.2 编写测试程序 123456789/*** @author: INFINITY https://developer.aliyun.com/profile/sagwrxp2ua66w* @date: 2022/7/10 23:37*/public class Hello &#123; public void showMessage()&#123; System.out.println(&quot;IDEA &amp; Maven&quot;); &#125;&#125; 1234567891011/** * @author: https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/7/10 23:50 */public class HelloTest &#123; @Test public void showMessage()&#123; Hello hello = new Hello(); hello.showMessage(); &#125;&#125; 4.2.2 执行 maven 命令 方式一： 执行下面【插件】内的运行命令效果一致 方式二： 记得选择执行命令的位置 方式三： 传统命令行终端的形式 4.3 创建 Maven-web【子】工程 4.3.1 创建过程演示 同 4.2.1 创建 maven-java【子】工程 4.3.2 web 环境配置 子工程pom.xml中添加&lt;packaging&gt;war&lt;/packaging&gt; 生成WEB-INF等文件： 方式一： 方式二： 这里由于使用方式一生成了，所以方式二不显示了 4.3.3 测试环境 1234567891011package iceriver.maventest;/** * @author: https://developer.aliyun.com/profile/sagwrxp2ua66w * @date: 2022/7/11 0:24 */public class Message &#123; public String getMsg()&#123; return &quot;hello maven-web&quot;; &#125;&#125; 12345678910111213141516&lt;%@ page import=&quot;iceriver.maventest.Message&quot; %&gt;&lt;%-- Created by IntelliJ IDEA. User: iceri Date: 2022/7/11 Time: 0:25 To change this template use File | Settings | File Templates.--%&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;%= new Message().getMsg()%&gt;&lt;/body&gt;&lt;/html&gt; 4.4 导入工程 4.4.1 导入自远程仓库 4.4.2 导入自本地磁盘 普通 Java 工程导入： 导入 web 工程 先按正常 Java 工程导入 导入后进入为其添加 web 框架支持即可【4.3.2】 5. 其他核心概念 5.1 生命周期 5.1.1 生命周期概述 作用：让构建自动化完成，生命周期的每个环节对应一系列操作 分类：clean、site、default 特点：3 个生命周期相互独立，执行任何一个操作，都是从本周起最初的位置开始执行。 5.2 插件和仓库 5.2.1 插件 Maven 的核心程序仅仅负责宏观调度，不做具体工作。具体工作都是由 Maven 插件完成的。 5.2.2 仓库 当 Maven 根据坐标寻找构件时，它会首先查看本地仓库，若本地仓库存在此构件，则直接使用；若本地仓库不存在此构件，Maven 就会去远程仓库查找，若发现所需的构件后，则下载到本地仓库使用。如果本地仓库和远程仓库都没有所需的构件，则 Maven 就会报错。 不要中央仓库和阿里云镜像混用，否则 jar 包来源不纯，彼此冲突。","tags":[{"name":"Maven","slug":"Maven","permalink":"https://sk370.github.io/tags/Maven/"}]},{"title":"SSM","date":"2022-06-27T16:33:11.000Z","path":"2022/06/28/ssm/SSM/","text":"SSM（SpringMVC+Spring+MyBatis）是目前市场上最流行的开发 web 项目的框架，它由 SpringMVC、Spring、MyBatis 整合而成。 SSM（SpringMVC+Spring+MyBatis）是目前市场上最流行的开发 web 项目的框架，它由 SpringMVC、Spring、MyBatis 整合而成。——即 springmvc 整合 spring 和 mybatis SpringMVC 框架负责接收浏览器发送的请求，并响应浏览器数据； Spring 框架使用其核心 IOC 思想管理服务器中各个组件，使用 AOP 思想面向切面编程，在不改变源码的基础上实现功能增强； MyBatis 框架封装 JDBC，负责访问数据库，完成持久化操作。 意义：一是不用使用 mapper.xml 文件，使用时自动注入简化开发过程，后续的操作由 spring 完成。二是对各种配置文件简单化，不要纠缠在一起。 1. SpringMVC Spring MVC 2. Spring Spring 3. MyBatis MyBatis 4. SSM 整合 4.0 三者的关系 spring 容器是 ApplicationContext/contextloaderlistener，管理的是 service 和 dao springmvc 容器是 webApplicationContext/dispatchservlet，管理的是 controller springmvc 容器是 spring 容器的子容器 4.1 引入 4.1.1 一个问题 SpringMVC 中，springMVC.xml 用于配置控制层组件。 Spring 中，bean.xml 用于配置各个层组件。 而在 SpringMVC 项目中，通过将控制层组件初始化提前到创建 tomcat 容器时（创建 servlet 时）完成。 但控制层组件依赖 spring 管理的 service 实体，此时要求 service 实体的自动装配必须在控制层组件装配前完成，否则会报错。 这就要求 spirng 的配置必须在创建 servlet 前完成。 由于监听器、过滤器、servlet 依次执行，所以可以把 Spring 的配置放到监听器中完成。不选择过滤器的原因是它是专业处理请求和响应的。 4.1.2 整合思路 spring 整合 mybatis 即虚线框外的是需要程序员做的。 spring 整合 springmvc： 总体思路：tomcat 服务器会读取 web.xml，所以在读取 web.xml 文件时，spring.xml、spring-mvc.xml、mybatis.xml 都要配置好。 spring.xml 配置 mybatis.xml，简化了 mybatis 的使用。在 web.xml 文件中，将 mybatix.xml 的文件配置在中，随着工程一启动加载。 web.xml 中加载 spring.xml。web.xml 中要用到 spring.xml 的数据信息，所以将 spring.xml 放在监听器启动时加载。 web.xml 中加载 spring-mvc.xml。web.xml 要使用 spring-mvc.xml 的 servlet，默认 servlet 的声明周期为第一次接到请求时创建，这时来不及，所以使用 load-on-startup 提前到 web 应用启动时创建。 启动时执行流程：监听器 → 过滤器 → 拦截器 4.1.3 ContextLoaderListener Spring 提供了监听器ContextLoaderListener，实现ServletContextListener接口，可监听 ServletContext的状态，在 web 服务器的启动，读取 Spring 的配置文件，创建 Spring 的 IOC 容器。 4.1.4 使用监听器加载 spring 配置文件 创建新工程，使用 Maven 作为构建工具。 引入依赖： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;iceriver.ssm.pro01-spring-listener&lt;/groupId&gt; &lt;artifactId&gt;pro01-spring-listener&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- SpringMVC --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- ServletAPI --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring5和Thymeleaf整合包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.thymeleaf&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-spring5&lt;/artifactId&gt; &lt;version&gt;3.0.12.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 配置 web 工程： 配置 springMVC 前端控制器，及 spring 配置文件 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd&quot; version=&quot;4.0&quot;&gt;&lt;!-- springmvc前端控制器配置--&gt; &lt;servlet&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;!-- 在监听器配置spring配置文件--&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring.xml&lt;/param-value&gt; &lt;/context-param&gt;&lt;/web-app&gt; 这里 spring 的配置文件采用了自定义文件名的方式，如果没有这段内容，则 spring 的配置文件名称要为：applicationContext.xml，位置要在/WEB-INF/下。 创建 springmvc.xml 和 spring.xml 配置文件 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc https://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!--扫描组件--&gt; &lt;context:component-scan base-package=&quot;iceriver.ssm.pro01.controller&quot;/&gt; &lt;!--配置视图解析器--&gt; &lt;bean id=&quot;viewResolver&quot; class=&quot;org.thymeleaf.spring5.view.ThymeleafViewResolver&quot;&gt; &lt;property name=&quot;order&quot; value=&quot;1&quot;/&gt; &lt;property name=&quot;characterEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;property name=&quot;templateEngine&quot;&gt; &lt;bean class=&quot;org.thymeleaf.spring5.SpringTemplateEngine&quot;&gt; &lt;property name=&quot;templateResolver&quot;&gt; &lt;bean class=&quot;org.thymeleaf.spring5.templateresolver.SpringResourceTemplateResolver&quot;&gt; &lt;!-- 视图前缀 --&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/templates/&quot;/&gt; &lt;!-- 视图后缀 --&gt; &lt;property name=&quot;suffix&quot; value=&quot;.html&quot;/&gt; &lt;property name=&quot;templateMode&quot; value=&quot;HTML5&quot;/&gt; &lt;property name=&quot;characterEncoding&quot; value=&quot;UTF-8&quot; /&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;mvc:annotation-driven/&gt; &lt;mvc:view-controller path=&quot;/&quot; view-name=&quot;index&quot;&gt;&lt;/mvc:view-controller&gt;&lt;/beans&gt; 1234567&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;iceriver.ssm.pro01.service&quot;/&gt;&lt;/beans&gt; srpingmvc 创建的 ioc 容器是 spring 创建的 ioc 容器的子容器（因为 spring 先执行创建容器）。spring 中，子容器能够访问父容器，但父容器不能访问子容器。 4.2 SSM 整合案例 4.2.1 创建工程 创建工程：【同上】 导入依赖： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;iceriver.ssm.pro02-ssm&lt;/groupId&gt; &lt;artifactId&gt;pro02-ssm-integration&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.3.1&lt;/spring.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- spring上下文依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring bean管理依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--springmvc--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 事务管理需要jdbc--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mybatis核心 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.7&lt;/version&gt; &lt;/dependency&gt; &lt;!--mybatis和spring的整合包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;2.0.6&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- junit测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- MySQL驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.16&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log4j日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- ServletAPI --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring5和Thymeleaf整合包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.thymeleaf&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-spring5&lt;/artifactId&gt; &lt;version&gt;3.0.12.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 创建 web 支持 创建测试表： 生成数据【任意数量】 4.2.2 配置 web.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd&quot; version=&quot;4.0&quot;&gt;&lt;!-- 1. 配置spring的编码过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt;&lt;!-- 2. 配置请求方式的过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt;&lt;!-- 3. 配置springmvc的前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;!-- 4. 在监听器配置spring配置文件--&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt;&lt;!-- 5. 配置spring配置文件自定义的位置和名称--&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring.xml&lt;/param-value&gt; &lt;/context-param&gt;&lt;/web-app&gt; 4.2.3 配置 springmvc.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc https://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!--扫描组件--&gt; &lt;context:component-scan base-package=&quot;iceriver.ssm.pro02.controller&quot;/&gt; &lt;!--配置视图解析器--&gt; &lt;bean id=&quot;viewResolver&quot; class=&quot;org.thymeleaf.spring5.view.ThymeleafViewResolver&quot;&gt; &lt;property name=&quot;order&quot; value=&quot;1&quot;/&gt; &lt;property name=&quot;characterEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;property name=&quot;templateEngine&quot;&gt; &lt;bean class=&quot;org.thymeleaf.spring5.SpringTemplateEngine&quot;&gt; &lt;property name=&quot;templateResolver&quot;&gt; &lt;bean class=&quot;org.thymeleaf.spring5.templateresolver.SpringResourceTemplateResolver&quot;&gt; &lt;!-- 视图前缀 --&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/templates/&quot;/&gt; &lt;!-- 视图后缀 --&gt; &lt;property name=&quot;suffix&quot; value=&quot;.html&quot;/&gt; &lt;property name=&quot;templateMode&quot; value=&quot;HTML5&quot;/&gt; &lt;property name=&quot;characterEncoding&quot; value=&quot;UTF-8&quot; /&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!--配置默认的servlet处理静态资源--&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!--开启mvc的注解驱动--&gt; &lt;mvc:annotation-driven/&gt; &lt;!--配置视图控制器--&gt; &lt;mvc:view-controller path=&quot;/&quot; view-name=&quot;index&quot;/&gt; &lt;!--配置文件上传解析器--&gt; &lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;/&gt; &lt;!--配置异常处理器--&gt; &lt;!--配置拦截器--&gt;&lt;/beans&gt; 4.2.4 配置 spring.xml 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;&lt;!-- 引入jdbc.properties--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt;&lt;!-- 扫描组件，除控制层--&gt; &lt;context:component-scan base-package=&quot;iceriver.ssm.pro02&quot;&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt;&lt;!-- 配置数据源--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 4.2.5 配置 mybatis-config.xml 编写配置文件 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//MyBatis.org//DTD Config 3.0//EN&quot; &quot;http://MyBatis.org/dtd/MyBatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;properties resource=&quot;jdbc.properties&quot;/&gt; &lt;typeAliases&gt; &lt;package name=&quot;&quot;/&gt; &lt;/typeAliases&gt; &lt;environments default=&quot;mysql_test&quot;&gt; &lt;environment id=&quot;mysql_test&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;package name=&quot;&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 编写 mapper 类 编写 mapper.xml 映射文件 4.2.6 spring.xml 整合 mybatis 核心配置文件 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;&lt;!-- 引入jdbc.properties--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt;&lt;!-- 扫描组件，除控制层--&gt; &lt;context:component-scan base-package=&quot;iceriver.ssm.pro02&quot;&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt;&lt;!-- 配置数据源--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt; &lt;/bean&gt;&lt;!-- 整合mybatis:--&gt;&lt;!-- 配置sqlsessionfactorybean，可以直接在spring的ioc中获取sqlsessionfactory对象--&gt; &lt;bean class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt;&lt;!-- 设置mybatis的核心配置文件路径，如果有这个设置，则允许spring.xml与mybatis-config.xml同时进行配置--&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-cofig.xml&quot;/&gt;&lt;!-- 设置数据源--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;!-- 设置类型别名对应的包--&gt; &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;iceriver.ssm.pro02.pojo&quot;/&gt;&lt;!-- 设置映射文件路径：如果映射文件与mapper类的包名一致时，可以不设置。注意，不设置时还需要设置下面的sqlsession实现类对象。不然还是要写--&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:iceriver/ssm/pro02/mapper/*.xml&quot;/&gt; &lt;/bean&gt;&lt;!-- 配置mapper接口的扫描，可以将指定包下所有的mapper接口通过sqlsession创建代理实现类对象，并将这些对象交给ioc容器管理--&gt;&lt;!-- 有了这个配置，就可以直接在serviceimple类中直接使用ampper类了，而不用创建sqlsession对象--&gt; &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;iceriver.ssm.pro02.mapper&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; **注 1：**配置后就不需要 mybatis-config.xml 了，但由于 spring.xml 中有&lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-cofig.xml&quot;/&gt;，所以还是允许 mybatis-config.xml 存在的。 注 2：29 行老师讲的时候说可以不配置，他的演示里也没有配置，但本地测试，必须配置，否则会报 500 错误： 4.2.7 spring.xml 整合事务管理 引入log4j.xml 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE log4j:configuration SYSTEM &quot;log4j.dtd&quot;&gt;&lt;log4j:configuration xmlns:log4j=&quot;http://jakarta.apache.org/log4j/&quot;&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;org.apache.log4j.ConsoleAppender&quot;&gt; &lt;param name=&quot;Encoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;layout class=&quot;org.apache.log4j.PatternLayout&quot;&gt; &lt;param name=&quot;ConversionPattern&quot; value=&quot;%-5p %d&#123;MM-dd HH:mm:ss,SSS&#125;%m (%F:%L) \\n&quot;/&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;logger name=&quot;java.sql&quot;&gt; &lt;level value=&quot;debug&quot;/&gt; &lt;/logger&gt; &lt;logger name=&quot;org.apache.ibatis&quot;&gt; &lt;level value=&quot;info&quot;/&gt; &lt;/logger&gt; &lt;root&gt; &lt;level value=&quot;debug&quot;/&gt; &lt;appender-ref ref=&quot;STDOUT&quot;/&gt; &lt;/root&gt;&lt;/log4j:configuration&gt; spring.xml 中配置事务： 同 spring5 事务配置Spring5 4.3 实践 完成查询及分页查询功能","tags":[{"name":"SSM","slug":"SSM","permalink":"https://sk370.github.io/tags/SSM/"}]},{"title":"Java语言基础（下）","date":"2022-06-25T05:05:06.000Z","path":"2022/06/25/javase/Java语言基础（下）/","text":"本文介绍了Java语言的企业开发特性，如JDBC，企业开发中常封装成各种工具、框架进行使用。 第 14 章 JDBC 和数据库连接池 要求：学习了 Mysql（sql 语句）之后 14.1 JDBC 概述 14.1.1 JDBC 基本介绍 含义：Java DataBase Connectivity，Java 数据库连接，用于执行 SQL 语句的 JavaAPI，由类和接口组成，提供了个可以构建更高级工具和接口去访问数据库的基准。 作用：java 程序员使用 JDBC，可以连接任何提供了 JDBC 驱动程序的数据库系统，从而完成对数据库的各种操作。JDBC 为访问不同的数据库提供了统一的接口，为使用者屏蔽了细节问题。 基本原理图： 模拟 JDBC 接口 123456789public interface JdbcInterface &#123; //连接 public Object getConnection() ; //crud public void crud(); //关闭连接 public void close();&#125; 1234567891011121314151617public class MysqlJdbcImpl implements JdbcInterface&#123; @Override public Object getConnection() &#123; System.out.println(&quot;得到 mysql 的连接&quot;); return null; &#125; @Override public void crud() &#123; System.out.println(&quot;完成 mysql 增删改查&quot;); &#125; @Override public void close() &#123; System.out.println(&quot;关闭 mysql 的连接&quot;); &#125;&#125; 14.1.2 JDBC API 14.1.3 MySQLJDBC 版本支持： 下载地址： 14.1.3 JDBC 程序编写步骤 前置工作：在项目文件目录中加载对应数据库的 JDBC 工具包 注册驱动——加载 Driver 类 有异常问题 获取连接——得到 Connection 执行 CRUD——发送 sql 语句（字符串）给 mysql 执行 释放资源——关闭相关连接 123456789101112131415161718//1. 注册驱动——加载Driver类Driver driver = new com.mysql.jdbc.Driver();//2. 获取连接——得到ConnectionString url = &quot;jdbc:mysql://localhost:13306/db01&quot;;Properties properties = new Properties();properties.setProperty(&quot;user&quot;,&quot;root&quot;);properties.setProperty(&quot;password&quot;,&quot;dimitre123&quot;);Connection connect = driver.connect(url, properties);//3. 执行CRUD——发送sql语句（字符串）给mysql执行String sql = &quot;crud的SQL语句&quot;;//3.1 statement对象用于向数据库发送sql语句Statement statement = connect.createStatement();int rows = statement.executeUpdate(sql);//4. 释放资源——关闭相关连接statement.close();connect.close(); 解读： url： jdbc:mysql：使用的连接协议 localhost：连接地址（数据库地址） 13306：数据库端口号 db01：数据库名称 mysql连接的本质是socket连接 properties对象： setProperty()的user、password是规定好的 excuteUpdate(sql)：dml语句时，返回的是影响的行数。 14.2 获取数据库连接（connection）的 5 种方式 14.2.1 使用 Driver 类的connect()方法 12345678910//1. 注册驱动——加载Driver类Driver driver = new com.mysql.jdbc.Driver();//2. 获取连接——得到ConnectionString url = &quot;jdbc:mysql://localhost:13306/db01&quot;;Properties properties = new Properties();properties.setProperty(&quot;user&quot;,&quot;root&quot;);properties.setProperty(&quot;password&quot;,&quot;dimitre123&quot;);Connection connect = driver.connect(url, properties); 存在问题：Driver()是第三方工具，依赖强、灵活性差，且属于静态加载。 14.2.2 使用反射获取 Driver，再使用connect()方法 1234567891011//1. 注册驱动——加载Driver类Class&lt;?&gt; aClass = Class.forName(&quot;com.mysql.jdbc.Driver&quot;);Driver driver = (Driver)aClass.newInstance();//2. 获取连接——得到ConnectionString url = &quot;jdbc:mysql://localhost:13306/db01&quot;;Properties properties = new Properties();properties.setProperty(&quot;user&quot;,&quot;root&quot;);properties.setProperty(&quot;password&quot;,&quot;dimitre123&quot;);Connection connect = driver.connect(url, properties); 好处：动态加载，减少依赖性，更加灵活。 14.2.3 使用 DriverManager 注册驱动 12345678910111213//1. 注册驱动——加载Driver类Class&lt;?&gt; aClass = Class.forName(&quot;com.mysql.jdbc.Driver&quot;);Driver driver = (Driver)aClass.newInstance();//2. 获取连接——得到ConnectionString url = &quot;jdbc:mysql://localhost:13306/db01&quot;;Properties properties = new Properties();properties.setProperty(&quot;user&quot;,&quot;root&quot;);properties.setProperty(&quot;password&quot;,&quot;dimitre123&quot;);DriverManager.registerDriver(driver);//注册Driver驱动Connection connection = DriverManager.getConnection(url, properties); 14.2.4 Class.forName 自动完成注册驱动，简化代码 12345678910//1. 注册驱动——加载Driver类Class.forName(&quot;com.mysql.jdbc.Driver&quot;);//2. 获取连接——得到ConnectionString url = &quot;jdbc:mysql://localhost:13306/db01&quot;;Properties properties = new Properties();properties.setProperty(&quot;user&quot;,&quot;root&quot;);properties.setProperty(&quot;password&quot;,&quot;dimitre123&quot;);Connection connection = DriverManager.getConnection(url, properties); mysql 实现 java.sql 的 Driver 接口时，创建了静态代码块，自动执行DriverManager.registerDriver(new Driver()); 不写Class.forName(&quot;com.mysql.jdbc.Driver&quot;);也会正常运行，是因为 jdk1.5 之后使用了 jdbc4，会自动调用……\\libs\\mysql-connector-java-5.1.37-bin.jar!\\META-INF\\services\\java.sql.Driver文件中的类名（com.mysql.jdbc.Driver）去注册 14.2.5 使用配置文件读取连接地址等信息 1234user=rootpassword=dimitre123url=jdbc:mysql://localhost:13306/hsp_db02driver=com.mysql.jdbc.Driver 123456789101112Properties properties = new Properties();properties.load(new FileInputStream(&quot;src\\\\mysql.properties&quot;));String user = properties.getProperty(&quot;user&quot;);String password = properties.getProperty(&quot;password&quot;);String driver = properties.getProperty(&quot;driver&quot;);String url = properties.getProperty(&quot;url&quot;);//1. 注册驱动——加载Driver类Class.forName(driver);//2. 获取连接——得到ConnectionConnection connection = DriverManager.getConnection(url, user,password); 14.3 结果集（ResultSet） 含义：执行查 DQL 语句返回的查询结果。 ResultSet 对象保持一个光标指向其当前的数据行，最初光标位于第一行之前，next 方法将光标移动到下一行，并且由于再 ResultSet 对象中没有更多行时返回 false。可以使用 while 循环遍历。 使用：ReultSet resultSet = statement.excuteQuery(sql); 关闭结果集：resultSet.close(); 方法： 底层：ResultSet 是个接口，底层的数据是表是一个 ArrayList 14.4 SQL 注入 含义：利用某些系统没有对用户输入的数据进行充分的检查，而在用户输入数据中注入非法的 SQL 语句段或命令，恶意攻击数据库。 访问数据库、执行 sql 语句的三种方法： Statement对象【存在 sql 注入问题】 PreparedStatement对象【预处理】 CallableStatement对象【存储过程】 万能用户名：1' or，万能密码：or '1'='1' 14.5 PreparedStatement 预处理的使用 PreparedStatement 是 Statement 的子接口，可以使用 Statement 的方法（怎么使用的，Statement 的方法都是抽象的，是因为加载了 JDBCjar 包的原因吗？） 语法变化： 书写 sql 语句（定义 sql 字符串）：使用?代替参数值，作为占位符。 String sql = &quot;select name , pwd from admin where name =? and pwd = ?&quot;; 创建 PreparedStatement 对象：PreparedStatement preparedStatement = connection.prepareStatement(sql); Statement 此时不需要传入 sql 语句 使用 PreparedStatement 的对象给 sql 语句的占位符赋值：使用setXxx(int n, String str) n 表示?在 sql 语句中从左至右出现的次序，从 1 开始。 str 表示参数值。 调用excuteUpade()或excuteQuery()方法，返回结果集。不需要再传入 sql 语句当作参数。 预处理好处： 不再使用+拼接 sql 语句，减少了语法错误。 有效避免了 sql 注入问题 减少了编译次数，提高了效率 14.6 JDBC 事务 14.6.1 JDBC 事务介绍 JDBC 创建一个 Connection 对象后，默认是自动提交事务，执行成功后不能回滚。 调用 Connetcion 对象的setAutoCommit(false)可以取消自动提交事务。 调用 Connetcion 对象的commit()可以提交事务。 调用 Connetcion 对象的rollback()可以回滚事务。 14.6.2 应用场景 银行转账。 14.7 批处理（batch） 14.7.1 批处理介绍 java 的批量更新机制允许多条语句一次性提交给数据库处理，提高处理效率。相当于公交车拉一车人。 批处理方法（statement 对象调用）： addBatch()：添加需要批处理的 SQL 语句或参数 excuteBatch()：执行批处理语句 clearBatch()：清空批处理包的语句 使用： url 添加参数?rewriteBatchedStatements=true 1234567891011121314Connection connection = JDBCUtils.getConnection();String sql = &quot;insert into admin2 values(null, ?, ?)&quot;;PreparedStatement preparedStatement = connection.prepareStatement(sql);for (int i = 0; i &lt; 5000; i++) &#123;//5000执行 preparedStatement.setString(1, &quot;jack&quot; + i); preparedStatement.setString(2, &quot;666&quot;); preparedStatement.addBatch(); if((i + 1) % 1000 == 0) &#123; preparedStatement.executeBatch(); preparedStatement.clearBatch(); &#125;&#125;preparedStatement.executeBatch();//避免没到1000条的数据存在遗漏JDBCUtils.close(null, preparedStatement, connection); 源码分析： 执行addBatch()会创建 ArrayList - elementData =&gt; Object[] elementData =&gt; Object[] 就会存放我们预处理的 sql 语句 当 elementData 满后,就按照 1.5 扩容 当添加到指定的值后，就 executeBatch 批量处理会减少我们发送 sql 语句的网络开销，而且减少编译次数，因此效率提高 14.8 数据库连接池 14.8.1 传统方式连接数据库的问题 传统 JDBC 连接数据库使用 DriverManager 来获取，每次向数据库简历连接时，需要将 Connection 加载到内存中，再验证 ip 地址、用户名、密码（0.05~1s 时间），频繁进行数据库连接操作会占用系统资源，导致服务器崩溃。 每次连接数据库操作后，使用结束都应断开，如果程序出现异常而未关闭，将导致数据库内存泄漏，导致数据库重启 传统连接方式，不能控制创建的连接数量，如果连接过多，也可能导致内存泄漏，mysql 崩溃 使用数据库连接池（connection pool）技术，可以解决上述问题。 14.8.2 数据库连接池介绍 预先再缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”取出连接，使用完毕后放回到“缓冲池” 数据库连接池负责分配、管理、释放数据库的连接，它允许用用程序重复使用一个现有的数据库连接，而不用重新建立。 当应用程序向连接池请求的连接数量超过最大连接数量时，这些请求会被加入到等待队列中。 JDBC 的数据库连接池使用javax.sql.DataSource表示，DataSource 是一个接口，该接口由第三方提供实现【提供.jar】 数据库连接池种类： C3P0：速度较慢，稳定性好 DBCP：速度较快，但不稳定 Proxool：有监控连接池状态的功能，稳定性不如 C3P0 BoneCP：速度快 Druid（德鲁伊）：阿里提供，具有 DBCP、C3P0、Proxool 优点 14.8.3 C3P0 使用 将 c3p0 的 jar 包添加到开发环境，同 jdbc。 连接方式一： 12345678910111213141516171819202122232425//1. 创建一个数据源对象ComboPooledDataSource comboPooledDataSource = new ComboPooledDataSource();//2. 通过配置文件mysql.properties 获取相关连接的信息Properties properties = new Properties();properties.load(new FileInputStream(&quot;src\\\\mysql.properties&quot;));String user = properties.getProperty(&quot;user&quot;);String password = properties.getProperty(&quot;password&quot;);String url = properties.getProperty(&quot;url&quot;);String driver = properties.getProperty(&quot;driver&quot;);//给数据源 comboPooledDataSource 设置相关的参数comboPooledDataSource.setDriverClass(driver);comboPooledDataSource.setJdbcUrl(url);comboPooledDataSource.setUser(user);comboPooledDataSource.setPassword(password);//设置初始化连接数comboPooledDataSource.setInitialPoolSize(10);//最大连接数comboPooledDataSource.setMaxPoolSize(50);//测试连接池的效率, 测试对mysql 5000次操作for (int i = 0; i &lt; 5000; i++) &#123; Connection connection = comboPooledDataSource.getConnection(); connection.close();&#125; 连接方式二：使用 c3p0 的配置文件（整合了登录名、用户密码、连接地址、设置了连接数量等），放到 src 路径下 123456789101112131415161718192021222324&lt;c3p0-config&gt; &lt;named-config name=&quot;hello&quot;&gt; &lt;!-- 数据源名称，可以任意 --&gt; &lt;!-- 驱动类，跟数据库相关 --&gt; &lt;property name=&quot;driverClass&quot;&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;!-- url--&gt; &lt;property name=&quot;jdbcUrl&quot;&gt;jdbc:mysql://127.0.0.1:3306/girls&lt;/property&gt; &lt;!-- 用户名 --&gt; &lt;property name=&quot;user&quot;&gt;root&lt;/property&gt; &lt;!-- 密码 --&gt; &lt;property name=&quot;password&quot;&gt;root&lt;/property&gt; &lt;!-- 每次增长的连接数--&gt; &lt;property name=&quot;acquireIncrement&quot;&gt;5&lt;/property&gt; &lt;!-- 初始的连接数 --&gt; &lt;property name=&quot;initialPoolSize&quot;&gt;10&lt;/property&gt; &lt;!-- 最小连接数 --&gt; &lt;property name=&quot;minPoolSize&quot;&gt;5&lt;/property&gt; &lt;!-- 最大连接数 --&gt; &lt;property name=&quot;maxPoolSize&quot;&gt;10&lt;/property&gt; &lt;!-- 可连接的最多的命令对象数 --&gt; &lt;property name=&quot;maxStatements&quot;&gt;5&lt;/property&gt; &lt;!-- 每个连接对象可连接的最多的命令对象数 --&gt; &lt;property name=&quot;maxStatementsPerConnection&quot;&gt;2&lt;/property&gt; &lt;/named-config&gt;&lt;/c3p0-config&gt; 1234567ComboPooledDataSource comboPooledDataSource = new ComboPooledDataSource(&quot;hello&quot;);//测试5000次连接mysqlfor (int i = 0; i &lt; 500000; i++) &#123; Connection connection = comboPooledDataSource.getConnection(); connection.close();&#125; 14.8.4 Druid 使用 将 Druid 的 jar 包添加到开发环境，同 jdbc。 1234567891011121314#key=valuedriverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost:3306/girls?rewriteBatchedStatements=true#url=jdbc:mysql://localhost:3306/girlsusername=rootpassword=root#initial connection SizeinitialSize=10#min idle connecton sizeminIdle=5#max active connection sizemaxActive=20#max wait time (5000 mil seconds)maxWait=5000 1234567Properties properties = new Properties();properties.load(new FileInputStream(&quot;src\\\\druid.properties&quot;));DataSource dataSource = DruidDataSourceFactory.createDataSource(properties);for (int i = 0; i &lt; 500000; i++) &#123; Connection connection = dataSource.getConnection(); connection.close();&#125; 14.9 Apache 之 DBUtils 14.9.1 基本介绍 commons-dbutils 是 Apache 组织提供的一个开源 JDBC 工具类库，它是对 JDBC 的封装。简化 Dao 层的操作。 树结构： 作用：将数据库查询得到的数据放到一个 ArrayList 中，即使连接关闭了，也能使用这些数据。（正常情况下连接关闭，就访问不到这些数据）。 使用步骤： 得到连接（使用数据库连接池等方式） 将 common-dbutils 加入库文件 创建 QueryRunner 的对象QueryRunner queryRunner = new QueryRunner() 执行 DQL，返回 ArrayList 结果集 返回多个对象（查询多行）：queryRunner.query(connection, sql, new BeanListHandler&lt;&gt;(类名.class) 返回单个对象（查询单行）：queryRunner.query(connection, sql, new BeanHandler&lt;&gt;(类名.class) 返回单行单列对象（对象类型为 Object）：queryRunner.query(connection, sql, new ScalarHandler&lt;&gt;() 执行 DML，返回受影响的行数。 queryRunner.update(connection, sql, ?的值) 底层分析： 调用 query()方法时，底层会创建 PreparedStatement、ResultSet、 一个接收结果的 Object 对象 底层调用 handle(查询结果)，利用反射机制将查询到的结果传入到 Object 对象里 执行完毕关闭结果集、statement ResultSetSHandler 接口实现类的主要作用：ResultSetSHandler 用于处理 java.sql.ResultSet，将数据按要求转换为另一种形式。 14.9.2 模仿 dbutils，理解原理 1234567891011121314151617181920212223242526272829303132333435363738public ArrayList&lt;Actor&gt; testSelectToArrayList() &#123; //1. 得到连接 Connection connection = null; //2. 组织一个sql String sql = &quot;select * from actor where id &gt;= ?&quot;; PreparedStatement preparedStatement = null; ResultSet set = null; ArrayList&lt;Actor&gt; list = new ArrayList&lt;&gt;();//创建ArrayList 对象,存放actor 对象 //3. 创建PreparedStatement 对象 try &#123; connection = JDBCUtilsByDruid.getConnection(); preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, 1);//给?号赋值 //执行, 得到结果集 set = preparedStatement.executeQuery(); //遍历该结果集 while (set.next()) &#123; int id = set.getInt(&quot;id&quot;); String name = set.getString(&quot;name&quot;);//getName() String sex = set.getString(&quot;sex&quot;);//getSex() Date borndate = set.getDate(&quot;borndate&quot;); String phone = set.getString(&quot;phone&quot;); //把得到的resultset 的记录，封装到Actor 对象，放入到list 集合 list.add(new Actor(id, name, sex, borndate, phone)); &#125; System.out.println(&quot;list 集合数据=&quot; + list); for(Actor actor : list) &#123; System.out.println(&quot;id=&quot; + actor.getId() + &quot;\\t&quot; + actor.getName()); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭资源 JDBCUtilsByDruid.close(set, preparedStatement, connection); &#125; //因为ArrayList 和connection 没有任何关联，所以该集合可以复用. return list;&#125; 14.10 Dao 和增删改查方法 14.10.1 Dao 层及 BasicDao：与数据库的相关操作 Dao：data access object：数据访问对象 Dao 中的方法均为”单精度方法“——一个方法只干一件事。 Service 层中的方法可以是”非单精度方法“，一个方法可以干好几件事。 如 Dao 中的添加只是添加，Service 层中的添加还有先查询（判断是否重复）的功能，然后才是添加。 根据数据访问对象设计的类叫 Dao，高度抽象的该类叫 BasicDao。 在 BasicDao 的基础上，设计继承自其的实现特定与数据库操作的 Dao 类，可以更好完成开发任务。 BasicDao：可以设计成类、也可以设计成接口 14.10.2 domain/JavaBean/pojo 层：数据库对应的 java 类 14.10.3 service 层：具体业务层 14.10.3 view 层：页面层 14.11 封装 JDBC 工具类及使用 14.11.1 封装数据库连接功能 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class JDBCUtils &#123; //定义相关的属性(4个), 因为只需要一份，因此，我们做出static private static String user; //用户名 private static String password; //密码 private static String url; //url private static String driver; //驱动名 //在static代码块初始化 static &#123; try &#123; Properties properties = new Properties(); properties.load(new FileInputStream(&quot;src\\\\mysql.properties&quot;)); user = properties.getProperty(&quot;user&quot;); password = properties.getProperty(&quot;password&quot;); url = properties.getProperty(&quot;url&quot;); driver = properties.getProperty(&quot;driver&quot;); &#125; catch (IOException e) &#123; throw new RuntimeException(e); &#125; &#125; //连接数据库, 返回Connection public static Connection getConnection() &#123; try &#123; return DriverManager.getConnection(url, user, password); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; //关闭相关资源 /* 1. ResultSet 结果集 2. Statement 或者 PreparedStatement 3. Connection 4. 如果需要关闭资源，就传入对象，否则传入 null */ public static void close(ResultSet set, Statement statement, Connection connection) &#123; try &#123; if (set != null) &#123; set.close(); &#125; if (statement != null) &#123; statement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 1234user=rootpassword=dimitre123url=jdbc:mysql://localhost:13306/hsp_db02driver=com.mysql.jdbc.Driver 14.11.2 使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import org.junit.jupiter.api.Test;import java.sql.*;public class JDBCUtils_Use &#123; @Test public void testSelect() &#123;//select Connection connection = null; String sql = &quot;select * from actor where id = ?&quot;; PreparedStatement preparedStatement = null; ResultSet set = null; try &#123; connection = JDBCUtils.getConnection(); preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, 5); set = preparedStatement.executeQuery(); while (set.next()) &#123; int id = set.getInt(&quot;id&quot;); String name = set.getString(&quot;name&quot;); String sex = set.getString(&quot;sex&quot;); Date borndate = set.getDate(&quot;borndate&quot;); String phone = set.getString(&quot;phone&quot;); System.out.println(id + &quot;\\t&quot; + name + &quot;\\t&quot; + sex + &quot;\\t&quot; + borndate + &quot;\\t&quot; + phone); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.close(set, preparedStatement, connection); &#125; &#125; @Test public void testDML() &#123;//insert , update, delete Connection connection = null; String sql = &quot;update actor set name = ? where id = ?&quot;; PreparedStatement preparedStatement = null; try &#123; connection = JDBCUtils.getConnection(); preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, &quot;周星驰&quot;); preparedStatement.setInt(2, 4); preparedStatement.executeUpdate(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.close(null, preparedStatement, connection); &#125; &#125;&#125; 14.11.3 封装成 Druid 工具类——将连接功能使用线程池实现 123456789101112131415161718192021222324252627282930public class JDBCUtilsByDruid &#123; private static DataSource ds; static &#123; Properties properties = new Properties(); try &#123; properties.load(new FileInputStream(&quot;src\\\\druid.properties&quot;)); ds = DruidDataSourceFactory.createDataSource(properties); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static Connection getConnection() throws SQLException &#123; return ds.getConnection(); &#125; public static void close(ResultSet resultSet, Statement statement, Connection connection) &#123; try &#123; if (resultSet != null) &#123; resultSet.close(); &#125; if (statement != null) &#123; statement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 1234567891011121314#key=valuedriverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost:3306/girls?rewriteBatchedStatements=true#url=jdbc:mysql://localhost:3306/数据库名?rew……表示批处理username=rootpassword=root#initial connection SizeinitialSize=10#min idle connecton sizeminIdle=5#max active connection sizemaxActive=20#max wait time (5000 mil seconds)maxWait=5000 14.11.4 使用 Druid 工具类 1234567891011121314151617181920212223242526272829303132public class JDBCUtilsByDruid_USE &#123; public void testSelect() &#123; System.out.println(&quot;使用 druid方式完成&quot;); //1. 得到连接 Connection connection = null; //2. 组织一个sql String sql = &quot;select * from actor where id &gt;= ?&quot;; PreparedStatement preparedStatement = null; ResultSet set = null; //3. 创建PreparedStatement 对象 try &#123; connection = JDBCUtilsByDruid.getConnection(); System.out.println(connection.getClass());//运行类型 com.alibaba.druid.pool.DruidPooledConnection preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, 1);//给?号赋值 //执行, 得到结果集 set = preparedStatement.executeQuery(); while (set.next()) &#123; int id = set.getInt(&quot;id&quot;); String name = set.getString(&quot;name&quot;);//getName() String sex = set.getString(&quot;sex&quot;);//getSex() Date borndate = set.getDate(&quot;borndate&quot;); String phone = set.getString(&quot;phone&quot;); System.out.println(id + &quot;\\t&quot; + name + &quot;\\t&quot; + sex + &quot;\\t&quot; + borndate + &quot;\\t&quot; + phone); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtilsByDruid.close(set, preparedStatement, connection); &#125; &#125;&#125; JDBCUtilsByDruid 的 connection 类型是 DruidPooledConnection，当 connection 关闭时，只是将连接的引用断开，连接还是在连接池内。 JDBCUtils 的 connection 类型是 JDBC4connection，当 connection 关闭时，就真正实现了连接的关闭。 14.11.5 封装 DBUtils——实现关闭连接也能使用数据 第 15 章 正则表达式 常用正则表达式.txt 15.1 基本介绍 用某种模式去匹配字符串的一个公式。 15.2 快速入门 15.3 底层实现 15.4 语法规则 15.4.1 元字符 转义号\\\\（注意是两个\\代表一个\\）：需要转义的符号：.*+()/\\?[]^&#123;&#125; 字符匹配符： 选择匹配符： 限定符： 用于指定其前面的字符和组合项连续出现多少次 定位符： 规定要匹配的字符串出现的位置， 比如在字符串的开始还是在结束的位置 分组： 15.5 常用类 15.6 分组、捕获和反向引用 15.7 String 类中的正则 15.7.1 替换 public String replaceAll(String regex,String replacement) 15.7.2 判断 public boolean matches(String regex){} //使用 Pattern 和 Matcher 类 15.7.3 分割 public String[] split(String regex)","tags":[{"name":"Java","slug":"Java","permalink":"https://sk370.github.io/tags/Java/"}]},{"title":"MySQL","date":"2022-05-31T10:29:20.000Z","path":"2022/05/31/mysql/MySQL/","text":"MySQL 是最流行的关系型数据库管理系统，在 WEB 应用方面 MySQL 是最好的 RDBMS(Relational Database Management System：关系数据库管理系统)应用软件之一。 1. 数据库概述 1.1 数据库与数据库管理系统 1.1.1 数据库相关概念 数据库：（DB、Database），存储数据的仓库，本质是一个文件系统，保存了一系列有组织的数据。 数据库管理系统：（DBMS、Database Management System），是一种操纵和管理数据库的大型软件，用于建立、使用和维护数据库，对数据库进行统一管理和控制。用户通过数据库管理系统访问数据库中表内的数据。 图形化管理工具实际上不是数据库管理系统。它是代替了数据库管理系统功能的工具而已。 结构化查询语言：（SQL、Structured Query Language），用来与数据库通信的语言。 1.1.2 数据库、表、数据库管理系统的关系 1.1.3 常见数据库 1.2 MySQL 介绍 1.2.1 特点 开放源代码。6.x 版本之后分为社区版和商业版。 是一种关联数据库管理系统。 使用标准的 SQL 数据语言形式。 支持大型数据库，支持 5000 万条记录的数据仓库，32 位系统表文件最大可支持 4GB ，64 位系统支持最大的表文件为 8TB 。 1.2.2 发展历史 1995 年瑞典 MySQL AB 公司开发。 2008 年被 Sun 公司收购。 2009 年 Sun 被 Oracle 收购。MySQL 的开发者创建了 MySQL 的分支项目 MarialDB 2010 年，发布 5.5 版本 2013 年，发布 5.6 版本 2015 年，发布 5.7 版本 2016 年 9 月，发布 8.0 版本 1.2.3 常见数据库对象 1.3 RDBMS 与非 RDBMS 1.3.1 RDBMS 关系型数据库以行(row) 和列(column) 的形式存储数据，以便于用户理解。这一系列的行和列被称为表(table) ，一组表组成了一个库(database)。 表与表之间的数据记录有关系(relationship)。现实世界中的各种实体以及实体之间的各种联系均用关系模型来表示。关系型数据库，就是建立在关系模型基础上的数据库。 优点： 复杂查询：可以用 SQL 语句方便的在一个表以及多个表之间做非常复杂的数据查询。 事务支持：使得对于安全性能很高的数据访问要求得以实现。 1.3.2 非 RDBMS 基于键值对存储数据，不需要经过 SQL 层的解析， 性能非常高 NoSQL 泛指非关系型数据库，包括了榜单上的键值型数据库、文档型数据库、搜索引擎和列存储等，除此以外还包括图形数据库。 日志收集、排行榜、定时器等使用性能更高、成本更低的非关系型数据库是更明智的选择。 键值型数据库: 通过 Key-Value 键值的方式来存储数据，其中 Key 和 Value 可以是简单的对象，也可以是复杂的对象。Key 作为唯一的标识符，优点是查找速度快，但无法像关系型数据库一样使用条件过滤（比如 WHERE），如果不知道 key，就要遍历所有的键。 键值型数据库典型的使用场景是作为内存缓存。 Redis 是最流行的键值型数据库。 文档型数据库： 此类数据库可存放并获取文档，可以是 XML、JSON 等格式。 在数据库中文档作为处理信息的基本单位，一个文档就相当于一条记录。 文档数据库所存放的文档，就相当于键值数据库所存放的“值”。 MongoDB 是最流行的文档型数据库。 搜索引擎数据库： 搜索引擎数据库是应用在搜索引擎领域的数据存储形式，由于搜索引擎会爬取大量的数据，并以特定的格式进行存储，这样在检索的时候才能保证性能最优。核心原理是“倒排索引”。 列式数据库： 列式数据库是相对于行式存储的数据库，Oracle、MySQL、SQL Server 等数据库都是采用的行式存储（Row-based），而列式数据库是将数据按照列存储到数据库中，这样做的好处是可以大量降低系统的 I/O，适合于分布式文件系统，不足在于功能相对有限。 图形数据库： 利用了图这种数据结构存储了实体（对象）之间的关系。 数据模型主要是以节点和边（关系）来实现。 能高效地解决复杂的关系问题。 1.4 关系型数据库的设计规则 1.4.1 基本原则 关系型数据库的典型数据结构就是数据表，这些数据表的组成都是结构化的（Structured）。 将数据放到表中，表再放到库中。 一个数据库中可以有多个表，每个表都有一个名字，用来标识自己。表名具有唯一性。 表具有一些特性，这些特性定义了数据在表中如何存储，类似 Java 和 Python 中 “类”的设计。 1.4.2 相关概念 E-R（entity-relationship，实体-联系）模型：中有三个主要概念是： 实体集、属性、联系集。 一个实体集（class）对应于数据库中的一个表（table）。 一个实体（instance）对应于数据库表中的一行（row），也称为一条记录（record）。 一个属性（attribute）对应于数据库表中的一列（column），也称为一个字段（field）。 ORM 思想 (Object Relational Mapping)体现： 数据库中的一个表 &lt;—&gt; Java 或 Python 中的一个类 表中的一条数据 &lt;—&gt; 类中的一个对象（或实体） 表中的一个列 &lt;----&gt; 类中的一个字段、属性(field) 1.4.3 表的关联关系 一对一关联（one-to-one）： 外键唯一：主表的主键和从表的外键（唯一），形成主外键关系，外键唯一。 外键是主键：主表的主键和从表的主键，形成主外键关系。 一对对关系（one-to-many）： 在从表(多方)创建一个字段，字段作为外键指向主表(一方)的主键 多对多（many-to-many）： 要表示多对多关系，必须创建第三个表，该表通常称为联接表，它将多对多关系划分为两个一对多关系。将这两个表的主键都插入到第三个表中。 自我引用（Self reference）： 2. 环境搭建 02._MySQL 环境搭建.pdf 2.1 MySQL 环境搭建 2.1.1 8.x 版本 Development Machine（开发机器） ：该选项代表典型个人用桌面工作站。此时机器上需要运行多个应用程序，那么 MySQL 服务器将占用最少的系统资源。 Server Machine（服务器） ：该选项代表服务器，MySQL 服务器可以同其他服务器应用程序一起运行，例如 Web 服务器等。MySQL 服务器配置成适当比例的系统资源。 Dedicated Machine（专用服务器） ：该选项代表只运行 MySQL 服务的服务器。MySQL 服务器配置成使用所有可用系统资源。 配置环境变量：将MySQL Server 8.0\\bin添加到环境变量。 2.1.2 5.x 版本 5.x 版本的端口设置如图 两个版本的服务器密码均为 dimitre123 2.2 连接 mysql 数据库 mysql -h 主机名 -P 端口号 -u 用户名 -p密码 -p 与密码之间不能有空格，其他参数名与参数值之间可以有空格也可以没有空格。 输入完 p 后，按下回车，将密码在下一行输入，保证安全 客户端和服务器在同一台机器上，所以输入 localhost 或者 IP 地址 127.0.0.1。 连接本机-h 和主机名可以省略；如果端口号没有修改，-P 和 3306 也可以省略 退出登录：exit或quit 2.3 命令行操作 MySQL 服务 删除服务：sc delete 服务名 新建服务：mysqlld --install 服务名须在 mysql/bin 目录下执行 不写服务名时为 mysql 启动服务：net start 服务名 停止服务：net stop 服务名 2.4 修改 mysql5.x 默认字符集 为解决中文乱码以及默认字符集为latin1的问题，需要修改配置文件。 在 mysql 命令模式下查看编码：show variables like 'character_%' 查看比较规则：show variables like 'collation_%'; 修改 mysql5.7 配置文件：C:\\ProgramData\\MySQL\\MySQL Server 5.7.37\\my.ini 配置完后需要重新启动 mysql 服务 2.5 卸载 MySQL 停止 mysql 服务。 卸载软件。 清理残余文件。 清理服务目录：mysql 服务的安装目录。 数据目录：默认在 C:\\ProgramData\\MySQL 清理注册表： HKEY_LOCAL_MACHINE\\SYSTEM\\ControlSet001\\Services\\Eventlog\\Application\\MySQL服务 目录删除 HKEY_LOCAL_MACHINE\\SYSTEM\\ControlSet001\\Services\\MySQL服务 目录删除 HKEY_LOCAL_MACHINE\\SYSTEM\\ControlSet002\\Services\\Eventlog\\Application\\MySQL服务 目录删除 HKEY_LOCAL_MACHINE\\SYSTEM\\ControlSet002\\Services\\MySQL服务 目录删除 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Eventlog\\Application\\MySQL服务目录 删除 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\MySQL服务 删除 删除 mysql 环境变量。 2.6 图形化管理软件 2.6.1 Navicat 清除注册表.bat 定期清除注册表，一直试用 2.6.2 创建数据库 双击数据库，连接数据库服务。右键创建新数据库。 设置数据库格式 打开数据库新建表 设置表字段名称、类型，保存，设置表名称 在 user 表内添加数据 2.7 Linux 系统下安装 2.8 重置密码 通过任务管理器或者服务管理，关掉 mysqld(服务进程) 通过命令行+特殊参数开启mysqld mysqld --defaults-file=&quot;D:\\ProgramFiles\\mysql\\MySQLServer5.7Data\\my.ini&quot; --skip-grant-tables 此时，mysqld 服务进程已经打开。并且不需要权限检查 mysql -uroot 无密码登陆服务器。另启动一个客户端进行 修改权限表 （1） use mysql; （2）update user set authentication_string=password(‘新密 码’) where user=‘root’ and Host=‘localhost’; （3）flush privileges; 通过任务管理器，关掉 mysqld 服务进程。 再次通过服务管理，打开 mysql 服务。 8: 即可用修改后的新密码登陆。 3. SQL 概述 3.1 背景 由 IBM20 世纪 70 年代开发。 由美国国家标准局（ANSI）定制 SQL 标准。 SQL-92、SQL-99 是最重要的两个标准。 不同数据库生产厂商都支持 SQL 语句，但都有其特有内容。 3.2 SQL 语句分类 DDL：数据定义语句【create、drop、alter 表，库……】 DML：数据操作语句【增加 insert、修改 update、删除 delete】 DQL：数据查询语句【select】 DCL：数据控制语句【管理数据库：如用户权限提升 grant、移除 revoke、commit、rollback、savepoint】 3.3 SQL 语言规则与规范 3.3.1 基本规则 SQL 可以写在一行或者多行。为了提高可读性，各子句分行写，必要时使用缩进 每条命令以 ; 或 \\g 或 \\G 结束 图形化界面中，根据执行情况的不同，该规则较为灵活，但必须确保执行的语句上下文按规则结束。 关键字不能被缩写也不能分行 关于标点符号 必须保证所有的()、单引号、双引号是成对结束的 必须使用英文状态下的半角输入方式 字符串型和日期时间类型的数据可以使用单引号''表示 列的别名，尽量使用双引号&quot;&quot;，而且不建议省略 as 3.3.2 SQL 大小写规范 MySQL 在 Windows 环境下是大小写不敏感的（因为 windows 系统对大小写不敏感） MySQL 在 Linux 环境下是大小写敏感的 数据库名、表名、表的别名、变量名是严格区分大小写的 关键字、函数名、列名(或字段名)、列的别名(字段的别名) 是忽略大小写的。 推荐采用统一的书写规范： 数据库名、表名、表别名、字段名、字段别名等都小写 SQL 关键字、函数名、绑定变量等都大写 3.3.3 注释 单行注释：#注释文字 单行注释：-- 注释文字 -- 多行注释：/* 注释文字 */ 3.3.4 命名规则 数据库、表名不得超过 30 个字符，变量名限制为 29 个 必须只能包含 A–Z、 a–z、 0–9、_共 63 个字符 数据库名、表名、字段名等对象名中间不要包含空格 同一个 MySQL 软件中，数据库不能同名；同一个库中，表不能重名；同一个表中，字段不能重名 必须保证你的字段没有和保留字、数据库系统或常用方法冲突。如果坚持使用，请在 SQL 语句中使用````（着重号）引起来 保持字段名和类型的一致性，同一个数据库的不同表在命名字段并为其指定数据类型的时候一定要保证一致性。 4. 数据库操作 4.1 创建、使用数据库 CREATE DATABASE [IF NOT EXISTS] db_name [CHARSET xxx COLLATE xxx ……] [IF NOT EXISTS]（可选）：不写时数据库已存在则报错 [CHARSET xxx]（可选）：指定数据库字符集，默认 uft8 [COLLATE xxx]（可选）：指定数据库字符集校对规则，默认 utf8_general_ci，不区分大小写。 utf8_bin：区分大小写 表不指定字符集及字符集校对规则时，按照数据库规则执行。 db_name 与关键字冲突时，可采用反引号包裹。 使用数据库：USE 数据库名 show tables：选中数据库后执行此操作可以查看当前数据库下所有的表。 show tables from 数据库名;查看制定数据库下的所有表。 4.2 查看、删除数据库 显示数据库：SHOW DATABASES，查看当前数据库服务器中的所有数据库 显示数据库（使用全局函数）：SELECT DATABASE() 显示数据库的创建信息：SHOW CREATE DATABASE db_name 例：SHOW CREATE DATABASE db01 删除数据库：DROP DATABSE db_name DROP DATABASE IF EXISTS 数据库名; 4.3 备份、恢复数据库 备份数据库： （DOS 命令下）mysqldump -h 主机名 -P 端口号 -u 用户名 -p -B 数据库1 数据库2 &gt; 文件绝对路径\\文件名.sql 省略规则同开启 mysql 服务。 备份表：mysqldump -h 主机名 -P 端口号 -u 用户名 -p 数据库 表1 表2 &gt; 文件绝对路径\\文件名.sql 恢复数据库： 方式一：（mysql 环境下）source 文件绝对路径\\文件名.sql 方式二：（图形界面下） 文本模式打开要恢复的数据库文件，拷贝至图形化界面的查询语句输入区，点击运行 安装（导入）数据库：方式同恢复数据库。 4.4 修改数据库 更改数据库字符集：ALTER DATABASE 数据库名 CHARACTER SET 字符集; 5. 表（结构）操作——DDL 5.1 创建表 5.1.1 方式一 1234567CREATE TABLE [IF NOT EXISTS] 表名( 字段1, 数据类型 [约束条件] [默认值], 字段2, 数据类型 [约束条件] [默认值], 字段3, 数据类型 [约束条件] [默认值], …… [表约束条件])character set 字符集 collate 校对规则 engine 存储引擎; character、collate、engine 为可选内容 character set 与 charset 等价 存储引擎默认为 INNODB CREATE TABLE table_name1 LIKE table_name2;将 table_name2 的结构复制到 table_name1。 5.1.2 方式二 使用 AS subquery 选项，将创建表和插入数据结合起来。subquery 表示查询条件，例如： 12345CREATE TABLE dept80AS SELECT employee_id, last_name, salary*12 ANNSAL, hire_date FROM employees WHERE department_id = 80; 5.1.3 MySQL8 新特性：计算列 某一列的值是通过别的列计算得来的。例如，a 列值为 1、b 列值为 2，c 列不需要手动插入，定义 a+b 的结果为 c 的值，那么 c 就是计算列，是通过别的列计算得来的。 在 MySQL 8.0 中，CREATE TABLE 和 ALTER TABLE 中都支持增加计算列。 123456CREATE TABLE tb1( id INT, a INT, b INT, c INT GENERATED ALWAYS AS (a + b) VIRTUAL); 5.1.4 阿里 MySQL 字段命名规范 【强制】 表达是与否概念的字段，必须使用 is_xxx 的方式命名，数据类型是 unsigned tinyint（ 1 表示是， 0 表示否）。 说明： 任何字段如果为非负数，必须是 unsigned。 注意： POJO 类中的任何布尔类型的变量，都不要加 is 前缀，所以，需要在设置从 is_xxx 到 Xxx 的映射关系。数据库表示是与否的值，使用 tinyint 类型，坚持 is_xxx 的命名方式是为了明确其取值含义与取值范围。 正例： 表达逻辑删除的字段名 is_deleted， 1 表示删除， 0 表示未删除。 【强制】 表名、字段名必须使用小写字母或数字， 禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。 说明： MySQL 在 Windows 下不区分大小写，但在 Linux 下默认是区分大小写。因此，数据库名、表名、字段名，都不允许出现任何大写字母，避免节外生枝。 正例： aliyun_admin， rdc_config， level3_name 反例： AliyunAdmin， rdcConfig， level_3_name 【强制】 表名不使用复数名词。 说明： 表名应该仅仅表示表里面的实体内容，不应该表示实体数量，对应于 DO 类名也是单数形式，符合表达习惯。 【强制】 禁用保留字，如 desc、 range、 match、 delayed 等， 请参考 MySQL 官方保留字。 【强制】 主键索引名为 pk字段名；唯一索引名为 uk字段名； 普通索引名则为 idx字段名。 说明： pk 即 primary key； uk* 即 unique key； idx* 即 index 的简称。 【强制】 小数类型为 decimal，禁止使用 float 和 double。 说明： 在存储的时候， float 和 double 都存在精度损失的问题，很可能在比较值的时候，得到不正确的结果。如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数并分开存储。 【强制】 如果存储的字符串长度几乎相等，使用 char 定长字符串类型。 【强制】 varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长度大于此值，定义字段类型为 text，独立出来一张表，用主键来对应，避免影响其它字段索引效率。 【强制】 表必备三字段： id, gmt_create, gmt_modified。 说明： 其中 id 必为主键，类型为 bigint unsigned、单表时自增、步长为 1。 gmt_create, gmt_modified 的类型均为 datetime 类型，前者现在时表示主动式创建，后者过去分词表示被动式更新。 【推荐】 表的命名最好是遵循“业务名称_表的作用” 。 正例： alipay_task / force_project / trade_config 【推荐】 库名与应用名称尽量一致。 【推荐】 如果修改字段含义或对字段表示的状态追加时，需要及时更新字段注释。 【推荐】 字段允许适当冗余，以提高查询性能，但必须考虑数据一致。冗余字段应遵循： 1） 不是频繁修改的字段。 2） 不是唯一索引的字段。 3） 不是 varchar 超长字段，更不能是 text 字段。 正例： 各业务线经常冗余存储商品名称，避免查询时需要调用 IC 服务获取。 【推荐】 单表行数超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表。 说明： 如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。 【参考】 合适的字符存储长度，不但节约数据库表空间、节约索引存储，更重要的是提升检索速度。 正例： 无符号值可以避免误存负数， 且扩大了表示范围。 5.2 删除表 在 MySQL 中，当一张数据表 没有与其他任何数据表形成关联关系 时，可以将当前数据表直接删除。 数据和结构都被删除 所有正在运行的相关事务被提交 所有相关索引被删除 DROP TABLE table_name MySQL8 由于使用了 InnoDB 引擎，所以删除多个表时，也支持事务的完整性（即 DDL 操作要么全部成功要么回滚）。 5.3 备份、恢复表 备份表：mysqldump -h 主机名 -P 端口号 -u 用户名 -p 数据库 表1 表2 &gt; 文件绝对路径\\文件名.sql 恢复表：同数据库恢复操作 5.4 修改表 5.4.1 添加列 ALTER TABLE table_name ADD (column datatype [NOT NULL DEFAULT 'XXX']) AFTER TARGETCOLUMN; 在目标列TARGETCOLUMN后添加指定列，目标列可以大写，也可以小写。 可以一次添加多列（使用()包裹），添加一列时，可以不使用括号。 不指定添加在哪一列后面时，默认添加到最后一列 5.4.2 修改列 ALTER TABLE table_name MODIFY(TARGETCOLUMN datatype [NOT NULL DEFAULT 'XXX']); 可以修改列的数据类型，长度、默认值和位置。 对默认值的修改只影响今后对表的修改。 5.4.3 重命名列 ALTER TABLE table_name CHANGE 旧名 新名 datatype 5.4.4 删除列 ALTER TABLE 表名 DROP 【COLUMN】字段名 5.4.5 修改表名 方式一：RENAME table_name TO new_name; 方式二：ALTER table dept RENAME [TO] detail_dept; -- [TO]可以省略 5.4.6 修改表字符集 ALTER TABLE table_name CHARSET utf8 5.4.7 修改表存储引擎 ALETER TABLE table_name ENGINE = 引擎类型 5.5 显示表结构 DESCRIBE employees;或 DESC employees; Field：表示字段名称。 Type：表示字段类型。 Null：表示该列是否可以存储 NULL 值。 Key：表示该列是否已编制索引。PRI 表示该列是表主键的一部分；UNI 表示该列是 UNIQUE 索引的一部分；MUL 表示在列中某个给定值允许出现多次。 Default：表示该列是否有默认值，如果有，那么值是多少。 Extra：表示可以获取的与给定列有关的附加信息，例如 AUTO_INCREMENT 等。 5.6 清空表 删除表中所有的数据 释放表的存储空间 TRUNCATE TABLE detail_dept; 对比的是delete from table_name 6. 常用数据类型（列类型） 其中，常用的积累数据类型如下： MySQL 数据类型.mmap 6.1 整数类型 6.1.1 类型介绍 整数类型一共有 5 种，包括 TINYINT、SMALLINT、MEDIUMINT、INT（INTEGER）和 BIGINT，区别如下： 6.1.2 可选属性 6.1.2.1 宽度 M M 的取值范围是(0, 255)。例如，int(5)：当数据宽度小于 5 位的时候在数字前面需要用字符填满宽度。该项功能需要配合“ ZEROFILL ”使用，表示用“0”填满宽度，否则指定显示宽度无效。 上图中可以看出，f3 的实际数据位数（123）不足 5 位时，前面使用 0 进行了补齐。当实际数据位数（123456）超出 5 位时，按照实际数据显示。 从 MySQL 8.0.17 开始，整数数据类型不推荐使用显示宽度属性。 之前的版本中，整型数据类型可以在定义表结构时指定所需要的显示宽度，如果不指定，则系统为每一种类型指定默认的宽度值 。 查看表结构 （MySQL5.7 中显式如下，MySQL8 中不再显式范围） TINYINT 有符号数和无符号数的取值范围分别为-128~127 和 0~255，由于负号占了一个数字位，因此 TINYINT 默认的显示宽度为 4。同理，其他整数类型的默认显示宽度与其有符号数的最小值的宽度相同。 查看表结构 （MySQL8 整数类型已经不显示范围） 6.1.2.2 UNSIGNED UNSIGNED ：无符号类型（非负），所有的整数类型都有一个可选的属性 UNSIGNED（无符号属性），无符号整数类型的最小取值为 0。所以，如果需要在 MySQL 数据库中保存非负整数值时，可以将整数类型设置为无符号类型。 int 类型默认显示宽度为 int(11)，无符号 int 类型默认显示宽度为 int(10)——MySQL5.7，MySQL8 不显示。 6.1.2.3 ZEROFILL ZEROFILL：0 填充,（如果某列是 ZEROFILL，那么 MySQL 会自动为当前列添加 UNSIGNED 属性），如果指定了 ZEROFILL 只是表示不够 M 位时，用 0 在左边填充，如果超过 M 位，只要不超过数据存储范围即可。 在 int(M) 中，M 的值跟 int(M) 所占多少存储空间并无任何关系。 int(3)、int(4)、int(8) 在磁盘上都是占用 4 bytes 的存储空间。 mysql5 和 mysql8 显示结果相同，均如上。当使用 ZEROFILL 时，自动添加 UNSIGNED。 6.1.3 适用场景 TINYINT ：一般用于枚举数据，比如系统设定取值范围很小且固定的场景。如性别，男 1，女 0。 SMALLINT ：可以用于较小范围的统计数据，比如统计工厂的固定资产库存数量等。 MEDIUMINT ：用于较大整数的计算，比如车站每日的客流量等。 INT、INTEGER ：取值范围足够大，一般情况下不用考虑超限问题，用得最多。比如商品编号。 BIGINT ：只有当你处理特别巨大的整数时才会用到。比如双十一的交易量、大型门户网站点击量、证券公司衍生产品持仓等。 6.1.4 选择依据 在评估用哪种整数类型的时候，需要考虑存储空间和可靠性的平衡问题：一方面，用占用字节数少的整数类型可以节省存储空间；另一方面，要是为了节省存储空间， 使用的整数类型取值范围太小，一旦遇到超出取值范围的情况，就可能引起系统错误 ，影响可靠性。 举个例子，商品编号采用的数据类型是 INT。原因就在于，客户门店中流通的商品种类较多，而且，每天都有旧商品下架，新商品上架，这样不断迭代，日积月累。 如果使用 SMALLINT 类型，虽然占用字节数比 INT 类型的整数少，但是却不能保证数据不会超出范围 65535。相反，使用 INT，就能确保有足够大的取值范围，不用担心数据超出范围影响可靠性的问题。 在实际工作中，系统故障产生的成本远远超过增加几个字段存储空间所产生的成本。因此，建议首先确保数据不会超过取值范围，在这个前提之下，再去考虑如何节省存储空间。 总结：可靠性优先，存储空间次之。 6.2 浮点类型 6.2.1 类型介绍 浮点数和定点数类型的特点是可以 处理小数，整数可以看成小数的一个特例。因此，浮点数和定点数的使用场景，比整数大多了。 MySQL 支持的浮点数类型，分别是 FLOAT、DOUBLE、REAL。 REAL 默认就是 DOUBLE。如果把 SQL 模式设定为启用 REAL_AS_FLOAT，那 么，MySQL 就认为 REAL 是 FLOAT。 启用 REAL_AS_FLOAT：SET sql_mode = &quot;REAL_AS_FLOAT&quot;; 问题 1：FLOAT 和 DOUBLE 这两种数据类型的区别是啥呢？ FLOAT 占用字节数少，取值范围小；DOUBLE 占用字节数多，取值范围也大。 问题 2：为什么浮点数类型的无符号数取值范围，只相当于有符号数取值范围的一半，也就是只相当于有符号数取值范围大于等于零的部分呢？ MySQL 存储浮点数的格式为： 符号(S) 、 尾数(M) 和 阶码(E) 。因此，无论有没有符号，MySQL 的浮点数都会存储表示符号的部分。因此， 所谓的无符号数取值范围，其实就是有符号数取值范围大于等于零的部分。 6.2.2 数据精度说明 对于浮点类型，在 MySQL 中单精度值使用 4 个字节，双精度值使用 8 个字节。 MySQL 允许使用 非标准语法（其他数据库未必支持，因此如果涉及到数据迁移，则最好不要这么用）： FLOAT(M,D) 或 DOUBLE(M,D) 。这里，M 称为 精度 ，D 称为 标度 。(M,D)中 M=整数位+小数位，D=小数位。 D&lt;=M&lt;=255，0&lt;=D&lt;=30。 例如，定义为 FLOAT(5,2)的一个列可以显示为-999.99-999.99。如果超过这个范围会报错。 FLOAT 和 DOUBLE 类型在不指定(M,D)时，默认会按照实际的精度（由实际的硬件和操作系统决定）来显示。 浮点类型也可以加 UNSIGNED ，但是不会改变数据范围，例如：FLOAT(3,2) UNSIGNED仍然只能表示 0-9.99 的范围。 不管是否显式设置了精度(M,D)，这里 MySQL 的处理方案如下： 如果存储时，整数部分超出了范围，MySQL 就会报错。 如果存储时，小数点部分若超出范围，就分以下情况： 若四舍五入后，整数部分没有超出范围，则只警告，但能成功操作并四舍五入删除多余的小数位后保存。例如在 FLOAT(5,2)列内插入 999.009，近似结果是 999.01。 若四舍五入后，整数部分超出范围，则 MySQL 报错，并拒绝处理。如 FLOAT(5,2)列内插入 999.995 和-999.995 都会报错。 从 MySQL 8.0.17 开始，FLOAT(M,D) 和 DOUBLE(M,D)用法在官方文档中已经明确不推荐使用，将来可能被移除。另外，关于浮点型 FLOAT 和 DOUBLE 的 UNSIGNED 也不推荐使用了，将来也可能被移除。 6.2.3 精度误差说明 浮点数类型有个缺陷，就是不精准。 MySQL 用 4 个字节存储 FLOAT 类型数据，用 8 个字节来存储 DOUBLE 类型数据。无论哪个，都是采用二进制的方式来进行存储的。比如 9.625，用二进制来表达，就是 1001.101，或者表达成 1.001101×2^3。如果尾数不是 0 或 5（比如 9.624），你就无法用一个二进制数来精确表达。进而，就只好在取值允许的范围内进行四舍五入。 在编程中，如果用到浮点数，要特别注意误差问题，因为浮点数是不准确的，所以要避免使用“=”来判断两个数是否相等。同时，在一些对精确度要求较高的项目中，千万不要使用浮点数，不然会导致结果错误，甚至是造成不可挽回的损失。那么，MySQL 有没有精准的数据类型呢？当然有，这就是定点数类型： DECIMAL 。 6.3 定点数类型 6.3.1 类型介绍 MySQL 中的定点数类型只有 DECIMAL 一种类型。 使用 DECIMAL(M,D) 的方式表示高精度小数。其中，M 被称为精度，D 被称为标度。0&lt;=M&lt;=65，0&lt;=D&lt;=30，D&lt;M。例如，定义 DECIMAL（5,2）的类型，表示该列取值范围是-999.99~999.99。 DECIMAL(M,D)的最大取值范围与 DOUBLE 类型一样，但是有效的数据范围是由 M 和 D 决定的。DECIMAL 的存储空间并不是固定的，由精度值 M 决定，总共占用的存储空间为 M+2 个字节。也就是说，在一些对精度要求不高的场景下，比起占用同样字节长度的定点数，浮点数表达的数值范围可以更大一些。 定点数在 MySQL 内部是以 字符串 的形式进行存储，这就决定了它一定是精准的。 当 DECIMAL 类型不指定精度和标度时，其默认为 DECIMAL(10,0)。当数据的精度超出了定点数类型的精度范围时，则 MySQL 同样会进行四舍五入处理。 浮点数 vs 定点数： 浮点数相对于定点数的优点是在长度一定的情况下，浮点类型取值范围大，但是不精准，适用于需要取值范围大，又可以容忍微小误差的科学计算场景（比如计算化学、分子建模、流体动力学等） 定点数类型取值范围相对小，但是精准，没有误差，适合于对精度要求极高的场景 （比如涉及金额计算的场景） 6.4 位类型 BIT 类型中存储的是二进制值，类似 010110。 BIT 类型，如果没有指定(M)，默认是 1 位。这个 1 位，表示只能存 1 位的二进制值。这里(M)是表示二进制的位数，位数最小值为 1，最大值为 64 。 使用 SELECT 命令查询位字段时，可以用 BIN() 或 HEX() 函数进行读取。 使用 b+0 查询数据时，可以直接查询出存储的十进制数据的值。 6.5 日期与时间类型 6.5.1 基本介绍 MySQL 有多种表示日期和时间的数据类型，不同的版本可能有所差异，MySQL8.0 版本支持的日期和时间类型主要有：YEAR 类型、TIME 类型、DATE 类型、DATETIME 类型和 TIMESTAMP 类型。 YEAR 类型通常用来表示年 DATE 类型通常用来表示年、月、日 TIME 类型通常用来表示时、分、秒 DATETIME 类型通常用来表示年、月、日、时、分、秒 TIMESTAMP 类型通常用来表示带时区的年、月、日、时、分、秒 为什么时间类型 TIME 的取值范围不是 -23:59:59 ～ 23:59:59 呢？原因是 MySQL 设计的 TIME 类型，不光表示一天之内的时间，而且可以用来表示一个时间间隔，这个时间间隔可以超过 24 小时。 6.5.2 YEAR 类型 YEAR 类型用来表示年份，在所有的日期时间类型中所占用的存储空间最小，只需要 1 个字节的存储空间。 在 MySQL 中，YEAR 有以下几种存储格式： 以 4 位字符串或数字格式表示 YEAR 类型，其格式为 YYYY，最小值为 1901，最大值为 2155。 以 2 位字符串格式表示 YEAR 类型，最小值为 00，最大值为 99。 当取值为 01 到 69 时，表示 2001 到 2069； 当取值为 70 到 99 时，表示 1970 到 1999； 当取值整数的 0 或 00 添加的话，那么是 0000 年； 当取值是日期/字符串的’0’添加的话，是 2000 年。 从 MySQL5.5.27 开始，2 位格式的 YEAR 已经不推荐使用。YEAR 默认格式就是 YYYY，没必要写成 YEAR(4)，从 MySQL 8.0.19 开始，不推荐使用指定显示宽度的 YEAR(4)数据类型。 6.5.3 DATE 类型 DATE 类型表示日期，没有时间部分，格式为 YYYY-MM-DD ，其中，YYYY 表示年份，MM 表示月份，DD 表示日期。需要 3 个字节 的存储空间。在向 DATE 类型的字段插入数据时，同样需要满足一定的格式条件。 以 YYYY-MM-DD 格式或者 YYYYMMDD 格式表示的字符串日期，其最小取值为 1000-01-01，最大取值为 9999-12-03。YYYYMMDD 格式会被转化为 YYYY-MM-DD 格式。 以 YY-MM-DD 格式或者 YYMMDD 格式表示的字符串日期，此格式中，年份为两位数值或字符串满足 YEAR 类型的格式条件为：当年份取值为 00 到 69 时，会被转化为 2000 到 2069；当年份取值为 70 到 99 时，会被转化为 1970 到 1999。 使用 CURRENT_DATE() 或者 NOW() 函数，会插入当前系统的日期。 6.5.4 TIME 类型 TIME 类型用来表示时间，不包含日期部分。在 MySQL 中，需要 3 个字节 的存储空间来存储 TIME 类型的数据，可以使用“HH:MM:SS”格式来表示 TIME 类型，其中，HH 表示小时，MM 表示分钟，SS 表示秒。 在 MySQL 中，向 TIME 类型的字段插入数据时，也可以使用几种不同的格式。 可以使用带有冒号的字符串，比如'D HH:MM:SS' 、'HH:MM:SS'、'HH:MM'、'D HH:MM'、'D HH'或'SS'格式，都能被正确地插入 TIME 类型的字段中。 其中 D 表示天，其最小值为 0，最大值为 34。 如果使用带有 D 格式的字符串插入 TIME 类型的字段时，D 会被转化为小时，计算格式为 D*24+HH。 当使用带有冒号并且不带 D 的字符串表示时间时，表示当天的时间，比如 12:10 表示 12:10:00，而不是 00:12:10。 可以使用不带有冒号的字符串或者数字，格式为’ HHMMSS '或者 ‘HHMMSS’。如果插入一个不合法的字符串或者数字，MySQL 在存储数据时，会将其自动转化为 00:00:00 进行存储。 比如 1210，MySQL 会将最右边的两位解析成秒，表示 00:12:10，而不是 12:10:00。 使用 CURRENT_TIME() 或者 NOW() ，会插入当前系统的时间。 6.5.5 DATETIME 类型 DATETIME 类型在所有的日期时间类型中占用的存储空间最大，总共需要 8 个字节的存储空间。在格式上为 DATE 类型和 TIME 类型的组合，可以表示为 YYYY-MM-DD HH:MM:SS ，其中 YYYY 表示年份，MM 表示月份，DD 表示日期，HH 表示小时，MM 表示分钟，SS 表示秒。 在向 DATETIME 类型的字段插入数据时，同样需要满足一定的格式条件。 以 YYYY-MM-DD HH:MM:SS 格式或者 YYYYMMDDHHMMSS 格式的字符串插入 DATETIME 类型的字段时，最小值为 1000-01-01 00:00:00，最大值为 9999-12-03 23:59:59。 以 YYYYMMDD HHMMSS 格式的数字插入 DATETIME 类型的字段时，会被转化为 YYYY-MM-DDHH:MM:SS 格式。 以 YY-MM-DD HH:MM:SS 格式或者 YYMMDDHHMMSS 格式的字符串插入 DATETIME 类型的字段时，两位数的年份规则符合 YEAR 类型的规则，00 到 69 表示 2000 到 2069；70 到 99 表示 1970 到 1999。 使用函数 CURRENT_TIMESTAMP() 和 NOW() ，可以向 DATETIME 类型的字段插入系统的当前日期和时间。 6.5.6 TIMESTAMP 类型 TIMESTAMP 类型也可以表示日期时间，其显示格式与 DATETIME 类型相同，都是 YYYY-MM-DD HH:MM:SS ，需要 4 个字节的存储空间。但是 TIMESTAMP 存储的时间范围比 DATETIME 要小很多，只能存储 1970-01-01 00:00:01 UTC 到 2038-01-19 03:14:07 UTC 之间的时间。其中，UTC 表示世界统一时间，也叫作世界标准时间。 存储数据的时候需要对当前时间所在的时区进行转换，查询数据的时候再将时间转换回当前的时区。因此，使用 TIMESTAMP 存储的同一个时间值，在不同的时区查询时会显示不同的时间。 向 TIMESTAMP 类型的字段插入数据时，当插入的数据格式满足 YY-MM-DD HH:MM:SS 和 YYMMDDHHMMSS 时，两位数值的年份同样符合 YEAR 类型的规则条件，只不过表示的时间范围要小很多。 如果向 TIMESTAMP 类型的字段插入的时间超出了 TIMESTAMP 类型的范围，则 MySQL 会抛出错误信息。 6.5.7 TIMESTAMP 和 DATETIME 的区别 TIMESTAMP 存储空间比较小，表示的日期时间范围也比较小 底层存储方式不同，TIMESTAMP 底层存储的是毫秒值，距离 1970-1-1 0:0:0 0 毫秒的毫秒值。 两个日期比较大小或日期计算时，TIMESTAMP 更方便、更快。 TIMESTAMP 和时区有关。TIMESTAMP 会根据用户的时区不同，显示不同的结果。而 DATETIME 则只能反映出插入时当地的时区，其他时区的人查看数据必然会有误差。 用得最多的日期时间类型，就是 DATETIME 。虽然 MySQL 也支持 YEAR（年）、 TIME（时间）、DATE（日期），以及 TIMESTAMP 类型，但是在实际项目中，尽量用 DATETIME 类型。因为这个数据类型包括了完整的日期和时间信息，取值范围也最大，使用起来比较方便。毕竟，如果日期时间信息分散在好几个字段，很不容易记，而且查询的时候，SQL 语句也会更加复杂。 但是，一般存注册时间、商品发布时间等，不建议使用 DATETIME 存储，而是使用时间戳 ，因为 DATETIME 虽然直观，但不便于计算。 6.6 文本字符串类型 6.6.1 类型介绍 MySQL 中，文本字符串总体上分为 CHAR 、 VARCHAR 、 TINYTEXT 、 TEXT 、 MEDIUMTEXT 、LONGTEXT 、 ENUM 、 SET 等类型。 6.6.2 CHAR 与 VARCHAR 类型 CHAR 类型： CHAR(M) 类型一般需要预先定义字符串长度。如果不指定(M)，则表示长度默认是 1 个字符。 如果保存时数据的实际长度比 CHAR 类型声明的长度小，则会在右侧填充空格以达到指定的长度。当 MySQL 检索 CHAR 类型的数据时，CHAR 类型的字段会去除尾部的空格。 定义 CHAR 类型字段时，声明的字段长度即为 CHAR 类型字段所占的存储空间的字节数。 VARCHAR 类型： VARCHAR(M) 定义时， 必须指定 长度 M，否则报错。 MySQL4.0 版本以下，varchar(20)：指的是 20 字节，如果存放 UTF8 汉字时，只能存 6 个（每个汉字 3 字节） ；MySQL5.0 版本以上，varchar(20)：指的是 20 字符。 检索 VARCHAR 类型的字段数据时，会保留数据尾部的空格。VARCHAR 类型的字段所占用的存储空间为字符串实际长度加 1 个字节。 哪些情况使用 CHAR 或 VARCHAR 更好？ 类型 特点 空间上 时间上 适用场景 CHAR(M) 固定长度 浪费存储空间 效率高 存储不大，速度要求高 VARCHAR(M) 可变长度 节省存储空间 效率低 非 CHAR 的情况 情况 1：存储很短的信息。比如门牌号码 101，201……这样很短的信息应该用 char，因为 varchar 还要占个 byte 用于存储信息长度，本来打算节约存储的，结果得不偿失。 情况 2：固定长度的。比如使用 uuid 作为主键，那用 char 应该更合适。因为他固定长度，varchar 动态根据长度的特性就消失了，而且还要占个长度信息。 情况 3：十分频繁改变的 column。因为 varchar 每次存储都要有额外的计算，得到长度等工作，如果一个非常频繁改变的，那就要有很多的精力用于计算，而这些对于 char 来说是不需要的。 情况 4：具体存储引擎中的情况： MyISAM 数据存储引擎和数据列：MyISAM 数据表，最好使用固定长度(CHAR)的数据列代替可变长度(VARCHAR)的数据列。这样使得整个表静态化，从而使 数据检索更快 ，用空间换时间。 MEMORY 存储引擎和数据列：MEMORY 数据表目前都使用固定长度的数据行存储，因此无论使用 CHAR 或 VARCHAR 列都没有关系，两者都是作为 CHAR 类型处理的。 InnoDB 存储引擎，建议使用 VARCHAR 类型。因为对于 InnoDB 数据表，内部的行存储格式并没有区分固定长度和可变长度列（所有数据行都使用指向数据列值的头指针），而且主要影响性能的因素是数据行使用的存储总量。由于 char 平均占用的空间多于 varchar，所以除了简短并且固定长度的，其他考虑 varchar。这样节省空间，对磁盘 I/O 和数据存储总量比较好。 6.6.3 TEXT 类型 在 MySQL 中，TEXT 用来保存文本类型的字符串，总共包含 4 种类型，分别为 TINYTEXT、TEXT、MEDIUMTEXT 和 LONGTEXT 类型。 在向 TEXT 类型的字段保存和查询数据时，系统自动按照实际长度存储，不需要预先定义长度。这一点和 VARCHAR 类型相同。 由于实际存储的长度不确定，MySQL 不允许 TEXT 类型的字段做主键。 在保存和查询数据时，并没有删除 TEXT 类型的数据尾部的空格。 TEXT 文本类型，可以存比较大的文本段，搜索速度稍慢，因此如果不是特别大的内容，建议使用 CHAR，VARCHAR 来代替。 TEXT 类型不用加默认值，加了也没用。而且 text 和 blob 类型的数据删除后容易导致“空洞”，使得文件碎片比较多，所以频繁使用的表不建议包含 TEXT 类型字段，建议单独分出去，单独用一个表。 6.7 ENUM 类型 ENUM 类型也叫作枚举类型，ENUM 类型的取值范围需要在定义字段时进行指定。设置字段值时，ENUM 类型只允许从成员中选取单个值，不能一次选取多个值。 其所需要的存储空间由定义 ENUM 类型时指定的成员个数决定。 当 ENUM 类型包含 1 ～ 255 个成员时，需要 1 个字节的存储空间； 当 ENUM 类型包含 256 ～ 65535 个成员时，需要 2 个字节的存储空间。 ENUM 类型的成员个数的上限为 65535 个。 12345678910111213141516CREATE TABLE test_enum( season ENUM(&#x27;春&#x27;,&#x27;夏&#x27;,&#x27;秋&#x27;,&#x27;冬&#x27;,&#x27;unknow&#x27;));# 忽略大小写INSERT INTO test_enumVALUES(&#x27;UNKNOW&#x27;);# 允许按照角标的方式获取指定索引位置的枚举值INSERT INTO test_enumVALUES(&#x27;1&#x27;),(3);# Data truncated for column &#x27;season&#x27; at row 1INSERT INTO test_enumVALUES(&#x27;ab&#x27;);# 当ENUM类型的字段没有声明为NOT NULL时，插入NULL也是有效的INSERT INTO test_enumVALUES(NULL); 6.8 SET 类型 SET 表示一个字符串对象，可以包含 0 个或多个成员，但成员个数的上限为 64 。设置字段值时，可以取取值范围内的 0 个或多个值。 当 SET 类型包含的成员个数不同时，其所占用的存储空间也是不同的，具体如下： SET 类型在存储数据时成员个数越多，其占用的存储空间越大。注意：SET 类型在选取成员时，可以一次选择多个成员，这一点与 ENUM 类型不同。 12345678CREATE TABLE test_set( s SET (&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;));#插入重复的SET类型成员时，MySQL会自动删除重复的成员INSERT INTO test_set (s) VALUES (&#x27;A,B,C,A&#x27;);#向SET类型的字段插入SET成员中不存在的值时，MySQL会抛出错误。INSERT INTO test_set (s) VALUES (&#x27;A,B,C,D&#x27;); 1234567891011CREATE TABLE temp_mul(gender ENUM(&#x27;男&#x27;,&#x27;女&#x27;),hobby SET(&#x27;吃饭&#x27;,&#x27;睡觉&#x27;,&#x27;打豆豆&#x27;,&#x27;写代码&#x27;));INSERT INTO temp_mul VALUES(&#x27;男&#x27;,&#x27;睡觉,打豆豆&#x27;); #成功# Data truncated for column &#x27;gender&#x27; at row 1INSERT INTO temp_mul VALUES(&#x27;男,女&#x27;,&#x27;睡觉,写代码&#x27;); #失败# Data truncated for column &#x27;gender&#x27; at row 1INSERT INTO temp_mul VALUES(&#x27;妖&#x27;,&#x27;睡觉,写代码&#x27;);#失败INSERT INTO temp_mul VALUES(&#x27;男&#x27;,&#x27;睡觉,写代码,吃饭&#x27;); #成功 6.9 二进制字符串类型 MySQL 中的二进制字符串类型主要存储一些二进制数据，比如可以存储图片、音频和视频等二进制数据。 MySQL 中支持的二进制字符串类型主要包括 BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB 类型。 6.9.1 BINARY 与 VARBINARY 类型 BINARY 和 VARBINARY 类似于 CHAR 和 VARCHAR，只是它们存储的是二进制字符串。 BINARY (M)为固定长度的二进制字符串，M 表示最多能存储的字节数，取值范围是 0~255 个字符。如果未指定(M)，表示只能存储 1 个字节 。例如 BINARY (8)，表示最多能存储 8 个字节，如果字段值不足(M)个字节，将在右边填充’\\0’以补齐指定长度。 VARBINARY (M)为可变长度的二进制字符串，M 表示最多能存储的字节数，总字节数不能超过行的字节长度限制 65535，另外还要考虑额外字节开销，VARBINARY 类型的数据除了存储数据本身外，还需要 1 或 2 个字节来存储数据的字节数。VARBINARY 类型 必须指定(M) ，否则报错。 6.9.2 BLOB 类型 BLOB 是一个 二进制大对象 ，可以容纳可变数量的数据。 MySQL 中的 BLOB 类型包括 TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB 4 种类型，它们可容纳值的最大长度不同。可以存储一个二进制的大对象，比如 图片 、 音频 和 视频等。 需要注意的是，在实际工作中，往往不会在 MySQL 数据库中使用 BLOB 类型存储大对象数据，通常会将图片、音频和视频文件存储到 服务器的磁盘上 ，并将图片、音频和视频的访问路径存储到 MySQL 中。 6.9.3 TEXT 和 BLOB 的使用注意事项 BLOB 和 TEXT 值也会引起自己的一些问题，特别是执行了大量的删除或更新操作的时候。删除这种值会在数据表中留下很大的&quot; 空洞 &quot;，以后填入这些&quot;空洞&quot;的记录可能长度不同。为了提高性能，建议定期使用 OPTIMIZE TABLE 功能对这类表进行 碎片整理 。 如果需要对大文本字段进行模糊查询，MySQL 提供了前缀索引 。但是仍然要在不必要的时候避免检索大型的 BLOB 或 TEXT 值。例如，SELECT * 查询就不是很好的想法，除非你能够确定作为约束条件的 WHERE 子句只会找到所需要的数据行。否则，你可能毫无目的地在网络上传输大量的值。 把 BLOB 或 TEXT 列分离到单独的表中。在某些环境中，如果把这些数据列移动到第二张数据表中，可以让你把原数据表中的数据列转换为固定长度的数据行格式，那么它就是有意义的。这会减少主表中的碎片 ，得到固定长度数据行的性能优势。它还使你在主数据表上运行 SELECT * 查询的时候不会通过网络传输大量的 BLOB 或 TEXT 值。 6.10 JSON 类型 JSON（JavaScript Object Notation）是一种轻量级的 数据交换格式 。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言。它易于人阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率。JSON 可以将 JavaScript 对象中表示的一组数据转换为字符串，然后就可以在网络或者程序之间轻松地传递这个字符串，并在需要的时候将它还原为各编程语言所支持的数据格式。 在 MySQL 5.7 中，就已经支持 JSON 数据类型。在 MySQL 8.x 版本中，JSON 类型提供了可以进行自动验证的 JSON 文档和优化的存储结构，使得在 MySQL 中存储和读取 JSON 类型的数据更加方便和高效。 创建数据表，表中包含一个 JSON 类型的字段 js。 123456CREATE TABLE test_json( js json);INSERT INTO test_json (js)VALUES (&#x27;&#123;&quot;name&quot;:&quot;songhk&quot;, &quot;age&quot;:18, &quot;address&quot;:&#123;&quot;province&quot;:&quot;beijing&quot;, &quot;city&quot;:&quot;beijing&quot;&#125;&#125;&#x27;); 当需要检索 JSON 类型的字段中数据的某个具体值时，可以使用-&gt;和-&gt;&gt;符号。 6.11 空间类型 MySQL 空间类型扩展支持地理特征的生成、存储和分析。这里的地理特征表示世界上具有位置的任何东西，可以是一个实体，例如一座山；可以是空间，例如一座办公楼；也可以是一个可定义的位置，例如一个十字路口等等。MySQL 中使用 Geometry（几何） 来表示所有地理特征。Geometry 指一个点或点的集合，代表世界上任何具有位置的事物。 MySQL 的空间数据类型（Spatial Data Type）对应于 OpenGIS 类，包括单值类型：GEOMETRY、POINT、LINESTRING、POLYGON 以及集合类型：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、GEOMETRYCOLLECTION 。 MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、GEOMETRYCOLLECTION 是多个 Point、LineString 或 Polygon 组合而成。 Geometry 是所有空间集合类型的基类，其他类型如 POINT、LINESTRING、POLYGON 都是 Geometry 的子类。 Point，顾名思义就是点，有一个坐标值。例如 POINT(121.213342 31.234532)，POINT(30 10)，坐标值支持 DECIMAL 类型，经度（longitude）在前，维度（latitude）在后，用空格分隔。 LineString，线，由一系列点连接而成。如果线从头至尾没有交叉，那就是简单的（simple）；如果起点和终点重叠，那就是封闭的（closed）。例如 LINESTRING(30 10,10 30,4040)，点与点之间用逗号分隔，一个点中的经纬度用空格分隔，与 POINT 格式一致。 Polygon，多边形。可以是一个实心平面形，即没有内部边界，也可以有空洞，类似纽扣。最简单的就是只有一个外边界的情况，例如 POLYGON((0 0,10 0,10 10, 0 10))。 6.12 小节及选择建议 在定义数据类型时，如果确定是整数，就用 INT；如果是小数，一定用定点数类型 DECIMAL(M,D) ； 如果是日期与时间，就用 DATETIME。 这样做的好处是，首先确保你的系统不会因为数据类型定义出错。不过，凡事都是有两面的，可靠性好，并不意味着高效。比如，TEXT 虽然使用方便，但是效率不如 CHAR(M) 和 VARCHAR(M)。 关于字符串的选择，建议参考如下阿里巴巴的《Java 开发手册》规范： 7. CRUD（表中数据操作）——DML 7.1 Insert 语句 7.1.1 VALUES 的方式添加 123456INSERT INTO table_name(column1 [, column2, …, columnn])VALUES(value1 [,value2, …, valuen]),(value1 [,value2, …, valuen]),……(value1 [,value2, …, valuen]); 添加的数据与字段类型须一致。 '34'这种字符型会自动转换，也能添加到int类型的字段中 数据长度不能超过字段设定的存储范围。 数据的添加数量、顺序须与字段的数量、顺序一致。 字符型和日期类型数据必须要用''包裹 字段可以插入空值NULL（前提是字段设置为允许为空） 给表中所有字段添加数据，可以不写前面的字段名称。数据的数量与顺序须与表定义时的一致。 如果某个字段未指定为NOT NULL，添加数据时如果没有给值，则会默认为null。 如果需要设定有意义的默认值，则需要在定义表时，在字段声明后增加NOT NULL DEAFULT 'XXX' VALUES 也可以写成 VALUE ，但是 VALUES 是标准写法 d。 使用 INSERT 同时插入多条记录时，MySQL 会返回一些在执行单行插入时没有的额外信息，这些信息的含义如下： Records：表明插入的记录条数。 Duplicates：表明插入时被忽略的记录，原因可能是这些记录包含了重复的主键值。 Warnings：表明有问题的数据值，例如发生数据类型转换。 一个同时插入多行记录的 INSERT 语句等同于多个单行插入的 INSERT 语句，但是多行的 INSERT 语句在处理过程中效率更高 。【批量插入优先】 7.1.2 将查询结果插入到表中 INSERT 还可以将 SELECT 语句查询的结果插入到表中，此时不需要把每一条记录的值一个一个输入，只需要使用一条 INSERT 语句和一条 SELECT 语句组成的组合语句即可快速地从一个或多个表中向一个表中插入多行。 123456INSERT INTO 目标表名(tar_column1 [, tar_column2, …, tar_columnn])SELECT(src_column1 [, src_column2, …, src_columnn])FROM 源表名[WHERE condition] 在 INSERT 语句中加入子查询。 不必书写 VALUES 子句。 子查询中的值列表应与 INSERT 子句中的列名对应。 7.1.3 复制表数据 从其他表拷贝数据： 12INSERT INTO table_name1 (col1, col2, col3) SELECT col1, col2, col3 FROM table_name2; 如果 table_name1 和 table_name2 有一样的表结构，可以采用INSERT INTO table_name1 SELECT * FROM table_name2; INSERT INTO table_name1 SELECT DISTINCT * FROM table_name2;可以去重。 自我复制（蠕虫复制） 12INSERT INTO table_name SELECT * FROM table_name; 执行多次，可以创建n×COUNT(*)条数据。 7.2 Update 语句 123UPDATE table_name SET col_name = expr[,col_name = expr……] [WHERE col_name = expr]; 不写 WHERE 语句时，表示将该字段的所有值都修改。 7.3 Delete 语句 12DELETE from table_name [WHERE col_name = expr]; 不写 WHERE 语句时，表示清空表中数据。 8. Select 语句——DQL 8.1 基本语法 8.1.1 没有子句 12SELECT 1; #没有任何子句SELECT 9/2; #没有任何子句 8.1.2 有子句 12SELECT 标识选择哪些列FROM 标识从哪个表中选择 注意：在生产环境下，不推荐直接使用 SELECT * 进行查询： 12SELECT *FROM departments; 8.1.3 列的别名 重命名一个列 便于计算 紧跟列名，也可以在列名和别名之间加入关键字 AS，别名使用双引号，以便在别名中包含空格或特殊的字符并区分大小写。 AS 可以省略 建议别名简短，见名知意 8.1.4 去除重复行 在 SELECT 语句中使用关键字 DISTINCT 去除重复行 12SELECT DISTINCT department_id, salaryFROM employees; DISTINCT 需要放到所有列名的前面，如果写成SELECT salary, DISTINCT department_id FROM employees 会报错。 DISTINCT 其实是对后面所有列名进行组合去重，即去除 department_id+salary 组合后仍重复的行。 8.1.5 空值参与运算 在 MySQL 里面， 空值不等于空字符串。一个空字符串的长度是 0，而一个空值的长度是空。而且，在 MySQL 里面，空值是占用空间的。所有运算符或列值遇到 null 值，运算的结果都为 null。 12SELECT employee_id,salary,commission_pct, 12 * salary * (1 + commission_pct) &quot;annual_sal&quot;FROM employees; 8.1.5 着重号（```） 表中的字段、表名等没有和保留字、数据库系统或常用方法冲突，则需要在 SQL 语句中使用一对````（着重号）引起来。 否则会报错。 8.1.6 查询常数 SELECT 查询还可以对常数进行查询，就是在 SELECT 查询结果中增加一列固定的常数列。这列的取值是指定的，而不是从数据表中动态取出的。 一般来说只从一个表中查询数据，通常不需要增加一个固定的常数列，但如果想整合不同的数据源，用常数列作为这个表的标记，就需要查询常数。 1SELECT &#x27;MySQL&#x27; AS corporation, last_name FROM employees; 8.2 WHERE 过滤 语法： 123SELECT 字段1,字段2FROM 表名WHERE 过滤条件 使用 WHERE 子句，将不满足条件的行过滤掉 WHERE 子句紧随 FROM 子句 123SELECT employee_id, last_name, job_id, department_idFROM employeesWHERE department_id = 90 ; 8.3 SELECT 的执行过程 8.3.1 查询结构 1234567891011SELECT ...,....,...FROM ... JOIN ...ON 多表的连接条件JOIN ...ON ...WHERE 不包含组函数的过滤条件AND/OR 不包含组函数的过滤条件GROUP BY ...,...HAVING 包含组函数的过滤条件ORDER BY ... ASC/DESCLIMIT ...,... 关键字顺序不能颠倒：SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ... LIMIT... 8.3.2 SELECT 执行顺序 在 MySQL 和 Oracle 中，SELECT 执行顺序基本相同：FROM -&gt; WHERE -&gt; GROUP BY -&gt; HAVING -&gt; SELECT 的字段 -&gt; DISTINCT -&gt; ORDER BY -&gt; LIMIT 例如： 1234567SELECT DISTINCT player_id, player_name, count(*) as num # 顺序 5FROM player JOIN team ON player.team_id = team.team_id # 顺序 1WHERE height &gt; 1.80 # 顺序 2GROUP BY player.team_id # 顺序 3HAVING num &gt; 2 # 顺序 4ORDER BY num DESC # 顺序 6LIMIT 2 # 顺序 7 在 SELECT 语句执行这些步骤的时候，每个步骤都会产生一个 虚拟表 ，然后将这个虚拟表传入下一个步骤中作为输入。 8.3.3 SQL 的执行原理 SELECT 是先执行 FROM 这一步的。在这个阶段，如果是多张表联查，还会经历下面的几个步骤： 先先通过 CROSS JOIN 求笛卡尔积，相当于得到虚拟表 vt（virtual table）1-1； 通过 ON 进行筛选，在虚拟表 vt1-1 的基础上进行筛选，得到虚拟表 vt1-2； 添加外部行。如果使用的是左连接、右链接或者全连接，就会涉及到外部行，也就是在虚拟表 vt1-2 的基础上增加外部行，得到虚拟表 vt1-3。 如果操作的是两张以上的表，还会重复上面的步骤，直到所有表都被处理完为止。这个过程得到是原始数据。 当拿到了查询数据表的原始数据，也就是最终的虚拟表 vt1 ，就可以在此基础上再进行 WHERE 阶段 。在这个阶段中，会根据 vt1 表的结果进行筛选过滤，得到虚拟表 vt2 。 然后进入第三步和第四步，也就是 GROUP 和 HAVING 阶段 。在这个阶段中，实际上是在虚拟表 vt2 的基础上进行分组和分组过滤，得到中间的虚拟表 vt3 和 vt4 。 当完成了条件筛选部分之后，就可以筛选表中提取的字段，也就是进入到 SELECT 和 DISTINCT 阶段 。 首先在 SELECT 阶段会提取想要的字段，然后在 DISTINCT 阶段过滤掉重复的行，分别得到中间的虚拟表 vt5-1 和 vt5-2 。 当提取了想要的字段数据之后，就可以按照指定的字段进行排序，也就是 ORDER BY 阶段 ，得到虚拟表 vt6 。 最后在 vt6 的基础上，取出指定行的记录，也就是 LIMIT 阶段 ，得到最终的结果，对应的是虚拟表 vt7 。 在写 SELECT 语句的时候，不一定存在所有的关键字，相应的阶段就会省略。同时因为 SQL 是一门类似英语的结构化查询语言，所以在写 SELECT 语句的时候，还要注意相应的关键字顺序。 所谓底层运行的原理，即上述执行顺序。 9. 运算符 9.1 算术运算 9.1.1 加减运算 12SELECT 100, 100 + 0, 100 - 0, 100 + 50, 100 + 50 -30, 100 + 35.5, 100 - 35.5FROM dual; 一个整数类型的值对整数进行加法和减法操作，结果还是一个整数； 一个整数类型的值对浮点数进行加法和减法操作，结果是一个浮点数； 加法和减法的优先级相同，进行先加后减操作与进行先减后加操作的结果是一样的； 在 Java 中，+的左右两边如果有字符串，那么表示字符串的拼接。但是在 MySQL 中+只表示数值相加。如果遇到非数值类型，先尝试转成数值，如果转失败，就按 0 计算。（补充：MySQL 中字符串拼接要使用字符串函数 CONCAT()实现） 9.1.2 乘除运算 SELECT 100, 100 * 1, 100 * 1.0, 100 / 1.0, 100 / 2,100 + 2 * 5 / 2,100 /3, 100 DIV 0 FROM DUAL; 一个数乘以整数 1 和除以整数 1 后仍得原数； 一个数乘以浮点数 1 和除以浮点数 1 后变成浮点数，数值与原数相等； 一个数除以整数后，不管是否能除尽，结果都为一个浮点数； 一个数除以另一个数，除不尽时，结果为一个浮点数，并保留到小数点后 4 位； 乘法和除法的优先级相同，进行先乘后除操作与先除后乘操作，得出的结果相同。 在数学运算中，0 不能用作除数，在 MySQL 中，一个数除以 0 为 NULL。 9.1.1 求模（求余）运算 1SELECT 12 % 3, 12 MOD 5 FROM dual; 9.2 比较运算 比较运算符用来对表达式左边的操作数和右边的操作数进行比较，比较的结果为真则返回 1，比较的结果为假则返回 0，其他情况则返回 NULL。 比较运算符经常被用来作为 SELECT 查询语句的条件来使用，返回符合条件的结果记录。 9.2.1 等号 1SELECT 1 = 1, 1 = &#x27;1&#x27;, 1 = 0, &#x27;a&#x27; = &#x27;a&#x27;, (5 + 3) = (2 + 6), &#x27;&#x27; = NULL , NULL = NULL; 等号运算符（=）判断等号两边的值、字符串或表达式是否相等，如果相等则返回 1，不相等则返回 0。 在使用等号运算符时，遵循如下规则： 如果等号两边的值、字符串或表达式都为字符串，则 MySQL 会按照字符串进行比较，其比较的是每个字符串中字符的 ANSI 编码是否相等。 如果等号两边的值都是整数，则 MySQL 会按照整数来比较两个值的大小。 如果等号两边的值一个是整数，另一个是字符串，则 MySQL 会将字符串转化为数字进行比较。 如果等号两边的值、字符串或表达式中有一个为 NULL，则比较结果为 NULL。 1SELECT 1 = 2, 0 = &#x27;abc&#x27;, 1 = &#x27;abc&#x27; FROM DUAL; 9.2.2 安全等于运算符 安全等于运算符&lt;=&gt;与等于运算符=的作用是相似的， 唯一的区别 是&lt;=&gt;可以用来对 NULL 进行判断。 在两个操作数均为 NULL 时，其返回值为 1，而不为 NULL；当一个操作数为 NULL 时，其返回值为 0，而不为 NULL。 1SELECT 1 &lt;=&gt; &#x27;1&#x27;, 1 &lt;=&gt; 0, &#x27;a&#x27; &lt;=&gt; &#x27;a&#x27;, (5 + 3) &lt;=&gt; (2 + 6), &#x27;&#x27; &lt;=&gt; NULL, NULL&lt;=&gt; NULL FROM DUAL; 9.2.3 不等于运算符 不等于运算符（&lt;&gt;和!=）用于判断两边的数字、字符串或者表达式的值是否不相等，如果不相等则返回 1，相等则返回 0。不等于运算符不能判断 NULL 值。 如果两边的值有任意一个为 NULL，或两边都为 NULL，则结果为 NULL。 1SELECT 1 &lt;&gt; 1, 1 != 2, &#x27;a&#x27; != &#x27;b&#x27;, (3+4) &lt;&gt; (2+6), &#x27;a&#x27; != NULL, NULL &lt;&gt; NULL; 9.3 关键字运算 9.3.1 空运算符 空运算符（IS NULL 或者 ISNULL）判断一个值是否为 NULL，如果为 NULL 则返回 1，否则返回 0。 1SELECT NULL IS NULL, ISNULL(NULL), ISNULL(&#x27;a&#x27;), 1 IS NULL; 9.3.2 非空运算符 非空运算符（IS NOT NULL）判断一个值是否不为 NULL，如果不为 NULL 则返回 1，否则返回 0。 1SELECT NULL IS NOT NULL, &#x27;a&#x27; IS NOT NULL, 1 IS NOT NULL; 9.3.3 最小值运算符 语法格式为：LEAST(值 1，值 2，…，值 n)。其中“值 n”表示参数列表中有 n 个值。在有两个或多个参数的情况下，返回最小值。 1SELECT LEAST (1,0,2), LEAST(&#x27;b&#x27;,&#x27;a&#x27;,&#x27;c&#x27;), LEAST(1,NULL,2); 9.3.4 最大值运算符 语法格式为：GREATEST(值 1，值 2，…，值 n)。其中，n 表示参数列表中有 n 个值。当有两个或多个参数时，返回值为最大值。假如任意一个自变量为 NULL，则 GREATEST()的返回值为 NULL。 1SELECT GREATEST(1,0,2), GREATEST(&#x27;b&#x27;,&#x27;a&#x27;,&#x27;c&#x27;), GREATEST(1,NULL,2); 9.3.5 BETWEEN AND 运算符 BETWEEN 运算符使用的格式通常为 SELECT D FROM TABLE WHERE C BETWEEN A AND B，此时，当 C 大于或等于 A，并且 C 小于或等于 B 时，结果为 1，否则结果为 0。 9.3.6 IN 运算符 IN 运算符用于判断给定的值是否是 IN 列表中的一个值，如果是则返回 1，否则返回 0。如果给定的值为 NULL，或者 IN 列表中存在 NULL，则结果为 NULL。 9.3.7 NOT IN 运算符 NOT IN 运算符用于判断给定的值是否不是 IN 列表中的一个值，如果不是 IN 列表中的一个值，则返回 1，否则返回 0。 9.3.8 LIKE 运算符 LIKE 运算符主要用来匹配字符串，通常用于模糊匹配，如果满足条件则返回 1，否则返回 0。如果给定的值或者匹配条件为 NULL，则返回结果为 NULL。 %：匹配 0 个或多个字符。 _：只能匹配一个字符。 9.3.9 ESCAPE 如果使用\\表示转义，要省略 ESCAPE。如果不用\\，则要加上 ESCAPE。 9.3.10 REGEXP 运算符 用来匹配字符串，语法格式为： expr REGEXP 匹配条件 。如果 expr 满足匹配条件，返回 1；如果不满足，则返回 0。若 expr 或匹配条件任意一个为 NULL，则结果为 NULL。 ^匹配以该字符后面的字符开头的字符串。 $匹配以该字符前面的字符结尾的字符串。 .匹配任何一个单字符。 [...]匹配在方括号内的任何字符。例如，[abc]匹配 a 或 b 或 c。为了命名字符的范围，使用一个-。[a-z]匹配任何字母，[0-9]匹配任何数字。 *匹配零个或多个在它前面的字符。例如，x*匹配任何数量的 x 字符；[0-9]*匹配任何数量的数字；*匹配任何数量的任何字符。 9.4 逻辑运算符 逻辑运算符的返回结果为 1、0 或者 NULL。 9.4.1 逻辑非（NOT 或!） 表示当给定的值为 0 时返回 1；当给定的值为非 0 值时返回 0；当给定的值为 NULL 时，返回 NULL。 9.4.2 逻辑与（AND 或&amp;&amp;） 当给定的所有值均为非 0 值，并且都不为 NULL 时，返回 1；当给定的一个值或者多个值为 0 时则返回 0；否则返回 NULL。 9.4.3 逻辑或（OR 或||） 当给定的值都不为 NULL，并且任何一个值为非 0 值时，则返回 1，否则返回 0；当一个值为 NULL，并且另一个值为非 0 值时，返回 1，否则返回 NULL；当两个值都为 NULL 时，返回 NULL。 OR 可以和 AND 一起使用，但是 AND 的优先级高于 OR。 因此先对 AND 两边的操作数进行操作，再与 OR 中的操作数结合。 9.4.4 逻辑异或（XOR） 当给定的值中任意一个值为 NULL 时，则返回 NULL；如果两个非 NULL 的值都是 0 或者都不等于 0 时，则返回 0；如果一个值为 0，另一个值不为 0 时，则返回 1。 9.5 位运算符 位运算符会先将操作数变成二进制数，然后进行位运算，最后将计算结果从二进制变回十进制数。 9.5.1 按位与（&amp;） 将给定值对应的二进制数逐位进行逻辑与运算。当给定值对应的二进制位的数值都为 1 时，则该位返回 1，否则返回 0。 9.5.2 按位或（|） 将给定的值对应的二进制数逐位进行逻辑或运算。当给定值对应的二进制位的数值有一个或两个为 1 时，则该位返回 1，否则返回 0。 9.5.3 按位异或（^） 将给定的值对应的二进制数逐位进行逻辑异或运算。当给定值对应的二进制位的数值不同时，则该位返回 1，否则返回 0。 9.5.4 按位取反（~） 将给定的值的二进制数逐位进行取反操作，即将 1 变为 0，将 0 变为 1。 9.5.6 按位右移（&gt;&gt;） 将给定的值的二进制数的所有位右移指定的位数。右移指定的位数后，右边低位的数值被移出并丢弃，左边高位空出的位置用 0 补齐。 9.5.7 按位左移（&lt;&lt;） 将给定的值的二进制数的所有位左移指定的位数。左移指定的位数后，左边高位的数值被移出并丢弃，右边低位空出的位置用 0 补齐。 9.6 运算符的优先级 9.7 正则表达式 10. 排序和分页 10.1 排序 排序规则： 使用 ORDER BY 子句排序 ASC（ascend）：升序 DESC（descend）：降序 可以使用不在 SELECT 列表中的列排序。 在对多列进行排序的时候，首先排序的第一列必须有相同的列值，才会对第二列进行排序。如果第一列数据中所有值都是唯一的，将不再对第二列进行排序。 单列排序： 123SELECT last_name, job_id, department_id, hire_dateFROM employeesORDER BY hire_date DESC ; 多列排序 123SELECT last_name, department_id, salaryFROM employeesORDER BY department_id, salary DESC; 10.2 分页 分页原理：所谓分页显示，就是将数据库中的结果集，一段一段显示出来需要的条件。 格式：LIMIT [位置偏移量,] 行数 第一个“位置偏移量”参数指示 MySQL 从哪一行开始显示，是一个可选参数，如果不指定“位置偏移量”，将会从表中的第一条记录开始（第一条记录的位置偏移量是 0，第二条记录的位置偏移量是 1，以此类推）；第二个参数“行数”指示返回的记录条数。 12345678-- 前10条记录：SELECT * FROM 表名 LIMIT 0,10;-- 或者SELECT * FROM 表名 LIMIT 10;-- 第11至20条记录：SELECT * FROM 表名 LIMIT 10,10;-- 第21至30条记录：SELECT * FROM 表名 LIMIT 20,10; MySQL 8.0 中可以使用“LIMIT 3 OFFSET 4”，和“LIMIT4,3”返回的结果相同，意思是获取从5 条记录开始后面的 3 条记录。 分页显式公式：（当前页数-1）*每页条数，每页条数 LIMIT 子句必须放在整个 SELECT 语句的最后。 使用 LIMIT 的好处： 约束返回结果的数量可以 减少数据表的网络传输量 ，也可以提升查询效率 。 如果我们知道返回结果只有 1 条，就可以使用 LIMIT 1 ，告诉 SELECT 语句只需要返回一条记录即可。 这样的好处就是 SELECT 不需要扫描完整的表，只需要检索到一条符合条件的记录即可返回。 10.3 其他 DMS 中的分页查询关键字 在 MySQL、PostgreSQL、MariaDB 和 SQLite 中使用 LIMIT 关键字，而且需要放到 SELECT 语句的最后面。 如果是 SQL Server 和 Access，需要使用 TOP关键字，比如： 1SELECT TOP 5 name, hp_max FROM heros ORDER BY hp_max DESC 如果是 DB2，使用 FETCH FIRST 5 ROWS ONLY 这样的关键字： 1SELECT name, hp_max FROM heros ORDER BY hp_max DESC FETCH FIRST 5 ROWS ONLY 如果是 Oracle，你需要基于 ROWNUM 来统计行数： 1SELECT rownum,last_name,salary FROM employees WHERE rownum &lt; 5 ORDER BY salary DESC; 需要说明的是，这条语句是先取出来前 5 条数据行，然后再按照 hp_max 从高到低的顺序进行排序。但这样产生的结果和上述方法的并不一样。【没看懂】 11. 单行函数 11.1 概述 在 SQL 语言中，函数包括内置函数和自定义函数。内置函数是系统内置的通用函数，而自定义函数是根据自己的需要编写的函数。 在使用 SQL 语言的时候，不是直接和这门语言打交道，而是通过它使用不同的数据库软件，即 DBMS。DBMS 之间的差异性很大，远大于同一个语言不同版本之间的差异。实际上，只有很少的函数是被 DBMS 同时支持的。比如，大多数 DBMS 使用（||）或者（+）来做拼接符，而在 MySQL 中的字符串拼接函数为 concat()。大部分 DBMS 会有自己特定的函数，这就意味着采用 SQL 函数的代码可移植性是很差的，因此在使用函数的时候需要特别注意。 MySQL 提供的内置函数从实现的功能角度可以分为数值函数、字符串函数、日期和时间函数、流程控制函数、加密与解密函数、获取 MySQL 信息函数、聚合函数等。 单行函数 操作数据对象 接受参数返回一个结果 只对一行进行变换 每行返回一个结果 可以嵌套 参数可以是一列或一个值 11.2 数值函数 11.2.1 基本函数 11.2.2 角度与弧度互换函数 12.2.3 三角函数 ATAN2(M,N)与与 ATAN(X)的对比：例如有两个点 point(x1,y1)和 point(x2,y2)，使用 ATAN(X)函数计算反正切值为 ATAN((y2-y1)/(x2-x1))，使用 ATAN2(M,N)计算反正切值则为 ATAN2(y2-y1,x2-x1)。当 x2-x1 等于 0 时，ATAN(X)函数会报错，而 ATAN2(M,N)函数则仍然可以计算。 12.2.4 指数与对数 12.2.5 进制间转换 11.3 字符串函数 注意：MySQL 中，字符串的位置是从 1 开始的。 11.4 日期和时间函数 11.4.1 获取日期、时间 11.4.2 日期与时间戳的转换 11.4.3 获取月份、星期、星期数、天数等函数 11.4.4 日期操作 type 取值及含义： 11.4.5 时间和秒钟转换 11.4.6 计算日期和时间 11.4.6.1 第一组 type 取值及含义： 11.4.6.2 第二组 举例：查询 7 天内的新增用户数有多少？ 1SELECT COUNT(*) as num FROM new_user WHERE TO_DAYS(NOW())-TO_DAYS(regist_time)&lt;=7 11.4.6.3 日期的格式化与解析 上述 非 GET_FORMAT 函数中 fmt 参数常用的格式符： GET_FORMAT 函数中 date_type 和 format_type 参数取值： 11.5 流程控制函数 流程处理函数可以根据不同的条件，执行不同的处理流程，可以在 SQL 语句中实现不同的条件选择。 MySQL 中的流程处理函数主要包括 IF()、IFNULL()和 CASE()函数。 12345SELECT employee_id,salary, CASE WHEN salary&gt;=15000 THEN &#x27;高薪&#x27; WHEN salary&gt;=10000 THEN &#x27;潜力股&#x27; WHEN salary&gt;=8000 THEN &#x27;屌丝&#x27; ELSE &#x27;草根&#x27; END &quot;描述&quot;FROM employees; 11.6 加密与解密函数 加密与解密函数主要用于对数据库中的数据进行加密和解密处理，以防止数据被他人窃取。这些函数在保证数据库安全时非常有用。 ENCODE(value,password_seed)函数与 DECODE(value,password_seed)函数互为反函数。 11.7 信息函数 主要用于帮助数据库开发或运维人员更好地对数据库进行维护工作。 11.8 其他函数 不方便分类，但开发和运维中很常用。 12. 多表查询（关联查询） 前提条件：这些一起查询的表之间是有关系的（一对一、一对多），它们之间一定是有关联字段，这个关联字段可能建立了外键，也可能没有建立外键。比如：员工表和部门表，这两个表依靠“部门编号”进行关联。 12.1 笛卡尔积（交叉连接） SELECT * FROM table1, table2： 该种情况下，会取出 table1 的每一行与 table2 的每一行组合进行显示，显示的行数为 table1 的行数 ×table2 的行数，称为笛卡尔积 table1 和 table2 共有的 col，需要使用table1.col的方式指定是哪个表的 col 列，否则会报错。 多表查询的条件（WHERE）不能少于表的个数-1，否则会出现笛卡尔积。 笛卡尔积也称为 交叉连接 ，英文是 CROSS JOIN 。 即使两个表不相关，如下语句也能出现笛卡尔积： 1234#案例：查询员工的姓名及其部门名称SELECT last_name, department_nameFROM employees, departmentsWHERE employees.department_id = departments.department_id; 笛卡尔积的错误会在下面条件下产生： 省略多个表的连接条件（或关联条件） 连接条件（或关联条件）无效 所有表中的所有行互相连接 为了避免笛卡尔积， 可以在 WHERE 加入有效的连接条件。 12.2 等值连接 vs 非等值连接 12.2.1 等值连接 等值连接即多表查询的 WHERE 条件使用等号判断。 由于避免 n 个表笛卡尔积需要 n-1 个条件，所以其他等值的条件可以使用 AND 操作符。 多个表中有相同列时，必须在列名之前加上表名前缀。 123SELECT employees.last_name, departments.department_name,employees.department_idFROM employees, departmentsWHERE employees.department_id = departments.department_id; 使用表的别名可以简化查询，同时可以提高查询效率。 如果使用了表的别名，在查询字段中、过滤条件中就只能使用别名进行代替，不能使用原有的表名，否则就会报错。 1234SELECT e.employee_id, e.last_name, e.department_id, d.department_id, d.location_idFROM employees e , departments dWHERE e.department_id = d.department_id; 12.2.2 非等值连接 非等值连接即多表查询的 WHERE 条件=变为其他符号，如： 123SELECT e.last_name, e.salary, j.grade_levelFROM employees e, job_grades jWHERE e.salary BETWEEN j.lowest_sal AND j.highest_sal; 12.3 自连接 VS 非自连接 12.3.1 自连接 含义：在同一个表查询该表内的多个数据进行显示。 原理：给表起个别名，把它当成多个表，然后用别名.col指定多个查询列，进行多表查询 这里只有 emp 一个表，给表起了个 worker 和 boss 两个名字 123SELECT CONCAT(worker.last_name ,&#x27;works for&#x27;, manager.last_name)FROM employees worker, employees managerWHERE worker.manager_id = manager.employee_id ; 特点： 把同一张表当作两张表使用 需要给表取别名 可以给查询的列取别名。 12.3.2 非自连接 即不同的表查询。 12.4 内连接 VS 外连接 12.4.1 内连接 内连接：合并具有同一列的两个以上的表的行，结果集中不包含一个表与另一个表不匹配的行。（不显示不匹配的行） 语法结构： 1234SELECT 字段列表FROM A表 INNER JOIN B表ON 关联条件WHERE 等其他子句; 关键字 JOIN、INNER JOIN、CROSS JOIN 的含义是一样的，都表示内连接。 案例 1，两个表。 123SELECT e.employee_id, e.last_name, e.department_id, d.department_id, d.location_idFROM employees e JOIN departments dON (e.department_id = d.department_id); 案例 2，三个表。 123456SELECT employee_id, city, department_nameFROM employees eJOIN departments dON d.department_id = e.department_idJOIN locations lON d.location_id = l.location_id; 12.4.2 外连接 出现背景：笛卡尔积筛选的数据会根据关联条件匹配，如果关联条件一个表有，另一个表没有，那么就会匹配不上，就不会显示该条数据。（即内连接的情况） 外连接：两个表在连接过程中除了返回满足连接条件的行以外还返回左（或右）表中不满足条件的行，这种连接称为左（或右）外连接。没有匹配的行时, 结果表中相应的列为空(NULL)。 如果是左外连接，则连接条件中左边的表也称为主表 ，右边的表称为从表。 如果是右外连接，则连接条件中右边的表也称为主表 ，左边的表称为从表。 左外连接：左侧表完全显示。 left join 和 left outer join 相同 A 表完全显示。 1234SELECT 字段列表FROM A表 LEFT JOIN B表ON 关联条件WHERE 等其他子句; 右外连接：右侧表完全显示。 right join 和 right outer join 相同 B 表完全显示。 1234SELECT 字段列表FROM A表 RIGHT JOIN B表ON 关联条件WHERE 等其他子句; 在 SQL92 中采用（+）代表从表所在的位置，但是 MySQL 不支持这种写法，Oracle 可以。LEFT JOIN 和 RIGHT JOIN 只存在于 SQL99 及以后的标准中，在 SQL92 中不存在，只能用 (+) 表示。 12345678#左外连接SELECT last_name,department_nameFROM employees ,departmentsWHERE employees.department_id = departments.department_id(+);#右外连接SELECT last_name,department_nameFROM employees ,departmentsWHERE employees.department_id(+) = departments.department_id; 满外连接： 满外连接的结果 = 左右表匹配的数据 + 左表没有匹配到的数据 + 右表没有匹配到的数据。 SQL99 是支持满外连接，使用 FULL JOIN 或 FULL OUTER JOIN 来实现。 MySQL 可以用LEFT JOIN UNION RIGHT JOIN语法代替。 12.5 UNION 合并查询结果 作用：合并多条 SELECT 单表查询语句的结果，进行多行显示。合并时，两个表对应的列数和数据类型必须相同，并且相互对应。 语法： 123SELECT column,... FROM table1UNION [ALL]SELECT column,... FROM table2 UNION：查询结果会去重。 UNION ALL：查询结果不会去重。 执行 UNION ALL 语句时所需要的资源比 UNION 语句少。如果明确知道合并数据后的结果数据不存在重复数据，或者不需要去除重复的数据，则尽量使用 UNION ALL 语句，以提高数据查询的效率。 12.6 SQL99 扩展 12.6.1 自然连接 SQL99 在 SQL92 的基础上提供了一些特殊语法，比如 NATURAL JOIN 用来表示自然连接。可以把自然连接理解为 SQL92 中的等值连接。它会帮你自动查询两张连接表中 所有相同的字段 ，然后进行等值连接 。 SQL92： 1234SELECT employee_id,last_name,department_nameFROM employees e JOIN departments dON e.`department_id` = d.`department_id`AND e.`manager_id` = d.`manager_id`; SQL99： 12SELECT employee_id,last_name,department_nameFROM employees e NATURAL JOIN departments d; 12.6.2 USING 连接 SQL99 还支持使用 USING 指定数据表里的 同名字段 进行等值连接。但是只能配合 JOIN 一起使用。比如： 123SELECT employee_id,last_name,department_nameFROM employees e JOIN departments dUSING (department_id); 与自然连接 NATURAL JOIN 不同的是，USING 指定了具体的相同的字段名称，USING 的括号()中填入要指定的同名字段。同时使用 JOIN…USING 可以简化 JOIN ON 的等值连接。它与下面的 SQL 查询结果是相同的： 123SELECT employee_id,last_name,department_nameFROM employees e ,departments dWHERE e.department_id = d.department_id; 12.7 小结 表连接的约束条件可以有三种方式：WHERE, ON, USING WHERE：适用于所有关联查询。 ON ：只能和 JOIN 一起使用，只能写关联条件。虽然关联条件可以并到 WHERE 中和其他条件一起写，但分开写可读性更好。 USING：只能和 JOIN 一起使用，而且要求两个关联字段在关联表中名称一致，而且只能表示关联字段值相等。 常用多表查询语句： 12345678910111213141516171819202122232425262728293031323334353637383940414243#左上图：左外连接SELECT employee_id,last_name,department_nameFROM employees e LEFT JOIN departments dON e.`department_id` = d.`department_id`;#右上图：右外连接SELECT employee_id,last_name,department_nameFROM employees e RIGHT JOIN departments dON e.`department_id` = d.`department_id`;#左中图：A - A∩BSELECT employee_id,last_name,department_nameFROM employees e LEFT JOIN departments dON e.`department_id` = d.`department_id`WHERE d.`department_id` IS NULL#中图：内连接 A∩BSELECT employee_id,last_name,department_nameFROM employees e JOIN departments dON e.`department_id` = d.`department_id`;#右中图：B-A∩BSELECT employee_id,last_name,department_nameFROM employees e RIGHT JOIN departments dON e.`department_id` = d.`department_id`WHERE e.`department_id` IS NULL#左下图：满外连接# 左中图 + 右上图 A∪BSELECT employee_id,last_name,department_nameFROM employees e LEFT JOIN departments dON e.`department_id` = d.`department_id`WHERE d.`department_id` IS NULLUNION ALL #没有去重操作，效率高SELECT employee_id,last_name,department_nameFROM employees e RIGHT JOIN departments dON e.`department_id` = d.`department_id`;#右下图#左中图 + 右中图 A ∪B- A∩B 或者 (A - A∩B) ∪ （B - A∩B）SELECT employee_id,last_name,department_nameFROM employees e LEFT JOIN departments dON e.`department_id` = d.`department_id`WHERE d.`department_id` IS NULLUNION ALLSELECT employee_id,last_name,department_nameFROM employees e RIGHT JOIN departments dON e.`department_id` = d.`department_id`WHERE e.`department_id` IS NULL 13. 聚合函数 做聚合（或聚集、分组）函数是对一组数据进行汇总的函数，输入的是一组数据的集合，输出的是单个值。 聚合函数不能嵌套调用。比如不能出现类似“AVG(SUM(字段名称))”形式的调用。 13.1 数学计算函数 13.1.1 AVG 和 SUM 可以对数值型数据使用 AVG 和 SUM 函数 123SELECT AVG(salary), MAX(salary),MIN(salary), SUM(salary)FROM employeesWHERE job_id LIKE &#x27;%REP%&#x27;; 13.1.2 MIN 和 MAX 可以对任意数据类型的数据使用 MIN 和 MAX 函数。 12SELECT MIN(hire_date), MAX(hire_date)FROM employees; 11.3.3 COUNT COUNT(*)返回表中记录总数，适用于任意数据类型。 COUNT(field) 返回 expr 不为空的记录总数。 count(*)，count(1)，count(列名）的对比： 对于 MyISAM 引擎的表是没有区别的。这种引擎内部有一计数器在维护着行数。 Innodb 引擎的表用 count(*),count(1)直接读行数，复杂度是 O(n)，因为 innodb 真的要去数一遍。但好于具体的 count(列名) 。 能不能使用 count(列名)替换 count(*)? 不能，count(*) 是 SQL92 定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。 count(*)会统计值为 NULL 的行，而 count(列名)不会统计此列为 NULL 值的行。 13.2 GROUP BY 13.2.1 基本使用 可以使用 GROUP BY 子句将表中的数据分成若干组： 12345SELECT column, group_function(column)FROM table[WHERE condition][GROUP BY group_by_expression][ORDER BY column]; WHERE 一定放在 FROM 后面。 在 SELECT 列表中所有未包含在组函数中的列都应该包含在 GROUP BY 子句中。 123SELECT department_id, AVG(salary)FROM employeesGROUP BY department_id ; 包含在 GROUP BY 子句中的列不必包含在 SELECT 列表中。 123SELECT AVG(salary)FROM employeesGROUP BY department_id ; 13.2.2 使用多个列分组 123SELECT department_id dept_id, job_id, SUM(salary)FROM employeesGROUP BY department_id, job_id ; 13.2.3 GROUP BY 中使用 WITH ROLLUP 使用 WITH ROLLUP 关键字之后，在所有查询出的分组记录之后增加一条记录，该记录计算查询出的所有记录的总和，即统计记录数。 1234SELECT department_id,AVG(salary)FROM employeesWHERE department_id &gt; 80GROUP BY department_id WITH ROLLUP; 当使用 ROLLUP 时，不能同时使用 ORDER BY 子句进行结果排序，即 ROLLUP 和 ORDER BY 是互相排斥。 13.3 HAVING 13.3.1 基本使用 使用前提： 行已经被分组。 使用了聚合函数。 满足 HAVING 子句中条件的分组将被显示。 HAVING 不能单独使用，必须要跟 GROUP BY 一起使用。 1234SELECT department_id, MAX(salary)FROM employeesGROUP BY department_idHAVING MAX(salary)&gt;10000 ; 不能在 WHERE 子句中使用聚合函数，会报错： 1234SELECT department_id, AVG(salary)FROM employeesWHERE AVG(salary) &gt; 8000GROUP BY department_id; 13.3.2 WHERE 和 HAVING 的对比 区别 1： WHERE 可以直接使用表中的字段作为筛选条件，但不能使用分组中的计算函数作为筛选条件； HAVING 必须要与 GROUP BY 配合使用，可以把分组计算的函数和分组字段作为筛选条件。 这决定了，在需要对数据进行分组统计的时候，HAVING 可以完成 WHERE 不能完成的任务。这是因为，在查询语法结构中，WHERE 在 GROUP BY 之前，所以无法对分组结果进行筛选。HAVING 在 GROUP BY 之后，可以使用分组字段和分组中的计算函数，对分组的结果集进行筛选，这个功能是 WHERE 无法完成的。另外，WHERE 排除的记录不再包括在分组中。 区别 2： 如果需要通过连接从关联表中获取需要的数据，WHERE 是先筛选后连接，而 HAVING 是先连接后筛选。 这一点，就决定了在关联查询中，WHERE 比 HAVING 更高效。因为 WHERE 可以先筛选，用一个筛选后的较小数据集和关联表进行连接，这样占用的资源比较少，执行效率也比较高。HAVING 则需要先把结果集准备好，也就是用未被筛选的数据集进行关联，然后对这个大的数据集进行筛选，这样占用的资源就比较多，执行效率也较低。 优点 缺点 WHERE 先筛选数据再关联，执行效率高 不能使用分组中的计算函数进行筛选 HAVING 可以使用分组中的计算函数 在最后的结果集中进行筛选，执行效率较低 开发中的选择： WHERE 和 HAVING 也不是互相排斥的，我们可以在一个查询里面同时使用 WHERE 和 HAVING。包含分组统计函数的条件用 HAVING，普通条件用 WHERE。这样，我们就既利用了 WHERE 条件的高效快速，又发挥了 HAVING 可以使用包含分组统计函数的查询条件的优点。当数据量特别大的时候，运行效率会有很大的差别。 14. 子查询 14.1 基本使用 子查询指嵌入在其他 sql 语句中的 select 语句，也叫嵌套查询。 子查询（内查询）在主查询之前一次执行完成。 子查询的结果被主查询（外查询）使用 。 子查询要包含在括号内。 将子查询放在比较条件的右侧。 单行操作符对应单行子查询，多行操作符对应多行子查询。 14.2 单行子查询 14.2.1 基本使用 子查询只返回一行数据的查询语句。 假如要查询部门中谁的工资比 Abel 高，有三种方式： 1234567891011121314151617181920#方式一分步查询：SELECT salaryFROM employeesWHERE last_name = &#x27;Abel&#x27;;SELECT last_name,salaryFROM employeesWHERE salary &gt; 11000;#方式二：自连接SELECT e2.last_name,e2.salaryFROM employees e1,employees e2WHERE e1.last_name = &#x27;Abel&#x27;AND e1.`salary` &lt; e2.`salary`#方式三：子查询SELECT last_name,salaryFROM employeesWHERE salary &gt; ( SELECT salary FROM employees WHERE last_name = &#x27;Abel&#x27;); 14.2.2 HAVING 中的子查询 首先执行子查询，其次向主查询中的 HAVING 子句返回结果。 查询最低工资大于 50 号部门最低工资的部门 id 和其最低工资： 1234567SELECT department_id, MIN(salary)FROM employeesGROUP BY department_idHAVING MIN(salary) &gt; (SELECT MIN(salary) FROM employees WHERE department_id = 50); 14.2.4 CASE 中的子查询 在 CASE 表达式中使用单列子查询： 显示员工的 employee_id,last_name 和 location。其中，若员工 department_id 与 location_id 为 1800 的 department_id 相同，则 location 为’Canada’，其余则为’USA’。 1234567SELECT employee_id, last_name, (CASE department_id WHEN (SELECT department_id FROM departments WHERE location_id = 1800) THEN &#x27;Canada&#x27; ELSE &#x27;USA&#x27; END) locationFROM employees; 14.2.5 子查询中的空值 子查询不返回任何行 123456SELECT last_name, job_idFROM employeesWHERE job_id = (SELECT job_id FROM employees WHERE last_name = &#x27;Haas&#x27;); 14.2.6 非法使用子查询 多行子查询使用单行比较符 123456SELECT employee_id, last_nameFROM employeesWHERE salary = (SELECT MIN(salary) FROM employees GROUP BY department_id) 14.3 多行子查询 14.3.1 基本使用 也称为集合比较子查询，内查询返回多行，使用多行比较操作符。 操作符 含义 IN 等于列表中的任意一个 ANY 需要和单行比较操作符一起使用，和子查询返回的某一个值比较 ALL 需要和单行比较操作符一起使用，和子查询返回的所有值比较 SOME 实际上是 ANY 的别名，作用相同，一般常使用 ANY 14.3.2 HAVING 中的子查询 例：查询平均工资最低的部门的 id 123456789101112131415161718192021#方式1：SELECT department_idFROM employeesGROUP BY department_idHAVING AVG(salary) = ( SELECT MIN(avg_sal) FROM ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) dept_avg_sal)#方式2：SELECT department_idFROM employeesGROUP BY department_idHAVING AVG(salary) &lt;= ALL ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id) 14.3.3 空值问题 同单行子查询。 14.4 相关子查询 14.4.1 执行流程 如果子查询的执行依赖于外部查询，通常情况下都是因为子查询中的表用到了外部的表，并进行了条件关联，因此每执行一次外部查询，子查询都要重新计算一次，这样的子查询就称之为 关联子查询（相关子查询）。 相关子查询按照一行接一行的顺序执行，主查询的每一行都执行一次子查询。 子查询中使用主查询中的列 14.4.2 在 FROM 中使用子查询 子查询是作为 from 的一部分，子查询要用()引起来，并且要给这个子查询取别名， 把它当成一张“临时的虚拟的表”来使用。 查询员工中工资大于本部门平均工资的员工的 last_name,salary 和其 department_id。 1234SELECT last_name,salary,e1.department_idFROM employees e1,(SELECT department_id,AVG(salary) dept_avg_sal FROM employees GROUP BY department_id) e2WHERE e1.`department_id` = e2.department_idAND e2.dept_avg_sal &lt; e1.`salary`; 14.4.3 在 ORDER BY 中使用子查询 查询员工的 id,salary,按照 department_name 排序。 1234567SELECT employee_id,salaryFROM employees eORDER BY ( SELECT department_name FROM departments d WHERE e.`department_id` = d.`department_id`); 14.4.4 EXISTS 与 NOT EXISTS 关键字 关联子查询通常也会和 EXISTS 操作符一起来使用，用来检查在子查询中是否存在满足条件的行。 如果在子查询中不存在满足条件的行： 条件返回 FALSE 继续在子查询中查找 如果在子查询中存在满足条件的行： 不在子查询中继续查找 条件返回 TRUE NOT EXISTS 关键字表示如果不存在某种条件，则返回 TRUE，否则返回 FALSE。 查询公司管理者的 employee_id，last_name，job_id，department_id 信息： 12345SELECT employee_id, last_name, job_id, department_idFROM employees e1WHERE EXISTS ( SELECT * FROM employees e2 WHERE e2.manager_id =e1.employee_id); 14.4.5 相关更新 使用相关子查询依据一个表中的数据更新另一个表的数据。 在 employees 中增加一个 department_name 字段，数据为员工对应的部门名称： 1234UPDATE employees eSET department_name = (SELECT department_name FROM departments d WHERE e.department_id = d.department_id); 14.4.6 相关删除 使用相关子查询依据一个表中的数据删除另一个表的数据。 删除表 employees 中，其与 emp_history 表皆有的数据： 12345DELETE FROM employees eWHERE employee_id in (SELECT employee_id FROM emp_history WHERE employee_id = e.employee_id); 14.5 自连接和子查询的选择 可以使用子查询，也可以使用自连接的查询中，一般情况使用自连接，因为在许多 DBMS 的处理过程中，对于自连接的处理速度要比子查询快得多。 可以这样理解：子查询实际上是通过未知表进行查询后的条件判断，而自连接是通过已知的自身数据表进行条件判断，因此在大部分 DBMS 中都对自连接处理进行了优化。 15. 约束 15.1 约束概述 15.1.1 约束的意义 作用：确保数据库的数据满足特定的商业规则。 数据完整性（Data Integrity）是指数据的精确性（Accuracy）和可靠性（Reliability）。它是防止数据库中存在不符合语义规定的数据和防止因错误信息的输入输出造成无效操作或错误信息而提出的。 为了保证数据的完整性，SQL 规范以约束的方式对表数据进行额外的条件限制。从以下四个方面考虑： 实体完整性（Entity Integrity） ：例如，同一个表中，不能存在两条完全相同无法区分的记录 域完整性（Domain Integrity） ：例如：年龄范围 0-120，性别范围“男/女” 引用完整性（Referential Integrity） ：例如：员工所在部门，在部门表中要能找到这个部门 用户自定义完整性（User-defined Integrity） ：例如：用户名唯一、密码不能为空等，本部门经理的工资不得高于本部门职工的平均工资的 5 倍。 15.1.2 约束的创建与查看 约束是表级的强制规定。 可以在创建表时规定约束（通过 CREATE TABLE 语句），或者在表创建之后通过 ALTER TABLE 语句规定约束。 查看某个表已有的约束 1234#information_schema数据库名（系统库）#table_constraints表名称（专门存储各个表的约束）SELECT * FROM information_schema.table_constraintsWHERE table_name = &#x27;表名称&#x27;; 15.1.3 约束的分类 根据约束数据列的限制，约束可分为： 单列约束：每个约束只约束一列 多列约束：每个约束可约束多列数据 根据约束的作用范围，约束可分为： 列级约束：只能作用在一个列上，跟在列的定义后面 表级约束：可以作用在多个列上，不与列一起，而是单独定义 根据约束起的作用，约束可分为： NOT NULL 非空约束，规定某个字段不能为空 UNIQUE 唯一约束，规定某个字段在整个表中是唯一的 PRIMARY KEY 主键(非空且唯一)约束 FOREIGN KEY 外键约束 CHECK 检查约束 DEFAULT 默认值约束 15.2 非空约束 15.2.1 作用 限定某个字段/某列的值不允许为空。 15.2.2 特点 默认情况（不设置 not null)所有的类型的值都可以是 NULL，包括 INT、FLOAT 等数据类型 非空约束只能出现在表对象的列上，只能某个列单独限定非空，不能组合非空 一个表可以有很多列都分别限定了非空 空字符串’'不等于 NULL，0 也不等于 NULL 15.2.3 添加非空约束 建表时 12345CREATE TABLE 表名称( 字段名 数据类型, 字段名 数据类型 NOT NULL, 字段名 数据类型 NOT NULL); 建表后 1alter table 表名称 modify 字段名 数据类型 not null; 15.2.4 删除非空约束 123alter table 表名称 modify 字段名 数据类型 NULL;#去掉not null，相当于修改某个非注解字段，该字段允许为空或alter table 表名称 modify 字段名 数据类型;#去掉not null，相当于修改某个非注解字段，该字段允许为空 15.3 唯一约束 15.3.1 作用 用来限制某个字段/某列的值不能重复。 15.3.2 特点 同一个表可以有多个唯一约束。 唯一约束可以是某一个列的值唯一，也可以多个列组合的值唯一。 唯一性约束允许列值为空。 在创建唯一约束的时候，如果不给唯一约束命名，就默认和列名相同。 MySQL 会给唯一约束的列上默认创建一个唯一索引。 15.3.3 添加唯一约束 建表时 1234567891011121314151617181920212223242526create table 表名称( 字段名 数据类型, 字段名 数据类型 unique, 字段名 数据类型 unique key, 字段名 数据类型);create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, [constraint 约束名] unique key(字段名));------------------------------create table student( sid int, sname varchar(20), tel char(11) unique, cardid char(18) unique key);CREATE TABLE USER( id INT NOT NULL, NAME VARCHAR(25), PASSWORD VARCHAR(16), -- 使用表级约束语法 CONSTRAINT uk_name_pwd UNIQUE(NAME,PASSWORD) #表示用户名和密码组合不能重复); 建表后添加 1234#方式1：alter table 表名称 add unique key(字段列表);#方式2：alter table 表名称 modify 字段名 字段类型 unique; 15.3.4 删除唯一约束 删除唯一约束只能通过删除唯一索引的方式删除。 删除时需要指定唯一索引名，唯一索引名就和唯一约束名一样。 如果创建唯一约束时未指定名称，如果是单列，就默认和列名相同；如果是组合列，那么默认和()中排在第一个的列名相同。 1SELECT * FROM information_schema.table_constraints WHERE table_name = &#x27;表名&#x27;; #查看都有哪些约束 1show index from 表名称; #查看表的索引 12ALTER TABLE USERDROP INDEX uk_name_pwd; 15.4 主键约束 15.4.1 作用 用来唯一标识表中的一行记录。 15.4.2 特点 主键约束相当于唯一约束+非空约束的组合，主键约束列不允许重复，也不允许出现空值。 一个表最多只能有一个主键约束，建立主键约束可以在列级别创建，也可以在表级别上创建。 主键约束对应着表中的一列或者多列（复合主键） 如果是多列组合的复合主键约束，那么这些列都不允许为空值，并且组合的值不允许重复。 MySQL 的主键名总是 PRIMARY，就算自己命名了主键约束名也没用。 当创建主键约束时，系统默认会在所在的列或列组合上建立对应的主键索引（能够根据主键查询的，就根据主键查询，效率更高）。如果删除主键约束了，主键约束对应的索引就自动删除了。 需要注意的一点是，不要修改主键字段的值。因为主键是数据记录的唯一标识，如果修改了主键的值，就有可能会破坏数据的完整性。 15.4.3 添加主键约束 建表时 1234567891011create table 表名称( 字段名 数据类型 primary key, #列级模式 字段名 数据类型, 字段名 数据类型);create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, [constraint 约束名] primary key(字段名) #表级模式); 建表后添加： 1ALTER TABLE 表名称 ADD PRIMARY KEY(字段列表); #字段列表可以是一个字段，也可以是多个字段，如果是多个字段的话，是复合主键 15.4.4 删除主键约束 1alter table 表名称 drop primary key; 删除主键约束，不需要指定主键名，因为一个表只有一个主键，删除主键约束后，非空还存在。 15.5 外键约束 15.5.1 作用 限定某个表的某个字段的引用完整性。 15.5.2 主表和从表/父表和子表 主表（父表）：被引用的表，被参考的表 从表（子表）：引用别人的表，参考别人的表 例如：员工表的员工所在部门这个字段的值要参考部门表：部门表是主表，员工表是从表。 例如：学生表、课程表、选课表：选课表的学生和课程要分别参考学生表和课程表，学生表和课程表是主表，选课表是从表。 15.5.3 特点 从表的外键列，必须引用/参考主表的主键或唯一约束的列。因为被依赖/被参考的值必须是唯一的。 在创建外键约束时，如果不给外键约束命名，默认名不是列名，而是自动产生一个外键名（例如 student_ibfk_1;），也可以指定外键约束名。 创建(CREATE)表时就指定外键约束的话，先创建主表，再创建从表。 删表时，先删从表（或先删除外键约束），再删除主表。 当主表的记录被从表参照时，主表的记录将不允许删除，如果要删除数据，需要先删除从表中依赖该记录的数据，然后才可以删除主表的数据。 一个从表可以建立多个外键约束。 从表的外键列与主表被参照的列名字可以不相同，但是数据类型必须一样，逻辑意义一致。如果类型不一样，创建子表时，就会出现错误“ERROR 1005 (HY000): Can’t create table ‘database.tablename’(errno: 150)” 当创建外键约束时，系统默认会在所在的列上建立对应的普通索引。但是索引名是外键的约束名。（根据外键查询效率很高） 删除外键约束后，必须手动删除对应的索引。 15.5.4 约束等级 Cascade 方式 ：在父表上 update/delete 记录时，同步 update/delete 掉子表的匹配记录 Set null 方式 ：在父表上 update/delete 记录时，将子表上匹配记录的列设为 null，但是要注意子表的外键列不能为 not null No action 方式 ：如果子表中有匹配的记录，则不允许对父表对应候选键进行 update/delete 操作 Restrict 方式【默认】：同 no action， 都是立即检查外键约束 Set default 方式 （在可视化工具 SQLyog 中可能显示空白）：父表有变更时，子表将外键列设置成一个默认的值，但 Innodb 不能识别。 对于外键约束，最好是采用：ON UPDATE CASCADE ON DELETE RESTRICT的方式。 15.5.4 添加外键约束 建表时 123456789create table 主表名称( 字段1 数据类型 primary key, 字段2 数据类型);create table 从表名称( 字段1 数据类型 primary key, 字段2 数据类型, [CONSTRAINT &lt;外键约束名称&gt;] FOREIGN KEY（从表的某个字段) references 主表名(被参考字段)); 建表时规定约束等级 1234567891011create table dept( did int primary key, #部门编号 dname varchar(50) #部门名称);create table emp( eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int, #员工所在的部门 foreign key (deptid) references dept(did) on update cascade on delete set null #把修改操作设置为级联修改等级，把删除操作设置为set null等级); 建表后 1ALTER TABLE 从表名 ADD [CONSTRAINT 约束名] FOREIGN KEY (从表的字段) REFERENCES 主表名(被引用字段) [on update xx][on delete xx]; 15.5.5 删除外键约束 先查看约束名和删除外键约束 12SELECT * FROM information_schema.table_constraints WHERE table_name = &#x27;表名称&#x27;;#查看某个表的约束名ALTER TABLE 从表名 DROP FOREIGN KEY 外键约束名; 查看索引名和删除索引。（注意，只能手动删除） 12SHOW INDEX FROM 表名称; #查看某个表的索引名ALTER TABLE 从表名 DROP INDEX 索引名; 15.5.6 外键约束规范 15.6 CHECK 约束 检查某个字段的值是否符合 xx 要求，一般指的是值的范围。 MySQL5.7 可以使用 check 约束，但 check 约束对数据验证没有任何作用。添加数据时，没有任何错误或警告。MySQL 8.0 中可以使用 check 约束。 12345create table employee( eid int primary key, ename varchar(5), gender char check (&#x27;男&#x27; or &#x27;女&#x27;)); 15.7 DEFAULT 约束 15.7.1 作用 给某个字段/某列指定默认值，一旦设置默认值，在插入数据时，如果此字段没有显式赋值，则赋值为默认值。 默认值约束一般不在唯一键和主键列上加。 15.7.2 添加约束 建表时 12345678910111213create table 表名称( 字段名 数据类型 primary key, 字段名 数据类型 unique key not null, 字段名 数据类型 unique key, 字段名 数据类型 not null default 默认值,);create table 表名称(再举例： 字段名 数据类型 default 默认值 , 字段名 数据类型 not null default 默认值, 字段名 数据类型 not null default 默认值, primary key(字段名), unique key(字段名)); 建表后： 123alter table 表名称 modify 字段名 数据类型 default 默认值;alter table 表名称 modify 字段名 数据类型 default 默认值 not null; 如果这个字段原来有非空约束，还需要保留非空约束时，那么在加默认值约束时，还得保留非空约束，否则非空约束就被删除了。 同理，在给某个字段加非空约束也一样，如果这个字段原来有默认值约束，还想保留，也要在 modify 语句中保留默认值约束，否则就删除了。 15.7.3 删除约束 12alter table 表名称 modify 字段名 数据类型 ;#删除默认值约束，也不保留非空约束alter table 表名称 modify 字段名 数据类型 not null; #删除默认值约束，保留非空约束 15.8 自增列：AUTO_INCREMENT 15.8.1 作用 某个字段的值自增. 15.8.2 特点 一个表最多只能有一个自增长列 当需要产生唯一标识符或顺序值时，可设置自增长 自增长列约束的列必须是键列（主键列，唯一键列） 自增约束的列的数据类型必须是整数类型。 如果自增列指定了 0 和 null，会在当前最大值的基础上自增；如果自增列手动指定了具体值，直接赋值为具体值。 15.8.3 添加约束 建表时 123456789101112create table 表名称( 字段名 数据类型 primary key auto_increment, 字段名 数据类型 unique key not null, 字段名 数据类型 unique key,字段名 数据类型 not null default 默认值,);create table 表名称( 字段名 数据类型 default 默认值 , 字段名 数据类型 unique key auto_increment, 字段名 数据类型 not null default 默认值,, primary key(字段名)); 建表后 1alter table 表名称 modify 字段名 数据类型 auto_increment; 15.8.4 删除约束 1alter table 表名称 modify 字段名 数据类型; #去掉auto_increment相当于删除 15.8.5 MySQL 8.0 新特性—自增变量的持久化 MySQL8.0 之前，删除自增的某一列，添加新数据时，会从删除前开始，但如果重启数据库，则延续既有的值。 比如，自增列值为 1，2，3，4，此时删除 4，添加一条新数据，则为 1，2，3，5。再删除 5，重启数据库，添加一条新数据，则为 1，2，3，4。 这是因为组件自增没有持久化，在 MySQL 5.7 系统中，对于自增主键的分配规则，是由 InnoDB 数据字典内部一个 计数器 来决定的，而该计数器只在内存中维护 ，并不持久化到磁盘中。当数据库重启时，该计数器会被初始化。 MySQL 8.0 将自增主键的计数器持久化到 重做日志 中。每次计数器发生改变，都会将其写入重做日志中。如果数据库重启，InnoDB 会根据重做日志中的信息来初始化计数器的内存值。 15.9 常见面试问题 问题 1：如果两个表之间有关系（一对一、一对多），比如：员工表和部门表（一对多），它们之间是否一定要建外键约束？ 答：不是的 问题 2：建和不建外键约束有什么区别？ 答：建外键约束，操作（创建表、删除表、添加、修改、删除）会受到限制，从语法层面受到限制。例如：在员工表中不可能添加一个员工信息，它的部门的值在部门表中找不到。 不建外键约束，操作（创建表、删除表、添加、修改、删除）不受限制，要保证数据的 引用完整性 ，只能依靠程序员的自觉，或者是 在 Java 程序中进行限定 。例如：在员工表中，可以添加一个员工的信息，它的部门指定为一个完全不存在的部门。 问题 3：那么建和不建外键约束和查询有没有关系？ 答：没有 问题 4：为什么建表时，加 not null default ‘’ 或 default 0 答：不想让表中出现 null 值。 问题 5：为什么不想要 null 的值。 答:（1）不好比较。null 是一种特殊值，比较时只能用专门的 is null 和 is not null 来比较。碰到运算符，通常返回 null。 （2）效率不高。影响提高索引效果。因此，往往在建表时使用not null default ''或 default 0 问题 6：带 AUTO_INCREMENT 约束的字段值是从 1 开始的吗？ 答：在 MySQL 中，默认 AUTO_INCREMENT 的初始值是 1，每新增一条记录，字段值自动加 1。设置自增属性（AUTO_INCREMENT）的时候，还可以指定第一条插入记录的自增字段的值，这样新插入的记录的自增字段值从初始值开始递增，如在表中插入第一条记录，同时指定 id 值为 5，则以后插入的记录的 id 值就会从 6 开始往上增加。添加主键约束时，往往需要设置字段自动增加属性。 问题 7：是不是每个表都可以任意选择存储引擎？ 答：MySQL 支持多种存储引擎，每一个表都可以指定一个不同的存储引擎，需要注意的是：外键约束是用来保证数据的参照完整性的，如果表之间需要关联外键，却指定了不同的存储引擎，那么这些表之间是不能创建外键约束的。所以说，存储引擎的选择也不完全是随意的。 9. 索引 作用：提高数据库的性能，在不增加内存、不修改程序、不调用 sql 的情况下，提升查询性能。 细节： 创建索引后，会导致表文件的的大小增加。 创建索引只会提升该列的查询速度，不会提升其他列。 原理：没有创建索引时，查询数据采用全表扫描的方式；创建索引后，会按照索引列创建一个易于查询的数据结构，比如二叉树。 类型： 主键索引：有PRIMAY KEY约束的索引 唯一索引：有UNIQUE约束的索引、 普通索引：通过INDEX创建的索引 全文索引：FULLTEXT，适用于MyISAM 一般不使用 mysql 自带的全文索引，而是使用全文搜索框架Solr或ElasticSearch(ES) 查询是否有有索引： SHOW INDEX FROM table_name SHOW INDEXES FROM table_name SHOW KEYS FROM table_name 创建： 主键索引：ALTER TABLE table_name ADD PRIMARY KEY (id)——添加主键的第三种方式，也是创建主键索引的方式 唯一索引：CREATE UNIQUE INDEX 索引名 ON table_name(col) 普通索引：CREATE INDEX 索引名 ON table_name(col) 删除： DROP INDEX 索引名 ON table_name ALTER TABLE table_name DROP PRIMARY KEY 修改：需要先删除，再创建 使用规律： 较频繁的查询字段应创建索引。 唯一性太差的字段即使查询频繁，但一般不作为索引。 更新频繁的字段不适合创建索引。 不会作为WHERE查询条件的字段不该创建索引。 10. 事务 10.1 基本概念 事务概念：一组相关的 DML 语句（增、删、改）在执行时，为了保证数据的一致性，需要全部成功，或者全部失败。 原理：当执行事务操作（DML 语句）时，mysql 会在表上加锁，防止其他用户改变表的数据。 事务的几个重要操作： start transaction | set autocommit=off：开始一个事务 savepoint 点名：设置保存点 执行commit会自动删除该事务所定义的所有保存点。 rollback to 点名：回退事务到该点 rollback：回退所有事务 回退所有事务后，不能再执行回退至指定点。 commit：提交事务，操作后不能再回退。 本事务提交后，所有的 DML 数据会更新到数据库中，其他连接可以看到更新后的数据。 使用细节： 不开始事务，默认情况下 DML 操作是自动提交的，没有保存点，也不能回滚。 开始一个事务，未创建保存点时执行rollback，会回退到事务开始的状态。 开启一个事务后，在commit前，可以创建多个保存点，并可以回退到指定点。 注意：回退操作是单向的，即一旦回退到更早的位置，想再回退到最近的位置，是不能回退的。（A、B、C 的顺序，选择了回退到 A，这不能再回退至 B 或 C） 事务机制需要INNODB引擎，MYISAM不起作用。 10.2 事务隔离级别 事务隔离：多个连接开启各自事务操作数据库时，数据库系统负责隔离操作，保证各个俩捏在获取数据时的准确性（保证获取到的数据时开启事务时的数据，每个事务对表中数据的更新，不影响其他事务看到的数据的改变）。 未隔离后果： 脏读（dirty read）：一个事务读取到另一个事务尚未提交的 DML 操作的数据。 不可重复读（nonrepeatable read）：一个事务在执行 DML（修改、删除）操作，另一个事务在进行 DQL（查询）操作，每次查询返回的结果集不同。 幻读（phantom read）：一个事务在执行 DML（插入）操作，另一个事务在进行 DQL（查询）操作，每次查询返回的结果集不同。 事务隔离级别：事务与事务之间的隔离程度。 分类： 隔离级别 脏读 不可重复读 幻读 加锁读 读未提交（read uncommitted) √ √ √ 不加锁 读已提交（read committed） × √ √ 不加锁 可重复读（repeatable read） × × × √ 不加锁 可串行化（serializable） × × × 加锁 默认级别为repeatable read，一般不需要修改。 可串行化级别的隔离会让本事务执行查询时处于等待状态，直到其他事务进行 commit 或者等待超时。 操作： 查看当前会话隔离级别：select @@tx_isolation 查看系统当前隔离级别：select @@global.tx_isolation 设置当前会话隔离级别：set session transaction isolation level 隔离级别 设置系统当前隔离级别：set global transaction isolation level 隔离级别 全局修改隔离级别：编辑数据库文件路径下的my.ini文件，在最后添加transaction-isolation = 隔离级别 10.3 ACID 原子性（atomicity）：事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（consistency）：事务必须使数据库从一个一致性状态变换到另一个一致性状态。 隔离性（isolation）：多个用户并发访问数据库时，多个事务的数据操作相互隔离，不会互相干扰。 持久性（durability）：一个事务一旦被提交，则它对数据库的改变就是永久性的，不会因为数据库发生故障等情况也不会有任何影响。 11. mysql 表类型和存储类型 表类型：mysql 的存储引擎（storage engines）决定了表类型。 分类： 事务安全型： innodb 非事务安全型： mysiam memory 显示当前数据库支持的存储引擎：show engines 主要的存储引擎/表的特点 MyISAM、InnoDB、MEMORY 对比 innodb：支持事务、支持外键、支持行级锁 myisam：添加速度快、不支持事务和外键、支持表级锁 memory：数据存储在内存中【关闭 mysql 服务数据丢失，表结构存在】、执行速度很快（没有 io 读写）、默认支持索引（hash 表） 表的存储引擎选择 16. 视图 16.1 视图的作用 16.1.1 使徒的理解 视图一方面可以帮我们使用表的一部分而不是所有的表，另一方面也可以针对不同的用户制定不同的查询视图。 比如，针对一个公司的销售人员，我们只想给他看部分数据，而某些特殊的数据，比如采购的价格，则不会提供给他。 再比如，人员薪酬是个敏感的字段，那么只给某个级别以上的人员开放，其他人的查询视图中则不提供这个字段。 16.1.2 视图的优点 操作简单 将经常使用的查询操作定义为视图，可以使开发人员不需要关心视图对应的数据表的结构、表与表之间的关联关系，也不需要关心数据表之间的业务逻辑和查询条件，而只需要简单地操作视图即可，极大简化了开发人员对数据库的操作。 减少数据冗余 视图跟实际数据表不一样，它存储的是查询语句。所以，在使用的时候，我们要通过定义视图的查询语句来获取结果集。而视图本身不存储数据，不占用数据存储的资源，减少了数据冗余。 数据安全 MySQL 将用户对数据的 访问限制 在某些数据的结果集上，而这些数据的结果集可以使用视图来实现。用户不必直接查询或操作数据表。这也可以理解为视图具有 隔离性 。视图相当于在用户和实际的数据表之间加了一层虚拟表。 同时，MySQL 可以根据权限将用户对数据的访问限制在某些视图上，用户不需要查询数据表，可以直接通过视图获取数据表中的信息。这在一定程度上保障了数据表中数据的安全性。 适应灵活多变的需求 当业务系统的需求发生变化后，如果需要改动数据表的结构，则工作量相对较大，可以使用视图来减少改动的工作量。这种方式在实际工作中使用得比较多。 能够分解复杂的查询逻辑 数据库中如果存在复杂的查询逻辑，则可以将问题进行分解，创建多个视图获取数据，再将创建的多个视图结合起来，完成复杂的查询逻辑 16.1.3 视图的不足 基表的结构变更了，需要及时对相关的视图进行相应的维护。特别是嵌套的视图（就是在视图的基础上创建视图），维护会变得比较复杂， 可读性不好 ，容易变成系统的潜在隐患。因为创建视图的 SQL 查询可能会对字段重命名，也可能包含复杂的逻辑，这些都会增加维护的成本。 实际项目中，如果视图过多，会导致数据库维护成本的问题。 16.2 视图的概念 基本概念：虚拟表，内容由查询定义，数据来源于真实表（基表）。 特点： 通过视图可以修改基表的数据。 基表的数据改变（MDL），会影响视图的数据。 视图的创建和删除只影响视图本身，不影响对应的基表。 可以将视图理解为存储起来的 SELECT 语句 创建视图后，本地数据库文件并没有视图的数据文件，只有一个视图结构的文件（形式：视图名.frm） 16.3 视图操作 16.3.1 创建视图 12345CREATE [OR REPLACE][ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;]VIEW 视图名称 [(字段列表)]AS 查询语句[WITH [CASCADED|LOCAL] CHECK OPTION] 精简版 12CREATE VIEW 视图名称AS 查询语句 在创建视图时，没有在视图名后面指定字段列表，则视图中字段列表默认和 SELECT 语句中的字段列表一致。如果 SELECT 语句中给字段取了别名，那么视图中的字段名和别名相同。 16.3.2 基于视图创建视图 举例：联合 emp_dept 视图和 emp_year_salary 视图查询员工姓名、部门名称、年薪信息创建 emp_dept_ysalary 视图。 【把视图看作一张表即可】 12345CREATE VIEW emp_dept_ysalaryASSELECT emp_dept.ename,dname,year_salaryFROM emp_dept INNER JOIN emp_year_salaryON emp_dept.ename = emp_year_salary.ename; 16.3.3 查看视图 查看数据库的表对象、视图对象 1SHOW TABLES; 查看视图结构 1DESC / DESCRIBE 视图名称; 查看视图信息（显示数据表的存储引擎、版本、数据行数和数据大小等） 1SHOW TABLE STATUS LIKE &#x27;视图名称&#x27;\\G 查看视图的详细定义信息 1SHOW CREATE VIEW 视图名称; 16.3.4 更新视图数据 MySQL 支持使用 INSERT、UPDATE 和 DELETE 语句对视图中的数据进行插入、更新和删除操作。当视图中的数据发生变化时，数据表中的数据也会发生变化，反之亦然。 要使视图可更新，视图中的行和底层基本表中的行之间必须存在 一对一 的关系。另外当视图定义出现如下情况时，视图不支持更新操作： 在定义视图的时候指定了“ALGORITHM = TEMPTABLE”，视图将不支持 INSERT 和 DELETE 操作； 视图中不包含基表中所有被定义为非空又未指定默认值的列，视图将不支持 INSERT 操作； 在定义视图的 SELECT 语句中使用了 JOIN 联合查询 ，视图将不支持 INSERT 和 DELETE 操作； 在定义视图的 SELECT 语句后的字段列表中使用了 数学表达式或子查询 ，视图将不支持 INSERT，也不支持 UPDATE 使用了数学表达式、子查询的字段值； 在定义视图的 SELECT 语句后的字段列表中使用 DISTINCT 、 聚合函数 、 GROUP BY、HAVING 、UNION 等，视图将不支持 INSERT、UPDATE、DELETE； 在定义视图的 SELECT 语句中包含了子查询，而子查询中引用了 FROM 后面的表，视图将不支持 INSERT、UPDATE、DELETE； 视图定义基于一个 不可更新视图 ； 常量视图。 16.3.5 修改视图 使用 CREATE OR REPLACE VIEW 子句修改视图 123456CREATE OR REPLACE VIEW empvu80(id_number, name, sal, department_id)ASSELECT employee_id, first_name || &#x27; &#x27; || last_name, salary, department_idFROM employeesWHERE department_id = 80; ALTER VIEW 123ALTER VIEW 视图名称AS查询语句 16.3.6 删除视图 删除视图只是删除视图的定义，并不会删除基表的数据。 1DROP VIEW IF EXISTS 视图名称; 17. 存储过程与函数 17.1 存储过程 17.1.1 概述 含义：存储过程的英文是 Stored Procedure 。就是一组经过预先编译 的 SQL 语句的封装。 执行过程：存储过程预先存储在 MySQL 服务器上，需要执行的时候，客户端只需要向服务器端发出调用存储过程的命令，服务器端就可以把预先存储好的这一系列 SQL 语句全部执行。 好处： 简化操作，提高了 sql 语句的重用性，减少了开发程序员的压力 减少操作过程中的失误，提高效率 减少网络传输量（客户端不需要把所有的 SQL 语句通过网络发给服务器） 减少了 SQL 语句暴露在网上的风险，也提高了数据查询的安全性 和视图、函数的对比： 它和视图有着同样的优点，清晰、安全，还可以减少网络传输量。 视图是 虚拟表，通常不对底层数据表直接操作，而存储过程是程序化的 SQL，可以直接操作底层数据表 ，相比于面向集合的操作方式，能够实现一些更复杂的数据处理。 一旦存储过程被创建出来，使用它就像使用函数一样简单，直接通过调用存储过程名即可。相较于函数，存储过程是没有返回值的。 分类：存储过程的参数类型可以是 IN、OUT 和 INOUT（IN、OUT、INOUT 都可以在一个存储过程中带多个）。根据这点分类如下： 没有参数（无参数无返回） 仅仅带 IN 类型（有参数无返回） 仅仅带 OUT 类型（无参数有返回） 既带 IN 又带 OUT（有参数有返回） 带 INOUT（有参数有返回） 17.1.2 创建存储过程 12345CREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名 参数类型,...)[characteristics ...]BEGIN 存储过程体END 参数名前面的符号的含义： IN ：当前参数为输入参数，也就是表示入参； 存储过程只是读取这个参数的值。如果没有定义参数种类， 默认就是 IN，表示输入参数。 OUT ：当前参数为输出参数，也就是表示出参； 执行完成之后，调用这个存储过程的客户端或者应用程序就可以读取这个参数返回的值了。 INOUT ：当前参数既可以为输入参数，也可以为输出参数。 形参类型可以是 MySQL 数据库中的任意类型。 characteristics 表示创建存储过程时指定的对存储过程的约束条件，其取值信息如下： 12345LANGUAGE SQL| [NOT] DETERMINISTIC| &#123; CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA &#125;| SQL SECURITY &#123; DEFINER | INVOKER &#125;| COMMENT &#x27;string&#x27; LANGUAGE SQL ：说明存储过程执行体是由 SQL 语句组成的，当前系统支持的语言为 SQL。 [NOT] DETERMINISTIC ：指明存储过程执行的结果是否确定。 DETERMINISTIC 表示结果是确定的。每次执行存储过程时，相同的输入会得到相同的输出。 NOT DETERMINISTIC 表示结果是不确定的，相同的输入可能得到不同的输出。 如果没有指定任意一个值，默认为 NOT DETERMINISTIC。 { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } ：指明子程序使用 SQL 语句的限制。 CONTAINS SQL 表示当前存储过程的子程序包含 SQL 语句，但是并不包含读写数据的 SQL 语句； NO SQL 表示当前存储过程的子程序中不包含任何 SQL 语句； READS SQL DATA 表示当前存储过程的子程序中包含读数据的 SQL 语句； MODIFIES SQL DATA 表示当前存储过程的子程序中包含写数据的 SQL 语句。 默认情况下，系统会指定为 CONTAINS SQL。 SQL SECURITY { DEFINER | INVOKER } ：执行当前存储过程的权限，即指明哪些用户能够执行当前存储过程。 DEFINER 表示只有当前存储过程的创建者或者定义者才能执行当前存储过程； INVOKER 表示拥有当前存储过程的访问权限的用户能够执行当前存储过程。 如果没有设置相关的值，则 MySQL 默认指定值为 DEFINER。 COMMENT ‘string’ ：注释信息，可以用来描述存储过程。 存储过程体中可以有多条 SQL 语句，如果仅仅一条 SQL 语句，则可以省略 BEGIN 和 END。 BEGIN…END：BEGIN…END 中间包含了多个语句，每个语句都以（;）号为结束符。 DECLARE：DECLARE 用来声明变量，使用的位置在于 BEGIN…END 语句中间，而且需要在其他语句使用之前进行变量的声明。 SET：赋值语句，用于对变量进行赋值。 SELECT… INTO：把从数据表中查询的结果存放到变量中，也就是为变量赋值。 需要设置新的结束标记：DELIMITER 新的结束标记 因为 MySQL 默认的语句结束符号为分号;。为了避免与存储过程中 SQL 语句结束符相冲突，需要使用 DELIMITER 改变存储过程的结束符。 比如：DELIMITER //语句的作用是将 MySQL 的结束符设置为//，并以END //结束存储过程。存储过程定义完毕之后再使用DELIMITER ;恢复默认结束符。DELIMITER 也可以指定其他符号作为结束符。 当使用 DELIMITER 命令时，应该避免使用反斜杠（‘\\’）字符，因为反斜线是 MySQL 的转义字符。 13. Mysql 管理 13.1 mysql 用户 mysql 用户存储在 mysql 数据库中的 user 表中 host：允许登陆的位置，localhost 表示只允许本机登录，可以通过 ip 地址修改 user：用户名 authentication_string：默认使用 password()函数加密之后的密码 13.2 创建用户 CREATE USER '用户名'@'允许登录位置' IDENTIFIED BY '密码' 不指定密码时，默认为空（无密码） 13.3 删除用户 DROP USER '用户名'@'允许登录位置' 13.4 修改用户密码 修改自己的密码： SET PASSWORD = PASSWORD('新密码') 修改别人的密码：（需要具有权限） SET PASSWORD FOR '用户名'@'登录位置' = PASWORD('密码') 13.5 MYSQL 中的权限 13.6 给用户授权 13.7 回收用户权限 13.8 权限生效指令 13.9 用户管理细节","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://sk370.github.io/tags/MySQL/"}]},{"title":"Java语言新特性","date":"2022-05-28T09:19:24.000Z","path":"2022/05/28/javase/Java新特性/","text":"本文是对Java语言新特性的介绍，对Java发展历史或当下生产环境中主要使用到的特性进行简单描述， 第 1 章 Java8 新特性 1.1 Lambda 表达式 Lambda 是一个匿名函数，可以理解为可以传递的代码。 语法：-&gt;Lambda 操作符、箭头操作符，符号左侧为参数列表，右侧为 Lambda 体（方法体） 格式一：无参、无返回值：Runnable r1 = () -&gt; &#123;System._out_.println(&quot;hello&quot;);&#125;; 如果 Lambda 体只有一条语句，则&#123;&#125;可以省略：Runnable r1 = () -&gt; System._out_.println(str); 格式二：一个参数、无返回值：Consumer&lt;String&gt; con = (String str) -&gt; &#123;System._out_.println(str);&#125;; 一句 Lambda 体可省略&#123;&#125;：Consumer&lt;String&gt; con = (String str) -&gt; System._out_.println(str); 省略参数类型（根据上下文环境自动类型推断）：Consumer&lt;String&gt; con = (str) -&gt; System._out_.println(str); 省略形参()：Consumer&lt;String&gt; con = str -&gt; System._out_.println(str); 格式三：一个参数，有返回值 return 可以省略，同时必须省略&#123;&#125;? 格式四：两个及以上参数，无返回值？ 格式五：两个及以上参数，有返回值: 多个 Lambda 体：Compartor&lt;Integer&gt; com = (Integer x, Integer y) -&gt; &#123;System._out_.println(&quot;hello&quot;);return Integer.compare(x,y&#125;; 省略参数类型（根据上下文环境自动类型推断）：Compartor&lt;Integer&gt; com = (x, y) -&gt; &#123;System._out_.println(&quot;hello&quot;);return Integer.compare(x,y&#125;; 一个 Lambda 体：Compartor&lt;Integer&gt; com = (Integer x, Integer y) -&gt; &#123;return Integer.compare(x,y&#125;; - 省略参数类型（根据上下文环境自动类型推断）：Compartor&lt;Integer&gt; com = (x, y) -&gt; &#123;return Integer.compare(x,y&#125;; 一句 Lambda 体可省略&#123;&#125;及return：Compartor&lt;Integer&gt; com = (x, y) -&gt; Integer.compare(x,y); 注意：省略&#123;&#125;必须省略return，即Compartor&lt;Integer&gt; com = (x, y) -&gt; return Integer.compare(x,y);为错误形式 总结： 左边： 形参列表的类型可以省略 只有一个参数()可以省略 右边： Lambda 一般使用&#123;&#125;包裹 一条 Lambda 可以省略&#123;&#125; 一条 return 省略语句，省略&#123;&#125;必须省略return 本质：作为接口的实例（且是该接口只有一个方法，这是唯一的实例） 注意点： Lambda 只是代替了声明部分，要想执行还得调用其方法。 Lambda 之所以能这样写，是因为这些接口内部定义的方法只有这一个。 使用场景： 函数式接口 函数式接口需要匿名实现类 Lambda 表达式作为参数传递（调用时） 1.2 Stream API 1.2.1 概述 Stream API 是设计用于处理 MongDB、Radis 等 NoSQL 数据库，因为这类数据库需要 Java 层面去处理数据 Stream API 位于java.util.stream包下。使用了函数式编程风格。 Stream API 是设计用于处理 MongDB、Radis 等 NoSQL 数据库的，而这类数据库是 key-val 类型的，即 Java 中集合类型。所以 Stream API 是处理集合的。 Stream API 特点： Stream 不会存储元素，而是处理元素 Stream 处理元素不会改变原数据，而会返回一个 Stream Stream 是面向 CPU 的，是完成计算的 Stream 的操作是延迟执行的，即需要得到结果时才执行 Stream 操作步骤： 创建 Stream，得到 Stream 的实例 中间操作（中间操作链），对数据原进行数据处理 终止操作，执行终止操作后，就开始执行中间操作链，并得到结果。执行完毕进行废弃。 1.2.2 创建 Stream 通过 Collection 集合创建：集合的接口中定义了两个获取 Stream 流的默认方法，通过该接口的实例化对象，可以创建 Stream。 顺序流：default Stream&lt;E&gt; stream() 并行流：default Stream&lt;E&gt; parallelStream() 通过 Arrays 的静态方法： static &lt;T&gt; Stream&lt;T&gt; stream(T[] array) 重载类型： public static IntStream stream(int[] array) public static LongStream stream(long[] array) public static DoubleStream stream(double[] array) 通过 Stream 类的静态方法 of()方法：public static&lt;T&gt; Stream&lt;T&gt; of(T... values)，有限流。 iterate()方法——迭代：public static&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f)，无限流 generate()方法——生成：public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s)，无限流 1.2.3 中间操作 筛选与切片： filter(Predicate p)：获取满足该 p 条件的元素，接收 Lambda 表达式参数 distinct()：去重，通过元素的 hasCode()和 equals()判断 limit(Long maxSize)：截断，获取指定个数的元素 skip(Long n)：跳过，跳过指定个数的元素 映射： map(Function f)：接收一个函数作为参数，该函数会被应用到每个元素上，并将其映射成一个新的元素。 mapToDouble(ToDoubleFunction f) mapToInt(ToIntFunction f) mapToLong(ToLongFunction f) flatMap(Function f)：接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流 排序： sorted()：产生一个新流，其中按自然顺序排序 sorted(Comparator com)：产生一个新流，其中按比较器顺序排序 1.2.4 终止操作 匹配：返回 Boolean allMatch(Predicate p)：是否匹配所有元素 anyMath(Predicate p)：是否存在匹配的元素 noneMatch(Predicate p)：是否没有匹配所有元素 查找： findFirst()：返回第一个元素 findAny()：返回当前流中的任意元素 count()：返回流中元素总数 max(Comparator c)：返回流中最大值 min(Comparator c)：返回流中最小值 forEach(Consumer c)：内部迭代，遍历元素 归约： reduce(T iden, BinaryOperator b)：可以将流中元素反复结合起来，得到一个值。返回 T reduce(BinaryOperator b)：可以将流中元素反复结合起来，得到一个值。返回 Optional 备注：map 和 reduce 的连接通常称为 map reduce 模式，因 Google 用它来进行网络搜索而出名。 收集： collect(Collector c)：将流转换为其他形式。接收一个 Collector 接口的实现，用于给 Stream 中元素做汇总的方法 1.3 Optional 类 1.3.1 概述 Optional 类(java.util.Optional) 是一个容器类， 是一个可以为 null 的容器对象。如果值存在则 isPresent()方法会返回 true，调用 get()方法会返回该对象。 好处：原来用 null 表示一个值不存在，现在 Optional 可以更好的表达这个概念。并且可以避免空指针异常。 1.3.2 创建 Optional 类对象 Optional.of(T t) : 创建一个 Optional 实例， t 必须非空 Optional.empty() : 创建一个空的 Optional 实例 Optional.ofNullable(T t)： t 可以为 null 1.3.3 判断 Optional 是否包含对象 boolean isPresent(): 判断是否包含对象 void ifPresent(Consumer&lt;? super T&gt; consumer)：如果有值，就执行 Consumer 接口的实现代码，并且该值会作为参数传给它。 1.3.4 获取 Optional 容器的对象 T get(): 如果调用对象包含值，返回该值，否则抛异常 T orElse(T other)：如果有值则将其返回，否则返回指定的 other 对象。 T orElseGet(Supplier&lt;? extends T&gt; other)：如果有值则将其返回，否则返回由 Supplier 接口实现提供的对象。 T orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier)：如果有值则将其返回，否则抛出由 Supplier 接口实现提供的异常。 1.4 函数式（Functional）接口 只能创建一个抽象方法的接口。 定义接口时使用@FunctionalInterface注解，用于编译时检查。同时 javadoc 会包含该声明 @FunctionalInterface注解声明的接口，只能包含一个抽象方法，可以包含多个默认方法，也可以包含多个静态方法。 Java8 的函数式接口定义在java.util.function中 函数式接口的理解： 函数式接口的出现，使 Java 既可以支持 OOP，还可以支持 OOF（面向函数编程） 但在 Java 语言中，Lambda 表达式是对象，而不是函数，恰恰是因为函数式接口——特别的对象类型 Lambda 表达式就是一个函数式接口的实例。 即，只要一个对象是函数式接口的实例，那么该对象就可以用 Lambda 表达式表示。 Java 内置四大核心函数式接口： 其他内置接口： 1.5 方法引用及构造器引用 1.5.1 方法引用 对 Lambda 表达式的优化简写，通过方法的名字指向一个方法。 语法：::将类（或对象）与方法名分隔开 格式： 对象::实例方法名 没有对象::静态方法名，因为没必要 类::静态方法名 类::实例方法名 使用要求：接口中的抽象方法的参数列表、返回值类型，与方法引用的方法的参数列表和返回值类型一致。 类::实例方法名会出现抽象方法与方法引用方法参数列表匹配的情况，但还是可以使用，原因是，参数列表中第一个参数是作为方法的调用者出现的。 1.5.2 构造器引用 使用要求：接口中的抽象方法的参数列表，与构造器的参数列表和一致，且抽象方法的返回值为构造器对应类的对象. 格式：ClassName::new 1.5.3 数组引用 格式：type[]::new","tags":[{"name":"Java8","slug":"Java8","permalink":"https://sk370.github.io/tags/Java8/"}]},{"title":"Java语言基础（上）","date":"2022-05-28T09:16:03.000Z","path":"2022/05/28/javase/Java语言基础（上）/","text":"本文介绍了Java语言的基本特性，如数据类型、数组、类和对象、接口、异常等，是步入Java编程语言的第一步。 第 1 章 java 语言概述 1.0 计算机基础 1.0.1 基本知识 现代计算机基础：冯诺依曼体系结构 IT 定律（硬件）： 摩尔定律：18-24 个月，计算机性能提升一倍。 反摩尔定律：如果现在和 18 个月前卖掉同样多的同样的产品，营业也就会下降一半。 安迪-比尔定律：硬件性能的提高，很快会被软件消耗掉。 DOS 命令： md：创建文件夹 rd：删除文件夹 del：删除文件 cd：切换路径 dir：查看当前路径下的文件和文件夹 cd：切换路径 cd \\：切换到根路径。 tree：指定目录下的所有文件（显示文件树） cls：清屏 exit：退出 dos 编码： ASCII：一个字节表示一个字符，共 128 个。 Unicode：两个字节表示一个字符，字母和汉字都用 2 个字符表示，存在浪费空间问题。兼容 ASCII 编码 UTF-8：大小可变编码，字母用一个字节，汉字用 3 个字节 gbk：字母用 1 个字节，汉字用 2 个字节。 gb2312：同 gbk，表示范围小于 gbk big5：繁体中文 1.0.2 整数的四种表示方式 二进制：0 和 1，以 0b 或 0B 开头。 八进制：0-7，以 0 开头。 十进制：0-9。 十六进制：0-9 以及 A-F（a-f），以 0x 或 0X 开头。 1.0.3 进制转换 其他进制转十进制： 二进制转十进制：每个位上的数 * 2^indx的和，index 从右向左依次为 0 至 n。 八进制转十进制：每个位上的数 * 8^indx的和，index 从右向左依次为 0 至 n。 十六进制转十进制：每个位上的数 * 16^indx的和，index 从右向左依次为 0 至 n。 十进制转其他进制： 十进制转二进制：该数不断除 2，直到商为 0，将余数倒序拼接。 十进制转八进制：该数不断除 8，直到商为 0，将余数倒序拼接。 十进制转十六进制：该数不断除 16，直到商为 0，将余数倒序拼接。 二进制转其他进制： 二进制转八进制：从右向左，3 个数一组，对应的十进制数组合后即为八进制。 二进制转十六进制：从右向左，4 个数一组，对应的十进制数组合后即为十六进制。 其他进制转二进制： 八进制转二进制：将八进制的每一个数，转成对应的 3 位数二进制即可。 十六进制转二进制：将十六进制的每一个数，转成对应的 4 位数二进制即可。 1.1 java 简介 1.1.1 Java 的常见概念及关系 java SE、Java EE、Java ME 的关系 JDK 和 JRE： JDK： Java Development Kit，把 Java 源码编译成 Java 字节码。 JDK=JRE+Java 开发工具 JRE： Java Runtime Environment，运行 Java 字节码的虚拟机。 JRE=JVM+核心类库 JSR 和 JCP： JSR 规范：Java Specification Request JCP 组织：Java Community Process，负责 JSR。 JDK 的可执行文件： java：这个可执行程序其实就是 JVM，运行 Java 程序，就是启动 JVM，然后让 JVM 执行指定的编译后的代码； javac：这是 Java 的编译器，它用于把 Java 源码文件（以.java 后缀结尾）编译为 Java 字节码文件（以.class 后缀结尾）； jar：用于把一组.class 文件打包成一个.jar 文件，便于发布； javadoc：用于从 Java 源码中自动提取注释并生成文档； jdb：Java 调试器，用于开发阶段的运行调试。 Java 的特点： 面向对象（oop） 健壮性：强类型机制、异常处理、垃圾自动收集等。 跨平台的。 解释性的： 解释性语言编译后不能直接被机器运行，需要解释器执行。 编译性语言编译后的代码可以直接被机器执行，如 C，C++ 1.1.2 下载 JDK 程序 由于发展原因：推荐下载 jdk8U201 版本（最后一稳定版本、长期支持版本） 安装 jdk8 时，已经带了 jre，但安装程序最后还会额外安装 jre8，为了避免开发工具配置出错（如 Eclipse），建议安装。（如果不装有什么问题？） 本机的安装参考： 1.1.3 JDK 环境变量配置 虽然安装 java 时，环境变量Path下已经有默认配置，能够在任何位置的 cmd 终端执行java命令。 但依赖 JDK 环境变量的其他程序配置仍然需要重新配置JAVA_HOME，如 Tomcat，不配置会出现执行 Tomcat 的开启命令会出现一闪而过的情况，Tomcat 也不会运行起来。 配置参数： JAVA_HOME： CLASSPATH：.;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar;——JDK5 之后不需要配置了，jre 会自动搜索当前路径下的 jar 包，并加载 dt.jar 和 tools.jar，这是 Oracle 公司的改进设计。这个路径表示 class 文件的路径，即 JVM 去哪里寻找要运行的 class 文件。 Path：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; 1.1.4 java 命令基本使用 运行 Java 程序： javac 文件名.java编译代码，生成文件名.class字节码文件。 java 文件名自动查找对应的文件.class文件并执行。 java11 后，**java 文件名.java**可以直接运行一个单文件源码，但多数情况下，无法直接这样操作，因为**文件名.java**文件需要依赖其他库。 1.2 Java 程序基础 1.2.1 Java 程序基本结构 Java 编程的特点： Java 源码的缩进不是必须的。 Java 代码保存为文件时，文件名和 main 方法外部的类名要完全一致。 一个源文件最多只能有一个 public 类，其他类的个数不限。 也可以没有 public 类。 编译后每一个类都对应生成一个.class文件。 每个类都可以有 main 方法，且都可以被执行。 被执行的前提是同文件内有一个 public 类。 执行方式是**java 类名**。 main 方法必须按 java 程序规定的入口语法编写，即public static void main(String[] args) 类名必须以英文字母开头（习惯首字母大写），后接字母、数字、下划线的组合。 public static void main(String[] args)规定的 java 程序入口 main 方法名必须以英文字母开头（习惯首字母小写），后接字母、数字、下划线。 Java 的每一行语句必须以分号结束。 java 注释 单行注释：// 多行注释： 多行注释不能嵌套多行注释。 123/*xxxx*/ - 文档注释：注释内容可以被 JDK 提供的 javadoc 所解析，生成一套以网页文件形式体现在该程序 123/**xxx*/ 转义字符 \\t：一个制表符，实现对齐功能。 \\n：换行符 \\\\：\\ \\r：回车（光标移至本行前面） 霜冷长河\\r寒冬---------&gt;输出后寒冬长河 Java 代码规范： 类、方法的注释，要以 javadoc 的方式书写。 非 javadoc 的注释，写给代码维护者看，为什么这样写、如何修改、注意什么问题。 使用 tab 缩进。 运算符“±*/=”两边各使用一个空格增加代码的可读性。 源文件使用 UTF-8 编码。 每行字符不要超过 80 字符。 代码编写行尾风格（起始大括号在同一行）或次行风格（起始大括号在下一行） 1.1.2 标识符规范 包名：多单词组成时全部写成小写。 文件名、类名、接口名：使用大驼峰形式。 变量名、方法名：小驼峰。 常量名：所有字母都大写，多个单词用下划线连接。 1.1.3 键盘输入语句 使用步骤： 导入Scanner所在的包：import java.util.Scanner; 创建Scanner实例对象：Scanner myScanner = new Scanner(System.in); 使用myScanner的实例对象方法： 接收字符串：String str = myScanner.next(); 接收整数：int num = myScanner.nextInt(); 接收浮点数：double sth = myScanner.netDouble(); 接收字符：char charNum = myScanner.next().charAt(0); 第 2 章 基本语法 2.1 变量和数据类型 2.1.1 变量 概念：相当于内存中一个数据存储空间的表示。 不同类型的变量，占用的内存空间大小不同。 该存储空间的值，可以在同种类型范围内不断变化。 三个基本要素：类型、名称、值。 java 中变量的声明和初始化可以是两条语句，但使用前必须必须赋值进行初始化。 **int num; num = 10;** 注意点： 变量必须先声明，后使用。 同一作用域内，变量不能重名。 int a = b = c;这种初始化方式会报错。 int a, b, c;这种初始化方式不会报错。 输出语句中+的含义： 左右两边都是数值时，做加法运算。 左右一边有字符串时，做拼接运算。 2.1.2 数据类型 基本数据类型 整数型：整数型默认使用 int 大小的存储空间，使用 long 大小的存储空间时结尾加 L。 类型 占用存储空间 表示范围 byte 1 字节 -218-1~218-1-1 short 2 字节 -228-1~228-1-1 int 4 字节 -248-1~248-1-1 long 8 字节 -288-1~288-1-1 浮点类型： 浮点类型都是近似值，因为存在小数位丢失的问题。 内存中的存放形式：浮点数=符号位+指数位+尾数位。 浮点型默认使用 double 大小的存储空间，使用 float 大小的存储空间时末尾加 f 或 F。 浮点型除以整形存在精度问题，在编程中，应当注意对运算结果是小数的运算、判断！应该以两个数差值的绝对值在某个精度范围内判断。 类型 占用存储空间 表示范围 float 4 字节 double 8 字节 字符： 字符型要用单引号，不能使用双引号。 字符型也可以直接赋值（整数），不需要用单引号，输出时显示对应的 Unicode 编码。 字符型本质上是一个整数，可以参与运算。 字符型还可以赋值为转义字符，需要单引号。 字符型也可以赋值为单个汉字，需要单引号。 布尔类型： Java 对布尔类型的存储占用空间大小没有做规定，理论上只需要 1bit（0 或者 1），但由于计算机中最小存储单元为字节（Byte），所以占用应为 1 字节，同时又因为编译后虚拟机（JVM）中布尔类型的值会被转换为 int 类型的数据代替，所以会占用 4 个字节。 Java 中不可以用 0 或者非 0 的整数代替 false 或 true，即 Java 中布尔类型的值只能为 true 或 false。 自动类型转换： Java 中程序在进行赋值或运算时，精度小的类型会自动转换为精度大的类型。 char→int→long→float→double byte、short→int→long→float→double 几点规律： 多种类型的数据混合运算时，系统会将所有数据自动转换为容量最大的那种数据类型，然后再进行计算。 （byte、short）和 char 之间不会相互自动转换。 无法进行容量大的类型向容量小的类型自动转换，会报错。 byte、short、char 在进行运算时，会自动转换为 int，所以需要存储空间大于等于 int 的类型。 int=byte+byte（对），short=byte+byte（错），其他同理。 布尔类型不参与自动类型转换。 强制类型转换 自动类型转换的逆过程。 转换过程中会造成精度降低或溢出。 char 类型可以保存 int 的常量值（自动类型转换，int 自动转 char），但不能保存 int 的变量值。【原因：类型转换问题】 基本数据类型和 String 类型的转换 基本数据类型—&gt;String 类型：基本数据类型 + &quot;&quot; String 类型—&gt;基本数据类型： Byte.parseByte() Short.parseShort() Integer.parseInt() Long.parseLong() Float.parseFloat() Double.parseDouble() Boolean.parseBoolean() char类型没有上述的方法，s.charAt(n)可以获取字符串s指定位置的字符。 注意点： String 类型转换为基本类型时，必须先确保可以转换为对应的类型。如123hello不能转换为int 如果格式不正确，会抛出异常，程序终止。 变量的赋值： 如果变量是基本数据类型，此时赋值的是变量所保存的数据值。 如果变量是引用数据类型，此时赋值的是变量所保存的数据的地址值。 2.2 运算符 2.2.1 算术运算符 %本质是a % b = a - a / b *b，所以**10 % -3** = 10 - 10 / (-3) * (-3) = **1** ++的经典面试题： i = i++：(1)temp=i，(2)i=i+1，(3)i=temp，结果为 i i = ++i：(1)i=i+1，(2)temp=i，(3)i=temp，结果为 i+1 进行b++时不会进行强制类型转换，如： byte b=127;b++不会报错，输出-128。 注意：前减减/前加加的优先级高于=，后加加/后减减的优先级低于= 整数运算在除数为 0 时会报错，而浮点数运算在除数为 0 时，不会报错，但会返回几个特殊值： NaN 表示 Not a Number，0.0 / 0 Infinity 表示无穷大，1.0 / 0 -Infinity 表示负无穷大，-1.0 / 0 2.2.2 关系运算符 关系运算符的结果都是 boolean 类型，要么是 true，要么是 false。 ==判断引用数据类型时，比较的是地址值。 左右两侧数据类型要一致，否则编译报错。 ==判断基本数据类型时，比较的是具体值。 左右两侧不一定类型要相同，会进行自动数据类型提升 boolean 除外，boolean 与不是 boolean 的比较时会报错。 判断字符串时： 字符串以字面量形式声明：比较内容 字符串以 new 关键字声明：比较地址值 规范情况：须使用 isequals（）方法 2.2.3 逻辑运算符 逻辑运算符的结果都是 boolean 类型，要么是 true，要么是 false。 &amp;&amp;：第一个条件为 false 时，立即得出结果为 false，效率高，而且后面部分不会被执行。 &amp;：即使第一个条件为 false，第二个条件的计算还会执行。 ||：第一个条件为 true 时，立即得出结果为 true，效率高，而且后面部分不会被执行。 |：即使第一个条件为 true，第二个条件的计算还会执行。 开发中常用&amp;&amp;和||。 2.2.4 赋值运算符 赋值运算符左边只能是变量，右边可以是变量、表达式、常量值。 复合运算符会进行强制类型转换。 2.2.5 三元运算符 本质实际上是 if-else 语句。 条件表达式1 ？ 表达式1 : 表达式2 表达式 1 和表达式 2 要求类型一致，所以会自动类型提升。 2.2.6 位运算符 原码、反码、补码： 二进制数的最高位为符号位，0 表示正数，1 表示负数。 正数的原码、反码、补码都一样。 负数的反码 = 原码符号位不变，其他位的数取反。 负数的补码 = 反码 + 1，负数的反码 = 补码 - 1。 0 的反码，补码都是 0。 java 没有无符号数 ，即 java 中的数都是有符号的。 0 在 java 中表示正数。 计算机运算的时候，都是以补码进行运算。 使用运算结果的时候，需要看它的原码。 算数右移&gt;&gt;：符号位不变，低位溢出，高位补 0。 m &gt;&gt; n本质是m / 2^n 注意：负数跨越 0 时输出均为-1。 负数的算数右移？ 算术左移&lt;&lt;：符号位不变，高位溢出，低位补 0。 m &lt;&lt; n本质是m * 2^n 逻辑右移或无符号右移&gt;&gt;&gt;：低位溢出，高位补 0（符号位也跟着移动）。 没有&lt;&lt;&lt;。 2.2.7 运算符优先级 ++优先级的问题： 算术运算（赋值）： 逻辑运算： 三元运算： 2.3 流程控制 2.3.1 顺序控制 java 程序默认的执行流程，从上到下逐行执行，所以 变量必须先声明后使用。 2.3.2 分支控制 单分支【if】： 即使执行的代码块语句只有一条，也建议加上&#123;&#125; 当只有一条语句时，可以不用&#123;&#125;直接换行写，也可以不用换行，与if写在同一行。 双分支【if……else】： 多分支【if……else if……else】： 多分支可以没有最后的 else。 多分支只有一个执行入口，即从这个入口执行了语句，其余入口的语句不会被执行 。【先入为主】 如下述的伪代码，最高分为 80 时，考了 90 分的只会从第一个入口进，输出 A，而不会再去判断后面的 B、C、D 123456789if(成绩成绩&gt;=最高分-10)&#123; 等级为A&#125;else if(成绩&gt;=最高分-20)&#123; 等级为B&#125;else if(成绩&gt;=最高分-30)&#123; 等级为C&#125;else&#123; 等级为D&#125; 嵌套分支： 一个分支结构中嵌套了另一个完整的分支结构。 为了保证可读性，不要超过三层。 1234567891011if()&#123; if()&#123; else if()&#123; xxx; &#125;else&#123; xxx; &#125; &#125;else&#123; xxx; &#125;&#125; switch 分支： 穿透：如果 case 语句没有 break 语句，程序会继续执行下一个 case 的语句，而不会进行 case 判断。 switch 表达式的数据类型，应和 case 后面的常量类型一致，或者表达式的数据类型可以自动转换为常量类型。 switch 表达式的返回值必须是 byte、short、int、char、enum【枚举】、String。 case 语句的值必须是常量或常量表达式，不能是变量或带变量的表达式。 123456789101112switch(表达式)&#123; case 常量1: 语句1; break; …… case 常量n: 语句n; break; default: default语句块; break；&#125; switch 和 if 的选择： 运算的结果数值不多，且是 byte、short、int、char、enmu、String 的某种类型，建议使用 swtich。 对于区间判断、结果为 boolean 的判断，建议使用 if。 2.3.3 循环控制 for 循环： 基本语法： 123for(循环变量初始化;循环条件;循环变量迭代)&#123; 循环操作;&#125; 四要素：循环变量初始化、循环条件、循环操作、循环变量迭代。 循环初始值可以有多条初始化语句，但要求类型一样，并用逗号隔开int i = 0, j = 0 循环变量迭代可以有多条变量迭代语句，中间用逗号隔开i++, j++ 如果循环操作只有一条语句，可以省略&#123;&#125;，建议不要省略。 for(;循环条件;) 循环变量初始化和循环变量迭代可以写到其他地方，但是;不能省略。 循环变量初始化写到其他地方时，作用域再循环体外。否则只能作用域循环体内。 for(;;)：表示无限循环。 while 循环： 基本语法： 12345循环变量初始化;while(循环条件)&#123;循环体;循环变量迭代;&#125; 四要素：同 for 循环。 do…while 循环 基本语法 12345循环变量初始化;do&#123; 循环体; 循环变量迭代;&#125;while(循环条件); 四要素：同 for 循环。 注意 while 后面的;。 增强 for 循环 123for(int i : nums) &#123; System.out.println(&quot;i=&quot; + i);&#125; 依次从 nums 数组、枚举类等中取出数据，赋给 i 2.3.4 多重循环控制 建议循环嵌套不要超过 2 层，否则可读性非常差。 外层循环 m 次，内层循环 n 次，则内层循环体实际上需要执行 m*n 次。 break：结束当前循环。 12345678910label1:for(...)&#123; label2: for(...)&#123; label3: for(...)&#123; break label1; &#125; &#125;&#125; break 配合 label 标签可以指定跳出哪层循环。 label 是标签名，由程序员指定，实际开发中，尽量不要使用标签。 如果 break 后没有指定的 label 标签名，则表示跳出最近的循环体. continue：结束当次循环 12345678910label1:for(...)&#123; label2: for(...)&#123; label3: for(...)&#123; continue label1; &#125; &#125;&#125; continue 配合 label 标签可以指定跳出哪层循环。 label 是标签名，由程序员指定，实际开发中，尽量不要使用标签。 如果 contineu 后没有指定的 label 标签名，则表示跳出最近的循环体。 continue 不能用于 switch-case 语句中 return: return 用在方法内时，表示跳出方法，用在 main 方法中表示退出程序。本质上不是退出循环。 第 3 章 数组和数组操作 3.1 数组介绍 3.1.1 一维数组 定义：存放同一类型数据的组合。是一种引用数据类型。 动态初始化——方式一： 声明、创建、定义： 数据类型 数组名[] = new 数据类型[大小] 数据类型[] 数组名 = new 数据类型[大小] 初始化：a[0] = 1;…… 动态初始化——方式二： 声明数组： 数据类型 数组名[]; 数据类型[] 数组名; 声明数组时，[]内不能写数组大小。 创建数组（分配空间）：数组名 = new 数据类型[大小]; 初始化：a[0] = 1;…… 静态初始化——方式一： 数据类型 数组名[] = &#123;值1, 值2,……&#125; 数据类型[] 数组名 = &#123;值1, 值2,……&#125; 静态初始化时，[]内不能写数组大小。 静态初始化——方式二： 数据类型 数组名[] = new 数据类型[]&#123;值1, 值2,……&#125; 数据类型[] 数组名 = new 数据类型[]&#123;值1, 值2,……&#125; 静态初始化时，[]内不能写数组大小。 注意事项： 数组创建后，如果没有进行初始化，默认初始值如下： byte、short、int、long：0 float、double：0.0 char：\\u0000（空字符） boolean：false String：null 数组是引用类型，是对象类型的一种。 数组定义后不能直接改变大小（容量），但可以通过以下两种方式改变大小 数组的拷贝：将一个大容量的数组赋值给小容量的数组 通过 new 关键字重新确定容量大小。 动态初始化情况下改变传入变量的大小控制数组的大小。 3.1.2 二维数组 动态初始化——方式一： 声明、创建、定义： 数据类型 数组名[][] = new 数据类型[大小][大小] 数据类型[][] 数组名 = new 数据类型[大小][大小] 数据类型[] 数组名[] = new 数据类型[大小][大小] 初始化： a[0][0] = 1;…… 或者a[0] = &#123;1,3……&#125; 未初始化的默认值： a[0]：指向内存地址。 a[0][0]：输出该数组对应数据类型的默认值。 动态初始化——方式二： 声明数组： 数据类型 数组名[][]; 数据类型[][] 数组名; 数据类型[] 数组名[]; 创建数组（分配空间）：数组名 = new 数据类型[大小][大小]; 初始化： a[0][0] = 1;…… 或者a[0] = &#123;1,3……&#125; 未初始化的默认值： a[0]：指向内存地址。 a[0][0]：输出该数组对应数据类型的默认值。 动态初始化——列数不确定： 声明数组： 数据类型 数组名[][] = new 数据类型[大小][] 数据类型[][] 数组名 = new 数据类型[大小][] 数据类型[] 数组名[] = new 数据类型[大小][] 数据类型 数组名[][];数组名 = new 数据类型[大小][]; 数据类型[][] 数组名;数组名 = new 数据类型[大小][]; 数据类型[] 数组名[];数组名 = new 数据类型[大小][]; 初始化： a[0] = new int[1];…… 或者a[0] = &#123;1,3……&#125; 注意点： 数据类型 数组名[][] = new 数据类型[a][]声明了具有 a 个一维数组的二维数组，但一维数组内部的元素个数还不确定。 数据类型 数组名[][] = new 数据类型[][a]这种声明方式非法。 未初始化的默认值： a[0] = new int[3];a[0]：指向内存地址。 没有a[0] = new int[3];直接a[0]：null。 a[0] = new int[3];a[0][0]：输出该数组对应数据类型的默认值，本例为 0。 没有a[0] = new int[3];直接a[0]：报错。 静态初始化——方式一： 数据类型 数组名[][] = &#123;&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;&#125; 数据类型[][] 数组名 = &#123;&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;&#125; 数据类型[] 数组名[] = &#123;&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;&#125; 静态初始化时，[]内不能写数组大小。 静态初始化——方式二： 数据类型 数组名[][] = new int[][]&#123;&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;&#125; 数据类型[][] 数组名 = new int[][]&#123;&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;&#125; 数据类型[] 数组名[] = new int[][]&#123;&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;,&#123;值1, 值2,……&#125;&#125; 静态初始化时，[]内不能写数组大小。 特殊写法情况：int[] x,y[]; x 是一维数组，y 是二维数组。 3.2 数组操作 3.2.1 基本操作 拷贝数组： 基本数据类型：b=0;a=b;，两个数值互不影响，拷贝的是数据。 引用数据类型：int[] arr1 = &#123;1,2……&#125;;arr2 = arr1; 默认情况下：拷贝的是内存中的地址，arr1 和 arr2 不论哪个的值变化，都会引起另外一个的变化。 值拷贝方式：给 arr1 和 arr2 分别开辟独立的内存空间。int[] arr1 = &#123;1,2……&#125;;int[] arr2 = new int[arr1.length]; 反转数组： 交换数据法： 逆序赋值法： 数组扩容： 创建新数组，新数组添加元素，将新数组赋值给旧数组。 数组缩减： 创建新数组，新数组长度比旧数组少 1，循环赋值。 数组排序： 内部排序：将所有数据加载到内存中进行排序。 冒泡排序： 外部排序：数据量过大，无法全部加载到内存中，借助外部的存储空间进行排序。 数组查找： 顺序查找： 二分查找： 3.2.2 Arrays 工具类（常用） Arrays.equals(int[] a, int[] b)： 返回boolean，boolean isEquals = Arrays.equals(a, b); 返回 true 的条件：两数组指向同一个内存地址（a=b）、a 和 b 内部的元素值相等 返回 false 的条件：两数组均为 null、a 和 b 内部的元素值不想等 除 int 外，其余各种类型的对象都可判断 Arrays.toString(int[] a)： 返回String，String aStr = Arrays.toString(a); 输出为[值1,,……]的形式，实际为字符串的拼接 Arrays.fill(int[] a,10) 返回为空，所以不能赋值给其他语句。 表示将数组 a 的所有元素替换为 10。 Arrays.sort(int[] a) 返回为空，所以不能赋值给其他语句。 表示将数组 a 按照从小到大的顺序排序，底层排序方式为快速排序。 Arrays.binarySearch(int[] a, 10)： 返回int，int index = Arrays.binarySearch(a, 10); 返回值为正数时，为目标数据（10）所在的元素下标。返回值为负数时，表示没找到。 底层排序方式为二分法查找。 使用上述方法需要引入对应的工具类：import java.util.Arrays; 1.4.5 数组常见异常 编译时不报错，运行出错。 ArrayIndexOutOfBoundsExcetion： 角标越界： 角标为负数 角标为数组长度（应为数组长度-1） NullPointerException： 数组指向 null 数组只声明、未初始化 数组赋值为 null 不定列二维数组某个一维数组未初始化，访问了该一维数组的具体值。 一维字符串数组的某个值赋值为空。 3.2.4 数组输出问题 详见（详见 Java 问答，1.4.14）. 一维数组： 声明未创建（没有指定数组大小）：编译报错 声明并初始化（指定数组大小）：输出数组显示内存地址、输出元素显示默认值或指定值 二维数组： 声明未创建（没有指定数组大小）：编译报错 声明并初始化（指定数组大小）： 指定两个大小：输出数组显示内存地址、输出一维元素显示内存地址或一维数组、输出二维元素为默认值或指定值 指定一个大小：输出数组显示内存地址、输出一位元素显式为空、输出二维元素编译不报错，运行报错 第 4 章 面向对象 4.0 面向对象的学习内容 4.1 类和对象 类和对象是面向对象的核心概念。 类是对一类事物的描述，是抽象、概念上的定义。 对象是实际存在的该类事物的每个个体，因而也被称为实例。 类和对象的书面理解： Java 语言范畴中，将功能、结构等封装到类中，通过类的实例化，来调用具体的功能、结构。 Scanner、String 等 文件：File 网络资源：URL 涉及到 Java 语言与 HTML、后端数据库交互时，前后端的结构在 java 层面交互时，都体现为类、对象。 4.1.1 类 类的语法格式： 1234修饰符 class 类名&#123; 属性声明; 方法声明;&#125; 123456public class Person&#123; private int age ; //声明私有变量 age public void showAge(int i) &#123; //声明方法showAge( ) age = i; &#125;&#125; 类的成员： 属性：对应类中的成员变量 Field = 属性 = 成员变量 行为：对应类中的成员方法 Method = 方法 = 函数 类权限修饰符只能缺省或 public 4.1.2 类的成员——属性 属性的定义（声明）：语法同变量：访问修饰符 属性类型 属性名; 细节和注意事项： 访问修饰符：public、proctected、默认、private。 属性的类型：可以是任意类型，包括基本数据类型和引用数据类型。 属性的默认值：属性不赋值时，有默认值，规则同数组 byte、byte、short、int、long：0 float、double：0.0 char：\\u0000（空字符） boolean：false String：null 属性赋值：遵循变量赋值的规定，即： 如果变量是基本数据类型，此时赋值的是变量所保存的数据值。 如果变量是引用数据类型，此时赋值的是变量所保存的数据的地址值。 4.1.3 类的成员——方法 方法的定义：1234访问修饰符 返回数据类型 方法名(形参列表……)&#123; 语句; return 返回值;&#125; 形参列表：表示成员方法的输入 返回数据类型：表示成员变量的输出类型。void 表示没有返回值。 &#123;&#125;：表示方法体，实现某一功能的代码块 return 语句：不是必须的。 无返回值类型的方法也可以用 return，用于结束方法。 细节和注意事项 访问修饰符： public、proctected、默认、private 控制方法的使用范围。 返回数据类型： 一个方法只能有一个返回值。返回多个值可以使用数组接收。 返回类型可以是任意类型包含基本类型或引用类型（数组、对象） 如果要求有返回值，则必须有 return 语句，且 return 语句的数据类型与方法的返回数据类型一致或兼容。 如果方法时 void，则方法体中可以没有 return 语句，或者只写 return。 方法名： 见名知义 小驼峰命名方法，遵循标识符的规则、规范 形参列表： 一个方法可以有 0 个参数，也可以有多个参数，多个参数用逗号分开。 参数类型可以是任意类型，包括基本类型和引用类型。 调用带参数的方法时，必须传入对应类型或兼容类型的实参。 调用方法时，实参和形参的数量、类型、顺序必须一致。 方法体： 方法体的语句可以为输入、输出、变量、运算、分支、循环、方法调用，但是不能嵌套定义——方法中不能再声明方法。 方法调用的细节 同一个类中的方法 A（非 main 方法）中调用方法 B 时，可以直接调用方法名() main 方法需要调用同一个类中的方法 B 时，需要通过实例化类，对象名.B方法名()调用 跨类中的方法 A 调用方法 B 时，需要通过对象名（实例化对象）调用类名 对象名 = new 类名();对象名.方法名() 跨类调用的方法和它的访问修饰符有关。 方法的调用机制： 当程序执行（main 方法）到方法时，会在 jvm 的栈内存内开辟一个独立的空间 当方法执行完毕时，或执行到 return 语句时，就会将方法的运行结果返回给栈内存中调用方法的地方。同时销毁开辟的独立空间。 返回后，程序继续执行后面的代码 当 jvm 内存内的 main 方法执行完毕，整个程序退出。 使用方法的优点： 提高了代码的复用性。 将实现的细节封装起来，供其他用户调用。 方法的传参机制（值传递）： 基本数据类型：传递的是值（值拷贝），形参的任何改变不影响实参。 引用数据类型：传递的是地址，可以通过形参影响实参。 形参不影响实参的情况：形参在方法内开辟了新的内存空间（需要在方法体内的语句实现）。 字符串的传参？ 克隆对象：创建两个独立的对象，只是属性、方法一致。 利用引用数据类型传参的机制（在方法体内创建新对象，逐一赋值）。 4.1.4 类的实例化——对象 创建对象： 方式一：类名 对象名 = new 类名(); 方式二：类名 对象名;对象名 = new 类名(); Person p1 = new Person(); p1 是对象的名（对象的引用），保存实例化对象的内存地址 new Person()创建的对象空间（数据）才是真正的对象 方式三：创建对象数组类名[] 对象数组名 = new 类名[n]; 访问成员：对象名.对象成员 访问属性：对象名.属性 访问方法：对象名.方法() 匿名对象： 含义：创建对象时，如果没有显式地给对象起名，只是进行了new 类名()，则new 类名()为匿名对象。 特征：匿名对象只能调用一次。 第二次调用时已经是新对象了，跟前一个对象没有关系。 使用： 临时使用、不需要保留 作为方法的参数 4.1.5 方法重载（OverLoad） 定义：在同一个类中，允许存在一个以上的同名方法，只要它们的参数个数或者参数类型不同即可。 判断是否是重载： “两同一不同”： 在同一个类中 相同方法名 参数列表不同： 参数个数不同 参数类型不同 同类型参数顺序不同 跟方法的权限修饰符、返回值类型、形参变量名、方法体都没有关系！ 在通过对象调用方法时，如何确定某一个指定的方法： 判断方法名是否一致 判断参数列表是否一致：参数类型、参数个数、参数顺序一致 注意点：子类可以重载父类同名不同参的方法。 4.1.6 方法递归（Recrusion） 含义：方法自己调用自己，每次调用时传入的参数不同 调用规则： 递归执行一次，会在栈内存内创建一个受保护的独立空间（栈空间） 方法的局部变量相互独立，不会相互影响 方法中使用引用数据类型的变量时，就会共享该引用类型的数据。 递归必须向退出递归的条件逼近，否则就是无限递归。 当一个方法执行完毕，或者遇到 return 语句，就会返回，遵循谁调用，结果就返回给谁。同时该方法执行完毕或返回时，该方法也就执行完毕。 4.1.7 构造方法/构造器（Constructor） 基本语法：[修饰符] 方法名(形参列表)&#123;&#125; 语法说明： 修饰符可以默认（无），也可以用 private、protected、public。 修饰符为默认（无）时，与类的修饰符一致。 构造器没有返回值，也不可以写void 方法名必须和类名一模一样 参数列表和方法的规则一致 在创建对象时，系统会自动调用该类的对象完成属性的初始化。 完成对象的初始化不是创建对象 不能被 static、final、synchronized、abstract、native 修饰，不能有 return 语句返回值 主要作用：完成新对象（属性）的初始化。 构造器重载： 一个类可以定义多个构造器 如果一个构造器没写，系统会自动生成一个无参、默认的构造器（该构造器的修饰符与类一致）。 一旦定义了一个构造器，默认的无参构造器就不能使用了，使用了会报错。 如果想继续使用，需要再重新显式定义一下。 对象创建流程： 加载类信息，加载到方法区，只会加载一次 在堆空间中分配空间（地址） 完成对象的初始化 默认初始化 显式初始化 构造器初始化 将对象在堆空间的地址返回给创建对象时定义的对象 父类的构造器不可被子类继承 注意点：使用了构造器后，实例化对象要用构造器的方法，否则会报错。 原因为：显式声明构造器后，默认的无参构造器会不可用。 4.1.8 方法重写（override/overwrite） 定义：子类继承父类以后，可以对父类中同名同参数的方法，进行覆盖操作 应用：重写以后，当创建子类对象以后，通过子类对象调用子父类中的同名同参数的方法时，实际执行的是子类重写父类的方法。 语法：权限修饰符 返回值类型 方法名(形参列表) throws 异常的类型&#123;&#125; 方法名：父子一致。 形参列表：父子一致。 权限修饰符：子类不小于父类 不能重写父类中 private 修饰的方法，因为 private 修饰的方法对外部不可见。 返回值类型： void：父子一致。 基本数据类型：父子一致。 引用数据类型：子类不大于父类 异常类型：子类不大于父类 注意点：子类和父类中的同名同参数的方法要么都声明为非 static 的（考虑重写），要么都声明为 static 的（不是重写）。 4.1.9 代码块 作用：用来初始化类、对象 修饰： 权限修饰：只能缺省。 关键字：只能用 static 或没有。 分类：静态代码块与非静态代码块， 相同点： 都可以用于对类的属性、声明初始化 都可以声明多个代码块，但一般每种最多写一个 多个代码块默认执行顺序都是先上后下 不同点： 静态代码块： 只能调用静态的属性和方法 随类的加载而执行，只执行一次 类加载的三个时机： 创建对象实例时（new） 创建子对象实例时，父类会被加载 使用类的静态成员时。 非静态代码块： 既可以调用静态的属性和方法，也可以调用非静态的属性方法 随对象的创建而执行，创建一次执行一次。先于构造器执行 可以将构造器相同的部分写到代码块内，减少不同构造器间的代码冗余。 创建对象时，类的调用顺序： 静态代码块和静态属性。取决于书写顺序。 普通代码块和普通属性。取决于书写顺序。 构造器。 创建子类对象时，类的调用顺序： 父类的静态代码块和静态属性。取决于书写顺序。 子类的静态代码块和静态属性。取决于书写顺序。 父类的普通代码块和普通属性。取决于书写顺序。 父类的构造器。 子类的普通代码块和普通属性。取决于书写顺序。 子类的构造器。 4.1.10 内部类（InenerClass） 含义：定义在类内部的一个类，包含内部的类叫外部类 外部引用时需要完整写出类名称（含包名） 编译以后生成 OuterClass$InnerClass.class 字节码文件 分类： 局部内部类：定义在方法、代码块、构造器中。 成员内部类（static 修饰和无修饰）：定义在成员位置，类内可定义类的五大组成部分（属性、方法、构造器、代码块、内部类） 可以被 final 修饰，表示此类不能被继承。 可以被 abstract 修饰，表示不能被实例化 局部内部类： 可以直接访问外部类的所有成员，包含私有的 但是如果调用局部内部类所在方法中的局部变量时，要求该方法中的局部变量为 final 修饰，JDK8 之前显式声明，JDK8 之后可省略。 不能添加权限修饰符（只能缺省，同局部变量的修饰符范围），可以被 final 修饰，修饰后不可被继承 不能被 static 修饰，也不能包含 static 成员 内部类的成员与外部类的成员重名时，内部类调用遵循就近原则，需要调用外部成员时，需要通过外部类.this.成员名的方式 由于局部内部类定义在方法、代码块、构造器中，实际上是一个局部变量，只能在定义它的位置生效，即只能在这个位置实例化。 外部类需要访问内部类的成员时，需要通过上述流程，在外部类的方法中，将内部类实例化。 外部其他类不能访问。 成员内部类： 可以直接访问外部类的所有成员，包含私有的 使用 static 修饰时，只能调用外部类声明为 static 的结构 非静态内部类，内部不能声明静态成员 前面省略了外部类.this.，不能使用this. 可以使用四种权限修饰符修饰 实例化内部类： 静态内部类：外部类.内部类 变量名 = new 外部类.内部类(); 非静态内部类：外部类.内部类 变量名 = 外部类的引用.new 内部类(); 外部类.内部类 变量名 = 外部类的引用.new 外部类.内部类(); 内部类的成员与外部类的成员重名时，内部类调用遵循就近原则，需要调用外部成员时，需要通过外部类.this.成员名的方式，调用内部类自身的属性，this.成员名 匿名内部类： 语法：new 父类构造器（实参列表）|实现接口()&#123; //匿名内部类的类体部分 &#125;; 可以基于接口实现、也可基于父类实现 分号不能少 可以是成员内部类、也可以是局部内部类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class Cellphone&#123; // 1.1 匿名内部类是成员内部类 double b = new Calculator() &#123; double c = 9.0; @Override public double work() &#123; // TODO Auto-generated method stub return 0; &#125; &#125;.c; // 1.2 匿名内部类是成员内部类 double a = new Calculator() &#123; @Override public double work() &#123; // TODO Auto-generated method stub return 0; &#125; &#125;.work(); public double testWork(Calculator c) &#123; double result; result = c.work(); // 2.1 匿名内部类是局部内部类 double b = new Calculator() &#123; double c = 9.0; @Override public double work() &#123; return 0; &#125; &#125;.c; // 2.2 匿名内部类是局部内部类 double a = new Calculator() &#123; @Override public double work() &#123; // TODO Auto-generated method stub return 0; &#125; &#125;.work(); return result; &#125; public static void main(String[] args) &#123; Cellphone cp = new Cellphone(); // 2.3 匿名内部类是局部内部类 double a = cp.testWork(new Calculator() &#123; @Override public double work() &#123; return 1 + 1; &#125; &#125;); System.out.println(a); &#125;&#125; 匿名内部类本身也是一个对象，因此它也可以调用内部类内部的方法，执行时遵循多态的规则（匿名内部类重写了就执行内部类里的方法） 其他有关问题参见 4.4.8 第 6 部分。 外部类不能被 static 修饰 成员内部类和局部内部类，在编译以后，都会生成字节码文件。 成员内部类：外部类$内部类名.class 局部内部类：外部类$数字 内部类名.class 4.2 jvm 内存分析 4.2.1 内存结构 图结构： 引用类型的变量只能存储两类值： null 包含变量类型的地址值。 4.2.2 内存分配 栈：一般指虚拟机栈，存储局部变量。 堆：存放对象（含对象的属性）、数组。 方法区：常量池（常量、字符串）、静态变量、即时编译后的代码。 4.2.3 成员变量与局部变量的异同 相同点： 定义变量的格式相同：数据类型 变量名 = 变量值 先声明，后使用 都有其对应的作用域以及生命周期 不同点： 在类中声明的位置的不同 属性：直接定义在类的一对&#123;&#125;内 局部变量：声明在方法内、方法形参、代码块内、构造器形参、构造器内部的变量 作用域范围不同： 属性：可以被本类使用、也可以被其他类使用（通过对象调用） 局部变量：只能在本类中对应的方法使用。 关于权限修饰符的不同 属性：可以在声明属性时，指明其权限，使用权限修饰符。 局部变量：不可以使用权限修饰符。 默认初始化值的情况不同： 属性：类的属性，根据其类型，都有默认初始化值。 局部变量：没有默认初始化值。在调用局部变量之前，一定要显式赋值。形参在调用时赋值即可。 在内存中加载的位置不同： 属性：加载到堆空间中（非 static），因为对象加载到了堆空间 局部变量：加载到栈空间 生命周期不同： 属性：生命周期长，伴随对象的创建而创建，伴随对象的销毁而销毁。 局部变量：生命周期短，伴随代码块的执行而创建，伴随代码块的执行结束而销毁。 属性和局部变量可以重名，访问时遵循就近原则。 属性赋值过程： 默认初始化 显式初始化 / 代码块 取决于类中书写的顺序 构造器中初始化 通过“对象.属性“或“对象.方法”的方式赋值 4.2.4 可变个数形参 语法： JDK 5.0 以前：采用数组形参来定义方法，传入多个同一类型变量 public static void test(int a, String[] books); JDK5.0：采用可变个数形参来定义方法，传入多个同一类型变量 public static void test(int a, String… books); 注意点： 调用方法时，可传入的参数个数可以是 0 个，1 个或多个 可变参数的方法可与其他方法构成重载，调用时，优先调用匹配度高的 用...和[]作为方法参数的同名方法视为同一个类里视为重复定义（编译报错），父子类内视为重写。 可变参数方法的使用与方法参数部分使用数组是一致的，方法体内使用 for 循环 方法的参数部分有可变形参时，需要放在形参声明的最后 一个方法最多只能声明一个可变个数形参 4.3 封装、继承和多态 2.3.1 封装与隐藏 含义：将类中的属性（数据）私有化，并提供外部可访问的类进行属性操作。 好处： 对（变量）数据进行验证，保证数据安全 隐藏实现细节 便于修改，增强代码可维护性 体现： 将属性使用 private 修饰，进行私有化，并提供公共的 set 和 get 方法获取和设置此属性的值 提供 public 修饰的 setter，用于判断并修改属性，无返回值 提供 public 修饰的 getter，用于判断并读取属性，有返回值 不对外暴露私有方法 单例模式（将构造器私有化） 如果不希望类在包外调用，可以将类设置为缺省的 权限修饰符： 修饰符 类内部 同一个包 不同包的子类（其父类是 protected） 同一个工程 private √ 缺省 √ √ 但修饰的属性子类访问不到 protected √ √ √ public √ √ √ √ 类的权限修饰符只能用 public 或缺省 使用 public 时，本工程下使用 需要导入全类名 使用缺省时，只可以被同一个包内部的类访问 局部变量（方法内、方法形参等）的修饰符只能缺省。 protected 修饰的属性、方法需要在不同包内访问时，一是需要父子继承关系，且 protected 修饰的内容是父类，二是需要在子类中导入包 4.3.2 继承（inheritance） 作用： 减少代码的冗余，提高代码的复用性，提高开发效率 有利于功能扩展 使类与类之间产生联系，为多态提供了前提 格式：class A extends B&#123;&#125; A：子类、派生类、subclass B：父类、超类、superclas 体现： 子类 A 继承父类 B 以后，子类 A 中就自动获取了父类 B 中声明的所有的属性和方法。 直接父类、间接父类、自身都有同名属性的情况下，访问时执行就近原则。 方法按重写规定执行。 父类中声明为 private 的属性或方法，子类继承父类以后，仍然认为获取了父类中私有的结构。只有因为封装性的影响，使得子类不能直接调用父类的结构而已。需要通过 setter 和 getter 调用属性。 通过super.属性的方法也访问不到。 子类继承父类以后，还可以声明自己特有的属性或方法：实现功能的拓展。 子类必须调用父类的构造器，完成父类的初始化。 实例化过程中会一直调用至 Object 对象。 注意点： 子类 A 继承自父类 B 后，如果父类 B 中声明了带参的构造器，则必须在子类 A 中声明 A 的构造函数，且必须在首行通过super(形参列表)调用 B 的构造函数。否则会编译报错。 原因为如果子类 A 中不写构造器，其默认构造器为无参构造器，无参构造器中默认会调用 B 的无参构造器super()。由于父类 B 中显式地声明了构造器，导致默认的无参构造器不可用，从而会报错。而如果不在首行写super(形参列表)语句，则表明调用的是super()，同样会报错。 解决方案一：父类 B 中显式声明无参构造器。 解决方案二：子类 A 中调用父类 B 中指定的构造器。声明 A 的构造器方法体内使用super(形参列表) 规定： 一个父类可以被多个子类继承。 子类直接继承的父类，称为：直接父类。间接继承的父类称为：间接父类。 子类继承父类以后，就获取了直接父类以及所有间接父类中声明的属性和方法。 如果我们没有显式的声明一个类的父类的话，则此类继承于 java.lang.Object 类。 所有的 java 类（除 java.lang.Object 类之外）都直接或间接的继承于 java.lang.Object 类 所有的 java 类具有 java.lang.Object 类声明的功能。 子类对象的实例化过程 从结果上来看：（继承性） 子类继承父类以后，就获取了父类中声明的属性或方法。 创建子类的对象，在堆空间中，就会加载所有父类中声明的属性。 从过程上来看： 通过子类的构造器创建子类对象时，一定会直接或间接的调用其父类的构造器，进而调用父类的父类的构造器，直到调用了 java.lang.Object 类中空参的构造器为止。 正因为加载过所有的父类的结构，所以才可以看到内存中有父类中的结构，子类对象才可以考虑进行调用。 虽然创建子类对象时，调用了父类的构造器，但是自始至终就创建过一个对象，即为 new 的子类对象。 4.3.3 多态性（Polymorphism） 理解：方法和对象的多种形态。实现代码的通用性。 体现： 方法多态： 重载多态（参数不同，方法体现出多重形态）。 重写多态。 对象多态： 对象的编译类型和运行类型可以不一致，编译类型在定义时确定，不能变化。 对象的运行类型可以是变化的，可以通过getClass()查看运行类型。 常将父类对象作为方法形参，执行方法时根据具体实例化的父/子类对象进行传入。 使用：虚拟方法调用 子类中定义了与父类同名同参数的方法，在多态情况下，将此时父类的方法称为虚拟方法，父类根据赋给它的不同子类对象，动态调用属于子类的该方法。这样的方法调用在编译期是无法确定的。 编译期只能调用父类中声明的方法，但在运行期，实际执行的是子类重写父类的方法。 不能调用子类中特有的成员。 需要调用子类特有成员时，需要使用向下转型。 总结：编译，看左边；运行，看右边。 使用前提： 类的继承关系 方法的重写 对象的多态性，只适用于方法，不适用于属性（编译和运行都看左边）。 使用向下转型声明的父类对象，调用父子类同名的属性时，实际调用的是父类的属性。 属性不具有多态性。 由于父类的属性通常设置为 private，所以需要 getter 方法才能获取到。 instance of操作符： x instanceof A：检验 x 的运行类是否为类 A 或其子类的对象，返回值为 boolean 型。 如果 x 是类 B 的对象，而 B 是 A 的子类，则仍返回 true。 1234AA aa = new BB();System.out.println(aa instanceof AA);System.out.println(aa instanceof BB);//设BB继承自AA 强制转型后的类型判断：这里均返回true，因为运行的是BB类 x instanceof Object：总是返回 true。 检验的 A 类必须与 x 的类有父子类关系，否则编译会报错。 造型：对 java 类型的强制类型转换 子类到父类可以自动类型转换——多态、向上转型 父类到子类必须通过造型()实现——向下转型。 无继承关系的引用类型造型非法，运行报错：ClassCastException 通常首先使用instanceof操作符进行判断后进行造型操作，避免报错。 java 的动态绑定机制： 当调用对象的方法时，该方法会和对象的内存地址/运行类型绑定。 当调用对象的属性时，没有动态绑定机制，哪里声明，哪里使用。 4.4 关键字 4.4.1 this 作用： 方法内部使用时，代表方法所属对象的引用 构造器内部使用时，代表该构造器正在初始化的对象。【子类构造器初始化时，父类构造器中的 this 指向的是子类正在实例化的对象】 使用范围：属性、方法、构造器 时机：方法内需要调用该方法的对象时，可以使用 this。 在任意方法或构造器内，如果使用当前类的成员变量或成员方法可以在其前面添加 this，增强程序的阅读性。不过，通常我们都习惯省略 this。 当形参与成员变量同名时，如果在方法内或构造器内需要使用成员变量，必须添加 this 来表明该变量是类的成员变量 使用 this 访问属性和方法时，如果在本类中未找到，会从父类中查找 this 调用构造器 可以在类的构造器中使用this(形参列表)的方式，调用本类中重载的其他的构造器！ 明确：构造器中不能通过this(形参列表)的方式调用自身构造器 如果一个类中声明了 n 个构造器，则最多有 n-1 个构造器中使用了this(形参列表) this(形参列表)必须声明在类的构造器的首行！ 在类的一个构造器中，最多只能声明一个this(形参列表) 有参的构造器如果没有显式地调用 this()，则不会调用无参的构造器。因为如果没写 this()，也没写 super(0，默认调用的是 super()。super()不能使用this.super() 4.4.2 package 作用：package 语句作为 Java 源文件的第一条语句，指明该文件中定义的类所在的包。(若缺省该语句，则指定为无名包)。 格式：package 顶层包名.子包名 一般：com.公司名.项目名.业务模块名 使用规范： 包对应于文件系统的目录，package 语句中，用.来指明包(目录)的层次 包通常用小写单词标识。通常使用所在公司域名的倒置 作用： 包帮助管理大型软件系统：将功能相近的类划分到同一个包中。比如：MVC 的设计模式 包可以包含类和子包，划分项目层次，便于管理 解决类命名冲突的问题 控制访问权限 注意点： 同一个包下，不能命名同名的接口、类。 不同的包下，可以命名同名的接口、类。 常见包： java.lang----包含一些 Java 语言的核心类，如 String、Math、Integer、 System 和 Thread，提供常用功能 java.net----包含执行与网络相关的操作的类和接口。 java.io ----包含能提供多种输入/输出功能的类。 java.util----包含一些实用工具类，如定义系统特性、接口的集合框架类、使用与日期日历相关的函数。 java.text----包含了一些 java 格式化相关的类 java.sql----包含了 java 进行 JDBC 数据库编程的相关类/接口 java.awt----包含了构成抽象窗口工具集（abstract window toolkits）的多个类，这些类被用来构建和管理应用程序的图形用户界面(GUI)。 B/S C/S 4.4.3 import 作用：为使用定义在不同包中的 Java 类，需用 import 语句来引入指定包层次下所需要的类或全部类(.*)。 语法格式：import 包名.类名; 使用细节： 在源文件中使用 import 显式的导入指定包下的类或接口 声明在包的声明和类的声明之间。 如果需要导入多个类或接口，那么就并列显式多个 import 语句即可 可以使用 java.util.*的方式，一次性导入 util 包下所有的类或接口。 如果导入的类或接口是 java.lang 包下的，或者是当前包下的，则可以省略此 import 语句。 如果在代码中使用不同包下的同名的类。那么就需要使用类的全类名的方式指明调用的是哪个类。 如果已经导入 java.a 包下的类。那么如果需要使用 a 包的子包下的类的话，仍然需要导入。 import static 组合的使用：调用指定类或接口下的静态的属性或方法 4.4.4 super 用途：子类调用父类的属性、方法、构造器 解决子父子类属性、方法冲突的问题。 在合适的位置调用父类的属性、方法、构造器。 操作对象：属性、方法、构造器 调用属性和方法： 子类的方法和构造器中，使用父类的属性和方法时，默认使用了super.的方式调用属性、方法。编程中习惯不写super.。 super 不能调用父类的私有属性。 当子类和父类中定义了同名的属性时，使用super.属性调用的是父类的属性；如果属性没有重名，使用super.属性同this.属性作用一样。 在子类中重写的方法中调用父类被重写的方法时，必须使用super.方法的方式。 super 不能调用父类的私有方法。 super 调用父类时，不局限于直接父类，有多个上级类有重名方法、属性时，遵循就近原则。 调用构造器： 可以在子类的构造器中，显式地使用super(形参列表)的方式调用父类的构造器。 super(形参列表)必须声明在首行。 由于this(形参列表)也必须出现在首行，所以一个构造器中this(形参列表)或super(形参列表)只能二选一，不能同时出现。 在构造器的首行，没有显式的声明**this(形参列表)**或**super(形参列表)**，则默认调用的是父类中空参的构造器：**super()**。 在类的多个构造器中，至少有一个类的构造器中使用了super(形参列表)，调用父类中的构造器。 因为所有的对象都是继承来的，都必然具有父类的特征。 默认的无参构造器默认调用的super() 无论哪个构造器创建子类对象，需要先保证初始化父类，当子类继承父类以后，继承了父类中的属性和方法，因此子类有必要知道父类如何对对象初始化。 4.4.5 static 设计思想： 属性：在各个对象间共享，不因对象的不同而改变。 方法：方法的调用与对象无关，不需要创建对象就可调用方法。 修饰范围： 可修饰：属性、方法、代码块、内部类 被 static 修饰的属性叫：静态属性、静态变量、类变量。 没有被 static 修饰的属性叫非静态属性、实例变量。 被 static 修饰的方法叫：静态方法。 不能修饰：局部变量、形参、构造器、外部类 不能修饰构造器的原因：static 随着类的加载而加载，根据构造器的加载时机区分 static 和非 static，先于构造器加载的为 static，后于构造器加载的为非 static 特点： 属性和方法随着类的加载而加载，与是否创建对象无关。 由于类只会在 JVM 的内存里加载一次，所以属性会只有一份。 存在于方法区的静态域中。 无论创建多少次对象，使用多少次类，都只有一份 属性和方法优先于对象而存在。 被 static 修饰的成员，被所有对象所共享。 访问权限允许时，可不创建对象，直接被类调用。 被 static 修饰的内部类实例化的特点呢？以及与多线程的关系？ 注意点： 静态属性和静态方法可以通过类名.静态属性、类名、静态方法名()的方式直接调用，也可以通过对象名.静态属性、对象名、静态方法名()的方式调用。 静态方法中，只能调用静态的方法或属性； 同类中的静态方法调用静态方法，可以直接使用方法名();或类名、静态方法名()，不能使用this 实例化当前类后，可以通过对象名.静态属性、对象名、静态方法名()的方式调用非静态的属性和方法。 非静态方法中，既可以调用非静态的方法或属性，也可以调用静态的方法或属性。 在静态的方法内，不能使用 this 关键字、super 关键字。 也不能通过this.、super.的方式调用静态属性、静态方法，只能通过类名.的方式调用。 static 修饰的方法不能被重写，但可以被继承。 不能被重写：父类、子类的同名、同参静态方法都会被加载。 可以被继承：子类可以直接通过类名.方法名()的方式调用。 static 修饰内部类，类中的属性、方法、代码块可以是非静态的 static 可与四种权限修饰符搭配使用，控制可见范围。 设置为 private 时，只能被类名.静态属性、类名.静态方法名()调用，不能被对象的引用调用。 使用时机： 属性： 属性需要被多个对象共享 常量 方法： 操作静态属性 工具类中的方法。如 Math、Collection、Arrays 4.4.6 main main()说明： main 方法的权限修饰符必须是 public main 方法的修饰符必须是 static，因为 main 方法也是一个内中的方法，需要加载对象的时候执行，而不是创建 main 方法所在类的对象时候执行。 由于 main 方法使用 static 修饰了，所有不能直接访问本类中的非 static 属性和方法，必须通过实例化创建本类对象的方式进行调用。 main 方法可以作为与控制台交互的方式 - 在命令行中执行（运行时，先要生成字节码文件）java 文件名 参数 - 参数可以带引号，也可以不带，多个参数使用空格分开 在 eclipse 工具中执行： main 方法中访问直接访问本类中的静态方法和静态静态属性可以不需要类名.的方式调用。 4.4.7 final 用途：修饰类、变量（成员变量及局部变量）、方法、内部类 变量分为局部变量和成员变量 final 修饰类：不能被继承，提高安全性、可读性。 通常 final 修饰类后，内部的方法没必要在用 final 修饰 String 类、StringBuffer 类、System 类 final 修饰方法：不能被子类重写 Object 的 getClass() 不能修饰构造器 final 修饰属性（成员变量）：表示常量，且必须在定义时初始化。 赋值的位置可以是类中显式初始化、代码块、构造器内。 搭配 static 使用时，初始化位置只能是显示初始化和静态代码块内 final 修饰局部变量：。 final 修饰形参时，表示给常量赋值，方法体内只能使用该常量，不能修改该形参。 修饰方法体内局部变量时，称为局部常量。 final 可以和 static 搭配使用， static final：只能用于修饰属性、普通方法，修饰属性时，表示全局常量。 4.4.8 abstract（抽象类与抽象方法） 修饰结构：类、方法、内部类 不能修饰属性、私有（private）方法、静态（static）方法、final 方法、final 类 通常用于处理父类方法不缺定时的需求。 先有抽象方法，后有抽象类。 不可修饰结构：属性、构造器等。 修饰类： 此类不能实例化 开发中一般提供抽象类的子类，该子类使用多态的方式实例化父类。否则匿名化。 修饰方法： 抽象方法只有方法名，没有方法体，用;结束 包含抽象方法的类，一定是抽象类；但抽象类不一定包含抽象方法。 抽象类不能被 private 修饰（因为要对外可见，需要被重写） 若子类重写了父类所有的抽象方法，则该子类可实例化； 若子类没有重写或部分重写了父类的所有抽象方法，则该子类也是一个抽象类，需要使用 abstract 修饰。 抽象类的应用：模板方法的设计模式。（见 2.6.2） 匿名类（不一定为抽象类）与匿名对象：共有四种格式 非匿名类非匿名对象：Worker worker = new Worker(); 非匿名类匿名对象：new Worker(); 匿名类非匿名对象：Person p = new Person()&#123;重写Person的虚拟方法&#125;; 注意分号不能省略。 Person 是一个虚拟类，按理不能实例化，实际上也正是通过方法体中的重写，将 Person 类变成了其他类，正常来说应该单独定义一个非虚拟具名的类继承自 Person，再将方法体写入其中，但正由于没有这样做，不知道这个子类叫什么名字，所以就是匿名类。 在匿名类非匿名对象的方法体中，如果声明了属性 a，通过对象引用.p.a的方式访问属性 a 的值，如果 Person 类中没有定义过 a，则报错，如果 Person 中定义了 a，则返回的是 Person 类中的 a 的值。即，匿名类非匿名对象中声明的自有属性访问不到。 如果要能访问到，可采用int p = new Person()&#123;int a = 1;重写Person的虚拟方法&#125;.a; int p = new Person()&#123;int a = 1;重写Person的虚拟方法&#125;.a;改变返回值类型、.a为.方法名()可以获取.方法名()的返回值。但是仅限于有返回值类型的方法（含自有方法）。 匿名类匿名对象：new Person()&#123;重写Person的虚拟方法&#125; 注意分号不能省略。 4.5 面向对象补充知识 4.5.1 JavaBean JavaBean 是一种 Java 语言编写的可重用组件。 特征： 类是公共的 有一个无参的公共构造器 有属性 属性一般定义为 private，有对应的 getter、setter 可以使用 JavaBean 将功能、处理、值、数据库访问和其他任何可以用 Java 代码创造的对象进行打包，并且其他的开发者可以通过内部的 JSP 页面、Servlet、其他 JavaBean、applet 程序或者应用来使用这些对象。用户可以认为 JavaBean 提供了一种随时随地的复制和粘贴的功能，而不用关心任何改变。 4.5.2 UML 类图 Accout：类名 +表示 public，-表示 private，#表示 protected 属性：：前表示属性名，：后表示属性类型 方法：方法的()外面有：表示有返回值，：后面为返回值类型 方法有下划线表示构造器 4.5.3 MVC 设计模式 内容：常用的设计模式之一，将整个程序分为三个层次：视图模型层，控制器层，与数据模型层。 优点：这种将程序输入输出、数据处理，以及数据的展示分离开来的设计模式使程序结构变的灵活而且清晰，同时也描述了程序各个对象间的通信方式，降低了程序的耦合性。 模型层（model）：主要处理数据 数据对象封装：model.bean/domain 数据库操作类：model.dao 数据库：model.db 控制层（controller）：处理业务逻辑 应用界面相关：controller.activity 存放 fragment：controller.fragment 显示列表的适配器：controller.adapter 服务相关的：controller.service 抽取的基类：controller.base 视图层（view ）：显示数据 相关工具类：view.utils 自定义 view：view.ui 4.5.4 Object 类的使用 Object 类是所有 Java 类的根父类。 如果在类的声明中未使用 extends 关键字指明其父类，则默认直接父类为 java.lang.Object 类。 Object 类中的属性：无 Object 类中的方法： equals() ： 只能比较引用数据类型，作用与==相同，比较是否指向同一对象。 类File、String、Date及包装类由于重写了equals()，所以比较的时内容是否相同。 重写equals()原则： 对称性：如果 x.equals(y)返回是 true，那么 y.equals(x)也应该返回是 true。 自反性：x.equals(x)必须返回是 true。 传递性：如果 x.equals(y)返回是 true，而且 y.equals(z)返回是 true，那么 z.equals(x)也应该返回是 true。 一致性：如果 x.equals(y)返回是 true，只要 x 和 y 内容一直不变，不管重复 x.equals(y)多少次，返回都是 true。 任何情况下，x.equals(null)，永远返回是 false，null。equals(x)会空指针异常。 x.equals(和 x 不同类型的对象)永远返回是 false。 toString()： 返回值为 String，返回类名和它的内存地址——全类名+@+哈希值的十六进制。 全类名：包名+类名 类File、String、Date及包装类由于重写了toString()，返回&quot;实体内容&quot;信息。 使用 String 类型的数据与+进行连接操作时，自动调用toString() getClass()：获取当前对象所处的类 hashCode()：返回该对象的哈希值，用于提高哈希表的性能。 两个引用指向同一对象，哈希码值一定一样。 两个引用指向不同对象，哈希码值一般不一样，（极少数情况一样）。 哈希值主要根据地址值生成，但与地址值不同。 clone() finalize()： 当对象被回收时，系统会自动调用该方法。 当某个对象没有任何引用，则 jvm 认为该对象是一个垃圾对象。在销毁该对象前，会先调用该方法。 可以通过 System.gc()主动出发垃圾回收机制。 wait() notify() notifyAll() Object 类只声明了一个空参的构造器 final、finally、finalize 的区别？ 4.5.5 JUnit 单元测试 步骤：warning 选中当前工程 - 右键选择：build path - add libraries - JUnit 5.4 - 下一步 创建 Java 类，进行单元测试。 简便方式：在测试方法上一行写上**@Test**，然后 Ctrl+1 修复。 此时的 Java 类要求：① 此类是 public 的 ② 此类提供公共的无参的构造器 此类中声明单元测试方法：方法的权限是 public,没有返回值，没有形参。 此单元测试方法上需要声明注解：@Test,并在单元测试类中导入：import org.junit.Test; 声明好单元测试方法以后，就可以在方法体内测试相关的代码。 写完代码以后，左键双击单元测试方法名，右键：run as - JUnit Test 说明： 如果执行结果没有任何异常：绿条 如果执行结果出现异常：红条 测试方法没有返回值，也没有参数 4.6 设计模式 概念：开发过程中经过大量的实践中总结和理论化之后优选的代码结构、编程风格、 以及解决问题的思考方式。 分类： 创建型（5 种）： 结构型（7 种）： 行为型（11 种）： 4.6.1 单例模式（Singleton） 定义：采取一定的方法保证在整个的软件系统中，对某个类只能存在一个对象实例，并且该类只提供一个取得其对象实例的方法。 实现原理： 构造器私有化，防止 new 类的内部创建静态实例对象 向外暴露一个静态的公共方法。 实现方式： 饿汉式： 1234567891011class Singleton &#123; // 1.私有化构造器 private Singleton() &#123;&#125; // 2.内部提供一个当前类的实例 // 4.此实例也必须静态化 private static Singleton single = new Singleton(); // 3.提供公共的静态的方法，返回当前类的对象 public static Singleton getInstance() &#123; return single; &#125;&#125; 懒汉式： 1234567891011121314class Singleton &#123; // 1.私有化构造器 private Singleton() &#123;&#125; // 2.内部提供一个当前类的实例 // 4.此实例也必须静态化 private static Singleton single; // 3.提供公共的静态的方法，返回当前类的对象 public static Singleton getInstance() &#123; if(single == null) &#123; single = new Singleton(); &#125; return single; &#125;&#125; 优缺点： 饿汉式： 优点：线程安全。 缺点：对象加载时间过长。全生命周期。 懒汉式： 优点：延迟对象的创建。 缺点线程不安全。 应用场景： 网站计数器： 应用程序的日志程序： 数据库的连接池 读取配置文件的类： Application： windows 系统的任务管理器 windows 系统的回收站 4.6.2 模板方法设计模式（TemplateMethod） 体现：就是一种模板模式的设计，抽象类作为多个子类的通用模板，子类在抽象类的基础上进行扩展、改造，但子类总体上会保留抽象类的行为方式。 也是多态行的一种体现。 解决的问题： 当功能内部一部分实现是确定的，一部分实现是不确定的。这时可以把不确定的部分暴露出去，让子类去实现。 换句话说，在软件开发中实现一个算法时，整体步骤很固定、通用，这些步骤已经在父类中写好了。但是某些部分易变，易变部分可以抽象出来，供不同子类实现。 应用：模板方法设计模式是编程中经常用得到的模式。各个框架、类库中都有他的影子，比如常见的有： 数据库访问的封装 Junit 单元测试 JavaWeb 的 Servlet 中关于 doGet/doPost 方法调用 Hibernate 中模板程序 Spring 中 JDBCTemlate、HibernateTemplate 等 实现过程： 定义抽象类 抽象类中写明确定的流程，定义在一个方法（模板方法）内，模板方法调用各个流程 将不确定的内容插入到方法中的合适位置调用 将不确定的内容定义成抽象方法。 子类继承抽象类，重写抽象方法，在方法中完成需要工作的代码。 实例化子类，子类由于继承了父类的方法，通过子类对象的引用调用父类的模板方法的那个方法。 4.6.3 代理模式（Proxy） 体现：类 A 实现了接口，实际需要类 A 去调用（操作）接口时，表面上通过实例化类 B 去操作，而将 A 作为一个参数调用。 步骤： 类 A 实现接口 C 类 B 实现接口 C 类 B 构造函数传入接口 C 在类 B 内定义检查、校验等方法，并在实现的接口 C 的方法内调用 创建C 变量名 = new B(new A()) 变量名.C的虚拟方法 应用场景 安全代理 远程代理 延迟加载 分类： 静态代理（上述描述） 动态代理（JDK 自带的静态代理，需要反射实现） 4.6.4 工厂模式（略） 4.7 Interface（接口） 概述：接口是一组规则的集成，是抽象方法和常量值的集合。 修饰符：public、缺省。 public 修饰的接口需要单独建一个文件 级别：接口是和类并列的一类结构 语法：123456789interface 接口名&#123; 属性; 抽象方法;&#125;class 类名 extends 父类 implements 接口1, 接口2&#123; 类的自有属性； 类的自有方法； 必须实现的接口的抽象方法;&#125; 属性为全局常量，public static final修饰，不写时仍然为public static final，一般不写。 可以省略任意多个修饰符。 声明时必须初始化，因为为 final 修饰。 接口中的抽象方法可以不用public abstract修饰，不写时仍然为public abstract，一般不写**（不写时注意不是缺省，override 方法时注意权限修饰符范围，即实现类的重写方法权限修饰符必须为 public，重写方法不能写 abstract）**。 可以省略任意多个修饰符。 不能 new 接口名有没有 abstract 修饰不影响 JDK7 之前：只能定义全局常量和抽象方法：没有方法体 JDK8 及之后：还可以定义静态方法、默认方法：有方法体 静态方法：static 修饰 可以通过接口直接调用并执行方法体。 实现类可以不用重写接口的静态方法 默认方法：default 修饰 可以通过类对象来调用 实现类可以不用重写接口的默认方法 注意点： 接口不能被实例化（不能声明构造器），匿名接口的方式可以 new 普通类实现接口，必须实现接口中的所有方法（重写接口中的所有抽象方法） 抽象类实现接口，可以不用实现（重写）接口中的方法 接口可以相互继承，子类接口含有了直接父类、间接父类的所有属性和方法，其实现接口的类必须实现所有方法 接口中属性的访问形式：接口名.属性名或实现接口的类名.属性名、对象名.属性名（同虚拟类访问静态属性） 类名.属性名、对象名.属性名的方式不能和继承的父类中的属性名冲突，否则会报错，原因为不明确 继承+实现情况下属性名重复会报错 实现+实现情况下属性名重复会报错 接口也具有多态性 接口名 变量名 = new 实现接口的类名() 类优先原则： class C extends A implements B中，如果 A、B、C 中均定义了同名的属性，实例化 C 后访问该属性时，返回 C 中的，如果 C 中没有，则会报错。 如果 A 中实现了 B 中的抽象方法，而 C 中什么也没写，这时根据继承原则，默认 C 中有了 A 的方法，从而重写了 B 中的方法。 两个接口同时定义了同名同参的默认方法： 如果返回值不一致（权限修饰符均一致）， 一个类都实现了接口时，会发生接口冲突（报错），无法通过重写两个同名同参不同返回值类型的方法，因为两个接口不知道谁是它的重写。 如果返回值一致，则一个实现类可只写一个重写方法，表示对两个接口方法的重写。 一个接口定义了默认方法，一个父类定义了同名同参数的非抽象方法，不会发生冲突，且类优先，接口中定义的同名同参方法会被忽略。 - 调用指定接口的方法：接口名.super.默认方法名 类 内部类 属性 局部变量 方法 构造器 代码块 接口 public 是 是 是 否 是 是 否 是 protected 否 是 是 否 是 是 否 否 缺省 是 是 是 否 是 是 是 是 private 否 是 是 否 是 是 否 否 public 修饰类和接口时，必须单独设置一个文件。 类 内部类 属性 局部变量 方法 构造器 代码块 接口 static 否 是 是 否 是 否 是 否 final 是 是 是 是 是 否 否 否 abstract 是 是 否 否 非 private 的方法 否 否 是 static、final、abstract 要写在语句的开头，但与权限修饰符的先后顺序没关系。 final 和 abstract 修饰符永远二选一（类、内部类、方法）。 final 和 static 永远可搭配（内部类、属性、方法） static 和 abstract 可同时修饰内部类，不可同时修饰方法。 第 5 章 常用类 5.1 包装类 5.1.1 来源原因 Java 定义了 8 种数据类型对应的引用类型（包装类、封装类），使得可以调用类中的方法。 5.1.2 分类： 5.1.3 基本数据类型、包装类、String 类型的相互转换： 基本数据类型转换为包装类： 装箱：基本数据类型包装成包装类的实例。 使用包装类的构造器：new Integer()。 可传入基本数据类型。 可传入字符串。 使用包装类的方法：Integer.valueOf(); 包装类转换为基本数据类型： 拆箱：获得包装类对象中包装的基本类型变量。 调用包装类的.intValue()方法。 基本数据类型转换为字符串： 调用字符串重载的valueOf()方法。 使用+拼接&quot;&quot;空字符串。 字符串转换为基本数据类型： 使用包装类的构造器：new Integer()，传入字符串。 使用包装类的parseInteger(String s)静态方法。 字符串转换为包装类： 调用包装类的构造器：new Integer()。 包装类转换为字符串： 调用包装类的tostring()方法。 JDK1.5 之后支持自动装箱、自动拆箱，但类型必须匹配 Integer a = 10; 自动封箱 int b = a; 自动拆箱 基本数据类型和包装类型使用功能==号比较时，比较的是数值值。 5.2 String 类 5.2.1 String 对象的特点 继承自 Object 使用双引号包裹。 字符串的字符使用 Unicode 字符编码，一个字符（不区分字母和汉字）占用 2 个字节。 常见创建方式（常用构造器）： new String()：空参 new String(String str)：字符串字面量 new String(char[] a)：字符型数组 new String(byte[] b)：btye 型数字（一个 byte 数占 1 个字节） new String()： String 由 final 修饰，不可被继承。源码**public final class String&#123;&#125;** String 内定义了private final char value[]，用于存放字符串内容。 由于 value 数组被 final 修饰，所以不能改变 value 的地址值。 如果改变字符串变量的字面量值，如将String str = s;改为String str = str;则表示 str 指向常量池中新的地址。而不是对常量池中 s 所在的地址空间重新填充内容。 体现： - 当对字符串重新赋值时，需要重写指定内存区域赋值，不能使用原有的 value 进行赋值。 当对现有的字符串进行连接操作时，也需要重新指定内存区域赋值，不能使用原有的 value 进行赋值。 - 当调用 String 的 replace()方法修改指定字符或字符串时，也需要重新指定内存区域赋值，不能使用原有的 value 进行赋值。 5.2.2 创建 String 对象细节 比较对象： 字面量方式：String str = &quot;string&quot;; 构造器方式：String str = new String(string); 字面量方式： 从常量池检查是否已有该字符串 常量池中有，str 指向常量池中该字符串的地址 常量池中没有，先创建该字符串，然后 str 指向该字符串在常量池中的地址 构造器方式： new String 在堆空间创建了地址空间，str 指向该地址 new String 开辟的内存空间中的属性 value 数组，在常量池中查找字符串 常量池中有，value 指向常量池中该字符串的地址 常量池中没有，先创建该字符串，然后 value 指向该字符串在常量池中的地址 字面方式创建注意点： String d = &quot;helloabc&quot;;与String e = &quot;hello&quot; + &quot;abc&quot;;等价，指向常量池同一地址。 String a = &quot;hello&quot;;String b = &quot;abc&quot;;String c = a + b;与String e = &quot;hello&quot; + &quot;abc&quot;;不等价。 String c = a + b;底层会先进行StringBuilder c = new StringBuilder()，然后调用 append()方法将 a 和 b 添加进去，此时创建的 StringBuilder()对象在堆内存中，所以 c 保存的是堆空间的地址。 如果再执行c.intern();则与String e = &quot;hello&quot; + &quot;abc&quot;;等价。 原因为 intern()方法会在常量池中查找是否存在该字符串，存在时返回常量池中的对象（含地址），不存在则创建。 String a = &quot;hello&quot;;String b = &quot;abc&quot; + a;只要有变量参与，就会调用 StringBuilder()创建对象。 但假如final String a = &quot;hello&quot; ，则结果仍在常量池，因为此时 a 变成了常量 结论： 常量与常量的拼接结果在常量池。且常量池中不会存在相同内容的常量。 只要其中有一个是变量，结果就在堆中。 如果拼接的结果调用 intern()方法，返回值就在常量池中 5.2.3 JVM 中涉及字符串的存放位置 【参见 JVM 虚拟机】 jdk 1.6：字符串常量池存储在方法区（永久区） jdk 1.7：字符串常量池存储在堆空间 jdk 1.8：字符串常量池存储在方法区（元空间） 5.2.4 String 常见方法 操作： int length()：返回字符串的长度： return value.length char charAt(int index)： 返回某索引处的字符 return value[index] boolean isEmpty()：判断是否是空字符串：return value.length == 0 String toLowerCase()：使用默认语言环境，将 String 中的所有字符转换为小写 String toUpperCase()：使用默认语言环境，将 String 中的所有字符转换为大写 String trim()：返回字符串的副本，忽略前导空白和尾部空白 boolean equals(Object obj)：比较字符串的内容是否相同 boolean equalsIgnoreCase(String anotherString)：与 equals 方法类似，忽略大小写 String concat(String str)：将指定字符串连接到此字符串的结尾。 等价于用“+” int compareTo(String anotherString)：比较两个字符串的大小 查找： boolean endsWith(String suffix)：测试此字符串是否以指定的后缀结束 boolean startsWith(String prefix)：测试此字符串是否以指定的前缀开始 boolean startsWith(String prefix, int toffset)：测试此字符串从指定索引开始的子字符串是否以指定前缀开始 boolean contains(CharSequence s)：当且仅当此字符串包含指定的 char 值序列时，返回 true int indexOf(String str)：返回指定子字符串在此字符串中第一次出现处的索引 int indexOf(String str, int fromIndex)：返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始 如果fromIndex为负数，表示从尾部开始，到头部的顺序查找，仍然是正序查找。 int lastIndexOf(String str)：返回指定子字符串在此字符串中最右边出现处的索引 int lastIndexOf(String str, int fromIndex)：返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索 替换： String replace(char oldChar, char newChar)：返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的。 String replace(CharSequence target, CharSequence replacement)：使用指定的字面值替换序列替换此字符串所有匹配字面值目标序列的子字符串。 String replaceAll(String regex, String replacement)：使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串。 String replaceFirst(String regex, String replacement)：使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串。 匹配: boolean matches(String regex)：告知此字符串是否匹配给定的正则表达式。 切片： String[] split(String regex)：根据给定正则表达式的匹配拆分此字符串。 String[] split(String regex, int limit)：根据匹配给定的正则表达式来拆分此字符串，最多不超过 limit 个，如果超过了，剩下的全部都放到最后一个元素中。 String substring(int beginIndex)：返回一个新的字符串，它是此字符串的从 beginIndex 开始截取到最后的一个子字符串。 String substring(int beginIndex, int endIndex) ：返回一个新字符串，它是此字符串从 beginIndex 开始截取到 endIndex(不包含)的一个子字符串。 String 与 char[]之间的转换 String --&gt; char[]:调用 String 的 toCharArray() char[] --&gt; String:调用 String 的构造器 String 与 byte[]之间的转换 编码：String --&gt; byte[]:调用 String 的 getBytes() 解码：byte[] --&gt; String:调用 String 的构造器 解码时，要求解码使用的字符集必须与编码时使用的字符集一致，否则会出现乱码。 String 与基本数据类型、包装类之间的转换。 String --&gt; 基本数据类型、包装类：调用包装类的静态方法：parseXxx(str) 基本数据类型、包装类 --&gt; String:调用 String 重载的 valueOf(xxx) 注意： 由于字符串是不可变的，所以操作字符串的方法基本都有返回值，而原字符串不会发生改变。 5.2.5 StringBuffer 特性： 可变的字符序列，对字符串进行增删，不会产生新的对象 作为参数传递时，方法内部可以改变值。 构造器： StringBuffer()：初始容量为 16 的 char 型字符串数组 StringBuffer(int size)：指定 size 容量的字符串 StringBuffer(String str)：将内容初始化为指定字符串内容。 常用方法： 增：StringBuffer append(xxx)： 删：StringBuffer delete(int start,int end)： 改： StringBuffer replace(int start, int end, String str)： public void setCharAt(int n ,char ch)： 查：public char charAt(int n)： 插：StringBuffer insert(int offset, xxx)： 遍历：for() + charAt()或者toString() 其他： StringBuffer reverse() ：把当前字符序列逆转 public int indexOf(String str) public String substring(int start,int end): public int length()： 注意点：返回值类型为 StringBuffer 的方法会对原数据进行修改，返回值类型为 String 类型的方法不会对元数据进行修改。 底层原理：StringBuffer 的方法底层返回了 this，既是对当前数据的返回，也表明支持链式操作 5.2.6 StringBuilder 同 StringBuffer，JDK5.0 新增，只是线程不安全，效率优于 StringBuffer。 开发中建议使用：StringBuffer(int capacity) 或 StringBuilder(int capacity)。 5.3 时间 API JDK8 之前： java.lang.System 类：currentTimeMillis()，获取当前时间距离 1970 年 1 月 1 日 0 时 0 分 0 秒的毫秒数 java.util.Date 类 ： 构造器： Date() Date(long date) 方法： getTime()：获取当前时间距离 1970 年 1 月 1 日 0 时 0 分 0 秒的毫秒数 toString()：【重写方法】按照 down mon dd hh:mm:ss zzz yyy 的格式输出 down：星期几 zzz：时间标准 java.sql.Date 类： java.util.Date 类的子类 创建时使用全包名：即new java.sql.Date() 相互转换： 方式一：创建 Date 对象的多态 java.sql.Date 对象；创建 java.sql.Date 对象，使用向上转型（强转）——注意：强转的前提是 Date 对象是由 java.sql.Date 创建的。 方式二：通过共有方法getTime()获取时间戳 java.text.SimpleDataFormat 类： 用于对 Date 类的格式化和解析 创建指定格式的对象： SimpleDateFormat()：默认格式 SimpleDateFormat(&quot;yyyy-MM-dd&quot;)：指定格式 格式化：将日期输出为字符串 调用foramt(Date date)方法： 解析：将字符串转换为日期 调用parse(String sourse)方法： 这里 sourse 的格式必须与创建SimpleDateFormat()对象的格式一致 有异常问题 java.util.Calendar(日历)类 ： 用于日期字段（fileld）间的相互操作 field：Calendar 类的静态属性：YEAR、MONTH、DAY_OF_WEEK、HOUR_OF_DAY 、 MINUTE、SECOND 获取 Calendar 实例：Calendar calendar = Calendar.getInstance(); Calendar 时抽象类，无法实例化，调用静态方法getInstance()实际上会调用其子类GregorianCalendar创建对象。 常用方法： int get(int field,int value) void set(int field,int value) void add(int field,int amount) Date getTime()：日历类转换 Date 类 void setTime(Date date)：Date 类转换日历类 日期时间 API 的迭代： 第一代：jdk 1.0 Date 类 第二代：jdk 1.1 Calendar 类，一定程度上替换 Date 类 第三代：jdk 1.8 提出了新的一套 API 前两代存在的问题举例： 可变性：像日期和时间这样的类应该是不可变的。 偏移性：Date 中的年份是从 1900 开始的，而月份都从 0 开始。 格式化：格式化只对 Date 用，Calendar 则不行。 此外，它们也不是线程安全的；不能处理闰秒等。 JDK8 新增 API java.time – 包含值对象的基础包 java.time.chrono – 提供对不同的日历系统的访问 java.time.format – 格式化和解析时间和日期 java.time.temporal – 包括底层框架和扩展特性 java.time.zone – 包含时区支持的类 JDK8 新增时间类（构造方法私有的，不能实例化）： LocalDate：获取 ISO 格式（yyyy-MM-dd)的日期 LocalTime：获取时间 LocalDateTime：获取日期和时间 常用方法： now()（静态方法）：获取当前的日期、时间、日期+时间，创建类对象 of()：设置指定的年、月、日、时、分、秒。没有偏移量 getXxx()：获取相关的属性 withXxx()：设置相关的属性 Instant（类似于 Calendar，不可实例化)： now()（静态方法）：获取本初子午线对应的标准时间。Instant instant = Instant.now() atOffset()：添加时间的偏移量 toEpochMilli()：获取自 1970 年 1 月 1 日 0 时 0 分 0 秒（UTC）开始的毫秒数 ofEpochMilli()：通过给定的毫秒数，获取 Instant 实例 DateTimeFormatter：格式化或解析日期、时间 预定义的标准格式： DateTimeFormatter.ISO_LOCAL_DATE_TIME DateTimeFormatter.ISO_LOCAL_DATE DateTimeFormatter.ISO_LOCAL_TIME 本地化相关格式： DateTimeFormatter._ofLocalizedDateTime_(FFormatStyle.LONG / FormatStyle.MEDIUM / FormatStyle.SHORT) DateTimeFormatter.ofLocalizedDate(FormatStyle.FULL / FormatStyle.LONG / FormatStyle.MEDIUM / FormatStyle.SHORT) 自定义格式： DateTimeFormatter._ofPattern_(&quot;yyyy-MM-dd hh:mm:ss&quot;) 格式化：时间格式.format(时间)，返回字符串 解析：时间格式.parse(符合时间格式的字符串)，返回 TemporalAccessor 类型日期 5.4 其他类 Math 类： 常用方法： double floor(double a)：——向坐标轴左侧取整，如 1.2、1.6 得 1.0；-1.2、-1.6 得-2.0。 int round(double a)：——本质是(int)Math.floor(a + 0.5f)。 正数四舍五入；负数不可记按正数四舍五入加负号，因为round(-1.5)得-1 System 类 构造器是 private 的，无法创建该类的对象，内部的成员变量和成员方法都是 static 的，可以直接进行调用。 方法： native long currentTimeMillis() void exit(int status)：退出当前程序 status：0 表示正常状态。 arrayCopy()： 参数 1：源数组 参数 2：源数组开始索引（包含） 参数 3：目标数组 参数 4：目标数组位置开始索引（包含） 参数 5：源数组拷贝数量 void gc() String getProperty(String key) BigInteger 类 BigInteger 可以表示不可变的任意精度的整数 不能使用+、-、*、/，需要使用相应的方法。 BigDecimal 类 Array 类： toString()：重写 Object 方法，默认调用，输出数组元素 对象数组时，输出的时数组内每个对象的地址，如果还要显示每个对象的内容，就需要重写类的 toString 方法。 sort()：默认从小到大排序 binarySearch()：通过二分法查找指定数，返回索引，要求必须排好序，无序数组不能使用。如果不存在，返回-(low+1) copyOf()：拷贝指定数量的数组元素。 新数组比就数组长，最后几位为 null 拷贝长度小于 0，抛出异常，等于 0，新数组为空 fill()：替换数组的全部元素 equals()：比较数组元素是否一致 asList()：将括号内数据转换为 List 集合。 Scanner 类： next()：接收数据，输入空格、回车、英文引号表示结束 5.5 比较器 作用：比较两个对象，用于对象数组排序。 自然排序： java.lang.Comparable包中的Comparable接口 String、包装类等实现了 Comparable 接口，重写了 compareTo(obj)方法，给出了比较两个对象大小的方式。 像 String、包装类重写 compareTo()方法以后，进行了从小到大的排列 重写 compareTo(obj)的规则： 如果当前对象 this 大于形参对象 obj，则返回正整数 如果当前对象 this 小于形参对象 obj，则返回负整数 如果当前对象 this 等于形参对象 obj，则返回零 对于自定义类来说，如果需要排序，可以让自定义类实现 Comparable 接口，重写 compareTo(obj)方法。在 compareTo(obj)方法中指明如何排序 定制排序： 当元素的类型没实现 java.lang.Comparable 接口而又不方便修改代码，或者实现了 java.lang.Comparable 接口的排序规则不适合当前的操作，那么可以考虑使用 匿名的 Comparator 的对象来排序 重写 compare(Object o1,Object o2)方法，比较 o1 和 o2 的大小： 如果方法返回正整数，则表示 o1 大于 o2； 如果返回 0，表示相等； 返回负整数，表示 o1 小于 o2。 补充： 包装类具有compare、compareTo两个方法，其中compareTo是静态方法。 String 具有compareTo、conpareToIngoreCase方法 比较： Comparable 位于 java.lang 包， Comparator 位于 java.util 包。 实现 Comparable 接口需要重写 compareTo()方法，实现 Comparator 方法需要重写 compare()方法，这两个方法的类型都是 int Comparable 是排序接口，相当于内部比较器，称为自然排序，Comparator 是比较器，相当于外部比较器，称为比较器排序，外部比较器可以对内部比较器进行扩充。 int test = s1.compareTo(s2); result = 0 , 则 s1=s2 ; result &lt; 0, 则 s1 0 , 则 s1 &gt; s2 。 int test= compare(T o1, T o2); 结果同上。 （若是 o2-o1，则反之。） 如果对象的排序需要基于自然顺序，使用 Comparable 比较合适，如果需要按照对象的不同属性进行排序，使用 Comparator 比较合适。 第 6 章 枚举和注解 6.1 枚举类型 6.1.1 枚举的概念 enumeration、enum 一组常量类型的集合。 是一种特殊的类，只包含有限的特定的对象，与 Object 并列 特点： 属性不允许被改动：属性使用 private final 修饰、且不设置 setter 为了能够初始化属性，使用构造器传参的方式，在构造器内赋值。 在枚举类内创建实例。 6.1.2 枚举类型的实现 自定义类（JDK1.5 之前）： 私有化属性，只配置 getter，不配置 seter：防止属性被修改 private final 修饰 私有化构造器：防止 new private 修饰 在类内部，创建公开的枚举对象 命名规范：属性名使用大写形式 对象使用 public static final 修饰，进行底层优化 三个可省略任意多个 enum 关键字（JDK1.5 之后）：1234567891011enum Season&#123; SPRING(&quot;春天&quot;, &quot;温暖&quot;）, SUMMER(&quot;夏天&quot;, &quot;炎热&quot;); private String name; private String desc; private Season(String name, String desc)&#123; this.name = name; this.desc = desc; &#125; nam getter; desc getter;&#125; 使用 enum 关键字的注意点： 枚举类： 枚举类前默认有 final 关键字，所以枚举类不能被继承 final 的特性导致 默认继承自 java.lang.Enum 类，所以也不能继承其他类 枚举对象： 实例化的枚举类对象必须声明在枚举类的首行 实例化的枚举类对象必须与构造器的参数对应，且枚举对象必须传入参数 实例化的枚举对象前默认有 public static final，可以省略任意多个 构造器： 使用无参构造器实例化枚举类对象时，枚举对象的括号可以省略 构造器默认使用 private 修饰，可以省略 属性： 默认使用 private final 修饰，可以省略 6.1.3 常用方法 enum 声明的类会隐式继承 Enum 类，因此可以使用 Enum 中的相关方法： toString()：返回当前对象名，声明 enum 类时可重写，用于返回类中的属性内容 name()：返回当前对象名，不能重写。 有限使用toString() ordinal()：返回当前对象的位置号，从 0 开始 values()：返回当前枚举类中的所有实例化对象，返回一个 enum 声明的枚举类型数组 valueOf()：返回指定字符串的实例化对象，如果传入的字符串枚举类中没有，会报错 传入的字符串名称要跟实例化对象名一致 compareTo()：比较两个枚举常量的位置编号，返回常量值。 调用的 - 参数的 6.1.4 enum 实现接口 语法：enum 类名 implements 接口 1，接口 2&#123;&#125; 6.1.5 枚举配合 switch-case switch()中传入一个枚举对象，case 后是枚举对象的值 6.2 注解（Anotation） 6.2.1 概念 含义：又称为元数据（Meatdata），用于修饰解释包、类、方法、属性、构造器、局部变量等数据信息。 特点：和注释一样，不影响程序运行，但注解可以被编译或运行，相当于嵌入代码的补充信息。 功能： javase 中，注解主要用于标记过时功能、忽略警告等 Javaee 中，可以用于配置应用程序的任何切面，代替旧版 javaee 中的繁冗代码和 XML 配置 6.2.2 常见类型及基本使用 @Override：方法前使用，表示重写父类方法 只能用在方法前。 重写的方法前不写该注解不影响编译运行 如果写了该注解，程序在编译阶段检查是否真正重写。 非重写时会报错。 @Deprecated：表示程序元素（类、方法等）已过时。 可以用在类、方法（构造器）、变量（成员变量、局部变量）、包等 用于新旧版本的兼容和过渡 @SuppressWarnings()：抑制编译器警告 可以用在类、方法（构造器）、变量（成员变量、局部变量）、包等 主要用在方法、类前 ()：传入字符串值，表示抑制的警告类型 多种类型时，可以传入数组字面量&#123;&#125; 作用范围与书写位置有关 @SuppressWarning 中的属性介绍以及属性说明 all，抑制所有警告 boxing，抑制与封装/拆装作业相关的警告 cast，抑制与强制转型作业相关的警告 dep-ann，抑制与淘汰注释相关的警告 deprecation，抑制与淘汰的相关警告 fallthrough，抑制与 switch 陈述式中遗漏 break 相关的警告 finally，抑制与未传回 finally 区块相关的警告 hiding，抑制与隐藏变数的区域变数相关的警告 incomplete-switch，抑制与 switch 陈述式(enum case)中遗漏项目相关的警告 javadoc，抑制与 javadoc 相关的警告 nls，抑制与非 nls 字串文字相关的警告 null，抑制与空值分析相关的警告 rawtypes，抑制与使用 raw 类型相关的警告 resource，抑制与使用 Closeable 类型的资源相关的警告 restriction，抑制与使用不建议或禁止参照相关的警告 serial，抑制与可序列化的类别遗漏 serialVersionUID 栏位相关的警告 static-access，抑制与静态存取不正确相关的警告 static-method，抑制与可能宣告为 static 的方法相关的警告 super，抑制与置换方法相关但不含 super 呼叫的警告 synthetic-access，抑制与内部类别的存取未最佳化相关的警告 sync-override，抑制因为置换同步方法而遗漏同步化的警告 unchecked，抑制与未检查的作业相关的警告 unqualified-field-access，抑制与栏位存取不合格相关的警告 unused，抑制与未用的程式码及停用的程式码相关的警告 源码中@interface xxx表示 xxx 是一个注解类。 元注解：修饰其他注解的注解 @Retention(RetentionPolicy.xxx)：指定注解的作用时机 SOURCE：只在源码中显示，编译后丢弃这种注解策略 CLASS：在字节码文件中显示，运行时不会保留。是注解的默认值。 RUNTIME：运行时仍然保留，可以通过反射获取该注解 @Target(ElementType.xxx)：指定注解用于修饰哪些元素。 作用时机是 RUNTIME 只能用于修饰注解 @Documented：被修饰的注解会在生成文档注释时保留。 作用时机时 RUNTIME @Inherited：被修饰的注解具有继承性，子类自动具有该注解 与文档相关的注解： @author 标明开发该类模块的作者，多个作者之间使用,分割 @version 标明该类模块的版本 @see 参考转向，也就是相关主题 @since 从哪个版本开始增加的 @param 对方法中某参数的说明，如果没有参数就不能写 标记方法 可以并列多个 @return 对方法返回值的说明，如果方法的返回值类型是 void 就不能写 标记方法 格式要求：@return 返回值类型 返回值说明 @exception 对方法可能抛出的异常进行说明 ，如果方法没有用 throws 显式抛出的异常就不能写 标记方法 格式要求：@exception 异常类型 异常说明 可以并列多个 @WebServle()： @Transactional(propagation=Propagation.REQUIRES_NEW, isolation=Isolation.READ_COMMITTED,readOnly=false,timeout=3)： 6.3.3 自定义注解warning 自定义注解使用@interface 关键字声明 自定义注解自动继承了 java.lang.annotation.Annotation 接口 自定义注解的成员变量以无参数方法的形式来声明。方法名和返回值类型称为配置参数。返回值类型只能是八种基本数据类型、String 类型、Class 类型、enum 类型、Annotation 类型、 及以上所有类型的数组。 自定义注解的成员变量声明时可使用 default 关键字指定初始值 如果只有一个参数成员，通常使用 value 使用自定义注解时，如果定义注解时声明了成员变量（配置参数），那么使用时必须指定参数值，除非它有默认 值。格式是“参数名 = 参数值” ，如果只有一个参数成员，且名称为 value， 可以省略“value=” 自定义注解通过都会指明两个元注解：Retention、Target 第 7 章 异常处理 7.1 异常理解 概念：程序运行过程中发生的不正常情况。 7.2 异常的体系结构 Error：Java 虚拟机无法解决的严重问题。一般不做处理。 StackOverflowError：栈溢出 OutOfMemoryError（OOM）：堆溢出 Exception： 编译时异常： 运行时异常： java.lang.Throwabe&gt; java.lang.Throwable |-----java.lang.Error:一般不编写针对性的代码进行处理。 |-----java.lang.Exception:可以进行异常的处理 |------编译时异常(checked) |-----IOException |-----FileNotFoundException |-----ClassNotFoundException |------运行时异常(unchecked,RuntimeException) |-----NullPointerException：空指针异常 |-----ArrayIndexOutOfBoundsException：数组下标越界 |-----ClassCastException：类型不匹配 |-----NumberFormatException：数字格式错误 |-----InputMismatchException：输入不匹配 |-----ArithmeticException：运算错误 7.3 异常处理机制 7.3.1 抓抛模型： “抛”：程序执行的过程中，一旦出现异常，就会在异常代码处生成一个对应异常类的对象，并会将此对象抛出给调用它的方法，直到 main 方法，如果 main 方法没能处理，则程序运行终止，发生异常位置后面的代码也不会执行。 异常产生有两种形式：一是系统自动产生，二是手动生成并抛出（throw） “抓”：即异常的处理方式： try-catch-finally throws 7.3.2 try-catch-finally 语法：123456789101112try&#123; ...... //可能产生异常的代码 &#125; catch( ExceptionName1 e )&#123; ...... //当产生ExceptionName1型异常时的处置措施 &#125; catch( ExceptionName2 e )&#123; ...... //当产生ExceptionName2型异常时的处置措施 &#125; [ finally&#123; ...... //无论是否发生异常，都无条件执行的语句&#125; ] try 不能单独使用 catch 和 finally 是可选的。 try-finally：没有 catch 语句，相当于捕获到了异常但没有处置，会先输出 finally 中的语句，再报异常，程序也会中止。 try-catch-finally：先报异常，再输出 finally 语句，程序不会中止 在 try 结构中声明的变量，出了 try 结构以后，就不能再被调用 try-catch-finally 结构可以嵌套 使用：IDE 快捷键： 选中-&gt;ctrl + alt + t 执行流程： 如果异常没有发生，则顺序执行 try 的代码块，不会进入到 catch。 如果异常发生了，则异常发生后 try 块中的代码不会执行，直接进入到 catch 块。执行完 catch 块后再执行 try-catch 语句后的代码。 如果希望不管是否发生异常，都执行某段代码(比如关闭连接，释放资源等)则使用 finally 即使 try、catch 语句里有 return（正常情况 return 结束方法），执行完之后仍然会执行 finally 中的语句，再返回结束方法。但当 finally 中有 return 语句，则不再执行 try 或 catch 的 return。 可以使用多个 catch 分别捕获不同的异常，要求子类异常写在前面，父类异常写在后面 try 中有多个异常，且父类一致（如 Exception），如果 catch 只匹配父类，则只捕获第一个异常，try 中后面的代码（所有代码）不会被执行 try 中有多个异常，catch 匹配多个异常，但只捕获第一个异常，try 块中后面的代码（所有代码）不会被执行，异常也不会捕获到 常用的异常对象处理的方式： getMessage() ：打印异常信息 printStackTrace()： 获取异常类名和异常信息，以及异常出现在程序中的位置。返回值 void。 7.3.3 throws + 异常类型 &quot;throws + 异常类型&quot;写在方法的声明处，指明此方法执行时，可能会抛出的异常类型。 抛出多个异常使用,分隔 一旦当方法体执行时，出现异常，仍会在异常代码处生成一个异常类的对象，此对象满足 throws 后异常类型时，就会被抛出。不满足则相当于异常没有处理。异常代码后续的代码，就不再执行！ throws 的方式只是将异常抛给了方法的调用者。并没有真正将异常处理掉，方法调用者再抛出时，异常类型要不小于接收到的异常类型。 运行异常的默认处理方式为 throws，可不处理，编译异常没有默认处理方式，必须处理 子类重写父类的方法时，对抛出异常的规定:子类重写的方法，抛出的异常类型要么和父类抛出的异常一致，要么为父类抛出的异常类型的子类型 在 throws 过程中，如果有方法 try-catch , 就相当于处理异常，就可以不必 throws 7.4 try-catch-finally 和 throws 的选择 拥有继承关系的父子类，如果父类中被重写的方法没有 throws 方式处理异常，则子类重写的方法也不能使用 throws，意味着如果子类重写的方法中有异常，必须使用 try-catch-finally 方式处理。 执行的方法 a 中，先后又调用了另外的几个方法，这几个方法是递进关系执行的。建议这几个方法使用 throws 的方式进行处理。而执行的方法 a 可以考虑使用 try-catch-finally 方式进行处理。 7.5 手动抛出异常 在可能发生异常的位置生成异常类对象，然后通过 throw 语句实现抛出操作 可以抛出的异常必须是 Throwable 或其子类的实例，否则会报错。 7.6 自定义异常类 继承于现有的异常结构：RuntimeException 、Exception 继承 exception 时为编译异常 一般使用 RuntimeException，且需要方法再 thows 异常 提供全局常量（static final 修饰）：serialVersionUID 提供重载的构造器，传入String msg参数 构造器内执行super(msg);","tags":[{"name":"Java","slug":"Java","permalink":"https://sk370.github.io/tags/Java/"}]}]